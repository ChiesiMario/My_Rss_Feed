<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[開源中國-綜合資訊]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[開源中國-綜合資訊 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Sun, 29 Oct 2023 17:50:01 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[Next.js 支持在前端代碼中寫 SQL，開倒車還是遙遙領先？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>下面這張圖來自近日舉辦的&nbsp;Next.js Conf 2023，裏面的代碼使用了名為<strong>「Server Actions」</strong>的特性——在前端代碼中使用 SQL 語句直接操作數據庫。</p><blockquote><p>Next.js 是流行的開源前端框架，其開發商是知名創業公司 Vercel。</p><p>Next.js 提供了包括服務器端渲染和為 Web 應用程序生成靜態網站在內的功能。Vercel 作為一個開放的雲平台提供了網站託管服務，讓開發者能夠在上面開發、預覽和發佈 Web 應用，同時優化了前端開發者的開發和部署體驗。</p></blockquote><p><img alt="" height="533" src="https://oscimg.oschina.net/oscnet/up-e254f1c847ae20e8c530b34f9021da3a4d0.png" width="400" referrerpolicy="no-referrer"></p><p>在最新發布的 Next.js 14 中，Server Actions 已到達穩定階段。其團隊表示，Server Actions 改進了開發者在編寫數據變更方面的體驗。</p><blockquote><p><em><u><a href="https://www.oschina.net/news/263948/next-js-14" target="_blank">Next.js 14 發佈：Server Actions 已穩定、部分預渲染進入預覽</a></u></em></p></blockquote><p>Server Actions 允許開發者定義異步服務器函數，他們可以使用 Server Actions 重新驗證緩存數據、重定向到不同的路由、設置和讀取 cookie 等等。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-77eb78e36979830a06c1f44ed2476bb4db1.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0ad921306554c0716b4b4b0a8bedb71b8ba.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-badb8152ef443eb8272d03c220e0450c723.png" referrerpolicy="no-referrer"></p><p>不過目前看來，大多數人對它的評價似乎並不太好 ——</p><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1029/122449_wEe4_2720166.png" referrerpolicy="no-referrer"></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Sat, 28 Oct 2023 04:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263921/nextjs-server-actions</guid>
            <link>https://www.oschina.net/news/263921/nextjs-server-actions</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Linux Mint]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Linux Mint 團隊在最新月度報告中<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.linuxmint.com%2F%3Fp%3D4591" target="_blank">提到</a></u>，他們已經開始着手開發對 Wayland 的支持。</p><p>團隊稱這項工作是他們在很長一段時間內必須面對的主要挑戰之一，雖然他們不期待 Wayland 能很快取代 Xorg 作為默認值，無論是在 21.3 中，還是在 22.x 中，但仍然希望做好準備。</p><p>按照計劃，Cinnamon 6.0 計劃在今年的 Mint 21.3 中推出，<strong>並將提供實驗性的 Wayland 支持</strong>。用戶可以從登錄界面在默認 Cinnamon 會話（在 Xorg 上運行）和 Cinnamon on Wayland 之間進行選擇。</p><p>下圖是 Cinnamon on Wayland 的運行截圖：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-21b9daccab538fe5909df6cef6b172695f5.png" referrerpolicy="no-referrer"></p><p>Linux Mint 團隊表示，他們可能在 2026 年實現對 Wayland 的穩定支持/默認支持。鑑於 Linux Mint 堅持以 Ubuntu LTS 為基礎，因此並不指望在 Ubuntu 24.04 LTS 之前就能支持 Wayland。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 28 Oct 2023 04:19:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263917/linux-mint-wayland-progress</guid>
            <link>https://www.oschina.net/news/263917/linux-mint-wayland-progress</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[網易雲課堂 Service Worker 運用與實踐]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p><img src="https://oscimg.oschina.net/oscnet/up-ac820c57f42ee1b582f9a6e7a1787375e7a.png" alt="" referrerpolicy="no-referrer"></p><h1>前言</h1><p>本文首先會簡單介紹下前端的常見緩存方式，再引入 Service Worker 的概念，針對其原理和如何運用進行介紹。然後基於 google 推出的第三方庫 Workbox，在產品中進行運用實踐，並對其原理進行簡要剖析。</p><blockquote><p>作者：劉放</p></blockquote><blockquote><p>編輯：Ein</p></blockquote><h1>前端緩存簡介</h1><p>先簡單介紹一下現有的前端緩存技術方案，主要分為 http 緩存和瀏覽器緩存。</p><h2>http 緩存</h2><p>http 緩存都是第二次請求時開始的，這也是個老生常談的話題了。無非也是那幾個 http 頭的問題：</p><h3>Expires</h3><p>HTTP1.0 的內容，服務器使用 Expires 頭來告訴 Web 客戶端它可以使用當前副本，直到指定的時間為止。</p><h3>Cache-Control</h3><p>HTTP1.1 引入了 Cathe-Control，它使用 max-age 指定資源被緩存多久，主要是解決了 Expires 一個重大的缺陷，就是它設置的是一個固定的時間點，客戶端時間和服務端時間可能有誤差。 所以一般會把兩個頭都帶上，這種緩存稱為強緩存，表現形式為： <img src="https://oscimg.oschina.net/oscnet/up-5d55a7877b12164c2b7f2fe4e870e072dc2.png" alt="" referrerpolicy="no-referrer"></p><h3>Last-Modified / If-Modified-Since</h3><p>Last-Modified 是服務器告訴瀏覽器該資源的最後修改時間，If-Modified-Since 是請求頭帶上的，上次服務器給自己的該資源的最後修改時間。然後服務器拿去對比。</p><p>若資源的最後修改時間大於 If-Modified-Since，説明資源又被改動過，則響應整片資源內容，返回狀態碼 200；</p><p>若資源的最後修改時間小於或等於 If-Modified-Since，説明資源無新修改，則響應 HTTP 304，告知瀏覽器繼續使用當前版本。</p><h3>Etag / If-None-Match</h3><p>前面提到由文件的修改時間來判斷文件是否改動，還是會帶來一定的誤差，比如註釋等無關緊要的修改等。所以推出了新的方式。</p><p>Etag 是由服務端特定算法生成的該文件的唯一標識，而請求頭把返回的 Etag 值通過 If-None-Match 再帶給服務端，服務端通過比對從而決定是否響應新內容。這也是 304 緩存。</p><h2>瀏覽器緩存</h2><h3>Storage</h3><p>簡單的緩存方式有 cookie，localStorage 和 sessionStorage。這裏就不詳細介紹他們的區別了，這裏説下通過 localStorage 來緩存靜態資源的優化方案。 localStorage 通常有 5MB 的存儲空間，我們以微信文章頁為例。 查看請求發現，基本沒有 js 和 css 的請求，因為它把全部的不需要改動的資源都放到了 localStorage 中： <img src="https://oscimg.oschina.net/oscnet/up-aa2899a96564193e2509884484b4f1eb12b.png" alt="" referrerpolicy="no-referrer"> 所以微信的文章頁加載非常的快。</p><h3>前端數據庫</h3><p>前端數據庫有 WebSql 和 IndexDB，其中 WebSql 被規範廢棄，他們都有大約 50MB 的最大容量，可以理解為 localStorage 的加強版。</p><h3>應用緩存</h3><p>應用緩存主要是通過 manifest 文件來註冊被緩存的靜態資源，已經被廢棄，因為他的設計有些不合理的地方，他在緩存靜態文件的同時，也會默認緩存 html 文件。這導致頁面的更新只能通過 manifest 文件中的版本號來決定。所以，應用緩存只適合那種常年不變化的靜態網站。如此的不方便，也是被廢棄的重要原因。</p><p>PWA 也運用了該文件，不同於 manifest 簡單的將文件通過是否緩存進行分類，PWA 用 manifest 構建了自己的 APP 骨架，並運用 Servie Worker 來控制緩存，這也是今天的主角。</p><h1>Service Worker</h1><p>Service Worker 本質上也是瀏覽器緩存資源用的，只不過他不僅僅是 Cache，也是通過 worker 的方式來進一步優化。 他基於 h5 的 web worker，所以絕對不會阻礙當前 js 線程的執行，sw 最重要的工作原理就是：</p><p>1、後台線程：獨立於當前網頁線程；</p><p>2、網絡代理：在網頁發起請求時代理，來緩存文件。</p><h2>兼容性</h2><p><img src="https://oscimg.oschina.net/oscnet/up-c50376e8514a0eda4c04fcf7bf1af3f24aa.png" alt="" referrerpolicy="no-referrer"> 可以看到，基本上新版瀏覽器還是兼容滴。之前是隻有 chrome 和 firefox 支持，現在微軟和蘋果也相繼支持了。</p><h2>成熟程度</h2><p>判斷一個技術是否值得嘗試，肯定要考慮下它的成熟程度，否則過一段時間又和應用緩存一樣被規範拋棄就尷尬了。 所以這裏我列舉了幾個使用 Service Worker 的頁面：</p><ul><li>淘寶</li><li>網易新聞</li><li>考拉</li></ul><p>所以説還是可以嘗試下的。</p><h2>調試方法</h2><p>一個網站是否啓用 Service Worker，可以通過開發者工具中的 Application 來查看：</p><p><img src="https://oscimg.oschina.net/oscnet/up-b631c480eabb3662b08968b60c0466ceefd.png" alt="" referrerpolicy="no-referrer"></p><p>被 Service Worker 緩存的文件，可以在 Network 中看到 Size 項為 from Service Worker：</p><p><img src="https://oscimg.oschina.net/oscnet/up-a94d21f7c7ca175656166c4224fae4ba3c9.png" alt="" referrerpolicy="no-referrer"></p><p>也可以在 Application 的 Cache Storage 中查看緩存的具體內容：</p><p><img src="https://oscimg.oschina.net/oscnet/up-7db236cf39ff32cf4e8f86a591a08431b07.png" alt="" referrerpolicy="no-referrer"></p><p>如果是具體的斷點調試，需要使用對應的線程，不再是 main 線程了，這也是 webworker 的通用調試方法：</p><p><img src="https://oscimg.oschina.net/oscnet/up-72a1c5ec7411b0e92530a737fe53db2b158.png" alt="" referrerpolicy="no-referrer"></p><h2>使用條件</h2><p>sw 是基於 HTTPS 的，因為 Service Worker 中涉及到請求攔截，所以必須使用 HTTPS 協議來保障安全。如果是本地調試的話，localhost 是可以的。 而我們剛好全站強制 https 化，所以正好可以使用。</p><h2>生命週期</h2><p>大概可以用如下圖片來解釋：</p><p><img src="https://oscimg.oschina.net/oscnet/up-385c15f6dc80d598f67579d5c0308bb98e5.png" alt="" referrerpolicy="no-referrer"></p><h3>註冊</h3><p>要使用 Service Worker，首先需要註冊一個 sw，通知瀏覽器為該頁面分配一塊內存，然後 sw 就會進入安裝階段。 一個簡單的註冊方式：</p><pre><code>(function() {
    if('serviceWorker' in navigator) {
        navigator.serviceWorker.register('./sw.js');
    }
})()
</code></pre><p>當然也可以考慮全面點，參考網易新聞的註冊方式：</p><pre><code>"serviceWorker" in navigator &amp;&amp; window.addEventListener("load",
    function() {
        var e = location.pathname.match(/\/news\/[a-z]{1,}\//)[0] + "article-sw.js?v=08494f887a520e6455fa";
        navigator.serviceWorker.register(e).then(function(n) {
            n.onupdatefound = function() {
                var e = n.installing;
                e.onstatechange = function() {
                    switch (e.state) {
                        case "installed":
                            navigator.serviceWorker.controller ? console.log("New or updated content is available.") : console.log("Content is now available offline!");
                            break;
                        case "redundant":
                            console.error("The installing service worker became redundant.")
                    }
                }
            }
        }).
        catch(function(e) {
            console.error("Error during service worker registration:", e)
        })
    })
</code></pre><p>前面提到過，由於 sw 會監聽和代理所有的請求，所以 sw 的作用域就顯得額外的重要了，比如説我們只想監聽我們專題頁的所有請求，就在註冊時指定路徑：</p><pre><code>navigator.serviceWorker.register('/topics/sw.js');
</code></pre><p>這樣就只會對 topics/下面的路徑進行優化。</p><h3>installing</h3><p>我們註冊後，瀏覽器就會開始安裝 sw，可以通過事件監聽：</p><pre><code>//service worker 安裝成功後開始緩存所需的資源
var CACHE_PREFIX = 'cms-sw-cache';
var CACHE_VERSION = '0.0.20';
var CACHE_NAME = CACHE_PREFIX+'-'+CACHE_VERSION;
var allAssets = [
    './main.css'
];
self.addEventListener('install', function(event) {

    //調試時跳過等待過程
    self.skipWaiting();


    // Perform install steps
    //首先 event.waitUntil 你可以理解為 new Promise，
    //它接受的實際參數只能是一個 promise，因為,caches 和 cache.addAll 返回的都是 Promise，
    //這裏就是一個串行的異步加載，當所有加載都成功時，那麼 SW 就可以下一步。
    //另外，event.waitUntil 還有另外一個重要好處，它可以用來延長一個事件作用的時間，
    //這裏特別針對於我們 SW 來説，比如我們使用 caches.open 是用來打開指定的緩存，但開啓的時候，
    //並不是一下就能調用成功，也有可能有一定延遲，由於系統會隨時睡眠 SW，所以，為了防止執行中斷，
    //就需要使用 event.waitUntil 進行捕獲。另外，event.waitUntil 會監聽所有的異步 promise
    //如果其中一個 promise 是 reject 狀態，那麼該次 event 是失敗的。這就導致，我們的 SW 開啓失敗。
    event.waitUntil(
        caches.open(CACHE_NAME)
            .then(function(cache) {
                console.log('[SW]: Opened cache');
                return cache.addAll(allAssets);
            })
    );

});
</code></pre><p>安裝時，sw 就開始緩存文件了，會檢查所有文件的緩存狀態，如果都已經緩存了，則安裝成功，進入下一階段。</p><h3>activated</h3><p>如果是第一次加載 sw，在安裝後，會直接進入 activated 階段，而如果 sw 進行更新，情況就會顯得複雜一些。流程如下：</p><p>首先老的 sw 為 A，新的 sw 版本為 B。 B 進入 install 階段，而 A 還處於工作狀態，所以 B 進入 waiting 階段。只有等到 A 被 terminated 後，B 才能正常替換 A 的工作。</p><p><img src="https://oscimg.oschina.net/oscnet/up-5d04b8ca78c8e2f2bc8f02e992c6a540426.png" alt="" referrerpolicy="no-referrer"></p><p>這個 terminated 的時機有如下幾種方式：</p><p>1、關閉瀏覽器一段時間；</p><p>2、手動清除 Service Worker；</p><p>3、在 sw 安裝時直接跳過 waiting 階段</p><pre><code>//service worker 安裝成功後開始緩存所需的資源
self.addEventListener('install', function(event) {
    //跳過等待過程
    self.skipWaiting();
});
</code></pre><p>然後就進入了 activated 階段，激活 sw 工作。</p><p>activated 階段可以做很多有意義的事情，比如更新存儲在 Cache 中的 key 和 value：</p><pre><code>var CACHE_PREFIX = 'cms-sw-cache';
var CACHE_VERSION = '0.0.20';
/**
 * 找出對應的其他 key 並進行刪除操作
 * @returns {*}
 */
function deleteOldCaches() {
    return caches.keys().then(function (keys) {
        var all = keys.map(function (key) {
            if (key.indexOf(CACHE_PREFIX) !== -1 &amp;&amp; key.indexOf(CACHE_VERSION) === -1){
                console.log('[SW]: Delete cache:' + key);
                return caches.delete(key);
            }
        });
        return Promise.all(all);
    });
}
//sw 激活階段,説明上一 sw 已失效
self.addEventListener('activate', function(event) {


    event.waitUntil(
        // 遍歷 caches 裏所有緩存的 keys 值
        caches.keys().then(deleteOldCaches)
    );
});
</code></pre><h3>idle</h3><p>這個空閒狀態一般是不可見的，這種一般説明 sw 的事情都處理完畢了，然後處於閒置狀態了。</p><p>瀏覽器會週期性的輪詢，去釋放處於 idle 的 sw 佔用的資源。</p><h3>fetch</h3><p>該階段是 sw 最為關鍵的一個階段，用於攔截代理所有指定的請求，並進行對應的操作。</p><p>所有的緩存部分，都是在該階段，這裏舉一個簡單的例子：</p><pre><code>//監聽瀏覽器的所有 fetch 請求，對已經緩存的資源使用本地緩存回覆
self.addEventListener('fetch', function(event) {
    event.respondWith(
        caches.match(event.request)
            .then(function(response) {
                //該 fetch 請求已經緩存
                if (response) {
                    return response;
                }
                return fetch(event.request);
                }
            )
    );
});
</code></pre><p>生命週期大概講清楚了，我們就以一個具體的例子來説明下原生的 serviceworker 是如何在生產環境中使用的吧。</p><h2>舉個栗子</h2><p>我們可以以網易新聞的 wap 頁為例,其針對不怎麼變化的靜態資源開啓了 sw 緩存，具體的 sw.js 邏輯和解讀如下：</p><pre><code>'use strict';
//需要緩存的資源列表
var precacheConfig = [
    ["https://static.ws.126.net/163/wap/f2e/milk_index/bg_img_sm_minfy.png",
        "c4f55f5a9784ed2093009dadf1e954f9"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/change.png",
        "9af1b102ef784b8ff08567ba25f31d95"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-download.png",
        "1c02c724381d77a1a19ca18925e9b30c"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-login-dark.png",
        "b59ba5abe97ff29855dfa4bd3a7a9f35"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-refresh.png",
        "a5b1084e41939885969a13f8dbc88abd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-video-play.png",
        "065ff496d7d36345196d254aff027240"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon.ico",
        "a14e5365cc2b27ec57e1ab7866c6a228"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.eot",
        "e4d2788fef09eb0630d66cc7e6b1ab79"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.svg",
        "d9e57c341608fddd7c140570167bdabb"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.ttf",
        "f422407038a3180bb3ce941a4a52bfa2"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.woff",
        "ead2bef59378b00425779c4ca558d9bd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/index.5cdf03e8.js",
        "6262ac947d12a7b0baf32be79e273083"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/index.bc729f8a.css",
        "58e54a2c735f72a24715af7dab757739"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-bohe.png",
        "ac5116d8f5fcb3e7c49e962c54ff9766"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-mail.png",
        "a12bbfaeee7fbf025d5ee85634fca1eb"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-manhua.png",
        "b8905b119cf19a43caa2d8a0120bdd06"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-open.png",
        "b7cc76ba7874b2132f407049d3e4e6e6"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-read.png",
        "e6e9c8bc72f857960822df13141cbbfd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-site.png",
        "2b0d728b46518870a7e2fe424e9c0085"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_no_pic.png",
        "aef80885188e9d763282735e53b25c0e"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_pc.png",
        "42f3cc914eab7be4258fac3a4889d41d"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_standard.png",
        "573408fa002e58c347041e9f41a5cd0d"]
];
var cacheName = 'sw-precache-v3-new-wap-index-' + (self.registration ? self.registration.scope : '');

var ignoreUrlParametersMatching = [/^utm_/];

var addDirectoryIndex = function(originalUrl, index) {
    var url = new URL(originalUrl);
    if (url.pathname.slice(-1) === '/') {
        url.pathname += index;
    }
    return url.toString();
};
var cleanResponse = function(originalResponse) {
    // If this is not a redirected response, then we don't have to do anything.
    if (!originalResponse.redirected) {
        return Promise.resolve(originalResponse);
    }
    // Firefox 50 and below doesn't support the Response.body stream, so we may
    // need to read the entire body to memory as a Blob.
    var bodyPromise = 'body' in originalResponse ?
        Promise.resolve(originalResponse.body) :
        originalResponse.blob();
    return bodyPromise.then(function(body) {
        // new Response() is happy when passed either a stream or a Blob.
        return new Response(body, {
            headers: originalResponse.headers,
            status: originalResponse.status,
            statusText: originalResponse.statusText
        });
    });
};
var createCacheKey = function(originalUrl, paramName, paramValue,
                              dontCacheBustUrlsMatching) {
    // Create a new URL object to avoid modifying originalUrl.
    var url = new URL(originalUrl);
    // If dontCacheBustUrlsMatching is not set, or if we don't have a match,
    // then add in the extra cache-busting URL parameter.
    if (!dontCacheBustUrlsMatching ||
        !(url.pathname.match(dontCacheBustUrlsMatching))) {
        url.search += (url.search ? '&amp;' : '') +
            encodeURIComponent(paramName) + '=' + encodeURIComponent(paramValue);
    }
    return url.toString();
};
var isPathWhitelisted = function(whitelist, absoluteUrlString) {
    // If the whitelist is empty, then consider all URLs to be whitelisted.
    if (whitelist.length === 0) {
        return true;
    }
    // Otherwise compare each path regex to the path of the URL passed in.
    var path = (new URL(absoluteUrlString)).pathname;
    return whitelist.some(function(whitelistedPathRegex) {
        return path.match(whitelistedPathRegex);
    });
};
var stripIgnoredUrlParameters = function(originalUrl,
                                         ignoreUrlParametersMatching) {
    var url = new URL(originalUrl);
    // Remove the hash; see https://github.com/GoogleChrome/sw-precache/issues/290
    url.hash = '';
    url.search = url.search.slice(1) // Exclude initial '?'
        .split('&amp;') // Split into an array of 'key=value' strings
        .map(function(kv) {
            return kv.split('='); // Split each 'key=value' string into a [key, value] array
        })
        .filter(function(kv) {
            return ignoreUrlParametersMatching.every(function(ignoredRegex) {
                return !ignoredRegex.test(kv[0]); // Return true iff the key doesn't match any of the regexes.
            });
        })
        .map(function(kv) {
            return kv.join('='); // Join each [key, value] array into a 'key=value' string
        })
        .join('&amp;'); // Join the array of 'key=value' strings into a string with '&amp;' in between each
    return url.toString();
};

var hashParamName = '_sw-precache';
//定義需要緩存的 url 列表
var urlsToCacheKeys = new Map(
    precacheConfig.map(function(item) {
        var relativeUrl = item[0];
        var hash = item[1];
        var absoluteUrl = new URL(relativeUrl, self.location);
        var cacheKey = createCacheKey(absoluteUrl, hashParamName, hash, false);
        return [absoluteUrl.toString(), cacheKey];
    })
);
//把 cache 中的 url 提取出來,進行去重操作
function setOfCachedUrls(cache) {
    return cache.keys().then(function(requests) {
        //提取 url
        return requests.map(function(request) {
            return request.url;
        });
    }).then(function(urls) {
        //去重
        return new Set(urls);
    });
}
//sw 安裝階段
self.addEventListener('install', function(event) {
    event.waitUntil(
        //首先嚐試取出存在客戶端 cache 中的數據
        caches.open(cacheName).then(function(cache) {
            return setOfCachedUrls(cache).then(function(cachedUrls) {
                return Promise.all(
                    Array.from(urlsToCacheKeys.values()).map(function(cacheKey) {
                        //如果需要緩存的 url 不在當前 cache 中,則添加到 cache
                        if (!cachedUrls.has(cacheKey)) {
                            //設置 same-origin 是為了兼容舊版本 safari 中其默認值不為 same-origin,
                            //只有當 URL 與響應腳本同源才發送 cookies、 HTTP Basic authentication 等驗證信息
                            var request = new Request(cacheKey, {credentials: 'same-origin'});
                            return fetch(request).then(function(response) {
                                //通過 fetch api 請求資源
                                if (!response.ok) {
                                    throw new Error('Request for ' + cacheKey + ' returned a ' +
                                        'response with status ' + response.status);
                                }
                                return cleanResponse(response).then(function(responseToCache) {
                                    //並設置到當前 cache 中
                                    return cache.put(cacheKey, responseToCache);
                                });
                            });
                        }
                    })
                );
            });
        }).then(function() {

            //強制跳過等待階段,進入激活階段
            return self.skipWaiting();

        })
    );
});
self.addEventListener('activate', function(event) {
    //清除 cache 中原來老的一批相同 key 的數據
    var setOfExpectedUrls = new Set(urlsToCacheKeys.values());
    event.waitUntil(
        caches.open(cacheName).then(function(cache) {
            return cache.keys().then(function(existingRequests) {
                return Promise.all(
                    existingRequests.map(function(existingRequest) {
                        if (!setOfExpectedUrls.has(existingRequest.url)) {
                            //cache 中刪除指定對象
                            return cache.delete(existingRequest);
                        }
                    })
                );
            });
        }).then(function() {
            //self 相當於 webworker 線程的當前作用域
            //當一個 service worker 被初始註冊時，頁面在下次加載之前不會使用它。 claim() 方法會立即控制這些頁面
            //從而更新客戶端上的 serviceworker
            return self.clients.claim();

        })
    );
});

self.addEventListener('fetch', function(event) {
    if (event.request.method === 'GET') {
        // 標識位,用來判斷是否需要緩存
        var shouldRespond;
        // 對 url 進行一些處理,移除一些不必要的參數
        var url = stripIgnoredUrlParameters(event.request.url, ignoreUrlParametersMatching);
        // 如果該 url 不是我們想要緩存的 url,置為 false
        shouldRespond = urlsToCacheKeys.has(url);
        // 如果 shouldRespond 未 false,再次驗證
        var directoryIndex = 'index.html';
        if (!shouldRespond &amp;&amp; directoryIndex) {
            url = addDirectoryIndex(url, directoryIndex);
            shouldRespond = urlsToCacheKeys.has(url);
        }
        // 再次驗證,判斷其是否是一個 navigation 類型的請求
        var navigateFallback = '';
        if (!shouldRespond &amp;&amp;
            navigateFallback &amp;&amp;
            (event.request.mode === 'navigate') &amp;&amp;
            isPathWhitelisted([], event.request.url)) {
            url = new URL(navigateFallback, self.location).toString();
            shouldRespond = urlsToCacheKeys.has(url);
        }
        // 如果標識位為 true
        if (shouldRespond) {
            event.respondWith(
                caches.open(cacheName).then(function(cache) {
                    //去緩存 cache 中找對應的 url 的值
                    return cache.match(urlsToCacheKeys.get(url)).then(function(response) {
                        //如果找到了,就返回 value
                        if (response) {
                            return response;
                        }
                        throw Error('The cached response that was expected is missing.');
                    });
                }).catch(function(e) {
                    // 如果沒找到則請求該資源
                    console.warn('Couldn\'t serve response for "%s" from cache: %O', event.request.url, e);
                    return fetch(event.request);
                })
            );
        }
    }
});
</code></pre><p>這裏的策略大概就是優先在 Cache 中尋找資源，如果找不到再請求資源。可以看出，為了實現一個較為簡單的緩存，還是比較複雜和繁瑣的，所以很多工具就應運而生了。</p><h1>Workbox</h1><p>由於直接寫原生的 sw.js，比較繁瑣和複雜，所以一些工具就出現了，而 Workbox 是其中的佼佼者，由 google 團隊推出。</p><h2>簡介</h2><p>在 Workbox 之前，GoogleChrome 團隊較早時間推出過 sw-precache 和 sw-toolbox 庫，但是在 GoogleChrome 工程師們看來，workbox 才是真正能方便統一的處理離線能力的更完美的方案，所以停止了對 sw-precache 和 sw-toolbox 的維護。</p><h2>使用者</h2><p>有很多團隊也是啓用該工具來實現 serviceworker 的緩存，比如説：</p><ul><li>淘寶首頁</li><li>網易新聞 wap 文章頁</li><li>百度的 Lavas</li></ul><h2>基本配置</h2><p>首先，需要在項目的 sw.js 文件中，引入 Workbox 的官方 js，這裏用了我們自己的靜態資源：</p><pre><code>importScripts(
    "https://edu-cms.nosdn.127.net/topics/js/workbox_9cc4c3d662a4266fe6691d0d5d83f4dc.js"
);
</code></pre><p>其中 importScripts 是 webworker 中加載 js 的方式。</p><p>引入 Workbox 後，全局會掛載一個 Workbox 對象</p><pre><code>if (workbox) {
    console.log('workbox 加載成功');
} else {
    console.log('workbox 加載失敗');
}
</code></pre><p>然後需要在使用其他的 api 前，提前使用配置</p><pre><code>//關閉控制枱中的輸出
workbox.setConfig({ debug: false });
</code></pre><p>也可以統一指定存儲時 Cache 的名稱：</p><pre><code>//設置緩存 cachestorage 的名稱
workbox.core.setCacheNameDetails({
    prefix:'edu-cms',
    suffix:'v1'
});
</code></pre><h2>precache</h2><p>Workbox 的緩存分為兩種，一種的 precache，一種的 runtimecache。</p><p>precache 對應的是在 installing 階段進行讀取緩存的操作。它讓開發人員可以確定緩存文件的時間和長度，以及在不進入網絡的情況下將其提供給瀏覽器，這意味着它可以用於創建 Web 離線工作的應用。</p><h3>工作原理</h3><p>首次加載 Web 應用程序時，Workbox 會下載指定的資源，並存儲具體內容和相關修訂的信息在 indexedDB 中。</p><p>當資源內容和 sw.js 更新後，Workbox 會去比對資源，然後將新的資源存入 Cache，並修改 indexedDB 中的版本信息。</p><p>我們舉一個例子：</p><pre><code>workbox.precaching.precacheAndRoute([
    './main.css'
]);
</code></pre><p><img src="https://oscimg.oschina.net/oscnet/up-d8a79ba50b83bfbc538b960e07f0c707b17.png" alt="" referrerpolicy="no-referrer"></p><p>indexedDB 中會保存其相關信息</p><p><img src="https://oscimg.oschina.net/oscnet/up-b9f1c514f24a2b5121771fa9b59fb0cc82b.png" alt="" referrerpolicy="no-referrer"></p><p>這個時候我們把 main.css 的內容改變後，再刷新頁面，會發現除非強制刷新，否則 Workbox 還是會讀取 Cache 中存在的老的 main.css 內容。</p><p>即使我們把 main.css 從服務器上刪除，也不會對頁面造成影響。</p><p>所以這種方式的緩存都需要配置一個版本號。在修改 sw.js 時，對應的版本也需要變更。</p><h3>使用實踐</h3><p>當然了，一般我們的一些不經常變的資源，都會使用 cdn，所以這裏自然就需要支持域外資源了，配置方式如下：</p><pre><code>var fileList = [
    {
        url:'https://edu-cms.nosdn.127.net/topics/js/cms_specialWebCommon_js_f26c710bd7cd055a64b67456192ed32a.js'
    },
    {
        url:'https://static.ws.126.net/163/frontend/share/css/article.207ac19ad70fd0e54d4a.css'
    }
];


//precache 適用於支持跨域的 cdn 和域內靜態資源
workbox.precaching.suppressWarnings();
workbox.precaching.precacheAndRoute(fileList, {
    "ignoreUrlParametersMatching": [/./]
});
</code></pre><p>這裏需要對應的資源配置跨域允許頭，否則是不能正常加載的。且文件都要以版本文件名的方式，來確保修改後 Cache 和 indexDB 會得到更新。</p><p>理解了原理和實踐後，説明這種方式適合於上線後就不會經常變動的靜態資源。</p><h2>runtimecache</h2><p>運行時緩存是在 install 之後，activated 和 fetch 階段做的事情。</p><p>既然在 fetch 階段發送，那麼 runtimecache 往往應對着各種類型的資源，對於不同類型的資源往往也有不同的緩存策略。</p><h3>緩存策略</h3><p>Workbox 提供的緩存策劃有以下幾種，通過不同的配置可以針對自己的業務達到不同的效果：</p><h3>Stale While Revalidate</h3><p>這種策略的意思是當請求的路由有對應的 Cache 緩存結果就直接返回，</p><p>在返回 Cache 緩存結果的同時會在後台發起網絡請求拿到請求結果並更新 Cache 緩存，如果本來就沒有 Cache 緩存的話，直接就發起網絡請求並返回結果，這對用戶來説是一種非常安全的策略，能保證用戶最快速的拿到請求的結果。</p><p>但是也有一定的缺點，就是還是會有網絡請求佔用了用戶的網絡帶寬。可以像如下的方式使用 State While Revalidate 策略：</p><pre><code>workbox.routing.registerRoute(
    new RegExp('https://edu-cms\.nosdn\.127\.net/topics/'),
    workbox.strategies.staleWhileRevalidate({
        //cache 名稱
        cacheName: 'lf-sw:static',
        plugins: [
            new workbox.expiration.Plugin({
                //cache 最大數量
                maxEntries: 30
            })
        ]
    })
);
</code></pre><h3>Network First</h3><p>這種策略就是當請求路由是被匹配的，就採用網絡優先的策略，也就是優先嚐試拿到網絡請求的返回結果，如果拿到網絡請求的結果，就將結果返回給客戶端並且寫入 Cache 緩存。</p><p>如果網絡請求失敗，那最後被緩存的 Cache 緩存結果就會被返回到客戶端，這種策略一般適用於返回結果不太固定或對實時性有要求的請求，為網絡請求失敗進行兜底。可以像如下方式使用 Network First 策略：</p><pre><code>//自定義要緩存的 html 列表
var cacheList = [
    '/Hexo/public/demo/PWADemo/workbox/index.html'
];
workbox.routing.registerRoute(
    //自定義過濾方法
    function(event) {
        // 需要緩存的 HTML 路徑列表
        if (event.url.host === 'localhost:63342') {
            if (~cacheList.indexOf(event.url.pathname)) return true;
            else return false;
        } else {
            return false;
        }
    },
    workbox.strategies.networkFirst({
        cacheName: 'lf-sw:html',
        plugins: [
            new workbox.expiration.Plugin({
                maxEntries: 10
            })
        ]
    })
);
</code></pre><h3>Cache First</h3><p>這個策略的意思就是當匹配到請求之後直接從 Cache 緩存中取得結果，如果 Cache 緩存中沒有結果，那就會發起網絡請求，拿到網絡請求結果並將結果更新至 Cache 緩存，並將結果返回給客戶端。這種策略比較適合結果不怎麼變動且對實時性要求不高的請求。可以像如下方式使用 Cache First 策略：</p><pre><code>workbox.routing.registerRoute(
    new RegExp('https://edu-image\.nosdn\.127\.net/'),
    workbox.strategies.cacheFirst({
        cacheName: 'lf-sw:img',
        plugins: [
            //如果要拿到域外的資源，必須配置
            //因為跨域使用 fetch 配置了
            //mode: 'no-cors',所以 status 返回值為 0，故而需要兼容
            new workbox.cacheableResponse.Plugin({
                statuses: [0, 200]
            }),
            new workbox.expiration.Plugin({
                maxEntries: 40,
                //緩存的時間
                maxAgeSeconds: 12 * 60 * 60
            })
        ]
    })
);
</code></pre><h3>Network Only</h3><p>比較直接的策略，直接強制使用正常的網絡請求，並將結果返回給客戶端，這種策略比較適合對實時性要求非常高的請求。</p><h3>Cache Only</h3><p>這個策略也比較直接，直接使用 Cache 緩存的結果，並將結果返回給客戶端，這種策略比較適合一上線就不會變的靜態資源請求。</p><h2>舉個栗子</h2><p>又到了舉個栗子的階段了，這次我們用淘寶好了，看看他們是如何通過 Workbox 來配置 Service Worker 的：</p><pre><code>//首先是異常處理
self.addEventListener('error', function(e) {
  self.clients.matchAll()
    .then(function (clients) {
      if (clients &amp;&amp; clients.length) {
        clients[0].postMessage({ 
          type: 'ERROR',
          msg: e.message || null,
          stack: e.error ? e.error.stack : null
        });
      }
    });
});

self.addEventListener('unhandledrejection', function(e) {
  self.clients.matchAll()
    .then(function (clients) {
      if (clients &amp;&amp; clients.length) {
        clients[0].postMessage({
          type: 'REJECTION',
          msg: e.reason ? e.reason.message : null,
          stack: e.reason ? e.reason.stack : null
        });
      }
    });
})
//然後引入 workbox
importScripts('https://g.alicdn.com/kg/workbox/3.3.0/workbox-sw.js');
workbox.setConfig({
  debug: false,
  modulePathPrefix: 'https://g.alicdn.com/kg/workbox/3.3.0/'
});
//直接激活跳過等待階段
workbox.skipWaiting();
workbox.clientsClaim();
//定義要緩存的 html
var cacheList = [
  '/',
  '/tbhome/home-2017',
  '/tbhome/page/market-list'
];
//html 採用 networkFirst 策略，支持離線也能大體訪問
workbox.routing.registerRoute(
  function(event) {
    // 需要緩存的 HTML 路徑列表
    if (event.url.host === 'www.taobao.com') {
      if (~cacheList.indexOf(event.url.pathname)) return true;
      else return false;
    } else {
      return false;
    }
  },
  workbox.strategies.networkFirst({
    cacheName: 'tbh:html',
    plugins: [
      new workbox.expiration.Plugin({
        maxEntries: 10
      })
    ]
  })
);
//靜態資源採用 staleWhileRevalidate 策略，安全可靠
workbox.routing.registerRoute(
  new RegExp('https://g\.alicdn\.com/'),
  workbox.strategies.staleWhileRevalidate({
    cacheName: 'tbh:static',
    plugins: [
      new workbox.expiration.Plugin({
        maxEntries: 20
      })
    ]
  })
);
//圖片採用 cacheFirst 策略，提升速度
workbox.routing.registerRoute(
  new RegExp('https://img\.alicdn\.com/'),
  workbox.strategies.cacheFirst({
    cacheName: 'tbh:img',
    plugins: [
      new workbox.cacheableResponse.Plugin({
        statuses: [0, 200]
      }),
      new workbox.expiration.Plugin({
        maxEntries: 20,
        maxAgeSeconds: 12 * 60 * 60
      })
    ]
  })
);

workbox.routing.registerRoute(
  new RegExp('https://gtms01\.alicdn\.com/'),
  workbox.strategies.cacheFirst({
    cacheName: 'tbh:img',
    plugins: [
      new workbox.cacheableResponse.Plugin({
        statuses: [0, 200]
      }),
      new workbox.expiration.Plugin({
        maxEntries: 30,
        maxAgeSeconds: 12 * 60 * 60
      })
    ]
  })
);
</code></pre><p>可以看出，使用 Workbox 比起直接手擼來，要快很多，也明確很多。</p><h2>原理</h2><p>目前分析 Service Worker 和 Workbox 的文章不少，但是介紹 Workbox 原理的文章卻不多。這裏簡單介紹下 Workbox 這個工具庫的原理。</p><p>首先將幾個我們產品用到的模塊圖奉上：</p><p><img src="https://oscimg.oschina.net/oscnet/up-b22e014db049eea325d28de53c9b6b9cd76.png" alt="" referrerpolicy="no-referrer"></p><p>簡單提幾個 Workbox 源碼的亮點。</p><h3>通過 Proxy 按需依賴</h3><p>熟悉了 Workbox 後會得知，它是有很多個子模塊的，各個子模塊再通過用到的時候按需 importScript 到線程中。 <img src="https://oscimg.oschina.net/oscnet/up-12e98258edc4bd13e5ad48a74c9835ede68.png" alt="" referrerpolicy="no-referrer"></p><p>做到按需依賴的原理就是通過 Proxy 對全局對象 Workbox 進行代理：</p><pre><code>new Proxy(this, {
  get(t, s) {
    //如果 workbox 對象上不存在指定對象，就依賴注入該對象對應的腳本
    if (t[s]) return t[s];
    const o = e[s];
    return o &amp;&amp; t.loadModule(`workbox-${o}`), t[s];
  }
})
</code></pre><p>如果找不到對應模塊，則通過 importScripts 主動加載：</p><pre><code>/**
 * 加載前端模塊
 * @param {Strnig} t 
 */
loadModule(t) {
  const e = this.o(t);
  try {
    importScripts(e), (this.s = !0);
  } catch (s) {
    throw (console.error(`Unable to import module '${t}' from '${e}'.`), s);
  }
}
</code></pre><h3>通過 freeze 凍結對外暴露 api</h3><p>Workbox.core 模塊中提供了幾個核心操作模塊，如封裝了 indexedDB 操作的 DBWrapper、對 Cache Storage 進行讀取的 Cache Wrapper，以及發送請求的 fetchWrapper 和日誌管理的 logger 等等。</p><p>為了防止外部對內部模塊暴露出去的 api 進行修改，導致出現不可預估的錯誤，內部模塊可以通過 Object.freeze 將 api 進行凍結保護：</p><pre><code>var _private = /*#__PURE__*/Object.freeze({
    DBWrapper: DBWrapper,
    WorkboxError: WorkboxError,
    assert: finalAssertExports,
    cacheNames: cacheNames,
    cacheWrapper: cacheWrapper,
    fetchWrapper: fetchWrapper,
    getFriendlyURL: getFriendlyURL,
    logger: defaultExport
  });
</code></pre><h1>總結</h1><p>通過對 Service Worker 的理解和 Workbox 的應用，可以進一步提升產品的性能和弱網情況下的體驗。有興趣的同學也可以對 Workbox 的源碼細細評讀，其中還有很多不錯的設計模式和編程風格值得學習。</p><p><img src="https://oscimg.oschina.net/oscnet/up-a546d4d52ff6cdf625f4d4a4890fd454bec.png" alt="" referrerpolicy="no-referrer"></p><p><strong>-END-</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 10:45:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/youdaotech/blog/5054309</guid>
            <link>https://my.oschina.net/youdaotech/blog/5054309</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Wasmer 開源 WinterJS：Rust 編寫的 Service Worker]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Wasmer 團隊開源了一款用 Rust 編寫的<strong> JavaScript Service Worker：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwasmer.io%2Fposts%2Fannouncing-winterjs-service-workers" target="_blank">WinterJS</a></u></strong>。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6382fe02fb5cbb80e1cb6951156b73e1143.png" referrerpolicy="no-referrer"></p><p><em>WinterJS 開源地址：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwasmerio%2Fwinterjs" target="_blank">https://github.com/wasmerio/winterjs</a></u></em></p><p>據介紹，WinterJS 使用 SpiderMonkey 運行時執行 JavaScript（與 Firefox 使用的運行時相同），並遵循 WinterCG 規範，目的是最大限度地兼容 Cloudflare Workers、Deno Deploy 和 Vercel 等其他服務（因此命名為 WinterJS）。</p><p>WinterJS 除了速度極快，還能通過 WASIX <strong>編譯成 WebAssembly</strong>，因此完全支持在 Wasmer 上運行。</p><ul><li><strong>使用示例</strong></li></ul><p><strong>創建<code>serviceworker.js</code>文件，並返回 "hello world"</strong></p><pre><code class="language-javascript">$ wasmer run wasmer/winterjs --net --mapdir /app:. /app/serviceworker.js</code></pre><pre><code class="language-javascript">addEventListener('fetch', (req) =&gt; {
  req.respondWith(`hello world from ${req.request.url.href}`);
});</code></pre><blockquote><p>Wasmer 是支持 WASI 和 Emscripten 的通用 WebAssembly 運行時，提供基於 WebAssembly 的超輕量級容器，專注於支持在任何平台上運行 WASM 代碼：從桌面端到雲端、以及 IoT 設備，並且能嵌入在任何編程語言中。</p><p><img alt="" src="https://static.oschina.net/uploads/space/2023/0627/173716_02s8_2720166.png" referrerpolicy="no-referrer"></p><p>Wasmer 憑藉其多樣化的支持和專注於從通用桌面應用程序到 「便攜式 ML/AI 應用程序」 的領域，目前仍然是領先的 WASM 運行時之一。</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 10:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263968/winterjs-service-workers</guid>
            <link>https://www.oschina.net/news/263968/winterjs-service-workers</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[扎克伯克：Meta 明年投入更多工程和計算資源到 AI 領域]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>當地時間 10 月 25 日，在 2023 財年第三季度財報電話會上，Meta CEO 扎克伯格強調，相信生成式 AI 的相關技術將讓人們使用各種應用程序的方式變得更有意義。在未來，Meta 甚至有可能會利用 AI 來根據用戶的興趣為他們直接生成內容。</p><p>扎克伯格表示，AI 將幫助使用 Meta 各大應用的創作者提升內容質量和生產效率，而隨着時間的推移，AI 參與生成的內容在用戶消費內容中的佔比將會越來越大。</p><p>對於公司的後續發展，扎克伯格表示在 2024 年，<strong>就工程和計算資源而言，AI 將成為 Meta 最大的投資領域</strong>。此外，扎克伯格補充道，為了避免引入大量的新員工，<strong>公司將降低一些非 AI 項目的優先級，並將相關人員轉向從事 AI 工作</strong>。</p><p>上月曾報道過，<u><a href="https://www.oschina.net/news/257670/meta-building-llm-rival-openais-gpt4">Meta 正在構建</a></u>新開源大模型，據稱性能超越 Llama 2、比肩 GPT-4，最終目標是加速開發下一代生成式人工智能模型，使其能夠生成更多類似人類的表達。</p><p><img alt="" src="https://static.oschina.net/uploads/space/2023/0911/152426_g2gp_2720166.png" referrerpolicy="no-referrer"></p><p>長期以來，Meta 一直在採用開源方法公開其大模型產品，是業內眾所周知的最大貢獻者之一。僅今年它就向人工智能社區發佈了大量人工智能模型和訓練數據集。其中包括針對編程任務優化的 Code Llama 大語言模型； 可實現數百種語言通用按需翻譯的 SeamlessM4T 模型； 用於創作音樂和聲音的生成式人工智能模型 AudioCraft；語音生成人工智能模型 Voicebox。它還推出了 I-JEPA（一種可以像人類一樣學習的計算機視覺模型）和 FACET（一種基準數據集，旨在幫助研究人員審核計算機視覺模型的偏差）。</p><hr><p>延伸閲讀</p><ul><li><a href="https://www.oschina.net/news/256830/meta-ai-belebele">Meta AI 多語言閲讀理解數據集 Belebele，涵蓋 122 種語言變體</a></li><li><a href="https://www.oschina.net/news/255350/meta-code-llama">Meta 開源基於 Llama 2 的 AI 代碼生成大模型：Code Llama</a></li><li><a href="https://www.oschina.net/news/255168/meta-seamless-m4t">Meta 推出&nbsp;SeamlessM4T，可轉錄和翻譯近 100 種語言</a></li><li><a href="https://www.oschina.net/news/252174/audiocraft-generative-ai-for-music-and-audio">Meta 發佈開源 AI 工具 AudioCraft，文本自動生成音樂</a></li><li><a href="https://www.oschina.net/news/249944/meta-llama-2">Meta 放大招：發佈開源大語言模型 Llama 2，可免費商用</a></li><li><a href="https://www.oschina.net/news/245895/meta-voicebox-generative-ai-model-speech">Meta 發佈語音生成 AI 模型：Voicebox</a></li><li><a href="https://www.oschina.net/news/245705/meta-musicgen">Meta 開源音樂生成模型 MusicGen</a></li><li><a href="https://www.oschina.net/news/242331/mate-multilingual-model-speech">Meta 開源大模型：支持 4000+ 語言識別，1100+ 種語音文本轉換</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 09:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263757</guid>
            <link>https://www.oschina.net/news/263757</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Ubuntu 24.04 進入開發階段，代號 Noble Numbat]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="color:#000000">Canonical 的 Utkarsh Gupta 在一封發送給 Ubuntu 開發郵件列表的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.ubuntu.com%2Farchives%2Fubuntu-devel%2F2023-October%2F042835.html" target="_blank">電子郵件中宣佈</a>，Ubuntu 24.04 現已開放供開發，並透露了該版本的代號為「Noble Numbat」。</span></p><blockquote><p><span style="color:#000000">我們很高興地宣佈，Noble Numbat 現已開放開發。自動同步已啓用，並將很快運行。和往常一樣，我們預計在初始階段會有大量的構建和自動測試湧入，這將導致一些延遲現象的出現。請協助修復出現的任何故障。</span></p></blockquote><p><span style="color:#000000">根據百度百科，Numbat（袋食蟻獸）是分佈於澳大利亞西南部的一種小型有袋動物，幾乎只以白蟻為食，每天可以吃約 20000 只白蟻。目前僅在少數地區存活，屬於瀕危物種，已被列入《世界自然保護聯盟瀕危物種紅色名錄》。袋食蟻獸的體型小而吻長，牙齒多達 52 枚，超過任何陸生哺乳動物的齒數，齒細，排成長列，長而能伸的舌（長約 10 釐米），用以捕捉白蟻。</span></p><p><span style="color:#000000"><img alt="" height="286" src="https://oscimg.oschina.net/oscnet/up-050f5f99cc77c3757686112b409fb2558f7.jpg" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">Ubuntu 24.04 將是 Ubuntu 自 2006 年以來的第 10 個 LTS 版本。Ubuntu 的 LTS 版本將獲得 5 年的安全更新、錯誤修復和精選應用程序更新。Ubuntu Pro 則會在此基礎上額外增加 5 年的安全保障，為現代的 LTS 版本提供了長達十年的支持。</span></p><p><span style="color:#000000">目前對於 Ubuntu 24.04 中將包含的新功能和改進仍然知之甚少。但 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2023%2F10%2Fubuntu-24-04-development-open" target="_blank">OMG! Ubuntu</a> 指出，對於長期支持版本而言，Ubuntu 在主要新功能、用戶界面的巨大變化等方面往往會比較保守，主要會更加專注于堅實、穩定的體驗。</span></p><p><span style="color:#000000">可以確定的是，24.04 肯定會配備新的 Linux 內核（6.7 或 6.8，視時間而定）、GNOME 46（預計將在三月份發佈）。Canonical 的 Oliver Grawert 還透露，一個不可變的、snap-based Ubuntu 24.04 鏡像將於 4 月份提供下載（但不會是默認推薦下載）。</span></p><p><span style="color:#000000">Ubuntu 24.04 計劃於 2024 年 4 月 25 日正式發佈。其功能凍結階段定於 2024 年 2 月 29 日，beta 版本計劃於 2024 年 4 月 4 日發佈。</span></p><p><img height="501" src="https://oscimg.oschina.net/oscnet/up-78523d04d02ccf21bda56aa13e0dcd0b317.png" width="300" referrerpolicy="no-referrer"></p><p><span style="color:#000000">可在此查看具體的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdiscourse.ubuntu.com%2Ft%2Fnoble-numbat-release-schedule%2F35649" target="_blank">發佈時間表</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 09:29:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263754/ubuntu-24-04-noble-numbat</guid>
            <link>https://www.oschina.net/news/263754/ubuntu-24-04-noble-numbat</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[3202 年了，為啥 SSR 並沒有預想中的流行？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><div><p style="text-align:left">有研究發現，網站加載時間每增加一秒，用戶便會流失 10%。為提高頁面的秒開率，各路人馬不斷探索着優化策略，僅僅在瀏覽器領域下的優化已經滿足不了極致的要求了，大家開始往服務端方向不斷探索，並一度讓【服務端渲染】這一古早的概念「翻紅」，且炒得火熱。</p><p>服務端渲染簡稱 SSR，全稱 Server Side Rendering，顧名思義是將渲染的工作放在 Server 端進行。這種辦法不僅有利於首屏渲染，提高 SPA 應用的首屏響應速度，還方便搜索引擎抓取，有利於 SEO 優化。不過，到 2023 年了，SSR 並沒有預想中的流行。</p><p>有評論認為，大部分用 SSR 的原因是為了服務 SEO，但現在搜索引擎已經跟上發展步伐了，對於用框架寫成的 SPA 支持也不錯，所以 SSR 必要性沒那麼大了。還有人覺得 SSR 就是偽需求，業務邏輯和控制器分離好了加載一樣快。</p><p>但也有評論認為，現在仍然有大量的用戶因為網絡環境或設備情況，在訪問 Web 頁面的時候無法達到很好的體驗，如果要提升這部分用戶的體驗，那麼 SSR 就是一種不可或缺的方式。</p><p style="text-align:left">對此，真實的情況是怎樣的？實際應用中，阻礙 SSR 成為 Web 主流開發模式的原因是什麼？這種方法放到今天的環境下過時了嗎？什麼樣的業務場景更適合 SSR 呢？對此，開源中國邀請了兩位前端大佬，來聽聽他們的看法。</p><ul><li><p><span style="color:#245bdb">劉奎，社區暱稱 kuitos 。支付寶體驗技術部前端工程師，開源微前端方案 qiankun 作者，目前在螞蟻負責 Web 基建研發相關工作。</span></p></li><li><p><span style="color:#245bdb">劉勇，社區暱稱天豬，某大廠 Node.js </span><span style="color:#245bdb">Infra</span><span style="color:#245bdb"> 負責人，EggJS / CNPM 核心開發者。</span></p></li></ul><p>&nbsp;</p><span id="OSC_h1_1"></span><h1>一、SSR，並不是偽需求</h1><p><span style="color:#245bdb"><strong>Q1：以你的經驗，什麼類型的項目和場景更常用 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> ？能舉些例子嗎？</strong></span></p><p><strong>劉奎：</strong>對首屏性能非常敏感，或者對 SEO 有強訴求的這類網站會更常用 SSR，如：</p><ul><li><p>電商平台：更快的首屏渲染可以讓用戶更快的看到商品信息，提升購買轉化率</p></li><li><p>營銷活動頁：秒開能有效提升營銷活動的業務效果</p></li><li><p>門戶網站：內容型站點通常對 SEO 有着比較強的訴求</p></li></ul><p>&nbsp;</p><p><span style="color:#245bdb"><strong>Q2：從你的實際體驗出發，你覺得 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 相比於 </strong></span><span style="color:#245bdb"><strong>CSR</strong></span><span style="color:#245bdb"><strong>（</strong></span><span style="color:#245bdb"><strong>Client-side rendering</strong></span><span style="color:#245bdb"><strong>）模式，優勢在哪？</strong></span></p><p><strong>劉奎：</strong>從我個人體驗來看，最大的優勢還是在首屏體驗上，SSR 模式下 HTML 加載過程中用戶就能看到有效的頁面內容，這個基本是 CSR 很難做到的。</p><p>&nbsp;</p><p><span style="color:#245bdb"><strong>Q3：如今</strong></span><span style="color:#245bdb"><strong>搜索引擎</strong></span><span style="color:#245bdb"><strong>已經支持渲染了，你認為還有必要因為 </strong></span><span style="color:#245bdb"><strong>SEO</strong></span><span style="color:#245bdb"><strong> 的原因使用 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 嗎？</strong></span></p><p><strong>劉奎：</strong>由於眾所周知的原因，國內的搜索引擎對 SPA 類型的應用支持的並不好，如果希望自己的網站能更好的被爬蟲索引到，基本上還是需要使用 SSR（或者 SSR 的變種）方案了。</p><p>&nbsp;</p><p><span style="color:#245bdb"><strong>Q4：有人認為 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 是偽需求，要改善首屏渲染性能的話，後端服務的業務邏輯和控制器分離，控制器分視圖控制器和接口控制器，調用相同的業務邏輯。第一次打開頁面，前端 </strong></span><span style="color:#245bdb"><strong>JavaScript</strong></span><span style="color:#245bdb"><strong> 加載頁面渲染的數據，用戶交互時再請求接口獲取數據。這個方案比性能着急的 SSR 強多了。你怎麼評價？</strong></span></p><p><strong>劉奎：</strong>這個方案本質還是 CSR，無法解決 CSR 方案原生的問題：即用戶必須等到 JS 下載完成 -&gt; 發起接口請求 -&gt; JS 獲取數據渲染頁面之後，才能看到有效內容的問題。在越苛刻的網絡環境及用戶設備條件下，這個問題會越明顯。</p><p><strong>劉勇：</strong>根據團隊的基建成熟度和業務場景做技術選型，這 2 個方案沒有絕對的優劣，也不是絕對的割裂，它們是可以通過前端工程化結合成一個方案的。</p><p>&nbsp;</p><span id="OSC_h1_2"></span><h1>二、SSR，想紅有點難</h1><p><span style="color:#245bdb"><strong>Q5：以當前的形勢來看，</strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 並沒能成為 Web 主流的開發模式，你覺得這其中的阻礙有哪些？</strong></span></p><p><strong>劉奎：</strong>我覺得主要有這幾類原因：</p><ul><li><p><strong>技術複雜度：</strong>SSR 需要在服務器端進行渲染，並與前端框架進行集成，對開發人員來説需要掌握更多的技術知識。</p></li><li><p><strong>SSR</strong><strong> 帶來的額外的開發及維護成本：</strong>相對於 CSR，SSR 方案需要前端額外去關注服務端相關的開發及運維，比如如何寫出更高性能的服務端渲染邏輯，如何處理潛在的內存泄露、變量污染等隔離問題，如何做 SSR 的容災（在 SSR 失敗時 fallback 到 CSR）等，這些都需要團隊有額外的資源及時間投入。</p></li><li><p><strong>場景匹配度：</strong>國內大量的服務是通過小程序、APP 這類載體進行分發的，純 Web 技術棧的產品相對較少，這點與國外的場景有着非常大的不同。</p></li></ul><p><strong>劉勇：</strong>首先，SSR 是需要服務器資源成本的，在降本提效的大背景下，會需要結合 Serverless 或邊緣計算等一些基建才能找到平衡點。同時既然是服務端，就有一定的運維能力要求，對前端團隊的技術積累有一定的要求。</p><p>其次，框架的封裝和維護如果做的不好的話，業務同學寫 SSR 很容易弄出內存泄露問題，這是非常常見的。而且目前的前端框架還沒有針對 SSR 場景進行優化，如果只是首屏展示快，但緊接着要下載超大的 Bundle 文件，從而用戶可交互時間太慢，就得不償失了。</p><p>最後，演進路徑問題，譬如螞蟻那邊，他們當年就已經跟把離線包的上下游基建都做的很完善了，APP 側、網絡側都有兄弟團隊配合一起打磨。這種模式會有一些缺陷，如離線包太多時的業務競爭問題，但就首屏性能這一點上，SSR 不一定比它好多少，這時候讓他們切換到 SSR 就會有不小的阻力。</p><p>&nbsp;</p><p><span style="color:#245bdb"><strong>Q6：有評論認為 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 開發和維護成本太高了，轉而投向了 </strong></span><span style="color:#245bdb"><strong>CSR</strong></span><span style="color:#245bdb"><strong> 的懷抱。CSR 能否取得跟 SSR 一樣的效果呢？有什麼具體的操作方案嗎？</strong></span></p><p><strong>劉勇：</strong>從首屏性能的關鍵點看，CSR 如果不做一定的優化的話，至少 3 次串行的 HTTP 請求，首屏時間肯定比不過 SSR（互操作時間就不一定）。</p><p>不過相應的解決方案也挺多的，如 ServiceWorker、離線包等等方式。</p><p><strong>劉奎：</strong>單從首屏渲染速度這一點來看，CSR 想取得 SSR 類似的效果，可以採取以下方案優化：</p><ol><li><p><strong>首屏頁面靜態資源優化：</strong>通過代碼切割 &amp; 懶加載等手段，確保首屏需要的 JS/CSS 是最小化的版本，並通過內聯等方式直接打到 HTML 中，減少首屏渲染需要的網絡請求；</p></li><li><p><strong>緩存和</strong><strong>預加載</strong><strong>：</strong>利用客戶端的緩存及預加載等機制，提升二次訪問速度；</p></li><li><p><strong>使用更輕量的框架：</strong>選擇更輕量的前端框架，從而減少首屏的 JS 體積，提升加載速度；</p></li><li><p><strong>優化關鍵接口響應速度：</strong>優化首屏需要的關鍵內容的接口響應速度，確保前端能更快的呈現頁面。</p></li></ol><p>但如果還有額外的 SEO 訴求，單純的 CSR 可能很難達到一樣的效果。</p><p>&nbsp;</p><p><span style="color:#245bdb"><strong>Q7：如果將原有的應用直接切換到 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 一體化應用中來，成本會有多大？對開發團隊會有哪些挑戰呢？</strong></span></p><p><strong>劉奎：</strong>成本及挑戰有以下幾點：</p><ol><li><p><strong>應用改造成本：</strong>大部分應用都是無法直接在服務端環境運行的，基本都需要做一定程度的改造，比如消除首屏渲染代碼中對 window、location 等瀏覽器特有 API 的依賴，構建出用於服務端運行的 JS 等。</p></li><li><p><strong>SSR</strong><strong> 函數研發及運維挑戰：</strong>同時具備豐富的前端及服務端開發經驗的團隊在大部分公司都是非常少見的，如前面提到的，SSR 帶來的額外的服務端的開發及運維挑戰，這個也是需要前端團隊考慮的。</p></li></ol><p>&nbsp;</p><span id="OSC_h1_3"></span><h1>三、也許，SSR + CSR 會是未來新方向？</h1><p><span style="color:#245bdb"><strong>Q8：現在一些網站採用了</strong></span><span style="color:#245bdb"><strong>首屏服務器端</strong></span><span style="color:#245bdb"><strong>渲染，即對於用戶最開始打開的那個頁面採用的是服務器端渲染，這樣就保證了渲染速度，而其他的頁面採用</strong></span><span style="color:#245bdb"><strong>客戶端渲染</strong></span><span style="color:#245bdb"><strong>，這樣就完成了前後端分離。你覺得這會是融合了兩者優勢的更完美的方案嗎？</strong></span></p><p><strong>劉奎：</strong>是的，這也是目前社區內的最佳實踐，能很好的保留 SSR 及 SPA 應用的優點。</p><p><strong>劉勇：</strong>這其實很多年前就有相關實踐了，譬如當年雲龍在 UC 的 Scrat Pagelet 就是類似的實踐，甚至當時做的是後續頁面也通過服務端局部渲染，按需更新前端頁面的階段。</p><p>這種方式在業界也有看到一些更近一步的實踐：開發者很自然的去寫邏輯，不用管什麼分離不分離的事，在前端工程化那一層自動拆分，SSG + SSR + CSR，一些可以靜態構建的直接在構建階段處理了，一些可以在服務端渲染的服務端，剩下的非剛需的組件直接前端渲染掉。這些都能做，前提是前端工程化這塊的基建是否足夠完善，研發模式是否足夠收斂。</p><p>最後提醒下，我所瞭解大部份 SSR 實踐，一般也會在前面再擋一個短時效的 CDN，然後通過 CSR 做千人千面的修飾和後續業務邏輯。</p><p>&nbsp;</p><p><span style="color:#245bdb"><strong>Q9：你如何看待 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 的未來發展？是會隨着硬件的升級逐步淘汰，還是會隨着技術的更新越發流行？</strong></span></p><p><strong>劉勇：</strong>優化思路是不會過時的，也許某一天我們現在熟悉的 SSR 的編程界面變了，譬如當年的 SSR 是用 nunjucks、ejs 之類的模版，現在是 react、vue。未來也會有新的技術出現，但它很有可能也屬於 SSR 的一種實踐模式。</p><p><strong>劉奎：</strong>按照我的經驗來看，很多時候新的技術方案大都會嘗試更多的壓榨硬件機能，從而獲得更好的交互體驗，所以任何時期都會存在相對「低端」的設備，這個應該是解決不掉的（笑</p><p>在我看來，SSR 最主要的落地成本還是在服務端的研發及運維上，這個對於大部分公司的前端團隊都是較大的負擔，進而因為 ROI 不高導致 SSR 落地困難。但是，隨着 Serverless 的發展，出現了許多幾乎「零運維」的 Serverless 方案，可以極大地降低前端團隊的運維成本。同時，從社區的趨勢來看，近年來流行的各種前端框架都在擁抱 Edge 和 SSR，例如 Next.js、remix-run、Qwik、Astro、Fresh 等。同時，React 等庫也推出了性能表現更佳的流式 SSR 能力。通過這些框架技術的集成和迭代，不僅可以顯著降低前端工程師開發 SSR 應用的研發成本，還能進一步提升傳統 SSR 的性能效果。</p><p>從目前的趨勢來看，我覺得 SSR 會隨着研發及運維成本的降低，變得越發的流行。</p><p>&nbsp;</p><p><span style="color:#245bdb"><strong>Q10：結合你的項目經驗，你會如何評價 </strong></span><span style="color:#245bdb"><strong>SSR</strong></span><span style="color:#245bdb"><strong> 這一模式呢？</strong></span></p><p><strong>劉勇：</strong>從前端的歷史演進看，是 SSR → CSR → SSR，粗一看似乎是在開歷史倒車，但實際不然。</p><p>舉個例子，當年前端的 HTML + CSS + JS 都是 all-in-one 的單文件方式，因為那時候前端沒有編譯能力只能寫在一起；隨着前端工程化的演進，開發期拆成多文件方式進行組織，構建時自動處理成為了主流；再進一步又出現了類似 Vue SFC 這樣的單文件方式，這是開倒車麼？其實不是，而是隨着基建的完善，用戶編程界面是可以更貼近直覺的，性能和部署之類的事交給工具去做即可。</p><p>因此，我認為 SSR 模式是有真實場景的，但在目前這個階段，我覺得它還有很多切實的性能問題和工程化問題需要解決才能更好的落地。</p><p><strong>劉奎：</strong>CSR 雖然也能獲得比較好的首屏體驗，但受限於用戶設備的機能，存在着明顯的性能天花板。而 SSR 則能更好的藉助邊緣計算（ESR）、流式渲染等服務端能力，有效的提升性能天花板，在大部分時候會是 Web 應用提升首屏性能的一個有效武器。</p><p>當然每個項目和團隊都有不同的特點和目標，在選擇開發模式時需要綜合考慮各種因素。</p><p>&nbsp;</p><p style="text-align:left">對此，你怎麼看？你的項目採取了 SSR 還是 CSR 呢？快來評論區説説你的體驗吧~</p></div></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 09:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6852546/blog/10136979</guid>
            <link>https://my.oschina.net/u/6852546/blog/10136979</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[推特年度工程總結，數據感人，什麼代碼減少 60 萬行、節省 1 億美元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>推特官方帳號發佈了<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2FXEng%2Fstatus%2F1717754398410240018" target="_blank">一年的工程總結</a>，亮點數據包括：</p><ul><li><p><span style="color:#000000"><span style="background-color:#ffffff">徹底重構 For you 服務和排名系統，代碼行數從 700K 減少到 70K，減少了 90%，計算佔用量減少了 50%，帖子吞吐量增加了 80%。</span></span></p></li><li><p><span style="color:#000000"><span style="background-color:#ffffff">重構了技術棧的 API 中間件層，並簡化了架構</span></span>，<span style="color:#000000"><span style="background-color:#ffffff">刪除超過 10 萬行代碼和數千個未使用的內部端，消除未採用的客戶端服務。</span></span></p></li><li><p><span style="color:#000000"><span style="background-color:#ffffff">關閉薩克拉門託數據中心並重新配置 5,200 個機架和 148,000 台服務器，每年節省超過 1 億美元。</span></span></p></li><li><p><span style="color:#000000"><span style="background-color:#ffffff">元數據獲取後延遲減少了 50%，全局 API 超時錯誤減少了 90%。</span></span></p></li><li><p><span style="color:#000000"><span style="background-color:#ffffff">優化了對雲服務提供商的使用，在本地進行更多工作，每月雲成本降低了 60%。具體是將所有媒體/blob 工件移出雲，使得整體雲數據存儲大小減少了 60%。同時，雲數據處理成本降低了 75%。</span></span></p></li><li><p><span style="color:#000000"><span style="background-color:#ffffff">構建本地 GPU 超級計算集羣，並設計、開發和交付 43.2Tbps 的新網絡結構架構以支持集羣。 </span></span></p></li><li><p><span style="color:#000000"><span style="background-color:#ffffff">擴展網絡主幹容量和冗餘，每年節省 1,390 萬美元。 </span></span></p></li></ul><hr><p><img src="https://static.oschina.net/uploads/space/2023/1027/161455_YSrE_2720166.png" referrerpolicy="no-referrer"></p><hr><p><strong>推薦閲讀</strong></p><ul><li><a href="https://www.oschina.net/news/231624/a-single-engineer-brought-down-twitter" target="_blank">一名工程師修改配置導致推特宕機，馬斯克迴應：要徹底重寫這堆 "ShitCode"</a></li><li><a href="https://www.oschina.net/news/259436/elon-musk-moved-twitter-servers-himself" target="news">馬斯克硬核遷移推特服務器</a></li><li><a href="https://www.oschina.net/news/247743/twitter-rate-limit" target="news">馬斯克稱 Twitter 數據被極端抓取，緊急上線「限流」機制</a></li><li><a href="https://www.oschina.net/news/228687/musk-twitter-recommend-algorithm" target="news">馬斯克連夜命令推特工程師修改算法</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 08:00:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263723/x-engineering-report</guid>
            <link>https://www.oschina.net/news/263723/x-engineering-report</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[全新的分佈式鎖，功能簡單且強大]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><blockquote><p>來源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftlnet.top%2Farticle%2F22425101" target="_blank">《全新的分佈式鎖，功能簡單且強大》</a></p><p>前言：分佈式鎖是分佈式系統中一個極為重要的工具。目前有多種分佈式鎖的設計方案，比如藉助 redis，mq，數據庫，zookeeper 等第三方服務系統來設計分佈式鎖。tldb 提供的分佈式鎖，主要是要簡化這個設計的過程，提供一個簡潔可靠，類似使用程序中對象鎖的方式來獲取分佈式鎖。</p></blockquote><p><strong>tldb 提供分佈式鎖使用方法：</strong></p><ol><li><p>lock 阻塞式請求鎖</p></li><li><p>trylock 嘗試加鎖，若鎖已被佔用，則失敗返回，反之，則獲取該鎖</p></li><li><p>unlock 釋放已經獲取的鎖</p><p>tldb 提供的分佈式鎖功能主要在 MQ 模塊中實現，調用的方法在 MQ 客戶端實現，客戶端的實現實際非常簡單，除了目前已經實現的幾種語言 java，golang，python，javaScript 寫的 simpleClient，其實其他開發者有興趣也可以實現其他語言的 MQ 客戶端，完全沒有技術門檻。分佈式鎖由 tldb 服務器控制，所以它相對客戶端來説，也是跨語言的，如，用 java 客戶端上鎖的對象，其他語言同樣無法獲取該對象鎖。</p></li></ol><hr><h4><strong>Lock(string,int) 方法的使用</strong></h4><p>tldb 提供的是以字符串為鎖對象的獨佔鎖， 如，lock("abc",3) 必須提供兩個參數：</p><ol><li>第一個參數為鎖對象，即服務器對「abc」對象分配一個鎖，所有對"abc"對象請求加鎖的線程爭用一個獨佔鎖，該方法為一個阻塞方法，請求到鎖則返回，如果鎖被其他線程佔用，則一直阻塞直至獲取到鎖。</li><li>第二個參數為持有該分佈式鎖的最長時間，單位為秒，例如 lock("abc",3)，意思是，如果超過 3 秒還沒有調用 unlock 釋放該鎖，服務器將強制釋放該鎖，繼續將鎖分配給其他請求的線程。</li></ol><hr><h4><strong>UnLock(string) 方法的使用</strong></h4><ul><li>UnLock 為釋放分佈式鎖時調用的方法。客戶端在成功獲取分佈式鎖後，服務器會返回一個該鎖的 key，客戶端執行完邏輯代碼的最後，必須顯式調用 UnLock(key) 來釋放該分佈式鎖。如果沒有調用 unlock 釋放鎖，tldb 將等待鎖釋放的超時時間直至超時後強制釋放該鎖。</li></ul><hr><h4><strong>TryLock(string,int) 方法的使用</strong></h4><ul><li>trylock 與 lock 相似，但是 lock 方法阻塞的，調用 lock 方法請求分佈式鎖時，如果該鎖已經被佔用，那麼 lock 方法將一直等待直至 tldb 服務器將鎖分配給它，這與程序中獲取獨佔鎖的方式一致。而 trylock 時非阻塞的，調用 trylock 後會立即返回，如果獲取到鎖，tldb 會將標識該鎖的 key 一併返回，如何該鎖已經被佔用，服務器將返回空數據。</li></ul><hr><p><strong>以下以 go 為例使用分佈式鎖</strong></p><p>因為 tldb 分佈式的實現是在 MQ 模塊，所以 go 程序必須使用 tlmq-go, tldb 的 mq 客戶端進行調用鎖方法。</p><pre><code>   import  "github.com/donnie4w/tlmq-go/cli"
</code></pre><p>調用 lock 的程序：lock 方法是阻塞的</p><pre><code>sc := cli.NewMqClient("ws://127.0.0.1:5001", "mymq=123")
sc.Connect()
//以上為，客戶端連接 MQ 服務器
key, err := sc.Lock("testlock", 3)
//lock 中兩個參數，第一個參數為字符串，即 tldb 服務器為「testlock」分配一個全局的分佈式鎖
//第二個參數 3 為客戶端持有該鎖的最長時間，表示超過 3 秒沒有釋放鎖時，tldb 服務器將在服務端強制釋放該鎖，並分配給其他請求鎖的線程
if err!=nil{
    //獲取鎖失敗，需查看 tldb 能正常訪問
}else{
    defer sc.UnLock(key) //獲取鎖成功後，必須在程序最後調用 Unlock
    //執行業務邏輯程序
}
</code></pre><p><strong>調用 tryLock 的程序，trylock 是非阻塞的</strong></p><pre><code>sc := cli.NewMqClient("ws://127.0.0.1:5001", "mymq=123")
sc.Connect()

if key, ok := sc.TryLock("testlock2", 3); ok {
    //ok 為 true，表示已經成功獲取到分佈式鎖
    defer sc.UnLock(key) //在程序最後釋放鎖對象
    ...        
}
</code></pre><p><strong>go 用自旋的方式使用 trylock 獲取分佈式鎖，實現程序的阻塞等待</strong></p><pre><code>sc := cli.NewMqClient("ws://127.0.0.1:5001", "mymq=123")
sc.Connect()
var key string
for {
if  v, ok := sc.TryLock("testlock", 3); ok {
    key = v
break
} else {
&lt;-time.After(100* time.Millisecond)
}
}
defer sc.UnLock(key)
...//業務邏輯代碼
</code></pre><p>這段程序應該比較易於理解，就是每隔 100 毫秒，循環獲取字符串「testlock」的分佈式鎖直至成功。</p><hr><p><strong>以下以 java 為例</strong> java 客戶端為 tlmq-j ：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdonnie4w%2Ftlmq-j" target="_blank">https://github.com/donnie4w/tlmq-j</a></p><p>maven 配置</p><pre><code>&lt;dependency&gt;        
   &lt;groupId&gt;io.github.donnie4w&lt;/groupId&gt;      
   &lt;artifactId&gt;tlmq-j&lt;/artifactId&gt;     
   &lt;version&gt;0.0.2&lt;/version&gt;   
&lt;/dependency&gt;
</code></pre><p>調用 lock 方法</p><pre><code>MqClient mc = new SimpleClient("ws://127.0.0.1:5001", "mymq=123");
mc.connect();
//java 連接服務器
String key = null;
try{
      key = mc.lock("testlock", 3); //獲取分佈式
      ... //執行業務邏輯程序
}finally {
     if (key!=null){
         mc.unLock(key); //釋放分佈式鎖
     }
}
</code></pre><p>調用 trylock 方法</p><pre><code>MqClient mc = new SimpleClient("ws://127.0.0.1:5001", "mymq=123");
mc.connect();
String key = null;
try{
      key = mc.tryLock("testlock", 3); //獲取分佈式
      ... //執行業務邏輯程序
} finally {
     if (key!=null){
         mc.unLock(key); //釋放分佈式鎖
     }               
}
</code></pre><p><strong>以下是 tldb 分佈式鎖的功能測試數據：</strong><strong>多線程併發，調用 lock 獲取同一個對象鎖後，程序的運行數據：</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-09276d085e420b382074c69dade4cd6372b.jpg" alt="" referrerpolicy="no-referrer"></p><p><strong>多線程併發使用自旋的方式調用 trylock 與 lock 獲取同一個對象鎖：</strong><img src="https://oscimg.oschina.net/oscnet/up-cb14a249e167dfb0eca3c06850a6017f182.jpg" alt="" referrerpolicy="no-referrer"></p><hr><p><a href="https://www.oschina.net/action/GoToLink?url=mailto%3A%E6%9C%89%E4%BB%BB%E4%BD%95%E9%97%AE%E9%A2%98%E6%88%96%E5%BB%BA%E8%AE%AE%E8%AF%B7Email%EF%BC%9Adonnie4w%40gmail.com%E6%88%96" target="_blank">有任何問題或建議請 Email：donnie4w@gmail.com 或</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftlnet.top%2Fcontact" target="_blank">https://tlnet.top/contact</a> 發信給我，謝謝！</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 07:38:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/donnie4w/blog/10114233</guid>
            <link>https://my.oschina.net/donnie4w/blog/10114233</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[libnop - C++ 本機對象協議]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="project_detail_above_text_link_1" data-tracepid="project_detail_above_text_link"><a style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代 <img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>libnop 是一個僅用於序列化和反序列化 C++數據類型的頭庫，無需外部代碼生成器或運行時支持庫。唯一的強制性要求是一個支持 C++14 標準的編譯器。</p><hr><p style="color:#1f2328; text-align:start"><strong>libnop 有以下目標：</strong></p><ul><li>使簡單的序列化任務變得容易，使複雜的任務變得易於處理。</li><li>在 C++語言中移除對代碼生成器和模式文件描述數據類型、格式和協議的依賴。</li><li>避免運行序列化操作時可能需要的額外運行時間。</li><li>提供現代功能，如雙向二進制兼容性、數據驗證、類型安全性和類型可替代性。</li><li>以最少的工作量處理內部類型、常見的 STL 類型和容器以及用戶定義的類型。</li><li>生成易於分析的代碼。</li><li>避免動態內存的分配時使用。</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 07:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/libnop</guid>
            <link>https://www.oschina.net/p/libnop</link>
        </item>
        <item>
            <title>
                <![CDATA[微軟已擁有超過 100 萬付費 Copilot 用戶]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="color:#000000">微軟 CEO 薩提亞·納德拉 (Satya Nadella) 日前透露，該公司 GitHub Copilot 軟件的付費用戶已超過 100 萬。</span></p><p><span style="color:#000000">「藉助 GitHub Copilot，我們將開發人員的工作效率提高了 55%，我們擁有超過 100 萬付費 Copilot 用戶。此外，已有超過 37,000 個組織訂閲了 Copilot for Business，環比增長 40%。本季度，我們通過 GitHub Copilot Chat 添加了新功能，Shopify 等數字原生企業以及馬士基和普華永道等領先企業已在使用這些功能，以提高其軟件開發人員的生產力。」</span></p><p><img height="264" src="https://oscimg.oschina.net/oscnet/up-1fbc03d629d9cf27183ca8ebfb858414029.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#000000">納德拉表示，Copilot 是其在公司產品線中推廣 AI 的眾多方式之一。該公司的必應搜索引擎已經與 OpenAI 的 ChatGPT 集成，迄今為止用戶參與的聊天數量已「超過 19 億次」。</span></p><p><span style="color:#000000">他指出，Microsoft Edge 瀏覽器的市場份額現已連續 10 個季度增長。「本季度，我們推出了新的個性化答案以及對 DALL-E 3 的支持，幫助人們獲得更相關的答案並創建極其逼真的圖像。迄今為止，我們已創建了超過 18 億張圖像。藉助 Copilot 在購物方面的應用，大家可以獲取更多量身定製的推薦以及達成更好的交易。」</span></p><p><span style="color:#000000">目前，微軟已經向初創公司 OpenAI 投資超過 100 億美元；華爾街預計微軟將從與 OpenAI 的合作中獲得巨大的財務回報。一些華爾街分析師估計，這種合作關係有一天可能會給微軟帶來 1000 億美元的價值。</span></p><p><span style="color:#000000">儘管納德拉沒有量化 GitHub Copilot 的收入，但微軟 CFO Amy Hood 表示，「高於預期的 AI 消費推動了 Azure 的收入增長」。</span></p><p><span style="color:#000000">該公司智能雲計算業務本季度的總收入超出了分析師的預期 (234 億美元)，同比增長 19%，達到 243 億美元。Hood 表示，其中 Azure 業務增長了 29%，有 3 個百分點來自「AI 服務」。</span></p><p><span style="color:#000000">此外，微軟方面還將 Copilot 引入了 Power Platform，使任何人都可以使用自然語言來創建應用程序，構建虛擬代理和分析數據。納德拉表示：「包括 3M、Equinor、Lumen Technologies、Nationwide、PG&amp;E 和豐田在內的超過 126,000 家組織都使用了 Copilot 和 Power Platform。」</span></p><p><span style="color:#000000">該公司還正在將生成式 AI 添加到其 LinkedIn 業務中。「我們發現本季度在 LinkedIn 上觀看 AI 相關學習課程的會員數量增加了近 80%」。以及將 Copilot 擴展到醫療保健等各行各業。目前，該公司正在將臨牀工具集成到 Azure 上的 Fabric 數據服務中。</span></p><p><span style="color:#000000">納德拉稱，GitHub Copilot 和其他產品的推出還處於早期階段。「我們正處於非常非常早期的階段，因此我們期待看到這些產品在未來的發展。Copilot 的早期發展給了我們很大的信心，更重要的是，讓我們的客戶對這些產品所代表的價值充滿了信心。」</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 07:33:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263710</guid>
            <link>https://www.oschina.net/news/263710</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[國家安全部：警惕一些境外 SDK 背後的「數據間諜」竊密]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><blockquote><p>本文轉載自 <u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fxq_0nAxzuZ4t0HLXLy8BEg" target="_blank"><strong>國家安全部微信公眾號</strong></a></u></p></blockquote><p>你知道 SDK 是什麼嗎？SDK 是英文 Software Development Kit 的縮寫，即軟件開發工具包，它的類型多種多樣。如果把開發一個軟件系統比作蓋一所「三室一廳」的房子，那麼不同的 SDK 就是這套房子的「客廳」「臥室」「衞生間」「廚房」等功能模塊。蓋好這套房子，我們只需要從不同的供應商那裏選擇這個功能模塊拼裝即可，而不再需要從「砌磚」「壘牆」做起，從而極大提高了軟件開發的效率。</p><p>近年來，國家安全機關工作發現，境外一些別有用心的組織和人員，正在通過 SDK 蒐集我用戶數據和個人信息，給我國家安全造成了一定風險隱患。</p><h4><span style="color:#ffffff"><strong><span style="background-color:#2980b9">SDK 帶來哪些數據安全問題？</span></strong></span></h4><p>當前，SDK 以其多樣化、易用性和靈活性等優勢成為移動供應產業鏈中最重要的一項服務，與此同時也帶來諸多數據安全問題。</p><ul><li><strong>過度收集用戶數據</strong></li></ul><p>有些 SDK 會收集與提供服務無關的個人信息，或強制申請非必要的使用權限，比如獲取地理位置、通話記錄、相冊照片等信息以及拍照、錄音等功能。當 SDK 的用戶覆蓋量達到一定規模時，可以通過蒐集的大量數據，對不同用戶羣體進行畫像側寫，從而分析出潛在的有用信息，比如同事關係、單位位置、行為習慣等。</p><p>一些境外 SDK 服務商，通過向開發者提供免費服務，甚至向開發者付費等方式來獲取數據。據相關網站披露，一款在美國擁有 5 萬日活躍用戶的應用程序，其開發者通過使用某 SDK，每月可以獲得 1500 美元的收入。作為回報，該 SDK 服務商可以從這款應用程序中收集用戶的位置數據。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-28cd143918cb6491c1493e0dcedaa1e2bd3.png" referrerpolicy="no-referrer"></p><p><em>SDK 蒐集個人信息類型</em></p><ul><li><strong>境外情報機構將 SDK 作為蒐集數據的重要渠道</strong></li></ul><p>據報道，美國特種作戰司令部曾向美國 SDK 服務商 Anomaly Six 購置了「商業遙測數據源」的訪問服務，而該服務商曾自稱將 SDK 軟件植入全球超過 500 款應用中，可以監控全球大約 30 億部手機的位置信息。</p><p>2022 年 4 月，有關媒體曝光巴拿馬一家公司通過向世界各地的應用程序開發人員付費的方式，將其 SDK 代碼整合到應用程序中，祕密地從數百萬台移動設備上收集數據，而該公司與為美國情報機構提供網絡情報蒐集等服務的國防承包商關係密切。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-b4bc6599b4e81d3e9a7883925c9f4c2c397.png" referrerpolicy="no-referrer"></p><p><em>《華爾街日報》：美國政府承包商在多個手機 APP 中嵌入跟蹤軟件</em></p><h4><span style="color:#ffffff"><strong><span style="background-color:#2980b9">消除 SDK 背後的數據風險</span></strong><strong><span style="background-color:#2980b9">我們應該怎麼做？</span></strong></span></h4><p>據國內權威機構掌握，截至 2022 年 12 月，我國 10 萬個頭部應用中，共檢測出 2.3 萬餘例樣本使用境外 SDK，使用境外 SDK 應用的境內終端約有 3.8 億台。對此，我們又應該做些什麼呢？</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-85cdd9629fe616749844499bf44f168c4aa.png" referrerpolicy="no-referrer"></p><p><em>SDK 申請收集用戶信息佔比</em></p><ul><li><strong>應用程序開發企業</strong>：應儘量選擇接入經過備案認證的 SDK，引入境外 SDK 前應做好安全檢測和風險評估，深入瞭解 SDK 的隱私政策，並利用 SDK demo 以及 APP 測試環境對 SDK 聲明內容進行一致性比對，並持續監測 SDK 是否有異常行為。</li><li><strong>個人用戶</strong>：個人用戶在使用手機應用程序時，要增強個人信息保護意識及安全使用技能，要選擇安全可靠的渠道下載使用應用程序，不安裝來路不明的應用，不盲目通過敏感權限的申請。特別是發現 SDK 申請與應用功能無關的權限時，需要保持高度警惕。</li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 07:17:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263704</guid>
            <link>https://www.oschina.net/news/263704</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[《2023 年三季度互聯網投融資運行情況》研究報告發布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">中國信息通信研究院政策與經濟研究所互聯網運行分析團隊於日前，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FxZbTT8DZ0Q79-2RGTXhFbg" target="_blank">發佈</a>了《2023 年三季度互聯網投融資運行情況》報告。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">報告構建了互聯網行業投融資研究框架，藉助 CB Insights 數據庫，深入挖掘我國和全球行業投融資整體態勢及重點領域情況，為行業趨勢預測、熱點問題預判提供重要參考。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">本季要點：</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><strong>1.&nbsp;</strong><strong>我國互聯網投融資略有反彈。</strong>2023Q3，我國互聯網投融資規模企穩，案例數環比下跌 5.8%，同比下跌 54%；披露的金額環比上漲 34.7%，同比下跌 36.4%。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><strong>2.<span>&nbsp;</span></strong><strong>全球互聯網投融資繼續下探。</strong>2023Q3，全球互聯網投融資案例數環比下跌 5.1%，同比下跌 23.4%；披露的金額環比下跌 15.5%，同比下跌 28.8%。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span><strong>3.</strong><strong>企業服務融資佔比保持領先。</strong>2023Q3，我國企業服務領域融資案例數佔比 25.7%，融資金額佔比 26%；全球企業服務領域融資案例數佔比 21.3%，融資金額佔比 19.8%。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>報告全文：</span></p><p><img height="3587" src="https://oscimg.oschina.net/oscnet/up-ee666d0691d7989a2ff66c9cfbe80f422d2.png" width="500" referrerpolicy="no-referrer"></p><p><img height="2012" src="https://oscimg.oschina.net/oscnet/up-2d7d99c36f9f1ebf6569235631c9a3771cd.png" width="500" referrerpolicy="no-referrer"></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.caict.ac.cn%2Fkxyj%2Fqwfb%2Fqwsj%2F202310%2FP020231027488865727077.pdf" target="_blank">報告全文下載</a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 06:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263690</guid>
            <link>https://www.oschina.net/news/263690</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Windows CE 徹底退役]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>誕生於 1996 年的 Windows 嵌入式操作系統 —— Windows CE (Windows Embedded Compact) 本月迎來了它的生命週期終點。</p><p>Windows CE 最初是 Windows 的精簡版本，之後逐漸發展成為全新的操作系統，它有自己的 CE 內核，而不是傳統 Windows 操作系統使用的 NT 內核。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-8f640a89f188c88148ea17ea44e6c74b915.png" referrerpolicy="no-referrer"></p><p>它的最後一個版本是 2013 年 8 月 11 日發佈的 Windows Embedded Compact 2013（或者叫 Windows CE 8.0），該版本於 2018 年 10 月結束主流支持 (Mainstream Support)，<strong>2023 年 10 月結束延長支持 (Extended Support)</strong>，成為不受支持的退役產品。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-bea6f86fc521c769ddd160731ae640284b0.png" referrerpolicy="no-referrer"></p><p>然而很少有人注意到 Windows CE 的生命週期已結束。作為一個產品，CE 8.0 的發佈未能獲得太多關注或宣傳。即使在 Windows CE 社區中，大多數人也認為 Windows Embedded Compact 2013 完全不適合使用。與之前的 Windows CE 7.0 一樣，基於 CE 8.0 平台發佈的設備很少，因此大多數人甚至從未見過 CE 8 的實體設備。</p><p>CE 8 唯一值得注意的特性是首次加入了 IPv6 支持，但更重要的是刪除了默認示例用戶界面，它規定任何 OEM 都必須發佈帶有完全自定義編碼界面的設備，然而很少有供應商願意這樣做，此舉事實上真正結束了廉價 Windows CE 上網本設備的時代。</p><blockquote><p>事實上微軟並未介紹過 CE 縮寫的由來，一般解釋則有 Compact Edition、Customer Embedded、Consumer Electronics 等等。</p><p>在 2008 年 4 月 15 日舉行的嵌入式系統大會上，微軟宣佈將 Windows CE 更名為 Windows Embedded Compact，與 Windows Embedded Enterprise、Windows Embedded Standard 和 Windows Embedded POSReady 組成 Windows Embedded 系列產品。</p></blockquote><p>===彩蛋分割線===</p><p>曾被稱為「國產機皇」的魅族 M8，其運行的操作系統正是基於 Windows CE 開發。</p><p><img alt="" height="533" src="https://oscimg.oschina.net/oscnet/up-786c41866ad15ca06fc5d0a683e8366c459.png" width="400" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 06:22:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263685/end-of-an-era-windows-ces-final-day</guid>
            <link>https://www.oschina.net/news/263685/end-of-an-era-windows-ces-final-day</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Bytebase 2.10.0 - 支持更靈活的變更發佈人]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><h2>🚀 新功能</h2><ul><li>發佈策略支持制定更靈活的變更發佈人：可以指定任意角色集合，也可以指定自定義審批流的最後一個審批人。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-232e6912ae4dfccc575951fdcfb6182eaae.png" alt="file" referrerpolicy="no-referrer"></p><ul><li>支持在項目中創建分支保護規則。</li><li>支持給數據庫設置標籤。</li><li>支持給字段設置標籤。</li><li>支持給表設置分類分級。</li></ul><h2>🎄 改進</h2><ul><li><p>支持 PostgreSQL 16。</p></li><li><p>SQL Editor：支持自定義數據庫樹的視圖。</p></li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-07ab1d1c2b98175941d8e28433fe617b896.png" alt="file" referrerpolicy="no-referrer"></p><ul><li>SQL Editor：允許提前終止查詢。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-80f111990561ce0c665fe4cce51db4fcb3a.png" alt="file" referrerpolicy="no-referrer"></p><ul><li>支持從指定分支中創建子分支。</li><li>支持在分支合併時選擇其他目標分支。</li><li>支持在工單中使用 SQL Server 局部變量。</li><li>基於 Parser 為 Postgres, MySQL, Oracle, SQL Server, Snowflake 查詢語句增加 LIMIT 子句。</li><li>表詳情頁展示「類型」的最大長度（如有）。</li></ul><h2>🐞 Bug 修復</h2><ul><li>修復分支和變更 Schema 中的列默認值問題。</li></ul><h2>📕 安裝及升級</h2><p>參考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fbytebase%2Fbytebase%23installation" target="_blank">升級指南</a>。如果從之前版本升級，獲取新版本後，重新啓動升級即可。</p><hr><p>💡 更多資訊，請關注 Bytebase 公號：Bytebase</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 06:21:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6148470/blog/10136864</guid>
            <link>https://my.oschina.net/u/6148470/blog/10136864</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[小米 14 開機動畫顯示澎湃 OS 基於 Android]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>小米 14、澎湃 OS 等一大波新品已經正式登場<strong>。澎湃 OS 發佈前，不少人都爭論，它不是小米自研的系統，對此雷軍還特意表示，確實不是。</strong></p><p>小米的澎湃 OS 由兩部分組成：一部分是基於安卓系統進行深度進化的，這使得澎湃 OS 可以與安卓系統保持同步，並且能夠使用安卓軟件。</p><p>另一部分則是小米自研發的 Vela 系統，主要用於實現小米產品之間的互聯互通。</p><p>這種系統架構使得澎湃 OS 能夠兼顧兼容性和自主性，既滿足了用戶對豐富應用的需求，又能夠提供更好的硬件軟件一體化體驗。</p><p><strong>發佈會後，有網友從現場展示的新機看到，小米澎湃 OS 開機頁面動畫還是和以前一樣，也有顯示 "Powered by android"，這也算是證實了雷軍之前的説法。</strong></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-4b428f25973338129e02686c3a80ff9faf1.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 05:17:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263829</guid>
            <link>https://www.oschina.net/news/263829</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[崑崙萬維 Q3 報告：實現經營性現金流 7.6 億]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="color:#000000">崑崙萬維發佈了 2023 年第三季度報告。據報告披露，崑崙萬維第三季度<strong>全面加速推進</strong>「All in」 AGI 與 AIGC 的戰略佈局，在多個方向均取得了突破性進展。截至本報告期末，崑崙萬維實現營業收入<strong>36.8 億</strong>元，同比增長 8%。實現經營性現金流<strong>7.6 億</strong>元，同比增長<strong>33%</strong>。</span></p><p><span style="color:#000000">今年前三季度，崑崙萬維海外業務收入佔比進一步提升至 84%，同比增加近 9 個百分點；整體毛利率達 80%，繼續保持在較高水平；實現歸屬於上市公司股東的淨利潤 3.3 億元，<strong>穩居行業第一梯隊</strong>。崑崙萬維第三季度實現經營性淨利潤 1.0 億元，環比增長 29%。</span></p><p><span style="color:#000000">為全面落實「All in」AGI 與 AIGC 的戰略佈局，崑崙萬維前三季度研發費用提升至 6.2 億元，加速推進相關業務發展。此外，9 月 21 日，崑崙萬維發佈公告，公司控股股東、實際控制人周亞輝先生及一致行動人盈瑞世紀承諾未來三年（從 2023 年 9 月 22 日到 2026 年 9 月 21 日）不以任何形式減持所持有的公司股票，包括承諾期間該部分股份因資本公積轉增、派送股票紅利、配股、增發等事項產生的新增股份，長期助力崑崙萬維成為一家全球領先的人工智能科技企業。</span></p><p><img height="282" src="https://static.oschina.net/uploads/space/2023/1027/113236_MDTf_4252687.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">報告稱，在國內 AGI 與 AIGC 領域，其「天工」大模型在邏輯推理、文本理解、多模態能力等多個方面均取得了顯著突破。基於大模型核心能力的提升，「天工」APP 進行了全面迭代升級，整合了 AI 搜索、AI 閲讀、AI 創作等核心功能，覆蓋了<strong>工作、學習和生活</strong>等多個應用場景。</span></p><p><span style="color:#000000">AI 搜索功能經過升級，提高了信息獲取的精準度和效率。AI 閲讀功能高效分析文章鏈接或文檔文件，生成 AI 摘要並提煉要點，幫助用戶快速瞭解文章內容。同時，支持問答式交互，使用戶能夠更便捷地查詢文檔信息。AI 創作提供輕鬆高效的創作體驗，滿足<strong>學術教育、職場文檔、創意寫作、廣告營銷</strong>等不同場景需求。</span></p><p><span style="color:#000000">作為崑崙萬維 AI 業務矩陣之一的 AI 遊戲也取得了重要進展。崑崙萬維旗下 Play for Fun 遊戲工作室自研的首款 AI 遊戲《Club Koala》於 8 月 25 日在德國科隆國際遊戲展亮相。Club Koala 引入了 AI NPC，並通過 Atom 系統控制 NPC 行為，使 AI NPC 擁有自我意識及記憶能力，為玩家提供更真實、更具沉浸感的遊戲體驗。</span></p><p><span style="color:#000000">海外佈局方面，崑崙萬維依託旗下信息分發及元宇宙業務 Opera 原生瀏覽器 AI 助手 Aria，推出了一系列前沿 AI 功能。</span></p><p><span style="color:#000000"><img alt="" height="236" src="https://oscimg.oschina.net/oscnet/up-b99565529a38b20bc30538e26fb9fe3d15d.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000"><em>Aria「重用」與「改述」功能</em></span></p><p><span style="color:#000000"><em><img alt="" height="238" src="https://oscimg.oschina.net/oscnet/up-00dfeb44b4d900099bac85a6a6d12e867cc.png" width="500" referrerpolicy="no-referrer"></em></span></p><p><span style="color:#000000"><em>Aria「Compose」與「My Style」功能</em></span></p><p><span style="color:#000000">Aria 已在包括歐盟在內的<strong>180</strong>多個國家和地區上線，用戶突破<strong>百萬</strong>。此外，Aria 已覆蓋 Mac、Windows、Linux、Android 和 iOS 等所有主要平台，以及公司元宇宙入口 Opera GX。</span></p><p><span style="color:#000000">截至本季度末，崑崙萬維此前採購及租賃芯片已到貨<strong>約 6000 張</strong>，另外還有約<strong>3000 張</strong>芯片待交付。目前崑崙萬維已有算力預計能夠滿足未來 1~2 年除視頻 AIGC 之外的大模型算力需求。</span></p><p><span style="color:#000000">報告期內，崑崙萬維宣佈通過增資方式控股 AI 算力芯片企業北京艾捷科芯科技有限公司（簡稱「艾捷科芯」），完成「算力基礎設施—大模型算法—AI 應用」全產業鏈佈局。艾捷科芯旨在開發一款可編程的、具有高性能的 NPU 產品，同時應用於模型訓練及推理。</span></p><p><span style="color:#000000">此外，崑崙萬維在「華為全聯接大會 2023」舉辦期間，發佈「天工大模型端雲一體化方案」。該方案具備開箱即用、定製調優、服務保障三大優勢，企業可以自主地訓練模型，也可以<strong>基於天工模型定製</strong>，實現從應用場景真實需求出發，賦能業務發展並提升競爭力。</span></p><p><span style="color:#000000">「崑崙萬維 2023 年第三季度報告展示了公司在 AGI 及 AIGC 領域的突飛猛進。隨着技術的不斷創新和業務的持續擴張，以及在全產業鏈佈局的驅動下，崑崙萬維有望加速成為全球領先的人工智能龍頭企業。」</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 03:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263657</guid>
            <link>https://www.oschina.net/news/263657</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[國外物價高，6 美元只能買 50 個 GitHub stars]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>《Wired》雜誌發表文章<em>"<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wired.com%2Fstory%2Fgithub-stars-black-market-coders-cheat%2F" target="_blank">The GitHub Black Market That Helps Coders Cheat the Popularity Contest</a></u>"</em>，介紹了交易 GitHub Stars 的地下黑市。</p><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1028/161939_TSqR_2720166.png" referrerpolicy="no-referrer"></p></blockquote><p>GitHub 平台託管項目的受歡迎程度能夠為部分程序員和創業公司打開一扇大門，他們通過 Stars 獲得關注度、影響力和聲譽。然而地下黑市出售的 Stars 提供了」以假亂真「的方式來讓他們進行作弊。這些虛假 Stars 在某種程度上能幫助程序員和創業公司在聯絡投資人或找工作時留下好印象。</p><p>據介紹，在交易 GitHub Stars 的平台上，支付價值&nbsp;6 美元的以太幣即可購買 50 個 Stars。除了 Stars，其他可量化的指標——如 Forks、Watchers 和 Follower 也可單獨或組合購買。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img src="https://static.oschina.net/uploads/space/2023/1024/174616_4UDX_2720166.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><em>via<span>&nbsp;</span><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaddhi.shop%2Fproduct%2Fbuy-github-followers%2F" target="_blank">https://baddhi.shop/product/buy-github-followers/</a></u></em></p><p>文章寫道，此前初創公司、程序員和投資者在決定僱用誰、為誰工作或投資誰時，會使用這些指標來篩選有潛力的程序員和初創公司。</p><p>但真正決定成功的不僅是這些指標，投資者開始意識到這種評估方式並不可靠，正在改變對 GitHub Stars 等指標的依賴，GitHub 平台也在打擊這些專門用於刷數據的虛假賬號。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 10:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263862/github-stars-black-market-coders-cheat</guid>
            <link>https://www.oschina.net/news/263862/github-stars-black-market-coders-cheat</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2023 CCF 中國開源大會開源商業化分論壇順利召開]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>10 月 21 日至 22 日，由中國計算機學會（CCF）、開放原子開源基金會主辦的 2023 CCF 中國開源大會在長沙順利舉行。其中，開源商業化分論壇由開源中國承辦，開源中國董事長馬越擔任主席。來自開源原生商業公司的諸多專家就開源項目商業化最佳實踐展開分享，為更多開發者和企業提供可借鑑的經驗，共同推動開源生態建設，助力開源生態發展。</p><p><strong>開源中國董事長馬越</strong>以《中國開源商業發展的現狀及思考》為題發表主旨演講。他指出，當前開源創業公司有「七大恨」：沒有品牌、沒有流量、沒有銷售能力、沒有資質、沒有交付能力、沒有現金流、沒有資本渠道。這些都嚴重阻礙了創業公司進一步發展壯大。而解決開源創業公司「七大恨」的關鍵就在於開源創業聯合體。所謂開源創業聯合體，就是提供通用服務模型的價值流平台，實現集成和自動化 IT 價值鏈的插件開放平台，融合市場各類開源或商業生態能力，落地客戶場景服務。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-729f0faa336b257003127266ac12c4b722d.png" width="800" referrerpolicy="no-referrer"></p><p>最後馬越提議，希望能夠集眾多開源力量共建這樣的插件開放平台，繁榮開源商業生態，互通有無。開源中國已積累了十幾年的商業化經驗以及商業化能力，旗下 Gitee 平台也已經入駐了 27 萬多家中小團隊，服務中國 600 多家 100 億元估值以上的大企業。未來，開源中國將通過該插件平台將這些經驗和能力賦能更多開源企業。</p><p><strong>CCF 開源發展委員會常委譚中意</strong>就 「AI to B 的開源和商業化」這兩大方向展開探討。譚中意認為，當前 LLM to B 業務的難點在於，需要找到一個 Killer 場景——有足夠的商業回報，能覆蓋大模型 Finetune 和 Serving 以及 LLM 應用開發和運維的成本。但是 LLM 技術存在先天上的約束：一是無法避免幻覺的問題，To C 業務需要滿足網信辦規定，合規成本很高；二是無法避免概率的問題，To B 的嚴肅場景可能不太適合。因此，突破口可能在於：一是企業內部，對生成內容更講究創意或者實時 Check 的場景，比如內部研發代碼生成工具、遊戲行業的場景；二是電商領域，比如促銷、廣告投放等，因為這是離錢最近的潛力市場，且容易形成數據閉環。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-cf8531b4308c6c6399ab200699cb0ef9b7d.png" width="800" referrerpolicy="no-referrer"></p><p>至於開源在 LLM 產業的關鍵作用，主要有兩個：一是降低成本，開源是降低整個產業創新成本的關鍵，即 AI 民主化。這需要整個學術界和產業界一起努力，來把成本降下來，而且開源底座模型是整個大生態最重要部分；二是建立信任，人工智能要讓人信任，它必須公開透明。一旦這個問題解決了，一個十萬億規模的市場可能起飛。</p><p><strong>PingCAP 副總裁劉松</strong>分享了 TiDB 從開源到 Serverless 的商業化演進邏輯。據其介紹，PingCAP 的商業化之路可以總結為「四部曲」：創建一個滿足時代剛需的開源項目，開源產品獲得規模化的用戶部署與反饋，打造一個全球化的商業化模式，持續創造滿足極致用戶需求的產品形態。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-c12ba68303c5e846f95912614c315de728c.png" width="800" referrerpolicy="no-referrer"></p><p>劉松表示，面對「開源 + 雲」互相推動和演進的新技術環境，TiDB 演進方向從技術領先走向了「技術+體驗」領先。當前，PingCAP 圍繞 TiDB 構建了三大產品形態：TiDB 企業版、TiDB Cloud（全託管）、TiDB Cloud Serverless。其中，TiDB Cloud 提供全託管的 DBaaS （Database-as-a-Service）服務，極大地降低了雲數據庫的使用門檻；TiDB Cloud Serverless 基於雲原生/多雲的設計，採用 AI-Ready 的架構，實現極致低成本、極致彈性，擁有自動化的資源調度能力以及靈活集成 AI 能力等特性。</p><p><strong>統信軟件解決方案中心專家任紫東</strong>以《中國開源操作系統商業發展探索》為題展開分享。任紫東認為，2020 年是國產 Linux 里程碑之年。2019 年前，我國有 10 多家國產操作系統企業，隨着政策引導和市場競爭戰略的選擇，國產操作系統廠商進行了全新的業態整合，2020 年起，初步形成兩家主流國產操作系統企業，統信就是其中之一。及至 2022 年，產業鏈的商業形態形成。頭部操作系統公司規模也呈現「幹百十」特徵：千人以上的操作系統開發隊伍，百人以上的內核研發團隊，十人以上的開源合規律師團隊。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-d9d35f21d4f87673628b2337118d6fb42dc.png" width="800" referrerpolicy="no-referrer"></p><p>當前，統信 UOS 生態是國內最大的自主操作系統生態圈之一。統信己基於服務器操作系統完成諸多主流國外商業軟件的適配，涵蓋了 Oracle、IBM、SAP、微軟等廠商的主流數據庫產品，Google、FaceBook、百度、華為等廠商的主流 AI 人工智能類產品，以及 Oracle、IBM 等廠商的主流中間件產品。</p><p>開源社區做得好，怎麼變成錢？在以《白鯨 DataOps 開源矩陣商業化之路》為題的演講中，<strong>白鯨開源 CEO 郭煒</strong>提到，白鯨花了 8 個月時間，摸索了一套開源商業轉化流程，積累數千萬的商業 Pipeline 以及上百個線索，而投入資源不過是一名銷售人員，沒有任何市場費用。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-c3550e7f84e4a38252e13b77a70f32bcd94.png" width="800" referrerpolicy="no-referrer"></p><p>郭煒把這些成果歸結於幾大原則：「別人做中石油，我做中石化」，即做所有人的朋友；開源項目定位要清晰，商業功能痛點要明確，因此白鯨開源採取了「開源矩陣+OpenCore」的路線；開源商業軟件要重視「行業屬性」，分行業洞察痛點，口碑營銷；不忘初心，牢記使命，不斷升級開源版，商業版才有機會；勇於探索，擁抱新技術，將大模型融入軟件，等等。</p><p>最後他提到，開源風口並沒有過去，而且溫度剛剛好。面對經濟下行週期、資本趨於冷靜以及收入體量要求更高等挑戰，也應看到行業展現出來更多的機會，比如惡性競爭減少，互聯網公司開始付費買工具，訂閲制更容易被接受，海外市場逐步增長等等。</p><p><strong>TDengine 聯合創始人&amp; 商業化 VP 李廣</strong>分享了 TDengine 是如何從開源時序數據庫到工業大數據處理平台的。他表示，一款軟件開源，就意味着可信、可控。TDengine 的商業邏輯就是重塑 2B 銷售模式，以開源建社區與品牌，以開源建 GTM 路徑。通過開源擴大影響力，樹立品牌，形成開發者社區，構建競爭壁壘，快速獲得市場反饋，快速迭代，快速打造生態，獲得用戶信任。另一方面，將傳統的 2B 銷售演變為 2C 的模式，將傳統的登門拜訪演變為線上銷售，將資源型銷售轉化為技術和產品型銷售。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-a48a9ecbd7606788031401fba0b8a83267d.png" width="800" referrerpolicy="no-referrer"></p><p>當前，TDengine 提供三種產品與服務，一是開源社區版——TDengine OSS，主要是為了建立開發者社區，建立生態；二是企業版——TDengine Enterprise，支持獨立部署並按照 TBL（Term Based License）年度服務訂閲或者永久 Licenese 模式銷售；三是雲服務版——TDengine Cloud，在阿里雲、AWS、華為雲等雲平台上直接提供 SaaS 服務，根據數據量和時長計費。</p><p>最後他表示，開源軟件的商業化邏輯已經發生了變化，從關注增長轉向強調利潤。2B 軟件羣龍紛爭的時代結束，只有深入行業黑土地才能生存。</p><p><strong>築棧（KodeRover / Zadig）創始人 &amp; CEO 李倩</strong>分析了公司為什麼選擇深耕中國而不是出海。據悉，在商業化過程中，KodeRover 也面臨過不少問題，比如花了時間打磨產品，但用戶付費意願不強烈；有需求有預算的大客戶找上門，卻因為自身團隊規模過小無法為其提供大型服務等等，不過 KodeRover 最終也地制定了相應策略。李倩將公司的商業化思路總結為：技術上用開源鑄造好基建，打造產品力、品牌力；商業上，中國場景助力「新 IT」 （比如硬科技創新、舊行業升級重整）升級，創造客戶價值；可持續創造價值，廣泛鏈接，為客戶提供最優質的解決方案。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-e0510f159c4ed52d504290463b06b22c059.png" width="800" referrerpolicy="no-referrer"></p><p>李倩提到，Zadig 是生產軟件的軟件，目的是交付數字業務。 Zadig 開源兩年，企業安裝總量近 3 萬，目前是國內雲原生 DevOps 領域落地最廣泛的平台，成為包括字節飛書、極氪、路特斯、小鵬、七牛雲、WiFi 萬能鑰匙、易快報、iMile、TT 語音、鍋圈、藥師幫、大參林、老百姓大藥房、 益豐大藥房、小天才等標杆企業的數千家企業研發工程師每日深度、高頻使用的軟件交付平台。</p><p><strong>EMQ 映雲科技聯合創始人兼 CPO 金髮華</strong>以《EMQ —— 開源數實融合基礎軟件的商業化》為題發表演進。據瞭解，作為全球領先的物聯網基礎軟件提供商，EMQ 創立並主導 LF Edge eKuiper、NanoMQ、Neuron 等多個全球知名邊緣軟件開源項目。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-086857ea6bb66926b0c38eb9fb66172ceae.png" width="800" referrerpolicy="no-referrer"></p><p>金髮華表示，Hosting(Cloud) 模式是未來產品的一個方向。當前，EMQ Cloud 商業服務有三種。一是 Serverless，輕鬆幾步即可獲得一個安全可伸縮的 ServerlessMQTT 服務。全託管特性讓用戶無需關心基礎設施和資源管理，特別適用於個人開發者、中小型項目、開發測試環境以及技術框架的評估。二是專有版，獨立部署的全託管 MQTT 服務，具有更高的性能保障和可定製能力，尤其適用於對性能、穩定性要求較高的企業級項目。三是 BYOC (Bring Your Own Cloud)，用戶在自己的雲上部署 EMQX 集羣,並交由 EMQX 團隊託管，適用於有嚴格數據安全和合規性要求的企業級項目，最大限度地利用現有的雲資源。</p><p><strong>天際科技投資副總裁江志桐</strong>闡述了開源與 AI 時代下的投資邏輯。她表示，在 AI 2.0 時代，從全球市場來看，基於大模型未來增長預期，資本給予顯著溢價。當前通用模型格局明確，出現了微軟、谷歌雙龍頭企業，工具層、應用層、垂直領域湧現大量獨角獸。全球市場都在關注大模型商業化的落地，圍繞效率、創意、情感陪伴 2C/2B 的應用生態繁榮。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-d423c2f9a083841554cb3cea84bb20c4056.png" width="800" referrerpolicy="no-referrer"></p><p>與此同時，開源正在加速 AI 2.0 落地，加速 AI 生態繁榮。開源是大模型基礎設施必然選擇，延伸出的服務、應用具有巨大商業機會。AI 開源時代的投資策略以核心人物為中心，佈局早期，發揮產業資源優勢，關鍵人物、網絡效應、稀缺數據，以及軟硬一體都有可能成為企業護城河的因素，也是 AI 開源時代投資重點。</p><p>那麼中國市場的機會在哪裏呢？江志桐認為，主要在於兩方面，一是基礎設施，二是垂直行業。國內大模型的應用還在快速成長，基於開源加速模型落地後，整個 AI 生態裏面也會出現能夠對標全球市場的公司。總之，國內 AI 市場還處於巨頭形成的階段。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 02:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/10136947</guid>
            <link>https://my.oschina.net/u/3859945/blog/10136947</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[智譜 AI 推出第三代基座大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>2023 年 10 月 27 日，智譜 AI 於 2023 中國計算機大會（CNCC）上，推出了<strong>全自研的第三代基座大模型 ChatGLM3</strong>及相關係列產品。</p><p><img height="281" src="https://static.oschina.net/uploads/space/2023/1028/102320_GQzP_2720166.jpg" width="500" referrerpolicy="no-referrer"></p><p>以下彙總摘錄自官方公告：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSVq458IhrR2GGezA9goumw" target="_blank">https://mp.weixin.qq.com/s/SVq458IhrR2GGezA9goumw</a></u></p><hr><h4><strong>全新技術升級</strong></h4><p><strong>1. 更強大的性能：</strong></p><p>今年以來，這是我們第三次對 ChatGLM 基座模型進行了深度優化。我們採用了獨創的多階段增強預訓練方法，更豐富的訓練數據和更優的訓練方案，使訓練更為充分。</p><p>評測顯示，與 ChatGLM 二代模型相比，在 44 箇中英文公開數據集測試中，ChatGLM3 在國內同尺寸模型中排名首位。其中，MMLU 提升 36%、CEval 提升 33%、GSM8K 提升 179% 、BBH 提升 126%。</p><p><strong>2. 瞄向 GPT-4V 的技術升級：</strong></p><p>瞄向 GPT-4V，ChatGLM3 本次實現了若干全新功能的迭代升級，包括：</p><p><strong>多模態理解</strong>能力的 CogVLM，看圖識語義，在 10 餘個國際標準圖文評測數據集上取得 SOTA；</p><p><strong>代碼增強</strong>模塊 Code Interpreter 根據用戶需求生成代碼並執行，自動完成數據分析、文件處理等複雜任務；</p><p><strong>網絡搜索增強</strong>WebGLM，接入搜索增強，能自動根據問題在互聯網上查找相關資料並在回答時提供參考相關文獻或文章鏈接。</p><p>ChatGLM3 的<strong>語義能力與邏輯能力</strong>大大增強。</p><p><strong>3. 全新的 Agent 智能體能力：</strong></p><p>ChatGLM3 本次集成了自研的 AgentTuning 技術，激活了模型智能體能力，尤其在智能規劃和執行方面，相比於 ChatGLM 二代提升 1000%；開啓國產大模型原生支持工具調用、代碼執行、遊戲、數據庫操作、知識圖譜搜索與推理、操作系統等複雜場景。</p><p><strong>4. Edge 端側模型：</strong></p><p>ChatGLM3 本次推出可手機部署的端測模型 ChatGLM3-1.5B 和 ChatGLM3-3B，支持包括 Vivo、小米、三星在內的多種手機以及車載平台，甚至支持移動平台上 CPU 芯片的推理，速度可達 20 tokens/s。</p><p>精度方面 ChatGLM3-1.5B 和 ChatGLM3-3B 在公開 Benchmark 上與 ChatGLM2-6B 模型性能接近。</p><p><strong>5. 更高效推理/降本增效：</strong></p><p>基於最新的高效動態推理和顯存優化技術，我們當前的推理框架在相同硬件、模型條件下，相較於目前最佳的開源實現，包括伯克利大學推出的 vLLM 以及 Hugging Face TGI 的最新版本，推理速度提升了 2-3 倍，推理成本降低一倍，每千 tokens 僅 0.5 分，成本最低。</p><h4><strong>新一代「智譜清言」上線</strong></h4><p>在全新升級的 ChatGLM3 賦能下，生成式 AI 助手智譜清言已成為國內首個具備代碼交互能力的大模型產品（Code Interpreter）。</p><p>傳送門：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchatglm.cn%2Fmain%2Fcode" target="_blank">https://chatglm.cn/main/code</a></u></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102833_nbbu_2720166.png" referrerpolicy="no-referrer"></p><p>在這一能力的加持下，ChatGLM3 可支持圖像處理、數學計算、數據分析等使用場景。以下分別為：</p><p><strong>處理數據生成圖表</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102853_C5gK_2720166.png" referrerpolicy="no-referrer"></p><p><strong>畫圖</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102920_iPm4_2720166.png" referrerpolicy="no-referrer"></p><p><strong>上傳 SQL 代碼分析</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102938_Bkdu_2720166.png" referrerpolicy="no-referrer"></p><p>隨着 WebGLM 大模型能力的加入，智譜清言現具有搜索增強能力。智譜清言可以幫助用戶整理出相關問題的網上文獻或文章鏈接，並整理出答案。</p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102955_TE2Q_2720166.jpg" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103014_ein5_2720166.png" referrerpolicy="no-referrer"></p><p>CogVLM 模型則提高了智譜清言的中文圖文理解能力，取得了接近 GPT-4V 的圖片理解能力。它可以回答各種類型的視覺問題，並且可以完成複雜的目標檢測，並打上標籤，完成自動數據標註。</p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103030_N8vg_2720166.png" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103042_fWLm_2720166.jpg" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103052_suIU_2720166.jpg" referrerpolicy="no-referrer"></p><hr><p>據介紹，自 2022 年初，智譜 GLM 系列模型已支持在昇騰、神威超算、海光 DCU 架構上進行大規模預訓練和推理，當前已支持 10 餘種國產硬件生態，包括昇騰、神威超算、海光 DCU、海飛科、沐曦曦雲、算能科技、天數智芯、寒武紀、摩爾線程、百度崑崙芯、靈汐科技、長城超雲等。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 02:31:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263818</guid>
            <link>https://www.oschina.net/news/263818</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
