<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[開源中國-綜合資訊]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[開源中國-綜合資訊 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Mon, 22 Jan 2024 11:32:16 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[德國程序員因報告漏洞被判罰 2.4 萬元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">德國於利希地方法院近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.heise.de%2Fnews%2FWarum-ein-Sicherheitsforscher-im-Fall-Modern-Solution-verurteilt-wurde-9601392.html" target="_blank">宣佈</a>了一項最新判決結果，認定一名程序員因未經授權訪問第三方計算機系統和刺探數據，違反《德國刑法典》（StGB）中所謂的黑客條款 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gesetze-im-internet.de%2Fstgb%2F__202a.html" target="_blank">202a</a> 而處以 3000 歐元的罰款（約 2.35 萬元），同時承擔所有的訴訟費用。</span></p><p><img height="203" src="https://oscimg.oschina.net/oscnet/up-1685fc08f7024fc47d741af2e54fc872cf5.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#000000">2021 年 6 月，這位名為 Hendrik H. 的研究人員在為 IT 服務公司 Modern Solution GmbH 的一位客户排除軟件故障時發現，Modern Solution 的代碼通過 MySQL 連接至一台 MariaDB 數據庫服務器。而訪問遠程服務器的密碼則以純文本形式存儲在程序文件 MSConnect.exe 中，任何人使用簡單的文本編輯器就能打開該文件查看內容，並找到未加密的硬編碼密碼。</span></p><p><span style="color:#000000">也正是因為這個唾手可得的密碼，導致任何人都可以登錄遠程服務器訪問 Modern Solution 的客户的數據，同時還可以訪問存儲在該數據庫服務器上的供應商所有客户的數據。總的來説，這個數據庫漏洞暴露了近 70 萬條客户記錄，包括姓名、電子郵件地址、電話號碼、銀行信息、密碼以及對話和通話記錄等。</span></p><p><span style="color:#000000">在發現這一漏洞後，該程序員在一名技術博客作者 Mark Steier 的幫助下聯繫了相關公司，後者隨後修復了安全漏洞，並報警追究這名程序員的責任。2021 年 9 月，德國警方扣押了 Hendrik H. 的電腦，因為 Modern Solution 指控他是通過內部信息獲得的密碼，並聲稱他是競爭對手。</span></p><p><span style="color:#000000">2023 年 6 月，德國於利希地方法院以 Modern Solution 軟件保護不力為由，支持了 Hendrik H 的訴訟請求。但亞琛地區法院指令於利希地方法院再次審理此案，原先的裁定被推翻。2024 年 1 月 17 日，於利希地方法院最終宣判對 Hendrik H. 處以罰款，並責令其支付訴訟費用。</span></p><p><span style="color:#000000">這一判決不可避免的在廣大網絡安全專家和研究人員當中引起了爭議。Steier 發帖<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwortfilter.de%2Fentdecker-des-datenlecks-modern-solution-heute-vor-gericht%2F" target="_blank">表示</a>，這一判決從根本上就是錯誤的。「幾乎以純文本形式保存的密碼並不構成第 202 條所要求的'special security'。法官無法對此作出判斷是可以理解的，但這樣一來就必須就這個問題聽取專家的意見。遺憾的是，這並沒有發生。」</span></p><p><span style="color:#000000">不過，該判決尚未具有法律約束力。</span><span style="background-color:#ffffff; color:#323232">被告的辯護律師辯稱，即使法院判定他有罪，他的當事人的行為也是為了公眾利益。</span><span style="color:#000000">被指控的程序員已於 1 月 19 日宣佈，正在對判決提出上訴。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 09:41:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276369/germany-programmer-fined-security</guid>
            <link>https://www.oschina.net/news/276369/germany-programmer-fined-security</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[北京獲準向公眾開放的生成式 AI 大模型產品佔全國近半]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>北京市第十六屆人民代表大會第二次會議於日前召開，會上透露，2023,年，北京獲準向公眾開放的生成式人工智能大模型產品佔全國近一半。今年，北京將推動人工智能模型對標國際先進水平，加快在政務、醫療、工業、生活服務等領域應用。</p><p>北京市市長殷勇作政府工作報告時指出，2023 年，北京加快建設國際科技創新中心，加強科技領軍人才尤其是青年人才培養引進，實施基礎研究領先行動和關鍵核心技術攻堅戰行動，推動在京國家實驗室高質量運行，支持新型研發機構開展有組織科研，加快構建以企業為主導的產學研深度融合新範式。</p><p>北京鞏固提升高精尖產業發展優勢，出台通用人工智能、人形機器人等 30 餘項細分產業支持政策，新設 4 支政府高精尖產業基金，一批創新藥品、醫療器械獲批上市，小米智能手機工廠、理想汽車旗艦工廠提前投產。</p><p>北京精心打造全球數字經濟標杆城市，率先建成全球性能領先的區塊鏈基礎設施，新增 5G 基站 3 萬個，獲準向公眾開放的生成式人工智能大模型產品佔全國近一半，「京通」「京辦」「京智」三個智慧城市應用終端快速升級拓展，高級別自動駕駛示範區實現 160 平方公里連片運行，全國首個數據基礎制度先行區啓動建設，數字經濟增加值佔地區生產總值比重達 42.9%。</p><p>殷勇説，今年，北京將加快發展新質生產力。實施製造業重點產業鏈高質量發展行動，提升產業鏈供應鏈韌性和安全水平。加強原創新藥和高端醫療器械研發，培育生物製造等醫藥健康產業新增長點。推動新能源汽車產業高質量發展，積極佈局電機、電池、電控等關鍵零部件產業鏈。推進超高清視頻全產業鏈優化升級。促進新能源、新材料、商業航天、低空經濟等戰略性新興產業發展，開闢量子、生命科學等未來產業新賽道。優化專精特新企業梯隊培育體系，助力更多企業發展壯大。</p><p>殷勇指出，今年，北京將促進平台經濟有序競爭、創新發展，推動先進數字技術向中小企業深度普及，構建開放共享、充滿活力的創新生態。提升人工智能底層技術和基礎底座自主可控能力，推動人工智能模型對標國際先進水平，加快在政務、醫療、工業、生活服務等領域應用，保持人工智能研發應用領先水平。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 05:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276347</guid>
            <link>https://www.oschina.net/news/276347</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[RustDesk 新增 2FA 雙重認證功能，增強遠程桌面訪問安全性]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>RustDesk <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frustdesk%2Frustdesk%2Freleases%2Ftag%2Fnightly" target="_blank">nightly</a>&nbsp;新增 2FA 雙重認證功能，增強遠程桌面訪問安全性，歡迎大家試用反饋。</p><p><img height="1246" src="https://oscimg.oschina.net/oscnet/up-0de6626da796bb7195b23fc861ee98e2f12.jpg" width="1708" referrerpolicy="no-referrer"></p><p><img height="623" src="https://oscimg.oschina.net/oscnet/up-eef5d3bce35b2b05039bb7678d9ebbea95b.jpg" width="854" referrerpolicy="no-referrer"></p><p><img height="1270" src="https://oscimg.oschina.net/oscnet/up-d4017aeb3ba844b73de27c16d258a40944d.jpg" width="1780" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:20:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276342/rustdesk-2fa</guid>
            <link>https://www.oschina.net/news/276342/rustdesk-2fa</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[到底什麼樣的 Java 項目用 Solon 好？？？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">就像華為講的，不要因為愛國而特意買華為手機。Solon 也是，<strong>有需要就用不需要就跳過</strong>（按正常的需求選擇）：</p><ul><li>信創需要國產化，應該用 Solon 或者 Solon Cloud（有案例）</li><li>軍工項目要國產化，應該用 Solon 或者 Solon Cloud（有案例）</li><li>嵌入式設備，內存有限，算力差，可以用 Solon 或者 Solon Native（有案例）</li><li>客户的希望你內存更少，可以用 Solon （有案例）</li><li>別的框架用膩了，可以用 Solon （有案例）</li><li>有新系統開發想嘗新的框架，可以用 Solon （有案例）</li><li>老系統要輕量化改造，可以用 Solon（有案例）</li></ul><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">作為後來者，大家的疑或是會多一些。有問題，可以去交流羣裏多交流。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276341</guid>
            <link>https://www.oschina.net/news/276341</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[如何寫好大模型提示詞？來自大賽冠軍的經驗分享（進階篇）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><blockquote><p><strong>編者按</strong>：近期，如何通過 Prompt Engineering 最大程度發揮大模型的潛力已成為一個熱點話題。人們越來越關注如何通過 Prompt Engineering 技術低成本地用好大模型。</p><p>今天我們推薦的這篇文章，作者認為 Prompt Engineering 需要結合藝術與科學，需要在理解技術背景的同時，發揮創造力和戰略思維。</p><p>本系列文章詳細介紹了作者在新加坡首屆 GPT-4 Prompt Engineering 大賽中使用的策略技巧，包括：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">使用 CO-STAR 框架構建提示語、使用分隔符明確語義單元</a>、利用 system prompts 添加行為約束、僅依靠 GPT-4 對數據集進行分析等。這些技巧都得到了實例驗證，證明瞭 Prompt Engineering 的重要作用。</p><p>本文屬於該系列文章的第二部分，詳細介紹適合進階使用的 Prompt Engineering 高級策略。</p></blockquote><p><strong>作者 |&nbsp;Sheila Teo</strong></p><p><strong>編譯&nbsp;|&nbsp;嶽揚</strong></p><p><strong>🚢🚢🚢歡迎小夥伴們加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技術軟件及技術交流羣</a>，追蹤前沿熱點，共探技術難題~</strong></p><p>上個月，我有幸獲得新加坡首屆 GPT-4 提示工程（Prompt Engineering）大賽相關獎項，該比賽由新加坡政府科技署（GovTech）組織，匯聚了超過 400&nbsp;位優秀的參與者。</p><p><strong>提示工程（Prompt Engineering）是一門融合了藝術和科學的學科——這門學科不僅需要理解技術，還需要一定的創造力和戰略思維。</strong> 以下是我在學習過程中學到的提示工程（Prompt Engineering）策略彙編，這些策略可以驅動任何大語言模型（LLM）精準執行需求，甚至超常發揮！</p><blockquote><p>作者注：</p><p>在撰寫本文時，我力圖摒棄已在網上廣泛討論和記錄的傳統提示工程（Prompt Engineering）技術。相反，撰寫本文的目的是給大家介紹我在實驗中學到的新見解，以及對某些技術的不同理解。希望你會喜歡閲讀這篇文章！</p></blockquote><p>本系列文章包括以下內容，其中🔵指的是適合初學者的提示語（prompt）技巧（<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">見基礎篇</a>），而🔴指的是高級策略（本文的重點）：</p><p>1.[🔵] 使用&nbsp;CO-STAR&nbsp;框架構建提示語</p><p>2.[🔵]&nbsp;使用分隔符（delimiters）將提示語分段</p><p><strong>3.[🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;創建&nbsp;system prompts</strong>（譯者注："guardrails"&nbsp;指的是一種保護機制或限制，用於確保大語言模型生成的內容符合特定標準或要求，防止產生不準確、不合適或有害的信息。）</p><p><strong>4.[🔴]&nbsp;僅使用&nbsp;LLM（無需插件或代碼）分析數據集——將介紹一個使用&nbsp;GPT-4&nbsp;分析真實&nbsp;Kaggle&nbsp;數據集的實踐示例</strong></p><h1><strong>01 [🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;創建&nbsp;system prompts</strong></h1><p>在進入正題之前，需要注意的是本節只適用於具有 System Prompt 功能的 LLM，而不像基礎篇和本文的其他章節那樣適用於任何 LLM。最著名的 LLM 當然是&nbsp;ChatGPT ，因此在本節中我們將以 ChatGPT 作為示例。</p><h2><strong>1.1 圍繞&nbsp;System Prompt&nbsp;的術語</strong></h2><p>首先，讓我們來理清術語，特別是關於 ChatGPT 的三種術語的使用：這三種術語在 ChatGPT 幾乎可以互換使用：&nbsp;"System Prompts"、"System Messages "和&nbsp;"Custom Instructions"。這讓很多人（包括我在內！）感到困惑，以至於&nbsp;OpenAI&nbsp;特意發佈了一篇文章來解釋這些術語。以下是其摘要：</p><ul><li><strong>"System Prompts"和"System Messages"是通過 Chat Completions API 以編程方式與 ChatGPT 進行交互時使用的術語。</strong></li><li><strong>另一方面，"Custom Instructions"是通過 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchat.openai.com%2F" target="_blank">https://chat.openai.com/</a>&nbsp; 用户界面與 ChatGPT 交互時使用的術語。</strong></li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-121921e25f98c551f3af8e6f70943254843.png" alt="" referrerpolicy="no-referrer"></p><p>Image from Enterprise DNA Blog</p><p>不過總的來説，這三個術語指的是同一件事，所以不要被這些術語混淆了！後續部分，本文將使用&nbsp;"System Prompts"一詞。現在，讓我們進入正題！</p><h2><strong>1.2 什麼是 System Prompts ？</strong></h2><p>System Prompts 是一種額外的提示語（prompt），我們可以在其中提供有關 LLM 行為方式的 instructions。它被認為是額外的提示語（prompt），因為它不屬於您給 LLM 的&nbsp;"正常"&nbsp;提示語（即 User Prompts）。</p><p>在聊天中，每當您給 LLM 發送新的提示語（prompt）時，System Prompts 都會像過濾器一樣，LLM 會在回答您的新提示語（prompt）之前自動應用這些提示語（prompt）。這意味着 System Prompts 在 LLM 做出回答時都會被考慮進去。</p><h2><strong>1.3 何時使用 System Prompt ？</strong></h2><p>您心中可能會想到的第一個問題是：為什麼我應該在 System Prompts 中提供 instruction，而不是在我向與 LLM 的新對話的第一個提示語（prompt）中提供 instruction，然後再與 LLM 進行更多的對話呢？</p><p>答案是，因為 LLM 的對話記憶是有限的。在後一種情況下，隨着對話的繼續，LLM 很可能會"忘記"您在聊天中提供的第一條提示語（prompt），從而使這些 instruction （指令）過時。</p><p>另一方面，如果在 System Prompts 中提供了 instruction （指令），那麼這些 System Prompts &nbsp;會與聊天中提供的每個新提示語一起發送。這可以確保 LLM 在聊天過程中繼續接收這些 instruction，無論聊天過程變得多長。</p><p>總結：</p><p>在整個聊天過程中，使用 System Prompts 提供您希望 LLM 在回答時記住的 instruction 。</p><h2><strong>1.4 System Prompt 應包括哪些內容？</strong></h2><p>System Prompt 通常應包括以下類別的 instruction ：</p><ul><li><strong>目標任務的定義（Task definition）</strong> ，這樣 LLM 在整個對話過程中都會記住它必須做什麼。</li><li><strong>輸出格式（Output format）</strong> ，這樣 LLM 在整個對話過程中都會記住它應該如何做出回答。</li><li><strong>防範措施（Guardrails）</strong> ，這樣 LLM 在整個對話過程中都會記住它不應該如何做出回答。Guardrails 是 LLM governance 中的新興領域，指的是 LLM 被允許操作的行為邊界。</li></ul><p>例如，System Prompt 可能是這樣的：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p><p>If the text does not contain sufficient information to answer the question,&nbsp;do not make up information and give the answer as&nbsp;「NA」.</p><p>You are only allowed to answer questions related to&nbsp;[insert scope].&nbsp;Never answer any questions related to demographic information such as age,&nbsp;gender,&nbsp;and religion.</p></blockquote><p>各部分內容涉及的類別如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-cab48977b545bba61bf371683a3bdc105af.png" alt="" referrerpolicy="no-referrer"></p><p>Breaking down a System Prompt&nbsp;—&nbsp;Image by author</p><h2><strong>1.5 但是，"正常"的聊天提示語又是什麼呢？</strong></h2><p>現在你可能會想：聽起來 System Prompt 中已經提供了很多信息。那我應該在聊天的"正常"提示語（即 User Prompts）中寫些什麼呢？</p><p>System Prompt 概述了當前的一般任務。在上面的 System Prompt 示例中，任務已被定義為只使用一段特定的文本來回答問題，並且 LLM 被指示以{"Question":&nbsp;"Answer"}的格式進行回答。</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p></blockquote><p>在這種情況下，聊天過程中的每個 User Prompt 都將簡化為你希望 LLM 用文本回答的問題。例如，某個用户的提問可能是「這段文本是關於什麼的？（What is the text about?）」然後 LLM 會回答説&nbsp;{"這段文本是關於什麼的？（What is the text about?）":&nbsp;"這段文本是關於……（The text is about..）"}。</p><p>但是，讓我們進一步概括這個任務示例。在這種情況下，我們可以將上述 System Prompt &nbsp;的第一行從：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p></blockquote><p>編輯為：</p><blockquote><p>You will answer questions using the provided text.</p></blockquote><p>現在，每個用户在聊天時的提示語（prompt）將包括進行問題回答的文本和要回答的問題，例如：</p><blockquote><p>&lt;text&gt;</p><p>[insert text]</p><p>&lt;/text&gt;</p><p>&lt;question&gt;</p><p>[insert question]</p><p>&lt;/question&gt;</p></blockquote><p>在這裏，還將使用 XML 標籤作為分隔符，以便以結構化的方式向 LLM 提供所需的兩個信息片段。<strong>XML 標籤中使用的名詞「text」和「question」與 System Prompt 中使用的名詞相對應，這樣 LLM 就能理解標籤與 System Prompt instructions 之間的關係。</strong></p><p>總之， System Prompt 應給出總體任務 instructions，而每個 User Prompt 應提供任務執行的具體細節。例如，在本例中，這些具體的細節是 text 和 question。</p><h2><strong>1.6 LLM guardrails&nbsp;動態化</strong></h2><p>上面通過 System Prompt 中的幾句話添加了 guardrails 。這些 guardrails 會被固定下來，在整個聊天過程中都不會改變。但是如果您希望在對話的不同階段設置不同的 guardrails ，該怎麼辦？</p><p>對於使用 ChatGPT Web 界面的用户來説，目前還沒有直接的方法來做到這一點。不過，如果您正在通過編程方式與 ChatGPT 進行交互，那你就走運了！隨着人們對構建有效的 LLM guardrail 的關注度越來越高，一些開源軟件包也應運而生，它們可以讓你以編程方式設置更詳細、更動態的 guardrail。</p><p>其中值得注意的是英偉達團隊開發的&nbsp;NeMo Guardrails[1]，它允許您配置用户和 LLM 之間預期的對話流程，從而在聊天的不同時間點設置不同的 guardrail ，實現隨着聊天進展而不斷演變的動態 guardrails 。我強烈推薦您去了解一下！</p><h1><strong>02 [🔴]&nbsp;僅使用&nbsp;LLM（無需插件或代碼）分析數據集</strong></h1><p>您可能已經聽説過 OpenAI 在 ChatGPT 的 GPT-4 中推出的高級數據分析插件，該插件僅高級（付費）賬户可以使用。它允許用户將數據集上傳到 ChatGPT，並直接在數據集上運行代碼，從而進行精確的數據分析。</p><p>但你知道嗎，使用 LLM 分析數據集並不一定需要這樣的插件？讓我們先來瞭解一下單純使用 LLMs 分析數據集的優勢和侷限性。</p><h2><strong>2.1 大語言模型不擅長的數據集分析類型</strong></h2><p>正如您可能已經知道的那樣，LLM 在進行精確數學計算方面的能力有限，因此它們不適合完成需要對數據集進行精確定量分析的任務，比如：</p><ul><li><strong>描述性統計（Descriptive Statistics）</strong> ：通過諸如均值或方差的測量來定量總結數值列。</li><li><strong>相關性分析（Correlation Analysis）</strong> ：獲取列之間精確的相關係數。</li><li><strong>統計分析（Statistical Analysis）</strong> ：比如假設檢驗（hypothesis testing），以確定各組數據點之間是否存在統計意義上的顯著差異。</li><li><strong>機器學習（Machine Learning）</strong> ：在數據集上執行預測建模，比如使用線性迴歸（linear regressions）、梯度提升樹（gradient boosted trees）或神經網絡（neural networks）。</li></ul><p>方便在數據集上執行這些定量任務正是 OpenAI 推出高級數據分析插件的原因，這樣編程語言就可以在數據集上運行代碼來執行此類任務。</p><p>那麼，為什麼有人只想使用 LLM 而不使用此類插件來分析數據集呢？</p><h2><strong>2.2 大語言模型擅長的數據集分析類型</strong></h2><p>LLM 非常擅長識別模式和趨勢（patterns and trends）。這種能力源自它們在多樣化和海量數據上的廣泛訓練，使它們能夠識別那些可能無法立即察覺的複雜模式。</p><p>這使它們非常適合執行基於數據集進行模式識別的任務，例如：</p><ul><li><strong>異常檢測（Anomaly detection）</strong> ：基於一列或多列的數值，識別偏離常規的異常數據點。</li><li><strong>聚類（Clustering）</strong> ：將具有相似特徵的數據點分組。</li><li><strong>跨列關係（Cross-Column Relationships）</strong> ：通過分析不同列之間的關係，可以揭示數據中的複雜模式和趨勢。</li><li><strong>文本分析（Textual Analysis）（針對基於文本的列）</strong> ：基於主題或情感進行分類。</li><li><strong>趨勢分析（Trend Analysis）（針對具有時間特徵的數據集）</strong> ：識別列中跨時間的模式、季節性變化或趨勢。</li></ul><p>對於這類基於模式的任務，單獨使用 LLM 可能會比使用代碼在更短的時間內獲得更好的結果！讓我們用一個例子來充分説明這一點。</p><h2><strong>2.3 僅使用&nbsp;LLM&nbsp;分析&nbsp;Kaggle&nbsp;數據集</strong></h2><p>我們將使用一個廣受歡迎的真實&nbsp;Kaggle&nbsp;數據集[2]，該數據集專為進行客户人格分析而準備，其中一家公司試圖對其客户羣體進行細分，以便更好地瞭解其客户。</p><p>為了便於之後驗證 LLM 的分析結果，我們取該數據集的 50&nbsp;行為一個子集，並只保留最相關的列。之後，用於分析的數據集將如下所示，其中每一行代表一位客户，每一列描述客户信息：</p><p><img src="https://oscimg.oschina.net/oscnet/up-7c8caf80c5954ae73d0746053b0109b7737.png" alt="" referrerpolicy="no-referrer"></p><p>First 3 rows of dataset&nbsp;—&nbsp;Image by author</p><p>假設你在公司的營銷團隊工作。你的任務是利用這些客户信息數據集來指導營銷工作。這是一項分兩步走的任務：首先，利用數據集劃分多個具有實際意義的客户細分羣體。接下來，提出如何最好地針對每個客户羣體進行創意營銷。現在，這是一個實際的商業問題，LLM 的模式發現（pattern-finding，對於步驟 1 ）能力在這個問題上確實可以大顯身手。</p><p>讓我們按照以下方式為這個任務制定提示語（prompt），將使用&nbsp;4&nbsp;種&nbsp;prompt engineering&nbsp;技術（後續會詳細介紹[3]）：</p><p><strong>1. 將複雜的任務分解為簡單的步驟</strong></p><p><strong>2. 參考每個步驟的中間輸出</strong></p><p><strong>3. 格式化 LLM 的回答</strong></p><p><strong>4. 將&nbsp;instructions&nbsp;與數據集分開</strong></p><blockquote><p>System Prompt:</p><p>I want you to act as a data scientist to analyze datasets.&nbsp;Do not make up information that is not in the dataset.&nbsp;For each analysis I ask for,&nbsp;provide me with the exact and definitive answer and do not provide me with code or instructions to do the analysis on other platforms.</p><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>GPT-4 的回答如下，接下來我們將數據集以 CSV 字符串的形式傳遞給它。</p><p><img src="https://oscimg.oschina.net/oscnet/up-e81a5f6ea27da5b058438541aa821009931.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p>隨後，GPT-4 將按照我們要求的 markdown 格式回覆分析結果：</p><p><img src="https://oscimg.oschina.net/oscnet/up-72040eca14932d7eb9289119eb13af4d0a8.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-03f08b1a6bfadee91b96716822f50f0c4a4.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-f81c09e878806d7f71c9773c8291d3611d5.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><h2><strong>2.4 驗證&nbsp;LLM&nbsp;的分析結果</strong></h2><p>為了簡潔起見，我們將挑選 LLM 生成的 2 個客户羣體進行驗證，比如 Young Families 和 Discerning Enthusiasts。</p><h3><strong>2.4.1 Young Families</strong></h3><ul><li>LLM 總結的該人羣特徵：1980 年後出生，已婚或同居，收入中等偏低，有孩子，經常進行小額購買。</li><li>LLM 將數據集中的這些行聚類到了 Young Families 這個羣體中：3、4、7、10、16、20</li></ul><p>深入數據集，這些行的完整數據如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-01030fc251ada675444cc2b84f87f861ce5.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Young Families&nbsp;—&nbsp;Image by author</p><p>LLM 識別出的這部分客户資料，確實對應於所識別的客户羣體。甚至能夠在我們事先未經過預處理的情況下對具有空值的資料進行聚類！</p><h3><strong>2.4.2 Discerning Enthusiasts</strong></h3><ul><li>LLM 總結的該人羣特徵：年齡跨度廣，可能是任何婚姻狀況，高收入，子女狀況各異，購買支出高。</li><li>LLM 認為該人羣對應的數據行：2、5、18、29、34、36</li></ul><p>深入數據集，這些行的完整數據如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-2844275d166caf555082adb378ad4e1e7b6.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Discerning Enthusiasts&nbsp;—&nbsp;Image by author</p><p>再次與 LLM 識別出的人羣資料非常吻合！</p><p>這個例子展示了 LLM 在發現模式、解釋和提煉多維數據集，並將其提煉為有意義的見解方面的能力，同時確保其分析深深紮根於數據集的事實。</p><h2><strong>2.5 如果我們使用 ChatGPT 的高級數據分析插件會怎樣呢？</strong></h2><p>為了保證分析的完整性，我嘗試使用相同的提示語（prompt），並請求 ChatGPT 使用代碼執行相同的分析，這就激活了它的高級數據分析插件。這個想法是讓插件直接在數據集上運行 K-Means 等聚類算法的代碼，以獲得各個用户羣體的特徵，然後綜合每個羣體的數據來提供營銷策略。</p><p>然而，儘管數據集只有 50&nbsp;行，進行了多次嘗試都導致出現以下錯誤信息而沒有任何輸出：</p><p><img src="https://oscimg.oschina.net/oscnet/up-10c7526d7a4b7d296cef8b48699d37762a9.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 1&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-fa3318a26ba4ab2800121fb5ab4295345f5.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 2&nbsp;—&nbsp;Image by author</p><p>現在使用高級數據分析插件，在數據集上執行較簡單的任務（如計算描述性統計數據或創建圖表）似乎很容易實現，但需要某些計算算法的較高級任務有時可能會由於計算限制或其他原因導致錯誤或無輸出。</p><h2><strong>2.6 那麼......何時使用 LLM 分析數據集？</strong></h2><p>答案是取決於分析的數據類型。</p><p>對於需要精確數學計算或複雜的、基於規則處理的任務，傳統的編程方法仍然更勝一籌。</p><p>對於基於模式識別（pattern-recognition）的任務，使用傳統的編程或算法方式可能比較具有挑戰性或更耗時。然而，LLM 擅長此類任務，甚至可以提供額外的內容輸出，比如用來支持其分析的附件和 Markdown 格式的完整分析報告。</p><blockquote><p><strong>歸根結底，是否使用 LLM 取決於手頭任務的性質，要在 LLM 在模式識別方面的優勢與傳統編程技術提供的精確性和特異性之間取得平衡。</strong></p></blockquote><h2><strong>2.7 現在回到提示工程（prompt engineering）！</strong></h2><p><strong>在本節結束之前，讓我們回顧一下用於生成此數據集分析的提示語（prompt），並分解所使用的 prompt engineering 關鍵技術。</strong></p><blockquote><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p><strong>技巧 1：將複雜任務分解為簡單步驟</strong></p><p>LLM 擅長執行簡單任務，但在複雜任務上表現一般。因此，<strong>對於像這樣的複雜任務，重要的是要把任務分解成簡單的步驟説明，讓 LLM 遵循。</strong> 這樣做的目的是向 LLM 提供你自己執行任務時會採取的步驟。</p><p>在本例中，步驟如下：</p><blockquote><p><strong>Use this step-by-step process and do not use code:</strong></p><p><strong>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</strong></p><p><strong>For each cluster found,</strong></p><p><strong>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</strong></p><p><strong>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</strong></p><p><strong>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</strong></p><p><strong>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</strong></p></blockquote><p>與簡單地將整體任務交給 LLM 相比，例如「將客户分組，然後提出針對每個羣體的營銷策略」。通過逐步説明，LLM 更有可能提供正確的結果。</p><p><strong>技巧 2：引用每個步驟的中間輸出</strong></p><p>在向 LLM 提供逐步説明時，可將每個步驟的中間輸出命名為大寫的變量名，例如 CLUSTERS、CLUSTER_INFORMATION、CLUSTER_NAME、MARKETING_IDEAS 和 RATIONALE。</p><p><strong>使用大寫字母是為了將這些變量名與給出的 instructions 內容區分開。稍後可以使用方括號引用這些中間輸出，如[VARIABLE_NAME]。</strong></p><p><strong>技巧 3：規範大模型回答的格式</strong></p><p>在這裏，要求使用 Markdown 報告格式，以美化 LLM 的回答。在這裏，中間輸出中的變量名又派上了用場，可以決定報告的結構。</p><blockquote><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;<br><strong>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</strong></p></blockquote><p>事實上，您甚至可以隨後要求 ChatGPT 將報告提供為可下載文件，這樣您就可以根據其回答來撰寫最終的報告文檔。</p><p><img src="https://oscimg.oschina.net/oscnet/up-792771629e0046fa10609d608023fce4df5.png" alt="" referrerpolicy="no-referrer"></p><p>Saving GPT-4's response as a file&nbsp;—&nbsp;Image by author</p><p><strong>技巧 4：將任務説明與數據集分開</strong></p><p>您會注意到我們在第一個提示語中並沒有將數據集提供給 LLM。相反，提示語只包含了數據集分析的任務説明，並在最後加上了以下內容：</p><blockquote><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>ChatGPT 隨後回覆説它理解了，我們將在下一個提示語中將數據集作為 CSV 字符串傳遞給它：</p><p><img src="https://oscimg.oschina.net/oscnet/up-3b139d01a6cf26df1f930d6e67dc5b8be24.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><strong>但為什麼要將 instructions 與數據集分開呢？</strong></p><p>簡單明瞭的答案是，LLM 的上下文窗口存在限制，即在一句提示語中可以輸入的 tokens 數量存在限制。同時包含 instructions 和數據的長提示語（long prompt）可能會超過這個限制，從而導致截斷（truncation）和信息丟失（loss of information）。</p><p>更復雜的答案是，<strong>將 instructions 和數據集分開可以幫助 LLM 保持清晰的理解，降低遺漏信息的可能性。</strong> 你可能遇到過這樣的情況，即 LLM "不小心忘記了"&nbsp;你發送的較長提示語給出的某個 instruction ——例如，如果你要求給出 100&nbsp;字的回答，而 LLM 卻給了您一個較長的段落。通過先接收 instructions ，再接收 instructions 所針對的數據集，LLM 可以先消化它應該執行的任務，然後再對接下來提供的數據集執行 instructions 。</p><p>不過請注意，只有聊天型&nbsp;LLM&nbsp;才能實現&nbsp;instruction&nbsp;和數據集的分離，因為它們會保留對話記憶，而&nbsp;completion LLM&nbsp;則不會（譯者注：completion LLM 指的是一種能夠根據給定的提示語來生成完整文本或完成特定任務的語言模型。這種模型通常不具備對話記憶，而是專注於根據提示語生成連貫的文本）。</p><p><strong>Thanks for reading!</strong></p><p><strong>END</strong></p><p><strong>🚢🚢🚢歡迎小夥伴們加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技術軟件及技術交流羣</a>，追蹤前沿熱點，共探技術難題~</strong></p><p><strong>參考資料</strong></p><p>[1]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FNVIDIA%2FNeMo-Guardrails" target="_blank">https://github.com/NVIDIA/NeMo-Guardrails</a></p><p>[2]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fimakash3011%2Fcustomer-personality-analysis" target="_blank">https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis</a></p><p>[3]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41%23544b" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41#544b</a></p><p><strong>本文經原作者授權，由 Baihai IDP 編譯。如需轉載譯文，請聯繫獲取授權。</strong></p><p><strong>原文鏈接：</strong></p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/10920438</guid>
            <link>https://my.oschina.net/IDP/blog/10920438</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[🔥 周熱點 | 純血 HarmonyOS NEXT 亮相；雲風宣佈開源自研遊戲引擎；ReiserFS 作者在獄中迴應被 Linux 內核棄用.....]]>
            </title>
            <description>
                <![CDATA[回顧一週熱門資訊。2024.01.15-2024.01.21]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</guid>
            <link>https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</link>
        </item>
        <item>
            <title>
                <![CDATA[助力 AI 技術共享，螞蟻開源又一核心技術 「因果學習系統 OpenASCE」]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">當地時間 2023 年 12 月 10 日，機器學習和人工智能領域的頂級國際會議 NeurIPS (Neural Information Processing Systems) 在美國路易斯安那州新奧爾良市開幕，來自全球產業界和學術領域的人工智能專家齊聚一堂。</p><h1>首個分佈式全鏈路因果學習系統 OpenASCE</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">大會第一天，螞蟻集團在主題為 「知識增強 AI 在垂直行業的應用探索」 的研討會上正式開源了業界首個分佈式全鏈路因果學習系統 OpenASCE (Open All-Scale Causal Engine) 。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">項目 GitHub：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpen-All-Scale-Causal-Engine%2FOpenASCE" target="_blank">https://github.com/Open-All-Scale-Causal-Engine/OpenASCE</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-8d931ba2b03a853fcf7d06c1a780d171b96.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">因果推斷主要研究如何從數據中推斷因果關係，是數據科學領域的重要分支，而傳統的機器學習則主要依賴數據中的相關關係。融合因果推斷和機器學習可以同時發揮兩者的強項，我們稱之為因果學習。因果學習作為一種深入理解數據和決策背後關係的技術，在數據驅動的運營和決策中扮演着重要的角色。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 根植於螞蟻集團多年積累的實踐經驗和技術突破，相較於業界已有的一些開源框架，支持全鏈路大規模因果學習，包含因果發現、因果效應估計和歸因，覆蓋了因果各個領域的相應實現。在因果發現上，OpenASCE 支持分佈式貝葉斯網絡結構搜索，能夠處理百節點百萬樣本數據；同時支持基於連續優化的因果發現，支持萬級節點億級樣本數據。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 實現的大規模分佈式因果糾偏樹可以在 4 小時內完成 1 億樣本的訓練任務，是業界唯一的分佈式因果提升樹實現。此外，OpenASCE 還沉澱了 20 多個工業級因果學習算法，包括 15 個以上因果技術和深度學習結合的因果表徵學習方法，有效降低了因果技術的工業應用門檻，在螞蟻集團內部多個場景實現了規模化應用。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-da7d981d440607195474f083b7a1cbb7862.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">在信貸風控領域，通過 OpenASCE 的因果學習方法，可以更準確地識別出風險因素和客户行為之間的因果關係，大幅提高了風險控制的精度和效率。在營銷優化方面，OpenASCE 能夠幫助營銷人員有效尋找 「營銷敏感人羣」，提升業務指標。在推薦場景中，因果推斷可以幫助機器學習糾正數據中的偏置，去除偽相關，學習更穩定的因果關係。</p><h1>開源開放，共建社區</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">作為一家科技公司，螞蟻集團將 OpenASCE 開源，為業界提供一套大規模、高性能的因果學習技術，並通過開源吸引全球開發者共同參與項目的建設和完善，促進全鏈路因果學習系統領域的發展和創新。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">技術開源是螞蟻集團的重要技術戰略，我們希望通過開源建立起開放、包容的技術生態，讓更多人共享技術紅利。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">截至目前，螞蟻集團已在數據庫、雲原生、中間件等基礎軟件領域開源了 1700 多個倉庫、積累了 100 多個社區頭部開源項目。《COPU2022 中國開源發展藍皮書》顯示，螞蟻開源影響力排名國內前三，其中重點開源的 9 大技術均為支撐支付寶的核心技術。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:47:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276338</guid>
            <link>https://www.oschina.net/news/276338</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[因政治濫用，OpenAI 將一家 AI 初創公司拉黑]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>OpenAI 於日前<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yahoo.com%2Fnews%2Fopenai-suspends-developer-over-chatgpt-bot-that-impersonated-a-presidential-candidate-214854456.html" target="_blank">封禁</a>了一家開發 Chatbot 的 AI 初創公司 Delphi。因為該公司基於 GPT-4，出於政治目的設計了一個模仿美國民主黨總統候選人 Dean Phillips 的機器人 Dean.Bot；以與潛在支持者互動並傳播候選人的信息，幫助其競選。</p><p><img height="220" src="https://oscimg.oschina.net/oscnet/up-5e13ec80f847c98829315c5bf8ebf46f766.png" width="500" referrerpolicy="no-referrer"></p><p>事實上，美國、英國、印度、巴基斯坦和南非等國都計劃在 2024 年進行大選。為了防止其技術被濫用，OpenAI 在本月早些時候曾發表了一篇<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Fblog%2Fhow-openai-is-approaching-2024-worldwide-elections" target="_blank">長文</a>，介紹其計劃採取的一些措施；其中明確表示不允許人們開發用於政治活動和遊説的應用，並且還特別提到了禁止"冒充候選人的聊天機器人 "。</p><p>雖然 Dean.Bot 網站有提供免責聲明，告知訪問者所有的交互都將由聊天機器人生成，而不是 Phillips 本人。但這種使用方式還是直接違反了 OpenAI 的政策。公司發言人在給《華盛頓郵報》的一份聲明中也證實了被 OpenAI 封禁的消息。Delphi 的 OpenAI 帳户據稱於上週五被封禁，隨後該公司就停止了對 Dean.Bot 的訪問權限。</p><p>現在訪問該網站的用户仍然會看到免責聲明，但會顯示聊天機器人本身已因"技術故障"而宕機："Apologies, DeanBot is away campaigning right now!"&nbsp;</p><p>這也是 OpenAI 首次因開發者違反其 AI 濫用準則而採取審查措施。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:34:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</guid>
            <link>https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CursusDB —— 面向文檔的內存數據庫]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CursusDB 是一種面向文檔的快速開源內存數據庫，提供安全性、持久性、分佈性、可用性和類似 SQL 的查詢語言。</p><p>CursusDB 的設想是創建無限可擴展的東西，同時又不會真正減慢速度。假設你有 10 億個文檔存儲在分佈在 100 個節點的 1 個集合中，當集羣同時在所有節點上啓動非插入操作時，集羣將在查詢 1000 萬個文檔所需的時間內查詢 10 億個文檔。這就是並行搜索的力量。Cursus 系統可同時在用户集合的多個部分中進行搜索。一個集羣可以同時查詢數千個節點。將主節點視為多個或一個集合的碎片。每個集合都會鎖定插入、更新和刪除，但由於 CursusDB 的分佈式設計，它就像一個併發交換機，允許大量併發事務。一個集羣或多個集羣採取操作，這些操作作為請求同時轉發到 1 個或多個節點。一致性和可靠性是設計 CursusDB 時的主要目標之一。</p><p style="text-align:start"><strong><span><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>特性</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></strong></p><ul><li>使用共享密鑰和 OR TLS 保護集羣和節點通信</li><li>運行時內存中的數據</li><li>並行搜索。同時搜索多個節點內的集合部分。</li><li>自動為所有節點生成唯一的所有文檔的 $id 鍵</li><li>具有基本（R、RW）權限的數據庫用户</li><li>集羣和節點認證</li><li>專門針對讀取的集羣節點數據複製和同步</li><li>JSON 對象插入</li><li>非結構化集合</li><li>集羣和客户端身份驗證</li><li>節點（插入、更新、刪除）實時轉發給觀察者</li><li>如果連接丟失，節點觀察者自動重新連接</li><li>類似 SQL 的查詢語言（CDQL - Cursus 文檔查詢語言）</li><li>低延遲</li><li>高可用</li><li>默認情況下使用共享密鑰和用户確保安全</li><li>高度可配置</li><li>輕量級核心代碼總共不到 6000 行代碼</li><li>基於<code>log-max-lines</code>配置的文件日誌記錄和自動日誌截斷</li><li>自動重新連接任何丟失的節點或節點副本</li><li>如果 .curodeconfig 中的 automatic-backup 設置為 true，則自動備份節點</li><li>如果 .curodeconfig 中的 automatic-backup-cleanup 設置為 true，則自動清理節點備份。</li><li>如果配置了自動備份，則在數據損壞時自動恢復節點</li><li>節點數據 (.cdat) 和節點備份 (/backups/.cdat.{unixtime}) 是在關機或備份時通過序列化-加密 (chacha20poly1305)-壓縮 (DEFLATE) 將內存中的數據序列化、加密並逐塊壓縮後創建的。</li><li>......</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:33:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/cursusdb</guid>
            <link>https://www.oschina.net/p/cursusdb</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推薦 | 中文對話 0.2B 小模型 ChatLM-Chinese-0.2B]]>
            </title>
            <description>
                <![CDATA[<div align="center"><h1><a id="user-content-中文對話 02b 小模型-chatlm-chinese-02b" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D02b%E5%B0%8F%E6%A8%A1%E5%9E%8B-chatlm-chinese-02b"></a>中文對話 0.2B 小模型 ChatLM-Chinese-0.2B</h1><p>中文  | <a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/README.en.md">English</a></p></div><h1><a id="user-content-一介紹" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%80%E4%BB%8B%E7%BB%8D"></a>一、👋介紹</h1><p>現在的大語言模型的參數往往較大，消費級電腦單純做推理都比較慢，更別説想自己從頭開始訓練一個模型了。本項目的目標是整理生成式語言模型的訓練流程，包括數據清洗、tokenizer 訓練、模型預訓練、SFT 指令微調、RLHF 優化等。</p><p>ChatLM-mini-Chinese 為中文對話小模型，模型參數只有 0.2B（算共享權重約 210M），可以在最低 4GB 顯存的機器進行預訓練（<code>batch_size=1</code>，<code>fp16</code>或者<code> bf16</code>），<code>float16</code>加載、推理最少只需要 512MB 顯存。</p><ul><li>公開所有預訓練、SFT 指令微調、DPO 偏好優化數據集來源。</li><li>使用<code>Huggingface</code>NLP 框架，包括<code>transformers</code>、<code>accelerate</code>、<code>trl</code>、<code>peft</code>等。</li><li>自實現<code>trainer</code>，支持單機單卡、單機多卡進行預訓練、SFT 微調。訓練過程中支持在任意位置停止，及在任意位置繼續訓練。</li><li>預訓練：整合為端到端的<code>Text-to-Text</code>預訓練，非<code>mask</code>掩碼預測預訓練。
<ul><li>開源所有數據清洗（如規範化、基於 mini_hash 的文檔去重等）、數據集構造、數據集加載優化等流程；</li><li>tokenizer 多進程詞頻統計，支持<code>sentencepiece</code>、<code>huggingface tokenizers</code>的 tokenizer 訓練；</li><li>預訓練支持任意位置斷點，可從斷點處繼續訓練;</li><li>大數據集（GB 級別）流式加載、支持緩衝區數據打亂，不利用內存、硬盤作為緩存，有效減少內存、磁盤佔用。配置<code>batch_size=1, max_len=320</code>下，最低支持在 16GB 內存+4GB 顯存的機器上進行預訓練；</li><li>訓練日誌記錄。</li></ul></li><li>SFT 微調：開源 SFT 數據集及數據處理過程。
<ul><li>自實現<code>trainer</code>支持 prompt 指令微調， 支持任意斷點繼續訓練；</li><li>支持<code>Huggingface trainer</code>的<code>sequence to sequence</code>微調；</li><li>支持傳統的低學習率，只訓練 decoder 層的微調。</li></ul></li><li>偏好優化：使用 DPO 進行全量偏好優化。
<ul><li>支持使用<code>peft lora</code>進行偏好優化；</li><li>支持模型合併，可將<code>Lora adapter</code>合併到原始模型中。</li></ul></li><li>支持下游任務微調：<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract">finetune_examples</a>給出<strong>三元組信息抽取任務</strong>的微調示例，微調後的模型對話能力仍在。</li></ul><p>如果需要做基於小模型的檢索增強生成（RAG），可以參考我的另一個項目<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese">Phi2-mini-Chinese</a>，代碼見<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese%2Fblob%2Fmain%2Frag_with_langchain.ipynb">rag_with_langchain.ipynb</a></p><p>🟢<strong>最近更新</strong></p><details><summary><b>2024-01-07</b></summary>
- 添加數據清洗過程中基於 mini hash 實現的文檔去重（在本項目中其實是數據集的樣本去重），防止模型遇到多次重複數據後，在推理時吐出訓練數據。<br>
- 添加`DropDatasetDuplicate`類實現對大數據集的文檔去重。<br></details><details><summary><b>2023-12-29</b></summary>
- 更新模型代碼（權重不變），可以直接使用`AutoModelForSeq2SeqLM.from_pretrained(...)`加載模型使用。<br>
- 更新 readme 文檔。<br></details><details><summary><b>2023-12-18</b></summary>
- 補充利用`ChatLM-mini-0.2B`模型微調下游三元組信息抽取任務代碼及抽取效果展示 。<br>
- 更新 readme 文檔。<br></details><details><summary><b>2023-12-14</b></summary>
- 更新 SFT、DPO 後的模型權重文件。 <br>
- 更新預訓練、SFT 及 DPO 腳本。 <br>
- 更新`tokenizer`為`PreTrainedTokenizerFast`。 <br>
- 重構`dataset`代碼，支持動態最大長度，每個批次的最大長度由該批次的最長文本決定，節省顯存。 <br>
- 補充`tokenizer`訓練細節。 <br></details><details><summary><b>2023-12-04</b></summary>
- 更新`generate`參數及模型效果展示。<br>
- 更新 readme 文檔。<br></details><details><summary><b>2023-11-28</b></summary>
- 更新 dpo 訓練代碼及模型權重。<br></details><details><summary><b>2023-10-19</b></summary>
- 項目開源， 開放模型權重供下載。 <br></details><h1><a id="user-content-二️chatlm-02b-chinese 模型訓練過程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%8C%EF%B8%8Fchatlm-02b-chinese%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>二、🛠️ChatLM-0.2B-Chinese 模型訓練過程</h1><h2><a id="user-content-21-預訓練數據集" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#21-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"></a>2.1 預訓練數據集</h2><p>所有數據集均來自互聯網公開的<strong>單輪對話</strong>數據集，經過數據清洗、格式化後保存為 parquet 文件。數據處理過程見<code>utils/raw_data_process.py</code>。主要數據集包括：</p><ol><li>社區問答 json 版 webtext2019zh-大規模高質量數據集，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbrightmart%2Fnlp_chinese_corpus">nlp_chinese_corpus</a>。共 410 萬，清洗後剩餘 260 萬。</li><li>baike_qa2019 百科類問答，見：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F107726">https://aistudio.baidu.com/datasetdetail/107726</a>，共 140 萬，清醒後剩餘 130 萬。</li><li>中國醫藥領域問答數據集，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FToyhom%2FChinese-medical-dialogue-data">Chinese-medical-dialogue-data</a>，共 79 萬，清洗後剩餘 79 萬。</li><li><del>金融行業問答數據，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F609821974">https://zhuanlan.zhihu.com/p/609821974</a>，共 77 萬，清洗後剩餘 52 萬。</del><strong>數據質量太差，未採用。</strong></li><li>知乎問答數據，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fwangrui6%2FZhihu-KOL">Zhihu-KOL</a>，共 100 萬行，清洗後剩餘 97 萬行。</li><li>belle 開源的指令訓練數據，介紹：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>，下載：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2FBelleGroup">BelleGroup</a>，僅選取<code>Belle_open_source_1M</code>、<code>train_2M_CN</code>、及<code>train_3.5M_CN</code>中部分回答較短、不含複雜表格結構、翻譯任務（沒做英文詞表）的數據，共 370 萬行，清洗後剩餘 338 萬行。</li><li>維基百科（Wikipedia）詞條數據，將詞條拼湊為提示語，百科的前<code>N</code>個詞為回答，使用<code>202309</code>的百科數據，清洗後剩餘 119 萬的詞條提示語和回答。Wiki 下載：<a href="https://gitee.com/link?target=https%3A%2F%2Fdumps.wikimedia.org%2Fzhwiki%2F">zhwiki</a>，將下載的 bz2 文件轉換為 wiki.txt 參考：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fapertium%2FWikiExtractor">WikiExtractor</a>。</li></ol><p>數據集總數量 1023 萬：Text-to-Text 預訓練集：930 萬，評估集：2.5 萬（因為解碼較慢，所以沒有把評估集設置太大）。<del>測試集：90 萬。</del>
SFT 微調和 DPO 優化數據集見下文。</p><h2><a id="user-content-22-模型" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#22-%E6%A8%A1%E5%9E%8B"></a>2.2 模型</h2><p>T5 模型（Text-to-Text Transfer Transformer），詳情見論文: <a href="https://gitee.com/link?target=https%3A%2F%2Farxiv.org%2Fabs%2F1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>。</p><p>模型源碼來自 huggingface，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftransformers%2Fblob%2Fmain%2Fsrc%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py%23L1557">T5ForConditionalGeneration</a>。</p><p>模型配置見<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese%2Fblob%2Fmain%2Fconfig.json">model_config.json</a>，官方的<code>T5-base</code>：<code>encoder layer</code>和<code>decoder layer </code>均為為 12 層，本項目這兩個參數修改為 10 層。</p><p>模型參數：0.2B。詞表大小：29298，僅包含中文和少量英文。</p><h2><a id="user-content-23-訓練過程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#23-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>2.3 訓練過程</h2><p>硬件：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 預訓練階段：</span></span><span id="LC2" class="line">CPU: 28 vCPU Intel<span class="o">(</span>R<span class="o">)</span> Xeon<span class="o">(</span>R<span class="o">)</span> Gold 6330 CPU @ 2.00GHz</span><span id="LC3" class="line">內存：60 GB</span><span id="LC4" class="line">顯卡：RTX A5000<span class="o">(</span>24GB<span class="o">)</span><span class="k">*</span> 2</span><span id="LC5" class="line"></span><span id="LC6" class="line"><span class="c"># sft 及 dpo 階段：</span></span><span id="LC7" class="line">CPU: Intel<span class="o">(</span>R<span class="o">)</span> i5-13600k @ 5.1GHz</span><span id="LC8" class="line">內存：32 GB</span><span id="LC9" class="line">顯卡：NVIDIA GeForce RTX 4060 Ti 16GB <span class="k">*</span> 1</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li><p><strong>tokenizer 訓練</strong>： 現有<code>tokenizer</code>訓練庫遇到大語料時存在 OOM 問題，故全量語料按照類似<code>BPE</code>的方法根據詞頻合併、構造詞庫，運行耗時半天。</p></li><li><p><strong>Text-to-Text 預訓練</strong>：學習率為<code>1e-4</code>到<code>5e-3</code>的動態學習率，預訓練時間為 8 天。訓練損失：</p></li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/train_loss.png" alt="traing loss" referrerpolicy="no-referrer"></p><ol start="3"><li><strong>prompt 監督微調（SFT）</strong>：使用<code>belle</code>指令訓練數據集（指令和回答長度都在 512 以下），學習率為<code>1e-7</code>到<code>5e-5</code>的動態學習率，微調時間 2 天。微調損失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/sft_loss.png" alt="finetune loss" referrerpolicy="no-referrer"></p><ol start="4"><li><strong>dpo 直接偏好優化</strong>：數據集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>作為<code>chosen</code>文本，步驟<code>2</code>中 SFT 模型對數據集中的 prompt 做批量<code>generate</code>，得到<code>rejected</code>文本，耗時 1 天，dpo 全量偏好優化，學習率<code>le-5</code>，半精度<code>fp16</code>,共<code>2</code>個<code>epoch</code>，耗時 3h。dpo 損失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/dpo_loss.png" alt="dpo loss" referrerpolicy="no-referrer"></p><h2><a id="user-content-24-對話效果展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#24-%E5%AF%B9%E8%AF%9D%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA"></a>2.4 對話效果展示</h2><h3><a id="user-content-241-stream-chat" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#241-stream-chat"></a>2.4.1 stream chat</h3><p>默認使用<code>huggingface transformers</code>的 <code>TextIteratorStreamer</code>實現流式對話，只支持<code>greedy search</code>，如果需要<code>beam sample</code>等其他生成方式，請將<code>cli_demo.py</code>的<code>stream_chat</code>參數修改為<code>False</code>。
<img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/stream_chat.gif" alt="" referrerpolicy="no-referrer"></p><h3><a id="user-content-242-對話展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#242-%E5%AF%B9%E8%AF%9D%E5%B1%95%E7%A4%BA"></a>2.4.2 對話展示</h3><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/show1.png" alt="" referrerpolicy="no-referrer"></p><p>存在問題：預訓練數據集只有 900 多萬，模型參數也僅 0.2B，不能涵蓋所有方面，會有答非所問、廢話生成器的情況。</p><h1><a id="user-content-三使用説明" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%89%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"></a>三、📑使用説明</h1><h2><a id="user-content-31-快速開始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#31-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"></a>3.1 快速開始：</h2><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="kn">from</span><span class="nn">transformers</span><span class="kn">import</span><span class="n">AutoTokenizer</span><span class="p">,</span><span class="n">AutoModelForSeq2SeqLM</span></span><span id="LC2" class="line"><span class="kn">import</span><span class="nn">torch</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="n">model_id</span><span class="o">=</span><span class="s">'charent/ChatLM-mini-Chinese'</span></span><span id="LC5" class="line"><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="k">if</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span><span class="k">else</span><span class="s">'cpu'</span><span class="p">)</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="n">tokenizer</span><span class="o">=</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span></span><span id="LC8" class="line"><span class="n">model</span><span class="o">=</span><span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span><span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="n">txt</span><span class="o">=</span><span class="s">'如何評價 Apple 這家公司？'</span></span><span id="LC11" class="line"></span><span id="LC12" class="line"><span class="n">encode_ids</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">([</span><span class="n">txt</span><span class="p">])</span></span><span id="LC13" class="line"><span class="n">input_ids</span><span class="p">,</span><span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">]),</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'attention_mask'</span><span class="p">])</span></span><span id="LC14" class="line"></span><span id="LC15" class="line"><span class="n">outs</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">my_generate</span><span class="p">(</span></span><span id="LC16" class="line"><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC17" class="line"><span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC18" class="line"><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span></span><span id="LC19" class="line"><span class="n">search_type</span><span class="o">=</span><span class="s">'beam'</span><span class="p">,</span></span><span id="LC20" class="line"><span class="p">)</span></span><span id="LC21" class="line"></span><span id="LC22" class="line"><span class="n">outs_txt</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outs</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">(),</span><span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></span><span id="LC23" class="line"><span class="k">print</span><span class="p">(</span><span class="n">outs_txt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Apple 是一家專注於設計和用户體驗的公司，其產品在設計上注重簡約、流暢和功能性，而在用户體驗方面則注重用户的反饋和使用體驗。作為一家領先的科技公司，蘋果公司一直致力於為用户提供最優質的產品和服務，不斷推陳出新，不斷創新和改進，以滿足不斷變化的市場需求。</span><span id="LC2" class="line">在 iPhone、iPad 和 Mac 等產品上，蘋果公司一直保持着創新的態度，不斷推出新的功能和設計，為用户提供更好的使用體驗。在 iPad 上推出的 iPad Pro 和 iPod touch 等產品，也一直保持着優秀的用户體驗。</span><span id="LC3" class="line">此外，蘋果公司還致力於開發和銷售軟件和服務，例如 iTunes、iCloud 和 App Store 等，這些產品在市場上也獲得了廣泛的認可和好評。</span><span id="LC4" class="line">總的來説，蘋果公司在設計、用户體驗和產品創新方面都做得非常出色，為用户帶來了許多便利和驚喜。</span><span id="LC5" class="line"></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-32-從克隆倉庫代碼開始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#32-%E4%BB%8E%E5%85%8B%E9%9A%86%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E5%BC%80%E5%A7%8B"></a>3.2 從克隆倉庫代碼開始</h2><p>本項目模型為<code>TextToText</code>模型，在預訓練階段、SFT 階段、RLFH 階段的<code>prompt</code>、<code>response</code>等字段，請務必加上<code>[EOS]</code>句子結束標記。<br>
本項目模型為<code>TextToText</code>模型，在預訓練階段、SFT 階段、RLFH 階段的<code>prompt</code>、<code>response</code>等字段，請務必加上<code>[EOS]</code>句子結束標記。<br>
本項目模型為<code>TextToText</code>模型，在預訓練階段、SFT 階段、RLFH 階段的<code>prompt</code>、<code>response</code>等字段，請務必加上<code>[EOS]</code>句子結束標記。</p><h3><a id="user-content-321-克隆項目" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#321-%E5%85%8B%E9%9A%86%E9%A1%B9%E7%9B%AE"></a>3.2.1 克隆項目：</h3><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone <span class="nt">--depth</span> 1 https://github.com/charent/ChatLM-mini-Chinese.git</span><span id="LC2" class="line"></span><span id="LC3" class="line"><span class="nb">cd </span>ChatLM-mini-Chinese</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-322-安裝依賴" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#322-%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"></a>3.2.2 安裝依賴</h3><p>本項目推薦使用<code>python 3.10</code>，過老的 python 版本可能不兼容所依賴的第三方庫。</p><p>pip 安裝：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pip <span class="nb">install</span><span class="nt">-r</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果 pip 安裝了 CPU 版本的 pytorch，可以通過下面的命令安裝 CUDA 版本的 pytorch：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># pip 安裝 torch + cu118</span></span><span id="LC2" class="line">pip3 <span class="nb">install </span>torch <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu118</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>conda 安裝：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">conda <span class="nb">install</span><span class="nt">--yes</span><span class="nt">--file</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-323-下載預訓練模型及模型配置文件" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#323-%E4%B8%8B%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"></a>3.2.3 下載預訓練模型及模型配置文件</h3><p>用<code>git</code>命令從<code>Hugging Face Hub</code>下載模型權重及配置文件，需要先安裝<a href="https://gitee.com/link?target=https%3A%2F%2Fdocs.github.com%2Fzh%2Frepositories%2Fworking-with-files%2Fmanaging-large-files%2Finstalling-git-large-file-storage">Git LFS</a>，然後運行:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 使用 git 命令下載 huggingface 模型，先安裝[Git LFS]，否則下載的模型文件不可用</span></span><span id="LC2" class="line">git clone <span class="nt">--depth</span> 1 https://huggingface.co/charent/ChatLM-mini-Chinese</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="nb">mv </span>ChatLM-mini-Chinese model_save</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>也可以直接從<code>Hugging Face Hub</code>倉庫<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>手工下載，將下載的文件移動到<code>model_save</code>目錄下即可。</p><h2><a id="user-content-33-tokenizer 訓練" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#33-tokenizer%E8%AE%AD%E7%BB%83"></a>3.3 Tokenizer 訓練</h2><p>原本打算直接用現成的<code>tokenizer</code>庫訓練的（如<code>sentencepiece</code>），但是數據集一大就容易 OOM。另外預訓練數據集各個領域的語料不平衡，會產生很多不必要的合併。最後使用<code>jieba</code>分詞對所有的預訓練語料切詞後統計詞頻，只保留出現 1500 次以上的字、詞，參照<code>PreTrainedTokenizerFast</code>的<code>BPE model</code>的保存格式，構造<code>tokenzier</code>，最後轉換為<code>PreTrainedTokenizerFast</code>。核心代碼如下，詳細的處理過程見<code>utils/train_tokenizer.py</code>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1"># 構造 merge 數組</span></span><span id="LC2" class="line"><span class="n">words_merge_list</span><span class="o">=</span><span class="p">[]</span></span><span id="LC3" class="line"><span class="k">for</span><span class="n">word</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span></span><span id="LC4" class="line"><span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span></span><span id="LC5" class="line"><span class="k">if</span><span class="n">n</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">:</span></span><span id="LC6" class="line"><span class="c1"># a, b 切分 12345 示例： 1 2345,  12 345,   123 45,   1234 5</span></span><span id="LC7" class="line"><span class="k">for</span><span class="n">i</span><span class="ow">in</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span></span><span id="LC8" class="line"><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]),</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="p">])</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="k">if</span><span class="n">a</span><span class="ow">in</span><span class="n">words_dict</span><span class="ow">and</span><span class="n">b</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">:</span></span><span id="LC11" class="line"><span class="n">words_merge_list</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>本項目還提供了使用預訓練模型自帶的<code>tokenizer</code>根據自己的語料重新訓練<code>tokenizer</code>的例子，見<code>train_tokenizer.ipynb</code>。注意，重新訓練<code>tokenizer</code>後，預訓練模型的權重將無法使用，需要重新訓練模型權重，因為<code>token</code>對應的<code>id</code>變了。</p><h2><a id="user-content-34-text-to-text-預訓練" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#34-text-to-text-%E9%A2%84%E8%AE%AD%E7%BB%83"></a>3.4 Text-to-Text 預訓練</h2><ol><li>預訓練數據集示例</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"對於花園街，你有什麼瞭解或看法嗎？"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"花園街（是香港油尖旺區的一條富有特色的街道，位於九龍旺角東部，北至界限街，南至登打士街，與通菜街及洗衣街等街道平行。現時這條街道是香港著名的購物區之一。位於亞皆老街以南的一段花園街，也就是</span><span class="se">\"</span><span class="s2">波鞋街</span><span class="se">\"</span><span class="s2">整條街約 150 米長，有 50 多間售賣運動鞋和運動用品的店舖。旺角道至太子道西一段則為排檔區，售賣成衣、蔬菜和水果等。花園街一共分成三段。明清時代，花園街是芒角村栽種花卉的地方。此外，根據歷史專家鄭寶鴻的考證：花園街曾是 1910 年代東方殷琴拿煙廠的花園。縱火案。自 2005 年起，花園街一帶最少發生 5 宗縱火案，當中 4 宗涉及排檔起火。2010 年。2010 年 12 月 6 日，花園街 222 號一個賣鞋的排檔於凌晨 5 時許首先起火，濃煙湧往旁邊住宅大廈，消防接報 4"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li><p>jupyter-lab 或者 jupyter notebook:</p><p>見文件<code>train.ipynb</code>，推薦使用 jupyter-lab，避免考慮與服務器斷開後終端進程被殺的情況。</p></li><li><p>控制枱：</p><p>控制枱訓練需要考慮連接斷開後進程被殺的，推薦使用進程守護工具<code>Supervisor</code>或者<code>screen</code>建立連接會話。</p><p>首先要配置<code>accelerate</code>，執行以下命令， 根據提示選擇即可，參考<code>accelerate.yaml</code>，<em>注意：DeepSpeed 在 Windows 安裝比較麻煩</em>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">accelerate config</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>開始訓練，如果要使用工程提供的配置請在下面的命令<code>accelerate launch</code>後加上參數<code>--config_file ./accelerate.yaml</code>，<em>該配置按照單機 2xGPU 配置。</em></p><p><em>預訓練有兩個腳本，本項目實現的 trainer 對應<code>train.py</code>，huggingface 實現的 trainer 對應<code>pre_train.py</code>，用哪個都可以，效果一致。本項目實現的 trainer 訓練信息展示更美觀、更容易修改訓練細節（如損失函數，日誌記錄等），均支持斷點繼續訓練，本項目實現的 trainer 支持在任意位置斷點後繼續訓練，按<code>ctrl+c</code>退出腳本時會保存斷點信息。</em></p><p>單機單卡：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer</span></span><span id="LC2" class="line">accelerate launch ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">python pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>單機多卡：
<code>2</code>為顯卡數量，請根據自己的實際情況修改。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>從斷點處繼續訓練：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train <span class="nt">--is_keep_training</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line"><span class="c"># 需要在`pre_train.py`中的`train`函數添加`resume_from_checkpoint=True`</span></span><span id="LC6" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li></ol><h2><a id="user-content-35-sft 微調" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#35-sft%E5%BE%AE%E8%B0%83"></a>3.5 SFT 微調</h2><p>SFT 數據集全部來自<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>大佬的貢獻，感謝。SFT 數據集分別為：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Fgenerated_chat_0.4M">generated_chat_0.4M</a>、<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_0.5M_CN">train_0.5M_CN</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_2M_CN">train_2M_CN</a>，清洗後剩餘約 137 萬行。
sft 指令微調數據集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"解釋什麼是歐洲啓示錄"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"歐洲啓示錄（The Book of Revelation）是新約聖經的最後一卷書，也被稱為《啓示錄》、《默示錄》或《約翰默示錄》。這本書從宗教的角度描述了世界末日的來臨，以及上帝對世界的審判和拯救。 書中的主題包括來臨的基督的榮耀，上帝對人性的懲罰和拯救，以及魔鬼和邪惡力量的存在。歐洲啓示錄是一個充滿象徵和暗示的文本，對於解讀和理解有許多不同的方法和觀點。"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>參考<code>data</code>目錄下的示例<code>parquet</code>文件製作自己的數據集，數據集格式：<code>parquet</code>文件分兩列，一列<code>prompt</code>文本，表示提示語，一列<code>response</code>文本，表示期待的模型輸出。
微調細節見<code>model/trainer.py</code>下的<code>train</code>方法, <code>is_finetune</code>設置為<code>True</code>時，將進行微調，微調默認會凍結 embedding 層和 encoder 層，只訓練 decoder 層。如需要凍結其他參數，請自行調整代碼。</p><p>運行 SFT 微調：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer， 添加參數`--is_finetune=True`即可, 參數`--is_keep_training=True`可從任意斷點處繼續訓練</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py <span class="nt">--is_finetune</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer, 多 GPU 請用 accelerate launch --multi_gpu --num_processes gpu 個數 sft_train.py</span></span><span id="LC5" class="line">python sft_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-36-rlhf 強化學習人類反饋優化方法" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#36-rlhf%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"></a>3.6 RLHF（強化學習人類反饋優化方法）</h2><p>偏好方法這裏介紹常見的兩種：PPO 和 DPO，具體實現請自行搜索論文及博客。</p><ol><li><p>PPO 方法（近似偏好優化,Proximal Policy Optimization）<br>
步驟 1：使用微調數據集做有監督微調（SFT， Supervised Finetuning）。<br>
步驟 2：使用偏好數據集（一個 prompt 至少包含 2 個回覆，一個想要的回覆，一個不想要的回覆。多個回覆可以按照分數排序，最想要的分數最高）訓練獎勵模型（RM， Reward Model）。可使用<code>peft</code>庫快速搭建 Lora 獎勵模型。<br>
步驟 3：利用 RM 對 SFT 模型進行有監督 PPO 訓練，使得模型滿足偏好。</p></li><li><p>使用 DPO（直接偏好優化，Direct Preference Optimization）微調（<strong>本項目採用 DPO 微調方法，比較節省顯存</strong>）
在獲得 SFT 模型的基礎上，無需訓練獎勵模型，取得正向回答（chosen）和負向回答（rejected）即可開始微調。微調的<code>chosen</code>文本來自原數據集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>，拒絕文本<code>rejected</code>來自 SFT 微調 1 個 epoch 後的模型輸出，另外兩個數據集：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FSkepsun%2Fhuozi_rlhf_data_json">huozi_rlhf_data_json</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fbeyond%2Frlhf-reward-single-round-trans_chinese">rlhf-reward-single-round-trans_chinese</a>，合併後共 8 萬條 dpo 數據。</p><p>dpo 數據集處理過程見<code>utils/dpo_data_process.py</code>。</p></li></ol><p>DPO 偏好優化數據集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="w"></span><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"為給定的產品創建一個創意標語。，輸入：可重複使用的水瓶。"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"chosen"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">保護地球，從擁有可重複使用的水瓶開始！</span><span class="se">\"</span><span class="s2">"</span><span class="p">,</span></span><span id="LC4" class="line"><span class="w"></span><span class="nl">"rejected"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">讓你的水瓶成為你的生活伴侶，使用可重複使用的水瓶，讓你的水瓶成為你的夥伴</span><span class="se">\"</span><span class="s2">"</span></span><span id="LC5" class="line"><span class="w"></span><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>運行偏好優化：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c">#  多 GPU 請用 accelerate launch --multi_gpu --num_processes gpu 個數 dpo_train.py</span></span><span id="LC2" class="line">python dpo_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-37-推理" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#37-%E6%8E%A8%E7%90%86"></a>3.7 推理</h2><p>確保<code>model_save</code>目錄下有以下文件，這些文件都可以在<code>Hugging Face Hub</code>倉庫<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>中找到：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">ChatLM-mini-Chinese</span><span id="LC2" class="line">├─model_save</span><span id="LC3" class="line">|  ├─config.json</span><span id="LC4" class="line">|  ├─configuration_chat_model.py</span><span id="LC5" class="line">|  ├─generation_config.json</span><span id="LC6" class="line">|  ├─model.safetensors</span><span id="LC7" class="line">|  ├─modeling_chat_model.py</span><span id="LC8" class="line">|  ├─special_tokens_map.json</span><span id="LC9" class="line">|  ├─tokenizer.json</span><span id="LC10" class="line">|  └─tokenizer_config.json</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li>控制枱運行：</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python cli_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li>API 調用</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python api_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>API 調用示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">curl <span class="nt">--location</span><span class="s1">'127.0.0.1:8812/api/chat'</span><span class="se">\</span></span><span id="LC2" class="line"><span class="nt">--header</span><span class="s1">'Content-Type: application/json'</span><span class="se">\</span></span><span id="LC3" class="line"><span class="nt">--header</span><span class="s1">'Authorization: Bearer Bearer'</span><span class="se">\</span></span><span id="LC4" class="line"><span class="nt">--data</span><span class="s1">'{</span></span><span id="LC5" class="line"><span class="s1">    "input_txt": "感冒了要怎麼辦"</span></span><span id="LC6" class="line"><span class="s1">}'</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/api_example.png" alt="api demo" referrerpolicy="no-referrer"></p><h2><a id="user-content-38-下游任務微調" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#38-%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83"></a>3.8 下游任務微調</h2><p>這裏以文本中三元組信息為例，做下游微調。該任務的傳統深度學習抽取方法見倉庫<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。抽取出一段文本中所有的三元組，如句子<code>《寫生隨筆》是冶金工業 2006 年出版的圖書，作者是張來亮</code>，抽取出三元組<code>(寫生隨筆,作者,張來亮)</code>和<code>(寫生隨筆,出版社,冶金工業)</code>。</p><p>原始數據集為：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F11384">百度三元組抽取數據集</a>。加工得到的微調數據集格式示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"請抽取出給定句子中的所有三元組。給定句子：《家鄉的月亮》是宋雪萊演唱的一首歌曲，所屬專輯是《久違的哥們》"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"[(家鄉的月亮,歌手,宋雪萊),(家鄉的月亮,所屬專輯,久違的哥們)]"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>可以直接使用<code>sft_train.py</code>腳本進行微調，腳本<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract/finetune_IE_task.ipynb">finetune_IE_task.ipynb</a>裏麪包含詳細的解碼過程。訓練數據集約<code>17000</code>條，學習率<code>5e-5</code>，訓練 epoch<code>5</code>。微調後其他任務的對話能力也沒有消失。</p><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/ie_task_chat.png" alt="信息抽取任務微調後的對話能力" referrerpolicy="no-referrer"></p><p>微調效果：
將<code>百度三元組抽取數據集</code>公開的<code>dev</code>數據集作為測試集，對比傳統方法<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。</p><table><thead><tr><th align="left">模型</th><th align="center">F1 分數</th><th align="center">精確率 P</th><th align="center">召回率 R</th></tr></thead><tbody><tr><td align="left">ChatLM-Chinese-0.2B 微調</td><td align="center">0.74</td><td align="center">0.75</td><td align="center">0.73</td></tr><tr><td align="left">ChatLM-Chinese-0.2B 無預訓練</td><td align="center">0.51</td><td align="center">0.53</td><td align="center">0.49</td></tr><tr><td align="left">傳統深度學習方法</td><td align="center">0.80</td><td align="center">0.79</td><td align="center">80.1</td></tr></tbody></table><p>備註：<code>ChatLM-Chinese-0.2B 無預訓練</code>指直接初始化隨機參數，開始訓練，學習率<code>1e-4</code>，其他參數和微調一致。</p><h1><a id="user-content-四引用" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E5%9B%9B%E5%BC%95%E7%94%A8"></a>四、🎓引用</h1><p>如果你覺得本項目對你有所幫助，歡迎引用。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">@<span class="n">misc</span>{<span class="n">Charent2023</span>,</span><span id="LC2" class="line"><span class="n">author</span>={<span class="n">Charent</span><span class="n">Chen</span>},</span><span id="LC3" class="line"><span class="n">title</span>={<span class="n">A</span><span class="n">small</span><span class="n">chinese</span><span class="n">chat</span><span class="n">language</span><span class="n">model</span><span class="n">with</span><span class="m">0</span>.<span class="m">2</span><span class="n">B</span><span class="n">parameters</span><span class="n">base</span><span class="n">on</span><span class="n">T5</span>},</span><span id="LC4" class="line"><span class="n">year</span>={<span class="m">2023</span>},</span><span id="LC5" class="line"><span class="n">publisher</span> = {<span class="n">GitHub</span>},</span><span id="LC6" class="line"><span class="n">journal</span> = {<span class="n">GitHub</span><span class="n">repository</span>},</span><span id="LC7" class="line"><span class="n">howpublished</span> = {\<span class="n">url</span>{<span class="n">https</span>://<span class="n">github</span>.<span class="n">com</span>/<span class="n">charent</span>/<span class="n">ChatLM</span>-<span class="n">mini</span>-<span class="n">Chinese</span>}},</span><span id="LC8" class="line">}</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h1><a id="user-content-五其他事項" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%94%E5%85%B6%E4%BB%96%E4%BA%8B%E9%A1%B9"></a>五、🤔其他事項</h1><p>本項目不承擔開源模型和代碼導致的數據安全、輿情風險或發生任何模型被誤導、濫用、傳播、不當利用而產生的風險和責任。</p>
]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:15:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/charent/ChatLM-mini-Chinese</guid>
            <link>https://gitee.com/charent/ChatLM-mini-Chinese</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | vivo 海量微服務架構最新實踐]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section style="font-size: 15px;line-height: 1.6;"><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(219, 219, 219);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgba(0, 0, 0, 0.5);font-size: 14px;text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">作者：來自 vivo 互聯網中間件團隊</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="text-align: left;justify-content: flex-start;display: flex;flex-flow: row;margin-bottom: 10px;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;align-self: flex-start;flex: 0 0 auto;background-color: rgb(234, 241, 255);border-style: solid;border-width: 0px 0px 0px 4px;border-color: rgb(48, 97, 207) rgb(48, 97, 207) rgb(48, 97, 207) rgb(21, 151, 239);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">本文根據羅亮老師在「2023 vivo 開發者大會"現場演講內容整理而成。公眾號回覆【2023 VDC】獲取互聯網技術分會場議題相關資料。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: left;" powered-by="xiumi.us"><section style="text-align: justify;line-height: 1.8;padding-right: 5px;padding-left: 5px;color: rgb(160, 160, 160);"><p style="text-wrap: wrap;">vivo 微服務平台為全球 5 億+用户背後的全網十萬級機器、萬級微服務提供服務，在高效實踐過程中，vivo 中間件平台團隊輸出了一套業務適用的微服務架構最佳實踐--架構能力矩陣、高效的開源中間件組件全生命週期管理策略，走出了一條從開源到開源+自研的技術演進路徑，通過微服務引擎升級和統一平台建設較好解決了面臨的問題與挑戰。</p></section></section><section style="margin-right: 0%;margin-bottom: -5px;margin-left: 0%;text-align: right;line-height: 1;font-size: 5px;transform: translate3d(5px, 0px, 0px);" powered-by="xiumi.us"><section style="width: 0px;display: inline-block;vertical-align: top;border-bottom: 0.6em solid rgb(160, 160, 160);border-right: 0.6em solid rgb(160, 160, 160);border-top: 0.6em solid transparent !important;border-left: 0.6em solid transparent !important;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>一、vivo 從 0 到 1 的微服務架構工程實踐</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.1 為什麼需要微服務及落地挑戰</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">伴隨業務的高速發展，業務的複雜度越來越高，用户規模和訪問量也越來越大；項目的迭代速度越來越快，交付效率要求也越來越高。與此同時，服務的集羣規模越來越大，部署架構越來越複雜，故障範圍也越來越不可控。此外，突增的業務流量時刻考驗着服務的水平擴容能力，創新業務的快速孵化也對服務的可擴展性提出了更高的要求。想要解決以上問題，業務架構會朝着微服務架構方向演進。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014317" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/0efbe187-74ec-4c6c-9f03-665534c87cb6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">正是在這樣的背景下，vivo 於 2015 年開始微服務架構改造，在落地過程中碰到了以下問題：</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>一是</strong>：服務數量多，配置修改生效、服務發佈等變更場景效率低下；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>二是</strong>：業務鏈路長，高可用保障難，問題與故障定位耗時長，服務的維護成本高；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>三是</strong>：大量的跨服務通訊，性能和訪問體驗優化提升難度大；</p><p style="text-wrap: wrap;"><strong>四是</strong>：一個業務鏈路涉及大量的上下游團隊，對接溝通的協作成本高；</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">為瞭解決以上落地過程中的開發、運維、團隊協作等難題，我們需要建設配套的微服務架構技術體系。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.2 vivo 微服務架構最佳實踐-架構能力矩陣</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">建設一套微服務架構技術體系，助力業務又快又好地構建微服務工程，需要哪些技術能力？我們對微服務架構的主要業務場景進行了分析，在業務實踐過程中，微服務主要會涉及同步、異步、定時任務三大核心業務場景。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014318" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/b246fe36-cacc-49af-a105-9c44ade187ea.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>同步調用</strong>場景：涉及的技術能力主要是 RPC 框架、註冊中心、服務治理；</p><p style="margin-bottom: 10px;">在<strong>異步調用</strong>場景：涉及的技術能力主要是消息中間件；</p><p>在<strong>定時任務</strong>場景：涉及的技術能力主要是分佈式任務調度。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了上面介紹的框架和系統，業務在微服務架構改造過程中，需要的能力全貌是怎樣的？</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在深度參與業務微服務架構改造過程中，我們對最佳實踐能力項進行了抽象，從而形成了 vivo 內部的微服務架構最佳實踐總結-架構能力矩陣，總計近 30 項能力。為了更直觀的呈現這些能力，我們從接入層、服務層、數據層的三層架構分層，開發、運維等 DevOps 的關鍵環節對架構能力進行了梳理，如下圖所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014319" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d1fb2470-8f85-4233-b3f2-010196469737.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;"><br></p></section><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在開發環節：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>在開發接口時</strong>，我們要實現內外網接口分離，保障接口的安全性，為此我們要接入網關來隔離內外網接口；在接入層和服務層，我們可以通過治理平台來實現限流、熔斷、降級等能力，保障業務的高可用。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在構建內部服務時</strong>，我們要儘可能實現服務無狀態，通過 RPC 框架實現內部接口的 RPC 相互調用，具備異常重試能力，提升服務的魯棒性；在編碼過程中，我們通過接入配置中心實現代碼與配置分離，具備運行時動態調整配置的能力，提高服務的變更效率。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在異步調用場景</strong>，我們可以通過接入消息中間件實現業務間的相互解耦、流量削峯；在定時任務場景，我們可以通過分佈式任務調度系統，實現失敗任務的自動轉移能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，我們可以通過落地存儲與計算分離能力，實現服務層和數據層的解耦，便於分層擴容，具備面向未來更大規模業務的擴展能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在數據層</strong>，通過落地讀寫分離、冷熱分離等能力，提升系統性能，節省存儲成本；同時將這些能力通過研發框架進行封裝，便於業務側複用。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在運維環節：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">我們可以藉助 CDN 實現網站的動靜分離訪問，減小系統的請求壓力；在日常運維過程中，我們要實現服務的可灰度、可回滾；服務節點無單點；同時藉助容器技術快速實現彈性伸縮能力；提升系統的故障恢復速度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在部署時</strong>，通過部署與發佈分離，可以較好規避發佈變更時產生的問題，即服務部署成功，並且健康檢查通過後再發布到生產環境，減小故障的影響範圍。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在遇到嚴重的系統故障時</strong>，需要具備使用備份數據從零恢復的能力，同時對所有已知的故障場景要有對應的預案，提升系統的故障應對能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在數據運維上</strong>，我們要確保數據屬主唯一，避免多個業務對同一個數據庫進行訪問；同時也要實現業務數據和大數據的存儲隔離，避免相互影響。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了以上能力之外，我們<strong>還要</strong>實現業務的安全合規，建設覆蓋 Metric、Trace、Log 的可觀測能力體系，便於對故障問題的定位排查；在多機房層面，需要具備同城雙活、異地多活等跨機房容災能力。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.3 vivo 微服務平台能力</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">為了更好落地以上最佳實踐，我們構建了一套從接入層、服務層、消息層、框架層到存儲層的平台能力，完整的平台能力地圖如下圖所示：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014320" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/3b0a6fb7-b253-48eb-921d-5056d3d9204a.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>接入層</strong>，我們提供了四層流量網關和七層微服務 API 網關；在服務層提供了服務/流量治理平台、配置中心、註冊中心、接口管理平台、分佈式任務調度等系統。</p><p style="margin-bottom: 10px;">在<strong>消息層</strong>提供了消息中間件；在框架層提供了腳手架，可快速集成日誌、配置、限流/熔斷、MySQL/Redis 等 SDK，以及 RPC 框架。</p><p style="margin-bottom: 10px;">在<strong>存儲層</strong>提供了 DaaS 平台，包含 MySQL、Redis、ElasticSearch、MongoDB、文件服務等系統能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">為了更好排查故障問題，我們在可觀測領域構建了監控中心、日誌中心、調用鏈等系統；此外，還有更好支撐服務構建、變更發佈的 CICD 系統和 IT 基礎設施的配置管理系統 CMDB。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">截止 2019 年，vivo 基本完成了從 0 到 1 的微服務平台能力煙囱式建設。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">快速構建這些能力的過程，離不開開源組件的賦能。例如微服務 API 網關背後的 zuul，註冊中心背後的 ZooKeeper 和 etcd，RPC 框架的 Dubbo 和 bRPC；配置中心的 Apollo 和 Nacos，流量治理的 hystrix 和 sentinel，消息中間件的 RabbitMQ 和 RocketMQ，任務調度的 xxl-job；如下圖所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014321" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d29efb2f-0028-42ec-b597-77c064aef74c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">在此，我們也通過 VDC(vivo 開發者大會) 平台，感謝開源社區的賦能，助力 vivo 微服務架構技術體系從 0 到 1 的快速構建。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.4 vivo 微服務現狀</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">截止當前，vivo 的微服務平台為全球分佈在 60+個國家/地區的 5 億+用户提供服務；其中 vivo 現有萬級的微服務，覆蓋全網機器規模十萬級，每天處理高達 8000 億次的 RPC 調用次數，流量的峯值 QPS 達到千萬級以上。<span style="text-align: center;letter-spacing: 0.034em;"></span></p><p><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014323" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/89e7af31-f3b1-48ab-8291-f6bdfc728e87.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在支撐如此規模的微服務過程中，特別是在 2020 年以後，我們碰到了較多的問題與挑戰，為瞭解決這些問題，我們使用了微服務引擎升級和統一平台建設的解決方案；下面來一起看看我們碰到了哪些問題與挑戰？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>二、微服務引擎升級與統一平台建設</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.1 面臨的問題與挑戰</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我們知道，註冊中心和配置中心是微服務架構領域的技術基石；下面給大家説明下我們在這兩個基石系統實踐過程中遇到的<strong>問題與挑戰</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014324" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/fd67ab5a-9617-4bd1-8251-85c51a7af775.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是註冊中心，眾所周知，ZK 是 CP 特性，在註冊中心場景有較多不可用的問題，此外還有跨機房多活能力缺失，集羣故障半徑大等問題；寫性能無法水平擴展，在大規模 Dubbo 服務場景中，接口級註冊模型註冊的數據量大，在業務高頻變更期間網卡的帶寬峯值會超過 1000Gbps。此外還有業務易混用，功能缺失；內部的多個技術棧使用不同的註冊中心，跨技術棧調用的研發運維成本高等問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在配置中心場景，存在應用、組件配置的變更通道不統一，故障場景配置回滾慢，變更審計日誌分散，業務恢復耗時長等問題；配置變更下發的時效不滿足業務要求，內部存在多套配置中心，都需要和業務研發流程打通，存在審批、審計、回滾等功能沒有對齊的問題；此外在功能和安全上，還需要實現內部的配置定時生效，配置加解密等需求，配置訪問通道符合公司的安全要求。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">從以上的問題與挑戰中可以看出，基於開源組件快速構建的微服務底層引擎在 vivo 的內部業務場景中存在較多的可用性、性能&amp;容量、研發運維、功能&amp;安全問題與挑戰。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="font-size: 16px;color: rgb(65, 95, 255);" powered-by="xiumi.us"><p style="text-wrap: wrap;">2.2 註冊中心引擎升級</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">為瞭解決以上的問題與挑戰，我們需要進行技術升級，首先給大家介紹的是註冊中心的<strong>解決方案</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014325" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bc83c41-6a74-4125-9924-22bc26ac900c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">針對 Dubbo 接口級服務發現導致 ZK 註冊中心流量過大的問題，業界同行都在往應用級服務發現遷移來構建解決方案；通過 Dubbo 開源社區官網的介紹，我們可以看到，應用級服務發現是適應雲原生，支持更大規模的服務發現模型；</p><p style="margin-bottom: 10px;">將 Dubbo 接口級服務發現模型升級為應用級，可降低單機 50% 的內存消耗，降低註冊中心集羣 90% 的存儲與推送壓力，從架構上支持百萬實例集羣規模；</p><p>因此我們需要將 Dubbo 框架服務發現模型從接口級升級為應用級，徹底解決註冊數據量大，對註冊中心請求壓力大的問題，同時具備面向雲原生微服務架構的擴展能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，針對註冊中心的可用性、性能&amp;容量、研發運維等問題，我們需要建設滿足 AP 特性、支持跨機房多活的統一註冊中心，使用 Session+Data 分離架構，Data 層持久化數據，Session 層處理和客户端的長連接，無狀態 Session 層能較好收斂客户端請求，實現讀寫流量隔離，具備較好的橫向擴展能力，真正解決註冊中心的性能、容量和擴展性問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>綜上</strong>，我們需要構建 Dubbo 應用級服務發現能力，構建 Session+Data 分離的統一註冊中心，內部的項目代號為 vns。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">從上面的技術方案分析中，我們可以看到，通過應用級註冊可以徹底解決註冊中心的流量突刺問題；通過 Session+Data 雙層分離架構可以實現業務無感知的多集羣拆分，有效縮小故障半徑，那如何來<strong>落地</strong>呢？</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014326" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/444085d9-cd4f-491a-b55b-5ec48ccb6c28.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">我們首先想到的就是上圖左側的技術方案，通過構建暴露 gRPC 協議、支持應用級註冊的 vns 系統，海量的 Dubbo 服務通過雙註冊來實現遷移；但是在經過詳細的技術分析之後，我們發現該方案存在明顯的<strong>耦合問題：</strong></p><p style="margin-bottom: 10px;">首先是 Dubbo 應用級註冊升級的進展依賴 vns 系統的建設進度，Dubbo 框架依賴穩定的 vns SDK，Dubbo 框架和 vns 系統之間存在進度依賴問題；</p><p style="margin-bottom: 10px;">其次還存在回滾依賴問題，當 vns 系統因灰度異常回滾時，Dubbo 應用級註冊升級進度也會同步回滾；</p><p style="margin-bottom: 10px;">同理當 Dubbo 流量切換異常回滾時，vns 的業務接入進度也會回退。</p><p style="margin-bottom: 10px;">此外，部分不迭代的業務可能需要繼續使用接口級註冊，無法實現 ZK 註冊中心的完全下線。</p><p>為瞭解決以上問題，我們對技術方案進行了升級，改用通過 vns 系統暴露和支持 ZK 協議，實現 Dubbo 應用級註冊升級和 vns 系統的能力建設解耦；當 vns 系統的能力建設進展還未達到生產環境要求時，我們可以通過引入一套新的 ZK 集羣來支持 Dubbo 的應用級註冊模型升級；當 vns 的能力成熟度達到生產環境的要求後，可以對引入的 ZK 集羣進行替代，整個過程可以根據系統建設進展和可用性保障要求，進行可控的灰度放量和回滾操作，控制變更風險；最終，vns 通過暴露 ZK+gRPC 雙協議滿足業務的接入訴求。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在整個技術方案落地過程中，我們始終堅持業務導向原則，實現業務升級和遷移的零|低成本；採用穩妥、完善的升級遷移方案，確保過程可灰度、可回滾、可觀測；大家可以看到，我們通過兼容 ZK 協議，最大限度的保障 Dubbo 業務的平滑升級，切換方案做到了可灰度可回滾可觀測，在減少升級成本的同時，降低項目落地風險，最終實現 ZK 註冊中心的完全下線。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.3 配置中心引擎升級</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介紹完註冊中心，我們再來看看配置中心的解決方案，配置中心主要解決的是配置通道不統一，性能不達標，無法滿足內部的業務需求等問題。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014327" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/4591d606-98ae-4070-823d-8aff13aa8f98.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">上圖左側是我們最新的配置中心技術架構圖，右側是統一配置通道的示意圖，我們通過支持應用配置與組件配置的統一配置通道，實現了配置管理能力的收斂統一，在此基礎上，建設一鍵審批/審計/回滾等能力，實現了和內部業務研發流程的打通，減少人力運維投入；此外，在新版配置中心上，我們也實現了較多的高可用、性能、安全、可觀測能力增強等業務訴求；在配置中心升級過程中，我們追求業務的無感知升級，通過兼容原有配置中心對外開放的接口，實現了新系統的平滑升級，原有系統優雅下線。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，和註冊中心的升級方案類似，在配置中心的技術方案設計中，我們也較好的遵循了業務導向原則。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.4 統一微服務平台建設</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介紹完註冊中心和配置中心等微服務引擎的技術升級方案，我們再來看下從 0 到 1 快速構建的煙囱式微服務平台會面臨哪些問題和挑戰？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014328" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/2dcc08b7-f604-43a4-85d3-b7c2ae6eedf3.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">從上圖左側示意圖中可以看到，我們快速構建的微服務平台存在 10 個以上的模塊，每個模塊都有獨立的入口，用户使用平台的易用性很低；此外，這些模塊在建設過程中，還需要重複對接雲平台、單點登錄、權限、工單、監控、CMDB 等公共服務系統；系統審計日誌分散，不便於快速定位因變更引起的問題；綜上，煙囱式微服務平台存在多入口，功能重複對接，運維、研發成本高，故障排查與恢復效率低，易用性不足等問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">要解決煙囱式微服務平台的問題，需要構建更合理的產品方案，我們對用户的使用現狀進行了分析：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014329" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/f3ad12b5-778a-41ec-9496-bd59e3ab55a6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">通過系統埋點數據發現，煙囱式微服務平台中用户使用頻率最高的兩個系統分別是配置中心、服務治理。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">通過上圖左側的 PV/UV 餅狀圖數據，大家可以發現：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: center;"><p style="text-align: left;">配置中心的用户訪問主要集中在配置的【查詢與變更】、【變更記錄與審批】和配置變更相關的 2 個頁面上，服務治理的用户訪問主要集中在【服務概覽】、【服務查詢】和服務相關的 2 個頁面上。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">基於埋點數據，我們可以看到用户的訪問集中在少數的幾個功能上，通過整合各個系統模塊高頻使用的功能，建設統一的平台入口，實現系統間聯動，這也給我們如何建設統一平台提供了較好的思路。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，在對各個模塊的技術架構進行分析時，我們識別到了位於最底層、技術依賴程度最高的兩個系統：配置中心、註冊中心，這兩個系統非常適合作為統一平台建設的技術底座。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014330" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bb1b5bf-7287-4f3d-884c-ea3b5ab7d7eb.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">區別於煙囱式微服務平台的多個系統模塊獨立對接 CICD 等研發平台，在統一微服務平台建設中，我們升級為統一平台對接 CICD 等研發平台；我們的建設思路是，以配置中心/註冊中心為底座來建設統一微服務平台：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>：基於統一的配置通道與 CICD 等研發平台系統進行聯動，建設一鍵審批、回滾能力，整合研發流程，降低對接成本；</p><p><strong>二是</strong>：通過統一平台的建設，實現平台間聯動，建設高階的自動化水平，支撐業務進一步提升持續服務能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.5 引擎升級&amp;統一平台建設總結</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">接下來，對我們前面講到的內容做一個總結：在大規模、海量業務的微服務架構實踐過程中，我們通過引擎升級和統一平台能力建設較好的解決了碰到的問題與挑戰。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014331" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/16e89ce4-7fa5-497c-9a09-4b45df61c722.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在升級和建設過程中，我們需要保證現有業務的連續性，保障不發生因底層引擎升級和平台建設導致的可用性問題。因此，引擎升級和統一平台建設的工作需要建立在高可用保障的基礎上；換句話來説，可用性是我們所有工作的底座。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在這個基礎上，我們實現註冊中心和配置中心的引擎升級，完成應用級註冊模型升級；在這個過程中，解決底層引擎的擴展性、容量、性能、可維護性和安全性等問題；最後，我們要建設統一的微服務平台能力，實現平台間聯動，構建自動/自助化使用能力；賦能業務。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，通過完整的方案介紹，在上圖右側我們呈現了微服務架構實踐過程中的價值分層邏輯，即在可用性的基礎上，提升系統的擴展性、容量、性能、可維護、安全性等能力；然後再在此基礎上，交付更高的研發效率，更好的用户使用體驗。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>三、微服務架構升級的總結與展望</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介紹完我們的解決方案後，最後來説明下我們對微服務架構升級的總結與思考，以及對未來的展望。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.1 擁抱開源的實用主義</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在構建微服務架構技術體系的過程中，我們始終堅持擁抱開源，迭代業務適用的技術平台；結合內部業務的實際情況，我們走出了一條從開源到開源+自研的研發路徑。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014332" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8f26fca0-069c-4375-850c-26e6d7c79812.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在從 0 到 1 的平台能力建設過程中，我們引入開源組件進行能力快速構建，快速交付滿足業務的需求；始終堅持業務適用原則，不過度設計，支撐業務的快速迭代；以上階段，我們稱之為「拿來主義」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在面向更大規模、海量業務實踐過程中，為瞭解決碰到的問題與挑戰，我們在開源的基礎上進行增強，自研部分能力來解決億級用户規模下內部業務的功能，性能，容量，研發流程打通等需求；這個階段，我們稱之為「實用主義」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在技術平台迭代過程中，我們始終堅持 2 個原則，一是簡單有效原則，堅持用最簡單的解決方案來解決問題；二是迭代和演進原則，堅持平台持續迭代和演進的原則；前期基於開源組件快速搭建能力，再基於實際的業務需求和痛點來落地自研架構；在這個過程中，始終堅持業務適用，不為了技術而技術，避免大而全的技術架構。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，也要説明一個常見的誤區，我們為什麼不完全自研？vivo 的微服務平台建設從開源社區獲益良多，堅持不閉門造車，站在巨人肩膀上，持續引入優秀特性來支撐業務的快速發展，同時也會考慮將部分行業適用的通用優秀特性反饋給社區，和社區共同成長。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.2&nbsp;中間件組件全生命週期管理</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">大家可以看到，vivo 的微服務架構技術體系引入了較多的開源組件，在實踐過程中，我們摸索出了一套完整的中間件組件全生命週期管理策略。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014333" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e436a13c-2e63-49df-a26a-8784d90e98a0.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我們先來看看業務的訴求和底層技術的<strong>特點</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是業務的訴求：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">業務期望更高的迭代交付效率；</span></p></li><li><p style="margin-bottom: 10px;">快速引入新技術，使用新技術助力業務創新，但很多時候新技術往往意味着成熟度不足，可能存在較多問題；</p></li><li><p style="margin-bottom: 10px;"><span style="letter-spacing: 0.034em;">業務的不斷創新與發展，對組件的性能、容量要求越來越高；</span><br></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">對業務來説，高效迭代交付需求是第一位的。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(14, 14, 13);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">然而，底層技術有它自己的特點：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">技術的發展有它的客觀規律，需要經歷萌芽期 → 膨脹期 → 低谷期→ 復甦期→ 成熟期等多個階段；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">缺</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">乏約束的技術體系必然隨着時間推移而腐化，治理不及時會成為技術債務，阻塞業務發展；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">同類中間件組件的快速引入會有重複建設的效率問題；</span></span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">中間件組件的技術升級週期客觀上都比較長。</span></span><span style="letter-spacing: 0.034em;"></span></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">實踐證明，只有足夠穩健的底層技術能力才能更好支撐業務的高效迭代。在這個過程中，如何兼顧效率與質量？尊重客觀規律，確保整個過程都有明確的目標和方向，避免走偏，慢就是快。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我們認為，完善的中間件組件全生命週期管理策略，首先需要在所有的技術團隊中形成價值共識；再通過組件掃描和組件地圖等手段及時對組件全貌進行洞察；在組件的標準化治理和運營階段實現有規範，補短板；同時在新技術引入時，通過完善的新技術引入規範，覆蓋功能/性能/容量/擴展性/成熟度/使用成本等維度；在組件的版本治理上，使用基線版本治理方案，輸出明確的使用標準/版本升級方案/版本收斂策略；最後，在組件的成熟度管理上，我們可以藉助 Gartner(高德納) 技術成熟度説明和組件能力矩陣，不斷提升組件的成熟度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">綜上，為更高效的支撐業務，在組件管理上我們使用了更加入寬鬆的引入策略，同時也會對組件的全生命週期進行嚴格管理，踐行寬入嚴出策略，通過完善的中間件組件全生命週期管理助力業務跑的更快，走的更遠。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.3 引擎升級探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">展望未來，我們會堅持和踐行引擎升級和平台建設的<strong>持續迭代思路</strong>：</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">首先是對引擎升級的探索，通過引入新技術來解決當前碰到的研發效率、成本等痛點問題：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014334" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8bd511dc-cb3c-4056-a1f1-5fca3b345d4b.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在研發效率方向</strong>，存在的痛點問題如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>，組件 SDK 的升級週期長，碎片化問題嚴重；</p><p><strong>二是</strong>，當前 vivo 內部主要的是 Java、C++技術棧，新業務形態孵化可能會引入新的技術棧，需能夠較好解決跨技術棧的問題。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">想要較好的解決以上問題，需要探索基於 Java Agent/SideCar 技術的標準 ServiceMesh 模式，將 RPC、MQ 等中間件能力下沉，透明化實現微服務治理、高可用等能力增強，同時組件具備熱升級能力。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">此外，<strong>在成本方向</strong>，存在的痛點問題如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>， MQ 等重資源型應用的 CPU、存儲資源利用率差異大；</p><p><strong>二是</strong>，部分事件驅動場景機器資源利用率低。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">要解決以上問題，我們可以通過升級 MQ 組件，落地存算分離技術，探索計算存儲資源利用率優化方案。另外，還可以探索 Serverless 技術，實現平台化託管運維，降低資源成本，天然適合小程序、快應用等事件驅動業務場景。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>綜上</strong>，在引擎升級探索上，我們會基於業務需求和痛點問題，探索和落地 ServiceMesh/Serverless/存算分離等雲原生技術。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.4 平台建設探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">講完引擎升級探索，我們再來看看在平台建設上的探索：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014335" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d67670bb-72fa-43e3-93a8-1c5f33dfca51.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">作為技術平台團隊，我們在持續積極的探索「平台工程」理念，從現在的 DevOps 實踐到平台工程，也是團隊協作理念的再次升級。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我們知道，DevOps 於 2009 年出現，2015 年在國內火起來，它是一種文化、方法論，是敏捷理念從開發到運維的延伸。DevOps 的理念是：踐行誰構建誰運行，開發運維一體化，實現業務的高效交付。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">但是，DevOps 在實際落地過程中存在以下問題：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">「DevOps 團隊」的中心化與去中心化取捨問題</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">【<strong>中心化</strong>】指的是，獨立的 DevOps 團隊，即不在業務團隊中配置 DevOps 能力，而把 DevOps 人員集中起來組建團隊，這種完全中心化的模式本質上和 DevOps 文化相矛盾。同時根據康威定律，可能會製造新的效能瓶頸。「獨立的 DevOps 團隊」在 2014 年被 Thoughtworks「技術雷達」列為 Hold (停止採用)。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">【<strong>去中心化</strong>】指的是，將 DevOps 能力分散在業務團隊，這種做法會將大量的和基礎設施相關的工作職責劃給業務團隊；這種方式會隨之出現基礎設施和服務治理缺失、系統穩定性降低、研發和 DevOps 效能浪費等諸多問題。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">因此，想要踐行好 DevOps，必須在中心化與去中心化之間取得平衡。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">此外，從平台能力上講，DevOps 平台往往更側重於建設流程和工具鏈，而在使用這些建設的工具技術平台過程中會大大增加業務開發團隊的認知負荷，存在無法較好向業務開發團隊屏蔽底層基礎設施複雜性的問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">平台工程的概念，是在 2017 年首次出現，於 2022 年在國內興起。平台工程的定義是，一套用來構建和運營支持軟件交付和生命週期管理的自助式內部開發者平台的機制和架構；它的特點是：平台在演進中提供足夠的透明度、敏捷性，在建設過程中形成適合業務架構的高效協作模式。在這一過程中逐步將知識體系固化到平台中，從而使得工程方式標準化、流程化和規模化並持續改善；它踐行的理念是：一個可用的、高效的平台並非一個技術團隊埋頭苦幹就可以產出的；恰恰相反，一個成功的平台工程需要企業各個組織部門合作、協調、推廣並根據實際使用反饋不斷迭代。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在具體實踐中，平台工程約定了「業務團隊」和「平台團隊」兩個團隊，其中「業務團隊」負責業務研發，「平台團隊」負責平台建設；「平台團隊」通過將技術知識沉澱到「平台工程」，隱藏和抽象底層基礎設施的複雜性，實現基礎設施即代碼，為「業務團隊」賦能增效；同時，基於「業務團隊」在使用「平台工程」的過程中的不斷反饋來持續改進平台的自助化產品能力，構建一整套覆蓋 DevOps 全鏈路的簡單易用平台產品；可以看到，平台工程是一種最佳實踐，和我們當前的團隊協作模式匹配度非常高。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014336" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e9b65caa-221d-4d7f-ae0c-c6722aa53455.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在平台建設的整體規劃上：</strong></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>當前階段</strong>：我們構建的統一微服務平台會持續探索「平台工程」理念，沉澱配置中心、註冊中心等平台的技術知識與最佳實踐，構建和打磨業務自助化使用的平台能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>展望未來</strong>：我們會通過明確的北極星指標，牽引平台提供更高的研發效率和更好的開發者體驗。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在研發效率上</strong>，我們追求單位時間內更多的代碼產出和需求交付；此外我們也追求更好的開發者體驗，通過降低用户使用平台的打斷次數和平台問題的人工支撐次數，提升業務團隊和平台團隊兩個團隊的開發體驗。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在具體的落地路徑上</strong>，我們始終以開發者用户為中心，針對研發工作中時間消耗較多的場景進行優化，通過北極星指標牽引，形成覆蓋 IDE+PaaS 的平台工程實踐路徑，持續迭代優化平台能力，提升研發效率與開發者體驗。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-right: 0%;margin-bottom: 20px;margin-left: 0%;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section><section style="display: inline-block;vertical-align: middle;width: 20%;align-self: center;flex: 0 0 auto;"><section style="text-align: center;color: rgb(45, 66, 87);font-size: 11px;" powered-by="xiumi.us"><p>END</p></section></section><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: left;" powered-by="xiumi.us"><section style="padding-left: 1em;padding-right: 1em;display: inline-block;text-align: center;"><span style="display: inline-block;padding: 0.3em 0.5em;border-radius: 0.5em;background-color: rgb(65, 94, 255);color: rgb(255, 255, 255);" title="" opera-tn-ra-cell="_$.pages:0.layers:0.comps:161.title1"><p>猜你喜歡</p></span></section><section style="border-width: 1px;border-style: solid;border-color: transparent;margin-top: -1em;padding: 20px 10px 10px;background-color: rgb(239, 239, 239);text-align: center;"><section style="font-size: 14px;text-align: left;" powered-by="xiumi.us"><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p><span style="font-size: 14px;letter-spacing: 0.578px;text-align: left;text-wrap: wrap;background-color: rgb(239, 239, 239);"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247498140%26idx%3D2%26sn%3D66854883c362d9145d89f72f267a7773%26chksm%3Debdb890edcac0018fa02b33bf9eff448548362f5174c829a6e20ee6ab656d9ea12b31c0a1e0f%26scene%3D21%23wechat_redirect" textvalue="Spring 七種事務傳播性介紹" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Spring 七種事務傳播性介紹</a></span><br></p></li><li><p><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497989%26idx%3D1%26sn%3Da98e270e4612356756966bd9d90d80ee%26chksm%3Debdb8997dcac0081e35a2c9ba681902e703f8c52406ee49fcedaafbba77b7dc3279f56305782%26scene%3D21%23wechat_redirect" textvalue="vivo 數據庫備份恢復系統演化" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 數據庫備份恢復系統演化</a></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497821%26idx%3D1%26sn%3D80e04511f5a5d5acfee4a44a8a8b3e31%26chksm%3Debdb88cfdcac01d954242fd24907b69c542e43fcb99ebe6a03d66858194e03ad14105281b62f%26scene%3D21%23wechat_redirect" textvalue="vivo 容器平台資源運營實踐" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 容器平台資源運營實踐</a></span></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497810%26idx%3D1%26sn%3Dfb5334c9637cdde4b5125f69ed32e89f%26chksm%3Debdb88c0dcac01d6faf82e4d44e8421616ec9128f46ea494339a599c346b13212b9f1d774886%26scene%3D21%23wechat_redirect" textvalue="Hudi 在 vivo 湖倉一體的落地實踐" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Hudi 在 vivo 湖倉一體的落地實踐</a></span></p></li></ul></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzI4NjY4MTU5Nw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/4g5IMGibSxt45QXJZicZ9gaNU2mRSlvqhQd94MJ7oQh4QFj1ibPV66xnUiaKoicSatwaGXepL5sBDSDLEckicX1ttibHg/0?wx_fmt=png" data-nickname="vivo 互聯網技術" data-alias="vivoVMIC" data-signature="分享 vivo 互聯網技術乾貨與沙龍活動，推薦最新行業動態與熱門會議。" data-from="0" data-is_biz_ban="0"></mp-common-profile></section></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公眾號 - vivo 互聯網技術（vivoVMIC）。<br>如有侵權，請聯繫 support@oschina.cn 刪除。<br>本文參與「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源創計劃</a>」，歡迎正在閲讀的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/10773883</guid>
            <link>https://my.oschina.net/vivotech/blog/10773883</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Stability AI 推出更小、更高效的 1.6B 語言模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Stability AI <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fintroducing-stable-lm-2" target="_blank">宣佈</a>推出迄今為止最強大的小語言模型之一 Stable LM 2 1.6B。以英語、西班牙語、德語、意大利語、法語、葡萄牙語和荷蘭語的多語言數據為基礎進行了訓練，體積小、速度快，降低了硬件門檻；並提供了完全透明的訓練細節，旨在讓開發人員和模型創建者能夠快速進行實驗和迭代。</p><p>Stable LM 是一種文本內容生成 LLM，Stability AI 於 2023 年 4 月首次推出了 30 億和 70 億參數模型。新的 StableLM 模型實際上是 Stability AI 在 2024 年發佈的第二個模型，此前該公司在早些時候還發布了一個 Stable Code 3B。</p><p>Stability AI 聲稱，Stable LM 2 1.6B 在大多數基準測試中均優於其他參數低於 20 億個的小語言模型，如微軟的 Phi-1.5 (1.3B) 和 Phi-2 (2.7B)、TinyLlama 1.1B 或 Falcon 1B。</p><p><img height="202" src="https://oscimg.oschina.net/oscnet/up-2d9d2c9ec7d678e945e32f4889d703808f8.png" width="500" referrerpolicy="no-referrer"></p><p><img height="188" src="https://oscimg.oschina.net/oscnet/up-b30279a4fcccd6dbb87084aa29fd54c9428.png" width="500" referrerpolicy="no-referrer"></p><p><img alt="" height="198" src="https://oscimg.oschina.net/oscnet/up-a33e04e9eca65dc6b9b1137e97e0f66d4e0.png" width="500" referrerpolicy="no-referrer"></p><p><img alt="" height="193" src="https://oscimg.oschina.net/oscnet/up-9336f78e90afcb31c1ce5bb73afb0d482bb.png" width="500" referrerpolicy="no-referrer"></p><p>不過他們也警告稱，由於小型、低容量語言模型的特性，Stable LM 2 1.6B 可能會出現高幻覺率、潛在的有毒語言等類似的常見問題。「我們要求社區在構建應用程序時牢記這一點，並採取適當措施確保以負責任的方式進行開發。」</p><p>Stable LM 2 1.6B 目前可在商業和非商業領域使用，只要擁有 Stability AI 會員資格，即可在 Hugging Face 上測試該模型。</p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 03:57:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276269/stable-lm-2-1-6b</guid>
            <link>https://www.oschina.net/news/276269/stable-lm-2-1-6b</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[X 正面向 Android 推出音頻和視頻通話]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#333333">埃隆·馬斯克旗下的社交網絡 X （原 Twitter）正面向 Android 客户端，推出直接從應用程序撥打音頻和視頻電話的功能。一位負責該項目的 X 工程師發佈了關於該功能發佈的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2Fenriquebrgn%2Fstatus%2F1748114599856193879" target="_blank">消息</a>，並表示 Android 用户將可在應用更新後使用該功能。</span></p><p><img height="220" src="https://oscimg.oschina.net/oscnet/up-d168ce5f0e35f1157f1ed9a8ba3f0e69408.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#333333">2023 年 8 月，其首席執行官 Linda Yaccarino 首次談到要在平台上引入視頻通話，並最終於 10 月向 iOS 用户推出了這一功能。但值得注意的是，任何用户都可以接聽電話，只有付費用户才能撥打電話。不過，X 在本月早些時候取消了高級用户將 NFT 設為個人照片的功能。</span></p><p><span style="color:#333333">用户可以通過 Settingle &gt; Privacy and safety &gt; Direct Messages &gt; Enable audio and video calling 來啓用或禁用通話。在同一個菜單中，用户還可以控制誰可以給自己打電話：通訊錄中的人、關注的人和驗證過的用户。可以從這些選項中選擇多個選項。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 03:25:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276266/x-audio-and-video-calls-to-android</guid>
            <link>https://www.oschina.net/news/276266/x-audio-and-video-calls-to-android</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Extism —— WebAssembly 插件實現框架]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Extism 是一個 WebAssembly 插件實現框架，它可以給你的應用開發出各種各樣的 WebAssembly 插件，支持多種編程語言。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img src="https://oscimg.oschina.net/oscnet/up-5d37397b3cababa42f8754739726d916b86.png" referrerpolicy="no-referrer"></p><p>Exism 團隊稱他們致力於構建一個可嵌入的、安全的、性能良好的運行時，為任何規模的軟件帶去可擴展性。</p></div>
                                                                ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 03:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/extism</guid>
            <link>https://www.oschina.net/p/extism</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推薦 | 龍蜥社區最佳安全加固實踐指南 security-benchmark]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-security-benchmark" class="anchor" href="https://gitee.com/anolis/security-benchmark#security-benchmark"></a>security-benchmark</h1><p>security-benchmark 是龍蜥下游各個廠商結合自己在安全合規/加固領域的大規模產品落地經驗和實踐打造的龍蜥社區最佳安全加固實踐指南，它包括安全基線（benchmark）、掃描腳本、修復腳本、安全合規鏡像製作、安全合規監控等多個方面。其中，Anolis OS 8 、Anolis OS 23 及其最佳安全基線已經完成與<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FComplianceAsCode%2Fcontent">OpenSCAP 國際知名社區</a>的映射與適配，並被 OpenSCAP 社區合入，詳見：</p><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FComplianceAsCode%2Fcontent%2Fblob%2Fmaster%2Fproducts%2Fanolis8%2Fprofiles%2Fstandard.profile">OpenSCAP 社區 Anolis OS 8 standard.profile</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FComplianceAsCode%2Fcontent%2Fblob%2Fmaster%2Fproducts%2Fanolis23%2Fprofiles%2Fstandard.profile">OpenSCAP 社區 Anolis OS 23 standard.profile</a></li></ul><h4><a id="user-content-介紹" class="anchor" href="https://gitee.com/anolis/security-benchmark#%E4%BB%8B%E7%BB%8D"></a>介紹</h4><p>Gitee 是 OSCHINA 推出的基於 Git 的代碼託管平台（同時支持 SVN）。專為開發者提供穩定、高效、安全的雲端軟件開發協作平台
無論是個人、團隊、或是企業，都能夠用 Gitee 實現代碼託管、項目管理、協作開發。企業項目請看 <a href="https://gitee.com/enterprises">https://gitee.com/enterprises</a></p><h4><a id="user-content-參與貢獻" class="anchor" href="https://gitee.com/anolis/security-benchmark#%E5%8F%82%E4%B8%8E%E8%B4%A1%E7%8C%AE"></a>參與貢獻</h4><p>請參考<a href="https://gitee.com/anolis/security-benchmark/blob/master/docs/development-guide.md">development-guide</a>和以下 gitee 貢獻步驟來貢獻您的代碼。</p><ol><li>Fork 本倉庫</li><li>新建 Feat_xxx 分支</li><li>提交代碼</li><li>新建 Pull Request</li></ol><h4><a id="user-content-特技" class="anchor" href="https://gitee.com/anolis/security-benchmark#%E7%89%B9%E6%8A%80"></a>特技</h4><ol><li>使用 Readme_XXX.md 來支持不同的語言，例如 Readme_en.md, Readme_zh.md</li><li>Gitee 官方博客 <a href="https://blog.gitee.com/" rel="nofollow">blog.gitee.com</a></li><li>你可以 <a href="https://gitee.com/explore">https://gitee.com/explore</a> 這個地址來瞭解 Gitee 上的優秀開源項目</li><li><a href="https://gitee.com/gvp">GVP</a> 全稱是 Gitee 最有價值開源項目，是綜合評定出的優秀開源項目</li><li>Gitee 官方提供的使用手冊 <a href="https://gitee.com/help">https://gitee.com/help</a></li><li>Gitee 封面人物是一檔用來展示 Gitee 會員風采的欄目 <a href="https://gitee.com/gitee-stars/">https://gitee.com/gitee-stars/</a></li></ol>]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 02:58:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/anolis/security-benchmark</guid>
            <link>https://gitee.com/anolis/security-benchmark</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 得物雲原生容器技術探索與落地實踐]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1>一、前言</h1><p style="color:#24292f; text-align:start">得物 App 作為互聯網行業的後起之秀，在快速的業務發展過程中基礎設施規模不斷增長，繼而對效率和成本的關注度也越來越高。我們在雲原生技術上的推進歷程如圖所示，整體上節奏還是比較快的。</p><p style="color:#24292f; text-align:start"><img alt="156.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/156.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">從 2021 年 8 月開始，我們以提升資源使用率和資源交付效率為目標，開始基於雲原生技術建設整個服務體系的高可用性、可觀測性和高運維效率，同時要保證成本可控。在容器化過程中我們遇到了很多的挑戰，包括：如何將存量的服務在保持已有研發流程不變的情況下，做到容器化部署和管理；容器化之後如何做到高效地運維；如何針對不同的業務場景，提供不同的容器化方案等等。此外，通過技術手段實現持續的成本優化是我們的長期目標，我們先後建設落地了畫像系統、混部方案和調度優化等方案。本文把得物在推進雲原生容器技術落地過程中相關方案和實踐做一些總結和梳理，歡迎閲讀和交流。</p><span id="OSC_h1_2"></span><h1>二、雲原生應用管理</h1><span id="OSC_h2_3"></span><h2>雲原生應用管理方式</h2><p style="color:#24292f; text-align:start">容器與 ECS 的資源形態是有差異的，所以會造成在管理流程上也會有不同之處。但是為了儘可能降低容器化帶來的使用體驗上的差異，我們參考業內容器應用 OAM 模型的設計模式，對容器的相關概念做了屏蔽和對等解釋。例如：以「應用集羣」的概念代表 CloneSet 工作負載（Kruise 提供的一種 Kubernetes 擴展工作負載）；將單個 Pod 約定為一個應用集羣的實例；以「應用路由/域名配置」的概念代表針對 Ingress/Service 的設置。</p><p style="color:#24292f; text-align:start">在應用集羣的構造上（即如何構造出 Kubernetes 工作負載對象），我們設計了「配置/特徵分層」的方案，將一個應用集羣所處歸屬的應用、環境組、環境上的配置進行疊加後，使用 Helm 工具渲染生成 Kubernetes 資源對象，提交給容器平台。</p><p style="color:#24292f; text-align:start"><img alt="679.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/679.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">CI 和 CD 過程均使用這種配置/特徵分層的方式，一方面可以解決應用依賴的中間件信息的管理問題（由相應的提供者統一維護）；另一方面，這種管理方式可以讓中間件組件/服務變更時按照不同維度進行，整體上降低了配置變更帶來的風險。</p><p style="color:#24292f; text-align:start">Sidecar 容器在應用集羣實例中除了扮演「協作者」的角色外，我們還基於它做了權限管理，以便對應在 ECS 形態下的不同用户的登陸權限，也算是一舉兩得。當然，在容器場景下也是可以定義不同的用户，賦予不同的角色，但是強依賴基礎鏡像的維護。</p><p style="color:#24292f; text-align:start"><img alt="564.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/564.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_4"></span><h2>多集羣管理方案</h2><p style="color:#24292f; text-align:start">雲原生場景下的解決方案對應用集羣而言本身就是高可用的，比如：容器編排引擎 Kubernetes 中支持 Pod 實例的拓撲分佈設置、支持可用區設置、副本數設置、 Service 負載均衡的設計等，這些都能保證應用集羣的高可用。那如果單個 Kubernetes 集羣不可用了，會有什麼的影響呢，該如何解決？多集羣管理方案就是我們解決 Kubernetes 的可用性問題的思路。</p><p style="color:#24292f; text-align:start">如果 Kubernetes 控制面不可用了，會導致應用發佈受損，較嚴重的情況也會影響容器服務的可用性。所以，為了保證 Kubernetes 的可用性，一方面要保證 Kubernetes 各組件的健壯性，另一方面要適當控制單個 Kubernetes 集羣的規模，避免集羣過大造成系統性風險升高。我們的解決思路就是「不要把雞蛋放在一個籃子裏」，用聯邦的方式管理多個 Kubernetes，將業務分散到不同的 Kubernetes 集羣。</p><p style="color:#24292f; text-align:start">聯邦的思想在 Kubernetes 誕生不久就被開始討論，逐步設計實現，從最初社區的 KubeFate V1.0 到 V2.0，再到企業開源的 Karmada、KubeAdmiral 逐漸成熟起來，並實際應用到了生產場景。那如果沒有集羣聯邦，多個 Kubernetes 集羣就沒法管理了嗎？當然不是的，容器管控平台其實也能做這件事情，筆者在幾年之前還對此深以為然，但現在已經完全改變看法了。因為在實際的生產落地過程中我們發現，相比在管控中用 if...else/switch 的方式，亦或配置的方式相比，基於 CRD 的方式來管理多集羣效率更高、邏輯更清晰。<img alt="435.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/435.png" referrerpolicy="no-referrer"><img alt="342.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/342.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">得物在使用聯邦思想管理多 Kubernetes 集羣的時候，參考華為開源的 Karmada 解決方案，在此基礎之上做了定製開發。容器管控平台負責管理應用集羣的原始特徵和配置，管理 CICD 流程，向 Host Kubernetes 集羣發起容器對象管控請求。Host Kubernetes 集羣通過 PropagationPolicies 管理工作負載如何分發到 Member Kubernetes 集羣，通過 OverridePolicies 管理差異化的配置。單 Kubernetes 集羣下我們使用了分批發布的方式來管理應用集羣的發佈，在引入聯邦管理之後，我們把分批發布的邏輯從容器管控層面下移到了 Host Kubernetes 集羣上。為了兼容存量的通過 Kubernetes Service 進行調用的服務，我們在 Member Kubernetes 集羣通過自定義的 MCS-Controller 來管理跨集羣的 Service/Endpoints 對象，在 Host Kubernetes 層通過 MCS-Validator 做雙重校驗，確保跨集羣的 Service 的一致性。</p><span id="OSC_h1_5"></span><h1>三、容器調度優化與混部</h1><p style="color:#24292f; text-align:start">落地雲原生容器技術的目標是期望在敏捷、彈性和可用的基礎上，最終實現資源利用率上的提升、成本上的節省。這通常有 2 個實現途徑，一個是通過技術的手段，另一個則是通過治理方法。本章重點介紹我們在容器精細化調度和混部實踐方面的技術方案設計和落地過程。</p><span id="OSC_h2_6"></span><h2>應用畫像</h2><p style="color:#24292f; text-align:start">應用服務的研發人員在部署應用集羣實例時，通常會申請超過應用集羣本身承載業務流量時所要消耗的資源量，這是可以理解的（要確保系統的資源利用率安全水位，防止過載造成系統夯住），但是不同的研發人員對這個「度」把握是不一樣的，因為合理地設置應用集羣的資源用量是依賴研發人員經驗的，也就是説主觀性會更強。</p><p style="color:#24292f; text-align:start">為瞭解決上述問題，業內的做法通常是通過分析應用集羣的過往資源利用率數據，來刻畫出應用集羣在業務流量下的實際資源利用率曲線，這就是應用畫像。如下圖所示是我們建設的畫像系統的架構框圖，該畫像系統不僅負責應用的畫像分析，也負責宿主機、Kubernetes 集羣的畫像分析，用來指導整個容器平台對資源的管理。<img alt="140.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/140.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">容器的監控數據通過 Prometheus 方案進行採集和管理，自研的 KubeRM 服務將它作為數據源，週期性計算產出應用畫像、宿主機畫像和 Kubernetes 集羣畫像（資源池畫像）。容器平台部署在線服務服務時，可參考畫像值來配置應用集羣的資源規格，這裏的畫像值就是指 Pod 的 Request 值，計算公式如下：</p><p style="color:#24292f; text-align:start">++Pod Request = 指標週期性利用率 / 安全水位++</p><p style="color:#24292f; text-align:start">公式中「指標週期性利用率」是畫像系統通過統計學手段、AI 模型等方法計算分析出的資源指標（CPU/內存/GPU 顯存）在實際業務流量下所表現出的週期性的規律。畫像值的生效我們通過以下 4 個策略進行實施：</p><ul><li>針對 P3/P4 等級的服務，默認在服務部署時生效畫像值。</li><li>針對非 P3/P4 等級的服務，將畫像值推薦給用户，由用户決定部署時是否採用畫像。</li><li>分資源池設置不同的生效策略（默認生效，或者用户決定生效）。</li><li>GPU 顯存的畫像不做默認生效，推薦給用户，讓用户決定。</li></ul><p style="color:#24292f; text-align:start">交由用户決定畫像是否生效時，如何讓用户更傾向於去生效畫像呢？我們使用差異化計費的策略：生效了畫像的應用集羣實例按照其 Pod 配置的 Request 值計費，未生效畫像的應用集羣實例按照其 Pod 配置的 Limit 值計費。用户可以根據自己服務的實際情況選擇生效畫像，以降低成本；平台也因為畫像而拿到了更多可以調度的資源，用於其它更多的場景。</p><p style="color:#24292f; text-align:start">此外，畫像系統也接入了 KubeAutoScale 自動伸縮器，在業務低峯期，可以指導自動伸縮器對部分場景在線服務做副本縮容操作，以便釋放出更多的資源供給其它場景使用（比如：混部任務場景），後面的章節會詳細介紹。</p><span id="OSC_h2_7"></span><h2>資源預佔</h2><p style="color:#24292f; text-align:start">當整個容器集羣的資源冗餘量不是很充足的時候，在以下幾種情況下是會出現 「雖然集羣層面總量資源是夠的，但是業務 Pod 卻無法調度」的問題，影響業務發佈效率和體驗。</p><ul><li>在集羣中容器實例變更比較頻繁的時候，某個大規格的業務集羣在做滾動更新時，釋放的舊的實例很可能被小規格的容器實例所搶佔，導致無法調度。</li><li>研發同學負責 2 個應用服務 A 和 B，它們的規格都是一樣的。為了保證總體成本不變會，選擇將 A 服務的實例縮掉一些，然後擴容 B 服務的實例。因為 Kubernetes 默認調度會按照 Pod 創建時間來依次調度新 Pod，當用户縮容完 A 服務的實例再去擴容 B 服務實例的時候，A 服務釋放的資源很可能被其他容器實例搶佔，導致 B 的實例無法調度。</li><li>在大促、全鏈路壓測等業務需要緊急擴容的情況下，容器平台會新擴宿主機節點以滿足業務需求，不曾想新擴容的機器資源卻被那些「小而快（拉起頻繁，執行時間短）」的任務給見縫插針地搶佔了，一方面會導致大規格的服務實例無法調度，另一方面還造成了較多的資源碎片。</li></ul><p style="color:#24292f; text-align:start">為瞭解決以上問題，我們在調度器中自定義實現了資源預佔的調度插件（通過 CRD 定義資源預佔期望，影響調度決策），用來提升用户體驗和提高調度效率。<img alt="650.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/650.png" referrerpolicy="no-referrer"><img alt="178.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/178.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_8"></span><h2>平衡調度</h2><p style="color:#24292f; text-align:start">為了更好地平衡集羣中節點的水位，以避免過熱節點的出現、儘量減少碎片資源等為目標來思考和設計，我們基於 Kubernetes 提供的調度器擴展框架，自定義實現了多個調度插件：</p><ul><li>CoolDownHotNode 插件：給最近調度過 Pod 的節點降低優先級，避免熱點節點</li><li>HybridUnschedulable 插件：阻止使用彈性資源的 Pod 調度到某些節點上。</li><li>NodeBalance 插件：用於平衡各節點上 CPU Request 值與畫像的比值，平衡各節點 CPU 使用率。</li><li>NodeInfoRt 插件：基於畫像打分數據和實時打分數據優化 Pod 調度。</li></ul><span id="OSC_h2_9"></span><h2>在實時混部</h2><p style="color:#24292f; text-align:start">從今年 1 月份開始，我們着手做在離線混部的落地，一期的目標着眼於將在線服務與 Flink 任務進行混部。之所以選擇 Flink 任務做混部，是因為它與在線服務有一個相似之處，那就是它是一種常駐的離線任務，在它啓動之後如果沒有特殊情況，一般不會下線，這種特質會使得我們的容器集羣調度頻次、Pod 的變更程度會低一些，進而對穩定性的挑戰也會小一點，整體混部風險也會低一些。</p><p style="color:#24292f; text-align:start">在沒有混部的情況下，我們的集羣整體利用率較低，即便畫像功能能幫助用户儘可能合理的為自己的服務實例設置資源規格，但對容器平台而言這依然很被動。所以為了挖掘出可以用來混部的資源，我們為不同等級的服務設置不同的綁核策略。如下表所示定義了 4 種應用類型（LSX、LSR、LS、BE），適用於 P0~P4 範圍和離線任務，綁核策略從完全綁核到部分綁核，再到完全共享。</p><p style="color:#24292f; text-align:start"><img alt="900.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/900.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">離線任務（Flink 任務）屬於 BE 類型，可以使用的資源是在宿主機所有 CPU 核心裏面單獨劃分出來的一部分專用 CPU 核心，再加上 LS 的共享 CPU 核心，以及 LSR、LS 類型的應用上共享出來的部分 CPU 核心。<img alt="·.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/%C2%B7.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">LSX、LSR 和 LS 類型的應用服務的容器實例均申請使用 Kubernetes 原生的資源 CPU/Memory 資源；BE 類型的任務需要申請使用我們自定義的資源 BE-CPU/BE-Memory 資源。<img alt=".png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/-.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">基於 Kubernetes 的 Device-Plugin 機制我們自研實現了 Kube-Agent 組件，該組件在集羣中的所有節點上以 Damonset 的方式部署，一方面負責根據自定義策略將本節點上可用的 BE 資源上報給 API-Server（通過 Kubelet 組件間接上報），另一方面負責執行 CPU 綁核操作。隨着混部的深入，該組件也承擔了更多的工作內容（例如：執行 CPU 算力壓制操作、參數動態調整操作，執行 VPA 操作等）。</p><span id="OSC_h2_10"></span><h2>在離線混部</h2><p style="color:#24292f; text-align:start">一期的混部在應用級別劃分的基礎上，應用了 CPU 核劃分策略來實現混部。站在 CPU 核心的角度來看，通過 CPU 核劃分策略之後，每個 CPU 核心已經有了自己的負載歸屬，能否充分利用取決於分配到它上面的業務特性。但站在整機的利用率上來看（或者站在整個集羣的利用率角度來看），依然有很大的提升的空間。混部二期的時候，我們考慮對 BE 資源進行二次超賣，以另一種新的自定義資源（OT 資源）進行分配使用。使用 OT 資源的任務不獨佔綁核，而是共享劃分給 BE 資源的所有 CPU 核心。<img alt="175.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/175.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">我們使用 OT 資源來混部 AI 訓練任務、其它數據處理任務，為了消除訓練任務對在線業務的影響，通過以下策略進行保證：</p><ul><li>設置宿主機安全水位，通過調度插件防止過熱節點出現。</li><li>通過 CPU Group Identity 進行優先級競爭，保障離線任務的調度優先級絕對低於在線服務。</li><li>對離線任務進行獨立掛盤，避免影響在線服務的磁盤 IO。</li><li>夜間時段通過 KubeAutoScaler 進行對在線服務進行彈性縮容，等比例提升內存的空閒率，保障離線任務有足夠的內存資源。</li></ul><span id="OSC_h2_11"></span><h2>彈性伸縮</h2><p style="color:#24292f; text-align:start">容器平台的彈性能力相較於傳統 IDC 資源管理模式、ECS 資源管理方式的要更上一個台階，因為它更側重將彈性伸縮的決策權交給應用服務，而不是資源管理方。雲原生技術中常説的彈性伸縮方案通常包含 2 種方式：</p><ul><li>HPA：Horizontal Pod Autoscaling，水平方向的 Pod 副本擴縮容。</li><li>VPA：Vertical Pod Autoscaling，垂直方向的 Pod 規格擴縮容。</li></ul><p style="color:#24292f; text-align:start">Kubernetes 中通過資源對象 HorizontalPodautoScalers 來支持工作負載的水平擴縮容，在較早的版本中只支持 CPU 和內存這兩個資源指標，較新版本中也開始支持自定義指標了。針對 VPA 的需求，目前 Kubernetes 層面還沒有比較穩定的可用功能，因為對一個 Pod 實例做資源規格的調整，會涉及到宿主機上資源賬本的管理問題、監控問題，也會涉及到 Pod 的重建/容器重啓動作，影響面會比較大，目前社區中依然在討論。但企業在 VPA 方面，也都是躍躍欲試，會設計自己的個性化 VPA 方案，本文前述應用畫像功能，就是我們得物在 VPA 方案上探索的第一步。</p><p style="color:#24292f; text-align:start">此外，我們在實際的支撐業務雲原生化轉型過程中發現，與通過服務的資源使用率指標來幫助業務來決策服務實例副本數的調整的方式相比，定時擴縮容反而能讓研發同學更有信心，研發同學可以根據自己負責的業務服務的流量特徵，來設置定時地縮容或者擴容自己的服務實例數量。</p><p style="color:#24292f; text-align:start">為了滿足彈性伸縮場景的所有需求，我們設計實現了 KubeAutoScaler 組件，用來統一管理 HPA、VPA、定時伸縮等彈性伸縮策略配置。此外，如前所述，該組件與畫像系統相互協作，在混部場景下可以幫助在夜間對部分低流量的服務做縮容操作，釋放更多的資源供離線任務使用。</p><p style="color:#24292f; text-align:start">彈性伸縮方案在 GPU 服務場景幫我們避免了很多的資源浪費，特別是在測試環境。如圖所示，我們為 GPU 服務注入了一個名為 Queue-Proxy 的 Sidecar 容器，用來採集服務流量，當流量低於某個閾值時，會按照比例減少實例數；當流量為 0 並持續了一段時間之後，會完全縮零。當冷啓動時，請求會經過激活器 Activator，激活器再通知 KubeAutoScaler 進行服務擴容。生產環境的部分服務也開啓了這一套機制，但在流量低峯期不會完全縮容至零，會維持一個最小的副本數。<img alt="1070.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/1070.png" referrerpolicy="no-referrer"></p><span id="OSC_h1_12"></span><h1>四、容器資源和成本治理優化</h1><p style="color:#24292f; text-align:start">為了更好地提升整體的資源利用、降低基礎設施成本，與技術方案的落地週期長、複雜度高的特點比起來，通過治理方法往往能在較短時間內達到不錯的效果，特別是在應用服務容器化部署改造進行到後期的時候。我們通過以下 5 個方面的治理實踐，降本效果明顯。</p><span id="OSC_h2_13"></span><h2>機型替換</h2><p style="color:#24292f; text-align:start">因為歷史原因，我們的模型推理服務在剛開始的時候使用的是 V100 的機型，該機型顯存較大、GPU 算力較優，更適合用在訓練場景，在推理場景的話有點大材小用了。經過機型對比分析，我們選用了一個性價比較高的 A10 機型，推理服務的成本整體降低了 20% 左右，而且因為 A10 機型配備的 CPU 架構有升級，對前後處理有較高要求的推理服務而言穩定性和性能均有提升。從 V100 切換到 A10，主要的工作在於基礎鏡像的替換，因為部分模型服務可能使用了較低版本的 CUDA，但是 A10 卡的算力需要配備較高的 CUDA。另外，因為兩種卡的 GPU 算力也是有差異的，替換之後需要對推理結果做對比驗證才可以上線。我們基於流量回放的思路，設計了 AB 實驗功能幫助業務做切換測試。<img alt="155.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/155.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">在使用 CPU 計算資源的場景上，我們對算力要求一般、對 CPU 指令集因無特殊要求、對單核/多核性能無要求的服務，均將其使用的機型從 Intel 的切換到了 AMD 的，整體成本降低 14% 左右。</p><span id="OSC_h2_14"></span><h2>資源池管理</h2><p style="color:#24292f; text-align:start">不得不承認的是容器化初期業務方是佔據主動的，業務側會基於穩定性、資源供給量上的考量要求容器平台獨立建集羣，或者資源池，容器平台也會選擇粗放式管理而應承這些需求。隨着容器化的推進，業務側的信心也會增強，容器平台對資源的把控程度也會更好，我們逐步採取以下幾個動作來收斂資源的管理，提高整體的資源分配率：</p><ul><li><strong>冗餘量控制</strong>：業務的發佈是有周期的，我們會根據發佈週期，動態調整容器平台管理的資源冗餘量。在保證日常的迭代開發的同時，儘可能縮小冗餘量。</li><li><strong>集羣合併</strong>：統一規劃 Kubernetes 集羣，按照區域（上海、杭州、北京等）、網絡環境類型（測試、預發、生產）、業務形態（普通業務、中間件、基礎設施、管控集羣等）等維度討論和決策，下線不必要的集羣。</li><li><strong>資源池合併、規整機型</strong>：合併資源需求特徵比較相近的資源池（例如：計算型的、內存型的），選擇合適的機型。與業務溝通，下線或者合併利用率過低的小資源池。</li><li><strong>碎片整理</strong>：單靠調度器的優化，在調度時儘可能避免碎片力量有限。加上在線服務的變更頻率一般比較低，如果不做重調度，長時間累積下來，集羣中會存在大量碎片。所以，針對多副本的應用集羣，在健壯的優雅停機機制基礎上，我們適當進行了一些碎片整理任務（重建 Pod 自由調度、重調度、宿主機騰挪等），有效地減少了資源碎片。</li></ul><span id="OSC_h2_15"></span><h2>工作負載規格治理</h2><p style="color:#24292f; text-align:start">用户自定義工作負載的規格在雲原生場景下也是一個常用的做法，這看似對用户友好的做法卻對容器平台造成了一些挑戰，因為如果對用户設置規格的自由度不做一些限制，很可能出現一些非常不合理的規格設置（例如：6C120G、20C4G），會產生調度碎片、成本分攤計算標準也難統一。</p><p style="color:#24292f; text-align:start">為瞭解決規格的問題，我們對在線服務的資源規格做了限制，不允許用户隨意指定，而是由平台給出規格列表，由用户選擇使用。規格列表可以分資源池設計、也可以分業務場景設置。針對任務型的工作負載，我們定義了 3 種 CPU 類型的資源規格（普通型、計算型、內存型，分別對應不同的 CPU:內存比例）。針對特殊的任務需求，我們約定了資源規格當中 CPU:內存的範圍。針對使用 GPU 的任務，因每種 GPU 卡的 CPU/內存/顯存規格配比都是不一樣的，我們定義了針對每種 GPU 卡的 CU 單位，用户只需要選擇相應的 CU，填寫 CU 數量即可。規格約定之後，我們針對不同的規格做了差異化計費，保證了規格申請和成本分攤上的合理性。關於規格的定義和計費標準，詳見下表。<img alt="133.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/133.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_16"></span><h2>產品自建</h2><p style="color:#24292f; text-align:start">得物的基礎設施是在雲上，所以在業務發展過程中，部分服務能力我們是會直接選用雲上產品的。算法側的模型訓練任務，最開始的時候就是選用雲上產品，隨着容器化的推進，我們自建的 AI 平台（KubeAI 平台）逐步承接模型訓練任務，使得訓練任務的成本大幅下降。</p><p style="color:#24292f; text-align:start">自建 KubeAI 平台，使得我們將訓練使用的資源與在線服務、其它離線任務場景使用的資源納入了統一的管理體系，便於從全局的視角去合理地調度分配資源，為 AI 模型訓練場景拿到更多的可用資源。本文前述 2.5 小節，我們就是通過混部的方式，將在線服務的資源供給了訓練任務使用，當前已經在常態混部。<img alt="3678.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/3678.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_17"></span><h2>多雲策略</h2><p style="color:#24292f; text-align:start">作為雲上用户，多雲策略是我們的長期目標。在多雲之間獲得議價主動權、符合合規性要求、獲得更充足的資源供給。尤其今年 4 月份以來，隨着 GPT/AIGC 方面的爆發、政策因素導致單個雲商的 GPU 資源對我們供給不足，阻礙業務發展。我們及時採用多雲策略，將 GPU 業務分散到不同的雲供應商，保障業務正常開展。當然，多雲的接入不是一蹴而就的，而是需要分業務場景逐步推進，週期較長，難度較大，我們需要考慮以下問題：</p><ul><li>梳理業務，找到適合多雲的業務場景，或者找到適合在多雲之間靈活遷移的業務場景。</li><li>因不同雲供應上的機房可能在不同的區域，所以需要考慮跨地域服務訪問、中間件依賴問題。</li><li>跨雲供應商的數據訪問和傳輸問題，涉及到專線建設、成本問題。</li></ul><span id="OSC_h1_18"></span><h1>五、雲原生 AI 場景建設</h1><p style="color:#24292f; text-align:start">我們期望雲原生容器技術的落地是要覆蓋全場景的，要將雲原生技術在普通服務、中間件產品和特殊的業務場景上都能發揮其巨大優勢。目前 MySQL、Redis、Miluvs、ElasticSearch 等產品都已經在推進容器化。雲原生 AI 場景的建設，我們通過 KubeAI 平台的建設在持續推進。<img alt="=.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/=-.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">KubeAI 是得物 AI 平台，是我們將雲原生容器技術落地得物全站業務過程中，逐步收集和挖掘公司各業務域在 AI 模型研究和生產迭代過程中的需求，逐步建設而成的一個雲原生 AI 平台。KubeAI 以模型為主線提供了從模型開發，到模型訓練，再到推理 (模型) 服務管理，以及模型版本持續迭代的整個生命週期內的解決方案。此外，隨着 AIGC 的火熱發展，我們經過調研公司內部 AI 輔助生產相關需求，上線了 AIGC/GPT 服務，為得物豐富的業務場景提供了 GAI 能力，助力業務效果提升。關於 KubeAI 平台相關解決方案，我們之前發佈過一些文章，歡迎大家閲讀交流，這裏不再贅述。</p><span id="OSC_h1_19"></span><h1>六、展望</h1><p style="color:#24292f; text-align:start">雲原生容器技術在得物的落地開展還是比較快的，業務覆蓋面也比較廣泛。經過 2 年時間的實踐落地，已經全面深入資源管理系統、預算/成本管理機制、應用服務發佈流程、AI 算法等管理體系和業務場景。接下來：</p><ul><li>在容器化，我們會繼續推進中間件產品的容器化，進一步提升基礎設施的資源效率。</li><li>我們會繼續鞏固混部方案，繼續探索彈性容量、調度優化等方案，進一步提升資源效率。</li><li>在穩定性方面，我們會繼續關注容器平台/Kubernetes 本身的穩定性建設，防範風險，切實保證業務平穩運行。</li><li>與業務場景一起探索快速接入多雲，以及多雲之間的快速切換能力，保障業務規模在持續增長的情況下，容器基礎設施切換靈活、堅如磐石。</li></ul><p style="color:#24292f; text-align:start">*<strong>文/weidong</strong></p><p style="color:#252933; margin-left:0; margin-right:0; text-align:start">本文屬得物技術原創，更多精彩文章請看：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftech.dewu.com" target="_blank">得物技術官網</a></p><p style="color:#252933; margin-left:0; margin-right:0; text-align:start">未經得物技術許可嚴禁轉載，否則依法追究法律責任！</p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 02:52:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/10861087</guid>
            <link>https://my.oschina.net/u/5783135/blog/10861087</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[谷歌華人工程師毆打妻子致死]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>當地時間 1 月 19 日，美國加州聖克拉拉縣檢察官辦公室官網發佈新聞稿，稱聖克拉拉一名男子被指控謀殺妻子。新聞稿寫道，警方在家中發現了身上有濺落血跡的 Liren Chen（27 歲），他妻子的屍體就在附近。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-ff37b9d973ac9b4aa4b6c2bf4abb7253fb0.png" referrerpolicy="no-referrer"></p><p>via&nbsp;<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcountyda.sccgov.org%2Fnews%2Fnews-release%2Fsanta-clara-man-charged-wifes-murder" target="_blank">https://countyda.sccgov.org/news/news-release/santa-clara-man-charged-wifes-murder</a></em></u></p></blockquote><p>當地媒體《The San Francisco Standard》報道，警方在法庭記錄中稱，Liren Chen 在聖克拉拉 Valley Way 家中多次毆打妻子的頭部。再結合聖克拉拉縣檢察官辦公室透露的信息，Liren Chen 被抓獲時右手極度腫脹並且發紫，他的身上和屋內到處都是血跡，初步可以判定該男子是用拳頭毆打妻子致死。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-acd15e46a086a302acdac8c5d0d5297f7ac.png" referrerpolicy="no-referrer"></p><p>via&nbsp;<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsfstandard.com%2F2024%2F01%2F19%2Fgoogle-engineer-murder-liren-chen-xuanyi-yu%2F" target="_blank">https://sfstandard.com/2024/01/19/google-engineer-murder-liren-chen-xuanyi-yu/</a></u></em></p></blockquote><p>檢察官證實 Liren Chen 是谷歌員工，目前還在醫院接受治療。截至美國當地時間 19 日早上，Liren Chen 尚未出庭面臨審訊。</p><p>據報道，Liren Chen 於 2018 年畢業於清華大學，獲得電子信息工程學位，隨後於 2018-2019 年在美國加州大學聖迭戈分校獲得計算機科學碩士學位。他在灣區進行了短暫的實習之後，於 2020 年 3 月加入谷歌，擔任軟件工程師；其妻子 Xuanyi Yu 也是 2018 年畢業於清華大學，獲得電子信息工程學位，隨後於 2018-2019 年在美國加州大學聖迭戈分校學習，同樣獲得計算機科學碩士學位。她畢業後先是在亞馬遜實習，隨後加入亞馬遜工作，2021 年 6 月加入谷歌。</p><p><img src="https://oscimg.oschina.net/oscnet/up-7fc2adccf7dd5164f1220c6e14d14a221d5.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">此前有消息稱涉案 2 人疑似被谷歌裁員，不過有自稱 Liren Chen 同事的知情人士告訴記者，涉案 2 人並未被裁員。</span></p><p>《The San Francisco Standard》的報道還指出，Liren Chen 因涉嫌殺害妻子被指控謀殺重罪，如果罪名成立，他將面臨終身監禁，不得假釋。公開信息顯示，當地檢方已對 Liren Chen 初步提起控訴，但由於他正在住院治療，對他的提審已經兩度推遲，目前聖克拉拉縣高等法院官網公開顯示最新聆訊日期為 1 月 24 日。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0143e73ce092391f1ccb990f17eb1460d69.png" referrerpolicy="no-referrer"></p><p>相關信源：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fm.mp.oeeee.com%2Fa%2FBAAFRD000020240120902417.html" target="_blank">https://m.mp.oeeee.com/a/BAAFRD000020240120902417.html</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 11:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276243</guid>
            <link>https://www.oschina.net/news/276243</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Altman 擬籌集數十億美元，建立 AI 芯片工廠網絡]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>彭博社援引知情人士<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2024-01-19%2Faltman-seeks-to-raise-billions-for-network-of-ai-chip-factories" target="_blank">消息稱</a>，OpenAI 首席執行官 Sam Altman 正在計劃募集數十億美元資金，用於<span style="background-color:#ffffff; color:#222222">建立</span>一家芯片合資企業。他的目標是利用這筆資金建立一個工廠網絡，生產半導體。</p><p><img alt="" height="281" src="https://oscimg.oschina.net/oscnet/up-e93d79d18a9c32a7a6caae67ce76611abfe.jpg" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">Altman 已與幾家大型潛在投資者進行了討論。</span><span style="background-color:#ffffff; color:#2b2b2b">一些知情人士表示，</span><span style="background-color:#ffffff; color:#222222">與 Altman 進行過討論的公司包括日本軟銀集團和阿聯酋 AI 企業 G42。</span><span style="background-color:#ffffff; color:#2b2b2b">該項目將涉及與頂級芯片製造商合作，晶圓廠網絡將覆蓋全球。</span></p><p><span style="background-color:#ffffff; color:#222222">不過談判尚處於早期階段，參與的合作伙伴和出資人的完整名單尚未確定。此前也曾有<a href="https://www.oschina.net/news/272549/openai-valuation-100-billion-funding-round">消息稱</a>，</span><span style="background-color:#ffffff; color:#000000">OpenAI 與阿布扎比 G42 進行了商討，為一家新的芯片合資企業募集資金，並已討論過從 G42 籌集 80 億至 100 億美元的資金。</span></p><p><span style="background-color:#ffffff; color:#222222">當下的一些市場預測指出，相關 AI 芯片的生產或將跟不上預期需求。部分知情人士稱，Altman 的籌資行動也反映出，他擔心隨着 AI 的普及，大範圍部署會缺少足夠的芯片。</span></p><p><span style="background-color:#ffffff; color:#222222">Altman 認為，為了確保到 2030 年有足夠的芯片供應，AI 行業需要現在就開始採取行動。此外，</span>OpenAI 的最大投資者<span style="background-color:#ffffff; color:#000000">微軟似乎也對該項目表現出了興趣。</span></p><p><span style="background-color:#ffffff; color:#000000">OpenAI 並沒有試圖涉足芯片代工領域。相反，OpenAI 的計劃似乎是將籌集到的資金注入台積電、三星電子和英特爾等尖端芯片製造商，</span><span style="background-color:#ffffff; color:#222222">這些公司都是 OpenAI 的潛在合作伙伴</span><span style="background-color:#ffffff; color:#000000">。</span></p><p><strong><span style="background-color:#ffffff; color:#000000">相關閲讀：</span></strong></p><ul><li><a href="https://www.oschina.net/news/269384/openai-buy-ai-chips-startup-sam-altma" target="news">OpenAI 承諾從 Altman 投資的初創公司購買 AI 芯片</a></li><li><p style="margin-left:0px; margin-right:0px; text-align:start"><a href="https://www.oschina.net/news/272549/openai-valuation-100-billion-funding-round" target="_blank">OpenAI 擬以 1000 億美元估值開啓新一輪融資</a></p></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 04:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276217/altman-raise-billions-ai-chip-factories</guid>
            <link>https://www.oschina.net/news/276217/altman-raise-billions-ai-chip-factories</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[夸克 App 上線搜索問答產品「元知」等大模型應用]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#222222">夸克 App 宣佈全面升級多個功能板塊和智能工具，推出搜索問答產品「元知」，上線夸克 PC 版以及夸克聽記等新產品。將圍繞智能助手的定位，為用户提供「內容產品+智能工具」的服務矩陣。</span></p><p><span style="background-color:#ffffff; color:#222222"><img alt="" height="534" src="https://oscimg.oschina.net/oscnet/up-30a3f66aba80ac6e349ddbb7c3ee127c67b.webp" width="300" referrerpolicy="no-referrer"></span></p><div style="text-align:start"><p style="color:#222222; margin-left:0; margin-right:0">據介紹，此次升級主要依託自研大模型能力，搜索問答產品「元知」綜合全網優質內容，用户可以在搜索結果中，查看到 AIGC 總結提煉出的回答內容，其中包含圖文、視頻等多種形式，輔助用户更便捷、高效地獲取信息。</p></div><div style="text-align:start"><p style="color:#222222; margin-left:0; margin-right:0">同時，為了更好地滿足用户在不同場景中的搜索體驗，夸克上線了 Windows 系統 PC 版，集合搜索、網盤、掃描等核心功能，給辦公、學習用户更好的大屏幕搜索體驗。進一步實現個人數字資產在手機、電腦、iPad 三端的一體化信息服務。</p></div><div style="text-align:start"><p style="color:#222222; margin-left:0; margin-right:0">此前，夸克 App 還升級了健康搜索和學習搜索的服務體驗。針對健康領域，通過 AIGC 首答、夸克健康百科、智能篩查和夸克健康助手等產品，打造健康信息查詢的新體驗。</p></div></div>
                                    ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 03:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276216</guid>
            <link>https://www.oschina.net/news/276216</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[QAnything —— 知識庫問答引擎]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#1f2328; text-align:start"><strong>QAnything</strong><span>&nbsp;</span>(<strong>Q</strong>uestion and<span>&nbsp;</span><strong>A</strong>nswer based on<span>&nbsp;</span><strong>Anything</strong>) 是致力於支持任意格式文件或數據庫的本地知識庫問答系統，可斷網安裝使用。</p><p style="color:#1f2328; text-align:start">你的任何格式的本地文件都可以往裏扔，即可獲得準確、快速、靠譜的問答體驗。</p><p style="color:#1f2328; text-align:start">目前已支持格式：<strong>PDF</strong>，<strong>Word(doc/docx)</strong>，<strong>PPT</strong>，<strong>Markdown</strong>，<strong>Eml</strong>，<strong>TXT</strong>，<strong>圖片（jpg，png 等）</strong>，<strong>網頁鏈接</strong>，更多格式，敬請期待...</p><h3 style="text-align:start">特點</h3><ul><li>數據安全，支持全程拔網線安裝使用。</li><li>支持跨語種問答，中英文問答隨意切換，無所謂文件是什麼語種。</li><li>支持海量數據問答，兩階段向量排序，解決了大規模數據檢索退化的問題，數據越多，效果越好。</li><li>高性能生產級系統，可直接部署企業應用。</li><li>易用性，無需繁瑣的配置，一鍵安裝部署，拿來就用。</li><li>支持選擇多知識庫問答。</li></ul><h3 style="text-align:start">架構</h3><p><img alt="" height="443" src="https://oscimg.oschina.net/oscnet/up-dd1c3f39b8916876b15ad0cc25aeaa24bfa.png" width="500" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/qanything</guid>
            <link>https://www.oschina.net/p/qanything</link>
        </item>
    </channel>
</rss>
