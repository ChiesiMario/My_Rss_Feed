<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 30 Dec 2023 02:52:50 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[Infinigen —— 无限高质量 3D 数据生成器]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Infinigen 是无限高质量 3D 数据生成器，使用程序生成的无限逼真世界。这些数据 100% 通过程序化生成，不需要外部资产，也不依赖 AI，并且是免费开源的，生成质量非常高，据称可以达到以假乱真的地步，甚至是花瓣上的皱纹都可定制。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-89f0a56c9b3a6cc8ff8cedb7501aecff0e0.png" referrerpolicy="no-referrer"></p><p>Infinigen 由普林斯顿视觉和学习实验室开发：</p><ul><li>基于 Blender 编写</li><li>每个小细节都是随机的和可定制的，甚至是花瓣上的皱纹</li><li>自然界中多样的物体和场景：植物、动物、地形；火、云、雨和雪</li><li>Groundtruth 自动标注：光流、3D 场景流、深度、表面法线、全景分割、遮挡边界</li></ul><p>其主要特性和功能包括：</p><p>1. 程序化：Infinigen 是一个程序生成器，它完全使用随机的数学规则来创建所有的形状和材料，从宏观结构到微观细节。Infinigen 可以创建无限的变化。用户可以通过覆盖随机化的默认参数来完全控制资产的生成。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-f9d8159ed3837b185ace5578b1e2d6b0973.png" referrerpolicy="no-referrer"></p><p>2. 多样化：Infinigen 为自然世界中的多样化对象和场景提供生成器，包括植物、动物、地形，以及火、云、雨、雪等自然现象。当前对自然的关注是由于观察到哺乳动物的视觉在自然世界中进化。然而，预计 Infinigen 将随着时间的推移扩展到覆盖建筑环境和人造物体。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e4874348efa3dd1ae71f5d170027ed35b43.png" referrerpolicy="no-referrer"></p><p>3. 真实的几何形状：Infinigen 针对计算机视觉研究进行了优化，特别是 3D 视觉。Infinigen 不使用 bump/normal-maps、全透明度或其他伪造几何细节的技术。Infinigen 的所有细微的几何细节都是真实的，确保了精确的 3D 地面真实性。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-2579dc0164572ce5aa1add312a321e16803.png" referrerpolicy="no-referrer"></p><p>4. 自动注释：Infinigen 可以自动生成各种计算机视觉任务的高质量注释，包括光流、3D 场景流、深度、表面法线、全景分割、遮挡边界。因为用户可以完全访问渲染过程，所以注释很容易定制。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-cbb0ac43739fd281ebc6ccdd23cead6aff3.png" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 02:40:45 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/infinigen</guid>
            <link>https://www.oschina.net/p/infinigen</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 嵌入式软件平台框架 VSF]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-vsf----versaloon-software-framework" class="anchor" href="https://gitee.com/vsfteam/vsf#vsf----versaloon-software-framework"></a>VSF -- Versaloon Software Framework</h1><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Fblob%2Fmaster%2FLICENSE"><img src="https://img.shields.io/github/license/vsfteam/vsf.svg" alt="GitHub" referrerpolicy="no-referrer"></a></p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Factions%2Fworkflows%2Fwindows-build.yml"><img src="https://github.com/vsfteam/vsf/actions/workflows/windows-build.yml/badge.svg" alt="windows-build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Factions%2Fworkflows%2Fcmake-native-build.yml"><img src="https://github.com/vsfteam/vsf/actions/workflows/cmake-native-build.yml/badge.svg" alt="cmake-native-build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Factions%2Fworkflows%2Fcmake-arm-cross-build.yml"><img src="https://github.com/vsfteam/vsf/actions/workflows/cmake-arm-cross-build.yml/badge.svg" alt="cmake-arm-cross-build" referrerpolicy="no-referrer"></a></p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsf-linux%2Fvsf.linux%2Factions%2Fworkflows%2Fwindows-build.yml"><img src="https://github.com/vsf-linux/vsf.linux/actions/workflows/windows-build.yml/badge.svg?branch=vsf-sync" alt="vsf.linux windows build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsf-linux%2Fvsf.linux%2Factions%2Fworkflows%2Fcmake-arm-cross-build.yml"><img src="https://github.com/vsf-linux/vsf.linux/actions/workflows/cmake-arm-cross-build.yml/badge.svg?branch=vsf-sync" alt="cmake-arm-cross-build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsf-linux%2Fvsf.linux%2Factions%2Fworkflows%2Fcmake-native-build.yml"><img src="https://github.com/vsf-linux/vsf.linux/actions/workflows/cmake-native-build.yml/badge.svg?branch=vsf-sync" alt="cmake-native-build" referrerpolicy="no-referrer"></a></p><p><a href="https://gitee.com/vsfteam/vsf/blob/master/README.md">English</a> |</p><p>VSF 全称是 Versaloon Software Framework，是一个基于 Apache2.0 协议的开源嵌入式软件平台框架。包含了从底层硬件的 hal 驱动、抢占式多任务内核、各种服务和组件。全部代码使用 C 语言，以及面向对象的方式实现。</p><h2><a id="user-content-整体框架" class="anchor" href="https://gitee.com/vsfteam/vsf#%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6"></a>整体框架</h2><h2><a id="user-content-目录" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%9B%AE%E5%BD%95"></a>目录</h2><table><thead><tr><th>目录名</th><th>描述</th></tr></thead><tbody><tr><td>document</td><td>文档</td></tr><tr><td>doxygen</td><td>doxygen 配置</td></tr><tr><td>example</td><td>示例代码</td></tr><tr><td>hardware</td><td>VSF 开发板硬件资料</td></tr><tr><td>patch</td><td>一些补丁（第三方库补丁等等）</td></tr><tr><td>script</td><td>一些工具脚本</td></tr><tr><td> cmake</td><td>cmake 工具脚本</td></tr><tr><td>source</td><td>VSF 源代码</td></tr><tr><td> component</td><td>组件（文件系统、协议栈、UI、外部芯片驱动）</td></tr><tr><td> hal</td><td>硬件抽象层（芯片 arch 支持、芯片驱动）</td></tr><tr><td> kernel</td><td>内核</td></tr><tr><td> osa_service</td><td>依赖内核的软件服务组件</td></tr><tr><td> service</td><td>软件服务组件</td></tr><tr><td> shell</td><td>「皮肤」</td></tr><tr><td> utilities</td><td>基础软件工具（一些预处理功能、编译器支持、列表等等）</td></tr></tbody></table><h2><a id="user-content-内核" class="anchor" href="https://gitee.com/vsfteam/vsf#%E5%86%85%E6%A0%B8"></a>内核</h2><p>基于事件驱动的抢占式多任务内核，支持 51、8bit MCU、32/64 bit arm、riscv、x86 等等各种构架的芯片。</p><ul><li>事件驱动，有事件运行，没事件休眠</li><li>抢占模式下，任务切换由硬件实现，任务优先级就是硬件 swi（software interrupt）的优先级</li><li>不同优先级抢占，同一优先级协作</li><li>可以运行在其他系统或者 RTOS 中，也可以运行在一个或者几个 SWI 中断中（和其他 RTOS 并存）。</li><li>多种任务形式
<ul><li>事件处理任务 -- 最小资源占用，最简配置下占用 20 字节 ram，常用配置下占用 40 字节 ram</li><li>pt 任务 -- 接近独立堆栈任务开发方式的共享堆栈任务</li><li>独立堆栈任务 -- 依赖 libc 中的 setjmp 库</li><li>fsm 状态机任务</li><li>「皮肤」中的其他任务封装形式，比如 pthread</li></ul></li><li>信号量、互斥量、触发器、队列等等常用 IPC 工具</li></ul><h2><a id="user-content-组件" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%BB%84%E4%BB%B6"></a>组件</h2><ul><li>合理的框架设计，软件高度可以复用</li><li>尽可能提供申明式的开发方式</li><li>标准化接口，第三方软件一次性移植，全平台适配</li><li>软件组件/框架
<ul><li>distbus -- 分布式总线框架</li><li>fifo</li><li>heap</li><li>json</li><li>pool -- 内存池</li><li>stream -- 流接口</li><li>trace</li></ul></li><li>组件
<ul><li>fs -- 文件系统，支持 VFS（可使用第三方的文件系统）</li><li>input -- 输入系统</li><li>mal -- 块设备</li><li>scsi -- SCSI 设备</li><li>tcpip -- TCPIP 协议栈以及 netdrv 网络设备（可使用第三方的 TCPIP 协议栈）</li><li>ui -- UI 以及显示设备（可使用第三方的 GUI）</li><li>usb -- USB 主从机协议栈</li><li>bt -- 蓝牙协议栈（使用第三方的 btstack）</li></ul></li></ul><h2><a id="user-content-硬件抽象层" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%A1%AC%E4%BB%B6%E6%8A%BD%E8%B1%A1%E5%B1%82"></a>硬件抽象层</h2><ul><li>标准 hal 接口，统一 API -- 比如：vsf_spi_init 可以用于所有 VSF 中支持的 SPI，包括芯片自带 SPI、GPIO 模拟的 SPI、通过 USB 外扩的 SPI，通过分布式总线访问的远端 SPI</li><li>简化开发的 IP 核驱动 -- 移植仅需要实现时钟、复位、中断等等 IP 核心之外的功能</li><li>各种接口封装模板</li><li>接口
<ul><li>PM</li><li>GPIO</li><li>SPI</li><li>I2C</li><li>PWM</li><li>ADC</li><li>SWI</li><li>USART</li><li>FLASH</li><li>USB</li><li>ethernet</li></ul></li></ul><h2><a id="user-content-皮肤" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%9A%AE%E8%82%A4"></a>「皮肤」</h2><p>「皮肤」可以把 VSF「伪装」成其他系统，使得可以直接使用基于其他系统的应用代码。</p><ul><li>SDL -- 可以直接使用一些基于 SDL 的应用层代码</li><li>linux -- 可以直接使用一些基于 linux 的应用层代码
<ul><li>posix</li><li>devfs</li><li>socket</li><li>console</li><li>一些 lib 库的实现
<ul><li>libusb</li><li>libgen</li></ul></li></ul></li></ul><h2><a id="user-content-第三方" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%AC%AC%E4%B8%89%E6%96%B9"></a>第三方</h2><table><thead><tr><th>名字</th><th>路径</th><th>许可</th><th>链接</th></tr></thead><tbody><tr><td>btstack</td><td>source/component/3rd-party/btstack/raw</td><td>Other</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbluekitchen%2Fbtstack">https://github.com/bluekitchen/btstack</a></td></tr><tr><td>coremark</td><td>source/component/3rd-party/coremark/raw</td><td>Apache</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Feembc%2Fcoremark">https://github.com/eembc/coremark</a></td></tr><tr><td>freetype</td><td>source/component/3rd-party/freetype/raw</td><td>FreeType</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Ffreetype.org%2F">https://freetype.org/</a></td></tr><tr><td>zlib</td><td>source/component/3rd-party/zlib/raw</td><td>zlib</td><td><a href="https://gitee.com/link?target=http%3A%2F%2Fzlib.net%2F">http://zlib.net/</a></td></tr><tr><td>nuklear</td><td>source/component/3rd-party/nuklear/raw</td><td>MTI</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FImmediate-Mode-UI%2FNuklear">https://github.com/Immediate-Mode-UI/Nuklear</a></td></tr><tr><td>nnom</td><td>source/component/3rd-party/nnom/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fmajianjia%2Fnnom">https://github.com/majianjia/nnom</a></td></tr><tr><td>lua</td><td>source/component/3rd-party/lua/raw</td><td>MIT</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.lua.org%2F">https://www.lua.org/</a></td></tr><tr><td>lwip</td><td>source/component/3rd-party/lwip/raw</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fsavannah.nongnu.org%2Fprojects%2Flwip%2F">https://savannah.nongnu.org/projects/lwip/</a></td></tr><tr><td>libpng</td><td>source/component/3rd-party/libpng/raw</td><td>PNG2</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flibpng.sf.net">https://libpng.sf.net</a></td></tr><tr><td>libjpeg-turbo</td><td>source/component/3rd-party/libjpeg-turbo/raw</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flibjpeg-turbo.org%2F">https://libjpeg-turbo.org/</a></td></tr><tr><td>SDL_ttf</td><td>source/shell/media/sdl2/3rd-party/SDL_ttf</td><td>zlib</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fhg.libsdl.org%2FSDL_ttf%2F">https://hg.libsdl.org/SDL_ttf/</a></td></tr><tr><td>SDL_image</td><td>source/shell/media/sdl2/3rd-party/SDL_image</td><td>zlib</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fhg.libsdl.org%2FSDL_image%2F">https://hg.libsdl.org/SDL_image/</a></td></tr><tr><td>lvgl</td><td>source/component/3rd-party/lvgl/raw/lvgl</td><td>MIT</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flvgl.io%2F">https://lvgl.io/</a></td></tr><tr><td>lv_lib_freetype</td><td>source/component/3rd-party/lvgl/extension/lv_lib_freetype/raw</td><td>MIT</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flvgl.io%2F">https://lvgl.io/</a></td></tr><tr><td>CMSIS</td><td>source/utilities/compiler/arm/3rd-party/CMSIS</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FARM-software%2FCMSIS_5">https://github.com/ARM-software/CMSIS_5</a></td></tr><tr><td>evm</td><td>source/component/3rd-party/evm/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fscriptiot%2Fevm">https://github.com/scriptiot/evm</a></td></tr><tr><td>LingLongGUI</td><td>source/component/3rd-party/LingLongGUI/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/gzbkey/LingLongGUI">https://gitee.com/gzbkey/LingLongGUI</a></td></tr><tr><td>PLOOC</td><td>source/utilities/3rd-party/PLOOC/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FGorgonMeducer%2FPLOOC">https://github.com/GorgonMeducer/PLOOC</a></td></tr><tr><td>mbedtls</td><td>source/component/3rd-party/mbedtls/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Ftls.mbed.org%2F">https://tls.mbed.org/</a></td></tr><tr><td>GuiLite</td><td>source/component/3rd-party/GuiLite/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fidea4good%2FGuiLite">https://github.com/idea4good/GuiLite</a></td></tr><tr><td>Segger_RTT</td><td>source/component/3rd-party/segger/raw/RTT</td><td>segger</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwiki.segger.com%2FRTT">https://wiki.segger.com/RTT</a></td></tr><tr><td>Segger_SystemView</td><td>source/component/3rd-party/segger/raw/SystemView</td><td>segger</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwiki.segger.com%2FSystemView">https://wiki.segger.com/SystemView</a></td></tr><tr><td>nuconsole</td><td>source/component/3rd-party/nuconsole/raw</td><td>nuvoton</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.nuvoton.com.cn%2F">https://www.nuvoton.com.cn/</a></td></tr><tr><td>AIC8800M_SDK</td><td>source/hal/driver/AIC/AIC8800/vendor</td><td>aic</td><td><a href="https://gitee.com/link?target=http%3A%2F%2Fwww.aicsemi.com%2F">http://www.aicsemi.com/</a></td></tr><tr><td>awtk</td><td></td><td>LGPL 2.1</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zlg.cn%2Findex%2Fpub%2Fawtk.html">https://www.zlg.cn/index/pub/awtk.html</a></td></tr><tr><td>littlefs</td><td>source/component/3rd-party/littlefs/raw</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Flittlefs-project%2Flittlefs">https://github.com/littlefs-project/littlefs</a></td></tr><tr><td>getopt_long</td><td>source/shell/sys/linux/lib/3rd-party/getopt</td><td>OpenBSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fopenbsd%2Fsrc">https://github.com/openbsd/src</a></td></tr><tr><td>regex</td><td>source/shell/sys/linux/lib/3rd-party/regex</td><td>OpenBSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fopenbsd%2Fsrc">https://github.com/openbsd/src</a></td></tr><tr><td>fnmatch</td><td>source/shell/sys/linux/lib/3rd-party/fnmatch</td><td>BSD</td><td><a href="https://gitee.com/link?target=http%3A%2F%2Fwww.jbox.dk%2Fsanos%2Fsource%2Flib%2Ffnmatch.c.html">http://www.jbox.dk/sanos/source/lib/fnmatch.c.html</a></td></tr><tr><td>glob</td><td>source/shell/sys/linux/lib/3rd-party/glob</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcloudius-systems%2Fmusl">https://github.com/cloudius-systems/musl</a></td></tr><tr><td>setjmp</td><td>source/hal/arch/x86/win</td><td>BSD</td><td></td></tr><tr><td>libtuv</td><td>source/shell/sys/linux/lib/3rd-party/libtuv/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FSamsung%2Flibtuv">https://github.com/Samsung/libtuv</a></td></tr></tbody></table><h2><a id="user-content-文档" class="anchor" href="https://gitee.com/vsfteam/vsf#%E6%96%87%E6%A1%A3"></a><a href="https://gitee.com/vsfteam/vsf/blob/master/document/README_zh.md">文档</a></h2>]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 02:35:45 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/vsfteam/vsf</guid>
            <link>https://gitee.com/vsfteam/vsf</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 混合专家模型 (MoE) 详解]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 编辑器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;" data-mpa-powered-by="yiban.io"><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">随着 Mixtral 8x7B (announcement, model card) 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的 Transformer 模型在开源人工智能社区引起了广泛关注。在本篇博文中，我们将深入探讨 MoEs 的核心组件、训练方法，以及在推理过程中需要考量的各种因素。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们开始吧！</p><span id="OSC_h2_1"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">简短总结</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoEs):</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      与稠密模型相比， 
     <strong style="color: black;">预训练速度更快</strong></section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      与具有相同参数数量的模型相比，具有更快的 
     <strong style="color: black;">推理速度</strong></section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      需要 
     <strong style="color: black;">大量显存</strong>，因为所有专家系统都需要加载到内存中 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      在 
     <strong style="color: black;">微调方面存在诸多挑战</strong>，但，近期的研究，表明，对混合专家模型进行 
     <strong style="color: black;">指令调优具有很大的潜力</strong>。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们开始吧！</p><span id="OSC_h2_2"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">什么是混合专家模型？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">那么，究竟什么是一个混合专家模型 (MoE) 呢？作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">稀疏 MoE 层</strong>: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干「专家」(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">门控网络或路由</strong>: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，「More」这个令牌可能被发送到第二个专家，而「Parameters」这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。 
    </section></li></ul><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006177" data-ratio="0.7703703703703704" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/8f46f9ec-4ee7-42d6-8b1d-68962ccb7e8b.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Switch Transformers paper 论文中的 MoE layer 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">尽管混合专家模型 (MoE) 提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">训练挑战</strong>: 虽然 MoE 能够实现更高效的计算预训练，但它们在微调阶段往往面临泛化能力不足的问题，长期以来易于引发过拟合现象。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">推理挑战</strong>: MoE 模型虽然可能拥有大量参数，但在推理过程中只使用其中的一部分，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到内存中，因此对内存的需求非常高。以 Mixtral 8x7B 这样的 MoE 为例，需要足够的 VRAM 来容纳一个 47B 参数的稠密模型。之所以是 47B 而不是 8 x 7B = 56B，是因为在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。此外，假设每个令牌只使用两个专家，那么推理速度 (以 FLOPs 计算) 类似于使用 12B 模型 (而不是 14B 模型)，因为虽然它进行了 2x7B 的矩阵乘法计算，但某些层是共享的。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">了解了 MoE 的基本概念后，让我们进一步探索推动这类模型发展的研究。</p><span id="OSC_h2_3"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">混合专家模型简史</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoE) 的理念起源于 1991 年的论文 Adaptive Mixture of Local Experts。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为「专家」) 处理训练样本的不同子集，专注于输入空间的特定区域。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络发挥作用的地方，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在 2010 至 2015 年间，两个独立的研究领域为混合专家模型 (MoE) 的后续发展做出了显著贡献:</p><ol data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">组件专家</strong>: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，Eigen、Ranzato 和 Ilya 的研究，探索了将 MoE 作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">条件计算</strong>: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活或停用网络组件的方法。 
    </section></li></ol><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这些研究的融合促进了在自然语言处理 (NLP) 领域对混合专家模型的探索。特别是在 2017 年，Shazeer 等人 (团队包括 Geoffrey Hinton 和 Jeff Dean，后者有时被戏称为 「谷歌的 Chuck Norris」) 将这一概念应用于 137B 的 LSTM (当时被广泛应用于 NLP 的架构，由 Schmidhuber 提出)。通过引入稀疏性，这项工作在保持极高规模的同时实现了快速的推理速度。这项工作主要集中在翻译领域，但面临着如高通信成本和训练不稳定性等多种挑战。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006175" data-ratio="0.48240635641316687" data-type="png" data-w="881" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/48db56ce-0292-4d84-a174-c5c7a03d5e8b.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Outrageously Large Neural Network 论文中的 MoE layer 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoE) 的引入使得训练具有数千亿甚至万亿参数的模型成为可能，如开源的 1.6 万亿参数的 Switch Transformers 等。这种技术不仅在自然语言处理 (NLP) 领域得到了广泛应用，也开始在计算机视觉领域进行探索。然而，本篇博客文章将主要聚焦于自然语言处理领域的应用和探讨。</p><span id="OSC_h2_4"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">什么是稀疏性?</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">稀疏性的概念采用了条件计算的思想。在传统的稠密模型中，所有的参数都会对所有输入数据进行处理。相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们深入分析 Shazeer 对混合专家模型 (MoE) 在翻译应用中的贡献。条件计算的概念 (即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这种稀疏性设置确实带来了一些挑战。例如，在混合专家模型 (MoE) 中，尽管较大的批量大小通常有利于提高性能，但当数据通过激活的专家时，实际的批量大小可能会减少。比如，假设我们的输入批量包含 10 个令牌， <strong style="color: black;">可能会有五个令牌被路由到同一个专家，而剩下的五个令牌分别被路由到不同的专家。这导致了批量大小的不均匀分配和资源利用效率不高的问题</strong>。在接下来的部分中，将会讨论让 MoE 高效运行的其他挑战以及相应的解决方案。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">那我们应该如何解决这个问题呢？一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E):</p><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="y = \sum_{i=1}^{n} G(x)_i E_i(x)
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><svg
                xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewBox="0 -1562.5 8246.1 2808.5" aria-hidden="true" style="-webkit-overflow-scrolling: touch;vertical-align: -2.819ex;width: 18.656ex;height: 6.354ex;max-width: 300% !important;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(1823.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(509.9, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mi" transform="translate(3434.2, 0)"><path data-c="47" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(4220.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4609.2, 0)"><path data-c="78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="msub" transform="translate(5181.2, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(389, -150) scale(0.707)"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msub" transform="translate(5864.2, 0)"><g data-mml-node="mi"><path data-c="45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(738, -150) scale(0.707)"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(6896.1, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7285.1, 0)"><path data-c="78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(7857.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在这种设置下，虽然所有专家都会对所有输入进行运算，但通过门控网络的输出进行加权乘法操作。但是，如果 G (门控网络的输出) 为 0 会发生什么呢？如果是这种情况，就没有必要计算相应的专家操作，因此我们可以节省计算资源。那么一个典型的门控函数是什么呢？一个典型的门控函数通常是一个带有 softmax 函数的简单的网络。这个网络将学习将输入发送给哪个专家。</p><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="G_\sigma(x) = \text{Softmax}(x \cdot W_g)
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -0.667ex;width: 24.749ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/addcdb77-1331-4c14-845f-06df5414abd2.svg" data-type="svg+xml" data-imgfileid="100006169"></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Shazeer 等人的工作还探索了其他的门控机制，其中包括带噪声的 TopK 门控 (Noisy Top-K Gating)。这种门控方法引入了一些可调整的噪声，然后保留前 k 个值。具体来说:</p><ol data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      添加一些噪声 
    </section></li></ol><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="H(x)_i = (x \cdot W_{\text{g}})_i + \text{StandardNormal()} \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i)
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -0.669ex;width: 60.565ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/26f83d98-1595-404e-9d4a-58c616d102c1.svg" data-type="svg+xml" data-imgfileid="100006173"></section></span><ol start="2" data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      选择保留前 K 个值 
    </section></li></ol><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="\text{KeepTopK}(v, k)_i = \begin{cases}
v_i &amp; \text{if } v_i \text{ is in the top } k \text{ elements of } v, \\
-\infty &amp; \text{otherwise.}
\end{cases}
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -2.148ex;width: 58.794ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/4505b4a9-5c66-45c5-9f72-1b2377d74dca.svg" data-type="svg+xml" data-imgfileid="100006171"></section></span><ol start="3" data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      应用 Softmax 函数 
    </section></li></ol><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k))
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -0.566ex;width: 37.6ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/70f4a245-7a7f-483c-b524-8795deb3ae04.svg" data-type="svg+xml" data-imgfileid="100006170"></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这种稀疏性引入了一些有趣的特性。通过使用较低的 k 值 (例如 1 或 2)，我们可以比激活多个专家时更快地进行训练和推理。为什么不仅选择最顶尖的专家呢？最初的假设是，需要将输入路由到不止一个专家，以便门控学会如何进行有效的路由选择，因此至少需要选择两个专家。Switch Transformers 就这点进行了更多的研究。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">我们为什么要添加噪声呢？这是为了专家间的负载均衡！</p><span id="OSC_h2_5"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">混合专家模型中令牌的负载均衡</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">正如之前讨论的，如果所有的令牌都被发送到只有少数几个受欢迎的专家，那么训练效率将会降低。在通常的混合专家模型 (MoE) 训练中，门控网络往往倾向于主要激活相同的几个专家。这种情况可能会自我加强，因为受欢迎的专家训练得更快，因此它们更容易被选择。为了缓解这个问题，引入了一个 <strong style="color: black;">辅助损失</strong>，旨在鼓励给予所有专家相同的重要性。这个损失确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。接下来的部分还将探讨专家容量的概念，它引入了一个关于专家可以处理多少令牌的阈值。在 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">transformers</code> 库中，可以通过 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">aux_loss</code> 参数来控制辅助损失。</p><span id="OSC_h2_6"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">MoEs and Transformers</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Transformer 类模型明确表明，增加参数数量可以提高性能，因此谷歌使用 GShard 尝试将 Transformer 模型的参数量扩展到超过 6000 亿并不令人惊讶。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">GShard 将在编码器和解码器中的每个前馈网络 (FFN) 层中的替换为使用 Top-2 门控的混合专家模型 (MoE) 层。下图展示了编码器部分的结构。这种架构对于大规模计算非常有效: 当扩展到多个设备时，MoE 层在不同设备间共享，而其他所有层则在每个设备上覆制。我们将在 「让 MoE 起飞」部分对这一点进行更详细的讨论。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006178" data-ratio="0.6046296296296296" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/8246ce4e-437b-4539-bf15-17cea720b24b.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     GShard 论文中的 MoE Transformer Encoder 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">为了保持负载平衡和训练效率，GShard 的作者除了引入了上一节中讨论的类似辅助损失外，还引入了一些关键变化:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">随机路由</strong>: 在 Top-2 设置中，我们始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">专家容量</strong>: 我们可以设定一个阈值，定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。专家容量是 MoE 中最重要的概念之一。为什么需要专家容量呢？因为所有张量的形状在编译时是静态确定的，我们无法提前知道多少令牌会分配给每个专家，因此需要一个固定的容量因子。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">GShard 的工作对适用于 MoE 的并行计算模式也做出了重要贡献，但这些内容的讨论超出了这篇博客的范围。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);"><strong style="color: black;">注意</strong>: 在推理过程中，只有部分专家被激活。同时，有些计算过程是共享的，例如自注意力 (self-attention) 机制，它适用于所有令牌。这就解释了为什么我们可以使用相当于 12B 稠密模型的计算资源来运行一个包含 8 个专家的 47B 模型。如果我们采用 Top-2 门控，模型会使用高达 14B 的参数。但是，由于自注意力操作 (专家间共享) 的存在，实际上模型运行时使用的参数数量是 12B。</p><span id="OSC_h2_7"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">Switch Transformers</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">尽管混合专家模型 (MoE) 显示出了很大的潜力，但它们在训练和微调过程中存在稳定性问题。Switch Transformers 是一项非常激动人心的工作，它深入研究了这些话题。作者甚至在 Hugging Face 上发布了一个 1.6 万亿参数的 MoE，拥有 2048 个专家，你可以使用 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">transformers</code> 库来运行它。Switch Transformers 实现了与 T5-XXL 相比 4 倍的预训练速度提升。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006176" data-ratio="0.5101851851851852" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/56bee9cf-5842-4bfd-b5a5-cdc79c60b93d.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Switch Transformer 论文中的 Switch Transformer Layer 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">就像在 GShard 中一样，作者用混合专家模型 (MoE) 层替换了前馈网络 (FFN) 层。Switch Transformers 提出了一个 Switch Transformer 层，它接收两个输入 (两个不同的令牌) 并拥有四个专家。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">与最初使用至少两个专家的想法相反，Switch Transformers 采用了简化的单专家策略。这种方法的效果包括:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      减少门控网络 (路由) 计算负担 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      每个专家的批量大小至少可以减半 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      降低通信成本 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      保持模型质量 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformers 也对 <strong style="color: black;">专家容量</strong> 这个概念进行了研究。</p><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="\text{Expert Capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) \times \text{capacity factor}
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -2.148ex;width: 58.45ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/52d4608e-4c61-4d62-b055-dc8f790f459a.svg" data-type="svg+xml" data-imgfileid="100006172"></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">上述建议的容量是将批次中的令牌数量均匀分配到各个专家。如果我们使用大于 1 的容量因子，我们为令牌分配不完全平衡时提供了一个缓冲。增加容量因子会导致更高的设备间通信成本，因此这是一个需要考虑的权衡。特别值得注意的是，Switch Transformers 在低容量因子 (例如 1 至 1.25) 下表现出色。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformer 的作者还重新审视并简化了前面章节中提到的负载均衡损失。在训练期间，对于每个 Switch 层的辅助损失被添加到总模型损失中。这种损失鼓励均匀路由，并可以使用超参数进行加权。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">作者还尝试了混合精度的方法，例如用 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">bfloat16</code> 精度训练专家，同时对其余计算使用全精度进行。较低的精度可以减少处理器间的通信成本、计算成本以及存储张量的内存。然而，在最初的实验中，当专家和门控网络都使用 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">bfloat16</code> 精度训练时，出现了不稳定的训练现象。这种不稳定性特别是由路由计算引起的，因为路由涉及指数函数等操作，这些操作对精度要求较高。因此，为了保持计算的稳定性和精确性，保持更高的精度是重要的。为了减轻不稳定性，路由过程也使用了全精度。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006174" data-ratio="0.2253731343283582" data-type="png" data-w="670" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/66532187-c893-46e4-8d55-3a04955edef9.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     使用混合精度不会降低模型质量并可实现更快的训练 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这个 Jupyter Notebook 展示了如何对 Switch Transformers 进行微调以进行摘要生成的详细指南。然而，在开始微调 Switch Transformers 之前，强烈建议您先阅读关于微调混合专家模型部分的内容。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformers 采用了编码器 - 解码器的架构，实现了与 T5 类似的混合专家模型 (MoE) 版本。GLaM 这篇工作探索了如何使用仅为原来 1/3 的计算资源 (因为 MoE 模型在训练时需要的计算量较少，从而能够显著降低碳足迹) 来训练与 GPT-3 质量相匹配的模型来提高这些模型的规模。作者专注于仅解码器 (decoder-only) 的模型以及少样本和单样本评估，而不是微调。他们使用了 Top-2 路由和更大的容量因子。此外，他们探讨了将容量因子作为一个动态度量，根据训练和评估期间所使用的计算量进行调整。</p><span id="OSC_h2_8"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">用 Router z-loss 稳定模型训练</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">之前讨论的平衡损失可能会导致稳定性问题。我们可以使用许多方法来稳定稀疏模型的训练，但这可能会牺牲模型质量。例如，引入 dropout 可以提高稳定性，但会导致模型质量下降。另一方面，增加更多的乘法分量可以提高质量，但会降低模型稳定性。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">ST-MoE 引入的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">Router z-loss</code> 在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">logits</code> 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要。为了深入了解这一机制，建议参考原始论文以获得更全面的细节。</p><span id="OSC_h2_9"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">专家如何学习？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">ST-MoE 的研究者们发现，编码器中不同的专家倾向于专注于特定类型的令牌或浅层概念。例如，某些专家可能专门处理标点符号，而其他专家则专注于专有名词等。与此相反，解码器中的专家通常具有较低的专业化程度。此外，研究者们还对这一模型进行了多语言训练。尽管人们可能会预期每个专家处理一种特定语言，但实际上并非如此。由于令牌路由和负载均衡的机制，没有任何专家被特定配置以专门处理某一特定语言。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006181" data-ratio="0.9258760107816711" data-type="png" data-w="742" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/1137474f-1950-4033-8f38-785166343282.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     ST-MoE 论文中显示了哪些令牌组被发送给了哪个专家的表格 
   </figcaption></figure><span id="OSC_h2_10"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">专家的数量对预训练有何影响？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">增加更多专家可以提升处理样本的效率和加速模型的运算速度，但这些优势随着专家数量的增加而递减 (尤其是当专家数量达到 256 或 512 之后更为明显)。同时，这也意味着在推理过程中，需要更多的显存来加载整个模型。值得注意的是，Switch Transformers 的研究表明，其在大规模模型中的特性在小规模模型下也同样适用，即便是每层仅包含 2、4 或 8 个专家。</p><span id="OSC_h2_11"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">微调混合专家模型</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);"><code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">4.36.0</code> 版本的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">transformers</code> 库支持 Mixtral 模型。你可以用以下命令进行安装: <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">pip install "transformers==4.36.0 --upgrade</code></p></blockquote><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">稠密模型和稀疏模型在过拟合的动态表现上存在显著差异。稀疏模型更易于出现过拟合现象，因此在处理这些模型时，尝试更强的内部正则化措施是有益的，比如使用更高比例的 dropout。例如，我们可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在微调过程中是否使用辅助损失是一个需要决策的问题。ST-MoE 的作者尝试关闭辅助损失，发现即使高达 11% 的令牌被丢弃，模型的质量也没有显著受到影响。令牌丢弃可能是一种正则化形式，有助于防止过拟合。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformers 的作者观察到，在相同的预训练困惑度下，稀疏模型在下游任务中的表现不如对应的稠密模型，特别是在重理解任务 (如 SuperGLUE) 上。另一方面，对于知识密集型任务 (如 TriviaQA)，稀疏模型的表现异常出色。作者还观察到，在微调过程中，较少的专家的数量有助于改善性能。另一个关于泛化问题确认的发现是，模型在小型任务上表现较差，但在大型任务上表现良好。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006182" data-ratio="0.38010471204188484" data-type="png" data-w="955" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/47346e7f-b742-4923-a63d-7ccecd2a6427.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     在小任务 (左图) 中，我们可以看到明显的过拟合，因为稀疏模型在验证集中的表现要差得多。在较大的任务 (右图) 中，MoE 则表现良好。该图来自 ST-MoE 论文 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">一种可行的微调策略是尝试冻结所有非专家层的权重。实践中，这会导致性能大幅下降，但这符合我们的预期，因为混合专家模型 (MoE) 层占据了网络的主要部分。我们可以尝试相反的方法: 仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006180" data-ratio="0.77" data-type="png" data-w="400" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/401ba984-a5f5-4772-95db-99c12f026ef7.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     通过仅冻结 MoE 层，我们可以在保持质量的同时加快训练速度。该图来自 ST-MoE 论文 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在微调稀疏混合专家模型 (MoE) 时需要考虑的最后一个问题是，它们有特别的微调超参数设置——例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006183" data-ratio="0.37570303712035996" data-type="png" data-w="889" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/0f29d064-6852-4055-99ce-fab522c4d6bd.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     降低学习率和调大批量可以提升稀疏模型微调质量。该图来自 ST-MoE 论文 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">此时，您可能会对人们微调 MoE 中遇到的这些挑战而感到沮丧，但最近的一篇论文 《MoEs Meets Instruction Tuning》 (2023 年 7 月) 带来了令人兴奋的发现。这篇论文进行了以下实验:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      单任务微调 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      多任务指令微调 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      多任务指令微调后接单任务微调 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">当研究者们对 MoE 和对应性能相当的 T5 模型进行微调时，他们发现 T5 的对应模型表现更为出色。然而，当研究者们对 Flan T5 (一种 T5 的指令优化版本) 的 MoE 版本进行微调时，MoE 的性能显著提升。更值得注意的是，Flan-MoE 相比原始 MoE 的性能提升幅度超过了 Flan T5 相对于原始 T5 的提升，这意味着 MoE 模型可能从指令式微调中获益更多，甚至超过了稠密模型。此外，MoE 在多任务学习中表现更佳。与之前关闭 <strong style="color: black;">辅助损失</strong> 函数的做法相反，实际上这种损失函数可以帮助防止过拟合。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006179" data-ratio="0.4456140350877193" data-type="png" data-w="855" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/dc2a1d41-2a2d-4e63-b2c3-015fb22f56f3.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     与稠密模型相比，稀疏模型从指令微调中受益更多。该图来自 MoEs Meets instructions Tuning 论文 
   </figcaption></figure><span id="OSC_h2_12"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">稀疏 VS 稠密，如何选择?</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">稀疏混合专家模型 (MoE) 适用于拥有多台机器且要求高吞吐量的场景。在固定的预训练计算资源下，稀疏模型往往能够实现更优的效果。相反，在显存较少且吞吐量要求不高的场景，稠密模型则是更合适的选择。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);"><strong style="color: black;">注意</strong>: 直接比较稀疏模型和稠密模型的参数数量是不恰当的，因为这两类模型基于的概念和参数量的计算方法完全不同。</p><span id="OSC_h2_13"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">让 MoE 起飞</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">最初的混合专家模型 (MoE) 设计采用了分支结构，这导致了计算效率低下。这种低效主要是因为 GPU 并不是为处理这种结构而设计的，而且由于设备间需要传递数据，网络带宽常常成为性能瓶颈。在接下来的讨论中，我们会讨论一些现有的研究成果，旨在使这些模型在预训练和推理阶段更加高效和实用。我们来看看如何优化 MoE 模型，让 MoE 起飞。</p><span id="OSC_h3_14"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">并行计算</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们简要回顾一下并行计算的几种形式:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">数据并行</strong>: 相同的权重在所有节点上覆制，数据在节点之间分割。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">模型并行</strong>: 模型在节点之间分割，相同的数据在所有节点上覆制。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">模型和数据并行</strong>: 我们可以在节点之间同时分割模型和数据。注意，不同的节点处理不同批次的数据。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">专家并行</strong>: 专家被放置在不同的节点上。如果与数据并行结合，每个节点拥有不同的专家，数据在所有节点之间分割。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在专家并行中，专家被放置在不同的节点上，每个节点处理不同批次的训练样本。对于非 MoE 层，专家并行的行为与数据并行相同。对于 MoE 层，序列中的令牌被发送到拥有所需专家的节点。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006184" data-ratio="0.5910194174757282" data-type="png" data-w="824" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/d166997f-8e65-483f-91e6-e5e4c1ccdf96.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Switch Transformers 论文中展示如何使用不同的并行技术在节点上分割数据和模型的插图 
   </figcaption></figure><span id="OSC_h3_15"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">容量因子和通信开销</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">提高容量因子 (Capacity Factor, CF) 可以增强模型的性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。一个合理的初始设置是采用 Top-2 路由、1.25 的容量因子，同时每个节点配置一个专家。在评估性能时，应根据需要调整容量因子，以在设备间的通信成本和计算成本之间找到一个平衡点。</p><span id="OSC_h3_16"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">部署技术</span><span style="display: none;"></span></h3><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);">您可以在 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">Inference Endpoints</code> 部署 mistralai/Mixtral-8x7B-Instruct-v0.1。</p></blockquote><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">部署混合专家模型 (MoE) 的一个关键挑战是其庞大的参数规模。对于本地使用情况，我们可能希望使用更小的模型。为了使模型更适合部署，下面是几种有用的技术:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      预先蒸馏实验: Switch Transformers 的研究者们进行了预先蒸馏的实验。他们通过将 MoE 模型蒸馏回其对应的稠密模型，成功保留了 30-40% 的由稀疏性带来的性能提升。预先蒸馏不仅加快了预训练速度，还使得在推理中使用更小型的模型成为可能。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      任务级别路由: 最新的方法中，路由器被修改为将整个句子或任务直接路由到一个专家。这样做可以提取出一个用于服务的子网络，有助于简化模型的结构。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。这样可以在不显著牺牲性能的情况下降低模型的复杂度。 
    </section></li></ul><span id="OSC_h3_17"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">高效训练</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">FasterMoE (2022 年 3 月) 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Megablocks (2022 年 11 月) 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。其核心优势在于，它不会丢弃任何令牌，并能高效地适应现代硬件架构 (支持块稀疏矩阵乘)，从而达到显著的加速效果。Megablocks 的创新之处在于，它不像传统 MoE 那样使用批量矩阵乘法 (这通常假设所有专家形状相同且处理相同数量的令牌)，而是将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的令牌分配。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006185" data-ratio="0.2851851851851852" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/d09a85e7-f59f-421c-8e9f-d75df34954c8.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     针对不同规模的专家和令牌数量的块稀疏矩阵乘法。该图来自 MegaBlocks 论文 
   </figcaption></figure><span id="OSC_h2_18"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">开源混合专家模型</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">目前，下面这些开源项目可以用于训练混合专家模型 (MoE):</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Megablocks: https://github.com/stanford-futuredata/megablocks 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Fairseq: https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      OpenMoE: https://github.com/XueFuzhao/OpenMoE 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">对于开源的混合专家模型 (MoE)，你可以关注下面这些:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Switch Transformers (Google): 基于 T5 的 MoE 集合，专家数量从 8 名到 2048 名。最大的模型有 1.6 万亿个参数。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      NLLB MoE (Meta): NLLB 翻译模型的一个 MoE 变体。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      OpenMoE: 社区对基于 Llama 的模型的 MoE 尝试。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Mixtral 8x7B (Mistral): 一个性能超越了 Llama 2 70B 的高质量混合专家模型，并且具有更快的推理速度。此外，还发布了一个经过指令微调的模型。有关更多信息，可以在 Mistral 的，公告博客文章，中了解。 
    </section></li></ul><span id="OSC_h2_19"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">一些有趣的研究方向</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">首先是尝试将稀疏混合专家模型 (SMoE) <strong style="color: black;">蒸馏</strong> 回到具有更少实际参数但相似等价参数量的稠密模型。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">MoE 的 <strong style="color: black;">量化</strong> 也是一个有趣的研究领域。例如，QMoE (2023 年 10 月) 通过将 MoE 量化到每个参数不到 1 位，将 1.6 万亿参数的 Switch Transformer 所需的存储从 3.2TB 压缩到仅 160GB。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">简而言之，一些值得探索的有趣领域包括:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      将 Mixtral 蒸馏成一个稠密模型。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      探索合并专家模型的技术及其对推理时间的影响。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      尝试对 Mixtral 进行极端量化的实验。 
    </section></li></ul><span id="OSC_h2_20"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">相关资源</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Adaptive Mixture of Local Experts (1991) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Learning Factored Representations in a Deep Mixture of Experts (2013) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Jun 2020) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      GLaM: Efficient Scaling of Language Models with Mixture-of-Experts (Dec 2021) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      ST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models(April 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (Nov 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (May 2023) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Mixtral-8x7B-v0.1, Mixtral-8x7B-Instruct-v0.1. 
    </section></li></ul><span id="OSC_h2_21"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">Citation</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">@misc {sanseviero2023moe,<br>    author       = { Omar Sanseviero and<br>                     Lewis Tunstall and<br>                     Philipp Schmid and<br>                     Sourab Mangrulkar and<br>                     Younes Belkada and<br>                     Pedro Cuenca<br>                   },<br>    title        = { Mixture of Experts Explained },<br>    year         = 2023,<br>    url          = { https://huggingface.co/blog/moe },<br>    publisher    = { Hugging Face Blog }<br>}<br></code></pre><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">Sanseviero,&nbsp;et&nbsp;al.,&nbsp;<span style="color: #d14;line-height: 26px;">"Mixture&nbsp;of&nbsp;Experts&nbsp;Explained"</span>,&nbsp;Hugging&nbsp;Face&nbsp;Blog,&nbsp;2023.<br></code></pre><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);">🤗 宝子们可以戳 <strong style="color: black;">阅读原文</strong> 查看文中所有的外部链接哟！</p></blockquote><hr data-tool="mdnice 编辑器" style="height: 1px;border-right: none;border-bottom: none;border-left: none;border-top-style: solid;border-top-color: rgb(249, 191, 69);margin-top: 20px;margin-bottom: 20px;"><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;color: rgb(91, 91, 91);background: rgba(158, 158, 158, 0.1);padding-top: 1px;padding-bottom: 1px;padding-left: 5px;margin-top: 0px;margin-bottom: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">英文原文:&nbsp;<span style="color: rgb(136, 136, 136);letter-spacing: 0px;">https://hf.co/blog/moe</span></p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">原文作者: Omar Sanseviero, Lewis Tunstall, Philipp Schmid, Sourab Mangrulkar, Younes Belkada, Pedro Cuenca</p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">译者: xinyu66 (Xinyu Yang)</p></blockquote></blockquote></blockquote></blockquote></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - Hugging Face（gh_504339124f0f）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 02:32:45 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10444582</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10444582</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[恭喜 LinkWeChat 荣获 2023 开源创新榜「优秀开源项目」]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span>&nbsp; &nbsp; &nbsp; 近日，由<strong>中国科协科学技术传播中心、中国计算机学会、中国通信学会、中国科学院软件研究所</strong>共同主办，CSDN 承办的 2023 开源创新榜专家评审会在国家科技传播中心成功举办。评委会主任、中国计算机学会开源发展委员会主任王怀民院士，评委会副主任、中国科协科学技术传播中心副主任陈锐，评委会副主任、中国通信学会副理事长兼秘书长张延川，评委会副主任、中国科学院软件研究所所长赵琛与来自全国学会、大学、科研院所、企业、开源基金会、行业联盟等二十多位开源专家共同参与了本届榜单评审工作，会议由陈锐主持。</span></p><p><span>&nbsp; &nbsp; &nbsp; &nbsp; 2023 年开源创新榜相较往年有以下几个变化。<strong>一是进一步提升权威性，</strong>主办单位新加入中国计算机学会、中国通信学会、中国科学院软件研究所，四家主办单位优势互补，共同推动榜单策划、征集申报、专家评审等工作重点。<strong>二是进一步提升公信力，</strong>由王怀民院士担任评委会主任，指导组建了结构更加科学、领域更加全面的评审专家库，从中提名形成最终评审专家。<strong>三是进一步提升专业度，</strong>围绕项目、社区、人物三大类别，四家主办单位打磨了更加客观、严谨、贴合实际的评审标准和更加开放、公平、科学的评审办法，在征集过程中公开标准细节，接受社会的意见反馈，形成良性循环。</span></p><p><span>评委会最终评选出优秀开源项目 20 个，LinkWeChat<span style="color:#f2622e"><strong>成功入选。</strong></span></span></p><p><span><span style="color:#f2622e"><strong><img alt="" height="472" src="https://oscimg.oschina.net/oscnet/up-c75591fff8654955776eeed8c0294671c38.png" width="284" referrerpolicy="no-referrer"></strong></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 15:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273361</guid>
            <link>https://www.oschina.net/news/273361</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[告别 2023，迎接 2024]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>顺便带一下开放签开源电子签章上线两周的小结，本周关键字「惊喜」。官网访问量稳定在 200 左右/天，github/gitee:start 总计 130。5 个企业版意向客户，以上便是开放签上线两周的运营数据。</span></span></span></span></span></span></span></span>&nbsp;</span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>用「坚持」「艰辛」来总结 2023 年，这是我们真实的写照。团队这几年确实很辛苦，经历了创业的艰辛，失去了几载青春、大把的票子，用很低的收入维持生活，这是我们的现状。但是这些依然打不倒，也压不跨我们，对我们来说也不是什么」苦「。因为我们早已看淡了这些，我们相信我们这些年的积累、经验总有一天有用武之地。因为我们有梦想（说梦想可能会有人笑话吧），我们可以把开放签做好。其实毫不夸张的说，直至开放签上线后，才体会到十年磨一剑的感受。不瞒大家，从开始做电子签直至开放签这个项目上线，真的已经十年了，期间被家人和朋友调侃十年才玩明白这点事儿。调侃归调侃，真正想做好一个产品，我们觉得十年只是一个开始，没有这十来年足够的客户、技术、服务等方方面面积累，以及我们对开放签的坚持，估计开放签上线后也是个没血肉、立不住的产品吧。对开放签来说，十年只是一个开始，我们深知没有任何一件事情可以随便、容易的完成的，接下来的几年、几十年，我们将持续的、顽强的深耕我们的产品、服务，让电子签章更简单不是说说而已。</span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>用「惊喜」「憧憬」来展望 2024 年，惊喜发生在开放签上线后，而且是持续的。惊喜的是没想到有特别多的朋友在关注、关心我们，给我们提了不少战略级别的建议，真的感谢你们。在此特别感谢@jack，感谢你给予我们的指导和建议。你给我们带来了更多的正能量，我们更加坚定了能做好开放签的信心。只要用心做好产品，我们相信 24 年会有更多的惊喜。24 年用「憧憬」一词来展望新年，代表了我们对新的一年美好的期冀。我们期待有更多用户可以低门槛的应用电子签章，电子签章可以更加简单的得到普及。新的一年，我们更加期待开放签可以更多的参与到公益和对社会有益的事业中，让开源、开放的意义更大。</span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>写在最后，我们还很年轻，还有很多不足，肯定在发展的道路上会犯很多错误，但是我们是开放的，有激情的，我们愿意接受各种批评和质疑，最后还是用一句我们自己坚信的话来总结我们「我们相信开源开放会为产品与用户之间带来更多信任「，这就是开放签的价值观，是我们坚定走下去的信念。 </span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>祝各位在新的一年顺遂、如意。新年好！</span></span></span></span></span></span></span></span></span></span></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 08:20:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273299</guid>
            <link>https://www.oschina.net/news/273299</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2023 大语言模型技术报告.pdf]]>
            </title>
            <description>
                <![CDATA[看之前先买瓶水，货实在太干了[Facepalm]]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 06:43:00 GMT</pubDate>
            <guid isPermaLink="false">https://talk.gitee.com/report/china-open-source-2023-llm-report.pdf?fr=news1229</guid>
            <link>https://talk.gitee.com/report/china-open-source-2023-llm-report.pdf?fr=news1229</link>
        </item>
        <item>
            <title>
                <![CDATA[Ubuntu 23.04 将于 2024 年 1 月 25 日结束支持]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">对 <a href="https://www.oschina.net/news/237763/ubuntu-23-04-released">Ubuntu 23.04「Lunar Lobster」</a>的官方支持将于 2024 年 1 月 25 日结束，还剩不到一个月的时间。</span></p><p><span style="color:#000000">Ubuntu 23.04 版本于 2023 年 4 月正式发布，作为短期支持版本获得 9 个月的支持。还在使用该版本的用户<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2023%2F12%2Fubuntu-2304-support-ends-january-2024" target="_blank">建议</a>可以考虑升级到 10 月份发布的 <a href="https://www.oschina.net/news/261571/ubuntu-23-10-ga">Ubuntu 23.10"Mantic Minotaur"</a>，以确保可继续收到来自 Canonical 的安全补丁、关键错误修复以及精选软件的重要更新。</span></p><p><span style="color:#000000"><img alt="" height="263" src="https://oscimg.oschina.net/oscnet/up-4bdb313301be4efb73863c20508102b5a86.jpg" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">Ubuntu 23.10 附带了最新的 GNOME 45 版本（其中包含大量改进）、使用 Linux 6.5 内核、更新了图形驱动程序，并首次发布了 2 个全新的应用程序，这些应用程序目前为该版本<span style="background-color:#ffffff">独有</span>： App Center 和 Firmware。</span></p><p><span style="color:#000000">同样作为短期支持版本，Ubuntu 23.10 计划将于 2024 年 7 月中旬达到 EOL。不过预计明年 4 月下旬，<span style="background-color:#ffffff"><a href="https://www.oschina.net/news/263525/ubuntu-24-04-release-date-april-25-2024">Ubuntu 24.04 LTS</a> 版本就会正式发布，LTS 版本将获得 5 年的安全更新、错误修复和精选应用程序更新；LTS 版本预期每 2 年发布一次。</span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 06:13:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273275/ubuntu-2304-support-ends-january-2024</guid>
            <link>https://www.oschina.net/news/273275/ubuntu-2304-support-ends-january-2024</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2023 mybatis-mp - 亮点一：可自定义默认值]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>1：默认值设置&nbsp;</p><div><pre><span style="color:#9e880d">@Table
</span><span style="color:#9e880d">@Data
</span><span style="color:#0033b3">public class </span><span style="color:#000000">DefaultValueTest </span>{

    <span style="color:#9e880d">@TableId
</span><span style="color:#9e880d"></span><span style="color:#0033b3">private </span><span style="color:#000000">Integer </span><span style="color:#871094">id</span>;

    <span style="color:#9e880d">@TableField</span>(defaultValue = <span style="color:#067d17">"{BLANK}"</span>)
    <span style="color:#0033b3">private </span><span style="color:#000000">String </span><span style="color:#871094">value1</span>;

    <span style="color:#9e880d">@TableField</span>(defaultValue = <span style="color:#067d17">"1"</span>)
    <span style="color:#0033b3">private </span><span style="color:#000000">Integer </span><span style="color:#871094">value2</span>;

    <span style="color:#9e880d">@TableField</span>(defaultValue = <span style="color:#067d17">"{NOW}"</span>)
    <span style="color:#0033b3">private </span><span style="color:#000000">LocalDateTime </span><span style="color:#871094">createTime</span>;
}
</pre></div><p>2：如何自定义默认值：</p><pre><code class="language-java">MybatisMpConfig.setDefaultValue("{NOW}", (type) -&gt; {
    if (type == LocalDateTime.class) {
        return LocalDateTime.now();
    } else if (type == LocalDate.class) {
        return LocalDate.now();
    } else if (type == Date.class) {
        return new Date();
    } else if (type == Long.class) {
        return System.currentTimeMillis();
    } else if (type == Integer.class) {
        return (int) (System.currentTimeMillis() / 1000);
    }
    throw new RuntimeException("Inconsistent types");
});</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 06:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273274</guid>
            <link>https://www.oschina.net/news/273274</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CNCF 报告：Kubernetes 推动云支出增长]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CNCF 发布的一份调查报告<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2023%2F12%2F20%2Fcncf-cloud-native-finops-cloud-financial-management-microsurvey%2F" target="_blank">指出</a>，Kubernetes 的到来导致了云支出急剧增加；有<span style="background-color:#fdfdfd; color:#000000">近一半 (49%) 的受访者表示 Kubernetes 推动了云支出增长。</span>其中，17% 的人表示成本大幅增加，32% 的人表示成本仅略有增加。</p><p>另一方面，13% 的受访者在实施 Kubernetes 后成功显着减少了云支出，11% 的受访者成功略微减少了支出。28% 的受访者表示采用 Kubernetes 后没有任何变化。</p><p><img height="273" src="https://oscimg.oschina.net/oscnet/up-153215e0c5e6743aa6ba642aee27efef80b.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">约 28% 的受访者表示，Kubernetes 占用了他们一半的预算，10% 的受访者表示，这一数字高达 75%，还有极少数 5% 的受访者表示，Kubernetes 占用了他们的全部预算。</span></p><p><img height="279" src="https://oscimg.oschina.net/oscnet/up-0189729f7a76027da4a44209791e910f940.png" width="500" referrerpolicy="no-referrer"></p><p>26% 的人每月在云计算上的支出高达 50000 美元；还有 22% 的人表示他们的支出是前者的 20 倍，每月高达 100 万美元以上。此外，21% 的人每月云计算支出不到 1 万美元。</p><p>在受访者中，Kubernetes 基础设施的规模存在很大差异。近一半的受访者 (49%) 只<span style="background-color:#fdfdfd; color:#000000">拥有最多 50 个节点</span>。15% 拥有 51-100 个节点，17% 拥有 101-250 个节点，18% 拥有超过 251 个节点。&nbsp;</p><p>许多人力和技术因素被认为是云环境中支出以及不必要和意外成本增加的原因。过度配置以 70% 的比例遥遥领先，个人或团队层面缺乏责任意识位居第二，为 45%。使用资源后未能停用资源以及存在技术债务（定义为尚未重新架构以利用云原生环境的可扩展性的工作负载）并列第三，各占 43%。</p><p><img alt="" height="417" src="https://oscimg.oschina.net/oscnet/up-4a03450fb93c2128ee80b363c64d6f5c9f6.png" width="500" referrerpolicy="no-referrer"></p><p>只有 19% 的受访者表示他们能够准确监控 Kubernetes 成本。40% 的人只是进行了估计，38% 的人表示他们根本没有进行任何监控。</p><p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2023%2F12%2F20%2Fcncf-cloud-native-finops-cloud-financial-management-microsurvey%2F" target="_blank">查看完整报告</a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 03:38:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273259/cncf-report-kubernetes-cloud-spen</guid>
            <link>https://www.oschina.net/news/273259/cncf-report-kubernetes-cloud-spen</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Bytebase 2.13.0 - 支持 StarRocks]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h2>🚀 新功能</h2><ul><li>支持 StarRocks。</li><li>支持 PostgreSQL, Redshift, RisingWave 高级自动补全。</li></ul><h2>🎄 改进</h2><ul><li>支持在 SQL 编辑器的表结构 DDL 弹窗中展示 index 语句。</li><li>支持在 SQL 编辑器中查询 PostgreSQL 外部表。</li><li>汉化钉钉 webhook 消息。</li></ul><h2>🎠 社区</h2><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1TG411r7rr%2F" target="_blank">盘点常用的 MySQL 可视化客户端</a></li></ul><h2>📕 安装及升级</h2><p>参考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fbytebase%2Fbytebase%23installation" target="_blank">升级指南</a>。如果从之前版本升级，获取新版本后，重新启动升级即可。</p><hr><p>💡 更多资讯，请关注 Bytebase 公号：Bytebase</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 03:22:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6148470/blog/10456200</guid>
            <link>https://my.oschina.net/u/6148470/blog/10456200</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[写给工程师的 MacBook 商用级大模型知识库部署方案]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section style="margin-bottom: 8px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);text-align: center;visibility: visible;" data-mpa-powered-by="yiban.io"><img class="rich_pages wxw-img __bg_gif" data-backh="96" data-backw="578" data-cropselx1="0" data-cropselx2="578" data-cropsely1="0" data-cropsely2="96" data-imgfileid="503041736" data-ratio="0.16666666666666666" src="https://oscimg.oschina.net/oscnet/990ffd45-801b-45e6-80cd-1b6ebb403d86.gif" data-type="gif" data-w="636" style="outline: 0px;letter-spacing: 0.544px;font-size: var(--articleFontsize);border-radius: 8px;text-align: justify;width: 100%;visibility: visible !important;background-size: 16px !important;height: auto;" referrerpolicy="no-referrer"><br style="outline: 0px;visibility: visible;"></section><section data-mpa-template="t" data-mpa-template-id="502" data-mpa-category="模板" style="outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);visibility: visible;"><section data-mpa-category="模板" data-mid="" style="padding-right: 1px;padding-left: 1px;outline: 0px;width: 677px;display: flex;justify-content: flex-start;align-items: center;flex-direction: column;visibility: visible;"><section data-mid="" style="outline: 0px;letter-spacing: 0.544px;width: 675px;display: grid;grid-template-columns: 26px auto;visibility: visible;"><section data-mid="" style="outline: 0px;width: 26px;height: 14px;display: flex;justify-content: center;align-items: center;align-self: center;visibility: visible;"><br style="outline: 0px;visibility: visible;"></section><section data-mid="" style="padding-left: 7px;outline: 0px;display: flex;justify-content: flex-start;align-items: center;visibility: visible;"><section data-mid="" style="margin-right: 7px;outline: 0px;text-align: left;visibility: visible;"><p data-mid="" style="outline: 0px;width: 0px;font-size: 14px;font-family: PingFangSC-Semibold, &quot;PingFang SC&quot;;font-weight: bold;color: rgb(58, 92, 244);line-height: 20px;visibility: visible;"><br style="outline: 0px;visibility: visible;"></p></section><section data-mid="" style="margin-bottom: 4px;outline: 0px;width: 635px;height: 1px;border-top: 1px solid rgb(58, 92, 244);align-self: flex-end;visibility: visible;"><br style="outline: 0px;visibility: visible;"></section></section></section><section data-mid="" style="padding: 7px 14px 9px 19px;outline: 0px;width: 675px;text-align: left;border-bottom: 1px solid rgb(58, 92, 244);visibility: visible;"><p style="margin-top: 8px;margin-bottom: 8px;outline: 0px;min-height: 24px;letter-spacing: 0.578px;text-align: justify;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;white-space-collapse: preserve;text-size-adjust: inherit;text-align: left;caret-color: rgb(23, 26, 29);letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;color: rgb(0, 0, 0);visibility: visible;font-size: 15px;">本文介绍了如何在<span style="font-size: 15px;letter-spacing: 1px;text-wrap: wrap;">自己的 MacBook 上部署一套知识库方案辅助自己的知识管理工作，</span><span style="font-size: 15px;letter-spacing: 1px;text-wrap: wrap;">希望能给每位计划自己搭建大模型知识库应用的工程师一点参考。</span></span></p></section></section></section><p style="margin-top: 8px;margin-bottom: 8px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);visibility: visible;line-height: 1.75em;"><br style="outline: 0px;visibility: visible;"></p><section style="margin-bottom: 0px;outline: 0px;box-sizing: inherit;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);color: rgba(25, 26, 31, 0.9);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 16px;text-align: center;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041735" data-ratio="0.3161764705882353" data-s="300,640" src="https://oscimg.oschina.net/oscnet/1447be7f-ff8f-4af0-a27e-b2863660d071.png" data-type="png" data-w="408" style="outline: 0px;color: rgb(34, 34, 34);font-family: system-ui, -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;visibility: visible !important;width: 113px !important;" referrerpolicy="no-referrer"></span></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);color: rgb(34, 34, 34);text-align: center;visibility: visible;line-height: normal;"><span style="outline: 0px;color: rgb(3, 69, 255);font-family: -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 18px;letter-spacing: 1px;visibility: visible;">背景</span></section><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></h4><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">历史的车轮滚滚向前，大模型技术发展日新月异，每天都有新鲜的技术出炉，让人目不暇接，同时具备可玩性和想象空间的各种应用和开源库，仿佛让自己回到了第一次设置 JAVA_HOME 的日子，作为一枚古典工程师，我专门挑了个可能对手上工作有帮助的方向小试一把，尝试在自己的 MacBook 上部署一套知识库方案，看看能不能辅助自己的知识管理工作。</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">﻿</span></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">我自己的 Macbook 配置情况如下，可以流畅地运行没问题。经过量化处理的大模型，还是对办公本很友好的。</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></p><p style="margin-top: 8px;line-height: 1.75em;text-align: center;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041733" data-ratio="0.24633431085043989" src="https://oscimg.oschina.net/oscnet/f4f317ea-3d61-4055-91c7-8f6534441328.jpg" data-type="other" data-w="682" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">为什么要在 MacBook 搭建而不是直接采用现成的云服务呢？最核心最重要的是我们手上的文档资料出于安全要求，不能随便上传云服务，也就无法实际验证知识库的实际效用；另外对于工程师来说，自己亲手搭建一个完整的方案、能灵活调整和对接各种不同的模型、评测各种模型不同的表现，也是出于对技术的探索本能使然。</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">﻿</span></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">鉴于大模型已经是大模型及其周边概念已经是大家耳熟能详的东西，我这里就不再重复阐述相关的基础概念和理论了，直接进入动手环节，以用最快的速度部署起一个可用的知识库平台为目标，先用起来，再分各个环节优化。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-bottom: 0px;outline: 0px;box-sizing: inherit;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);color: rgba(25, 26, 31, 0.9);font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 16px;text-align: center;visibility: visible;line-height: normal;"><span style="outline: 0px;font-size: 15px;letter-spacing: 1px;visibility: visible;"><img class="rich_pages wxw-img" data-imgfileid="503041732" data-ratio="0.3056872037914692" data-s="300,640" src="https://oscimg.oschina.net/oscnet/476e87ec-3926-4421-826d-d28a2c78789f.png" data-type="png" data-w="422" style="outline: 0px;color: rgb(34, 34, 34);font-family: system-ui, -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;visibility: visible !important;width: 117px !important;" referrerpolicy="no-referrer"></span></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);color: rgb(34, 34, 34);text-align: center;visibility: visible;line-height: normal;"><span style="outline: 0px;font-family: -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 18px;letter-spacing: 1px;visibility: visible;color: rgb(0, 17, 255);">方案概述</span></section><section style="margin-bottom: 8px;"><br></section><span id="OSC_h4_1"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;margin-bottom: 8px;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">应用架构</span></strong></span></h4><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">首先来看一下最终方案的应用架构是什么样子（下图）。在这套方案中，我们采用实力排上游、并且在使用上对学术和商业都友好的国产大模型 ChatGLM3-6B 对话模型和基于 m3e-base 模型的 embedding search RAG 方案；基于这两个模型封装和 ChatGPT 兼容的 API 接口协议；通过引入 One API 接口管理&amp;分发系统，形成统一的 LLM 接口渠道管理平台规范，并把封装好的接口协议注册进去；搭建与 Dify.ai 齐名的开源大模型知识库平台管理系统 FastGPT，实现集私有知识数据源预处理、嵌入检索、大模型对话一体的完整知识库应用流程。麻雀虽小五脏俱全，最终形成一套既满足商用标准、又能在 MacBook 跑起来的的方案。虽然智能程度和实际需求还有一定差距，但至少我们在不用额外购买显卡或云服务的情况下，以最小成本部署运行、并且能导入实际业务数据（如语雀知识库）进行实操验证，值得每位工程师都来动手尝试一下。</span></section><p style="margin-top: 8px;line-height: 1.75em;text-align: center;margin-bottom: 8px;"><img class="rich_pages wxw-img" data-imgfileid="503041734" data-ratio="0.7546296296296297" src="https://oscimg.oschina.net/oscnet/aa80fe6e-e7fa-4823-9906-d7c8f3ddcd3e.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></p><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><br></h4><span id="OSC_h4_2"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">成型展示</span></strong></span></h4><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"></h4><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">在用户终端，我们基于 FastGPT 提供知识库管理及使用方案。引用其官网介绍：FastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">﻿</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">先放一张官网上的图片，来增加一点吸引朋友们动手操作的动力：</span></section><section style="margin-bottom: 8px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503041740" data-ratio="0.4842592592592593" src="https://oscimg.oschina.net/oscnet/b61b175a-2352-47d3-a57f-826fbb292f26.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></section><section style="margin-bottom: 8px;"><br></section><span id="OSC_h4_3"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">部署要点</span></strong></span></h4><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"></h4><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">本套方案部署分为四个主要环节、14 个具体步骤，只要一步步实操下去，每位朋友都可以在自己的本本上拥有属于自己的私有大模型知识库系统，步骤清单如下：</span></section><table width="628"><colgroup style="box-sizing: inherit;"><col width="192" style="box-sizing: inherit;"><col width="436" style="box-sizing: inherit;"></colgroup><tbody style="box-sizing: inherit;"><tr data-cangjie-key="87" data-sticky="false" style="box-sizing: inherit;height: 33px;"><td data-cangjie-key="89" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-weight: bold;letter-spacing: 1px;font-size: 15px;">主要环节</span></section></td><td data-cangjie-key="94" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-weight: bold;font-size: 15px;letter-spacing: 1px;">详细步骤</span></section></td></tr><tr data-cangjie-key="99" data-sticky="false" style="box-sizing: inherit;height: 33px;"><td data-cangjie-key="101" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">一、准备大模型</span></section></td><td data-cangjie-key="106" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">1.1 下载对话语言模型 ChatGLM3-6B</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">1.2 下载文本嵌入模型 m3e-base</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">1.3 使用 chatglm.cpp 对 ChatGLM3-6B 进行量化加速</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">1.4 验证模型问答效果</span></section></td></tr><tr data-cangjie-key="120" data-sticky="false" style="box-sizing: inherit;height: 33px;"><td data-cangjie-key="122" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">二、搭建模型 API 服务</span></section></td><td data-cangjie-key="127" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">2.1 搭建模型 API</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">2.2 搭建 One API 接口管理/分发系统</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">2.3 验证模型接口能力</span></section></td></tr><tr data-cangjie-key="138" data-sticky="false" style="box-sizing: inherit;height: 33px;"><td data-cangjie-key="140" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">三、搭建知识库应用</span></section></td><td data-cangjie-key="145" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">3.1 安装 MongoDB</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">3.2 安装 PostgreSQL &amp; pgvector</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">3.3 搭建 FastGPT 知识库问答系统</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">3.4 验证模型对话能力</span></section></td></tr><tr data-cangjie-key="159" data-sticky="false" style="box-sizing: inherit;height: 33px;"><td data-cangjie-key="161" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">四、知识库问答实战</span></section></td><td data-cangjie-key="166" data-type="table-cell" rowspan="1" colspan="1" data-container-block="true" style="box-sizing: inherit;padding: 8px;border-color: rgba(88, 104, 144, 0.24);"><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">4.1 准备知识库语料</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">4.2 导入知识库数据</span></section><section style="line-height: normal;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">4.3 验证知识库问答效果</span></section></td></tr></tbody></table><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">部分步骤可以简单地通过 Docker 镜像一键部署完成，但本着对细节一杆子插到底的部署思路，还是采取了纯手工作业的方法。注意，下面的步骤中仅包含了关键的命令，完整的命令可以参考对应系统的官网介绍。部分安装步骤如果速度不够理想，可以考虑采用国内源，包含但不限于 go、brew、pip、npm 等。</span></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;visibility: visible;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);text-align: center;line-height: normal;"><img class="rich_pages wxw-img" data-imgfileid="503041737" data-ratio="0.3056872037914692" data-s="300,640" data-type="png" data-w="422" src="https://oscimg.oschina.net/oscnet/c8d6ad94-b394-4e67-a5c3-8464690e8f51.png" style="outline: 0px;letter-spacing: 0.544px;font-size: 14px;visibility: visible !important;width: 117px !important;" referrerpolicy="no-referrer"></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;visibility: visible;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);text-align: center;line-height: normal;"><span style="outline: 0px;font-family: -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 18px;letter-spacing: 1px;color: rgb(0, 17, 255);">详细步骤</span></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;visibility: visible;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);text-align: center;line-height: normal;"><br></section><section style="margin-bottom: 8px;"><br></section><span id="OSC_h4_4"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">准备离线模型</span></strong></span></h4><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"></h4><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">这个环节我们的主要任务是把模型文件准备好、完成量化，并通过命令行的方式，进行交互式对话验证。</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_5"></span><h4 data-cangjie-key="195" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">下载对话语言模型 ChatGLM3-6B</span></h4></li></ul><section style="margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">为什么选择 ChatGLM3-6B？常年霸榜的开源国产之光。ChatGLM3 一共开源了对话模型 ChatGLM-6B、基础模型 ChatGLM-6B-Base、长文本对话模型 ChatGLM3-6B-32K，对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。无论是用来做上手实践还是微调练习，目前看来都是比较好的选择。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">其实最重要的是，看看排行榜上的可选项，我的 MacBook 16G 内存只能带得动 ChatGLM3-6B 量化版本：</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041741" data-ratio="0.37777777777777777" src="https://oscimg.oschina.net/oscnet/08ae148e-39dc-45c8-9196-b121dbef2503.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">ChatGLM3-6B 现在比较方便的下载渠道有 HuggingFace 和 ModelScope，但是很明显能直接下载下来的可能性不大，所以我用家里的旧电脑科学下载后放到私有云 CDN 上，然后再用公司电脑下载，也方便未来随时随地取用，就是要花点小钱。ModelScope 也试过，不能直接下载文件，并且用 git clone 速度也不太理想，遂放弃。</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">如果用老一点的版本 ChatGLM2-6B 的话，网上也能找到一些比较好用的第三方镜像站。</span></p><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: left;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">HuggingFace:THUDM/chatglm3-6b﻿</span></p></li><li><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: left;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">ModelScope:ZhipuAI/chatglm3-6b（</span><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">地址：https://modelscope.cn/models/ZhipuAI/chatglm3-6b/summary）</span></p></li></ol><section style="line-height: 1.75em;margin-bottom: 8px;margin-top: 8px;"><br></section><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="php"><code><span class="code-snippet_outer">// 从 Git 仓库下载模型文件</span></code><code><span class="code-snippet_outer">// HuggingFace</span></code><code><span class="code-snippet_outer">git lfs install</span></code><code><span class="code-snippet_outer">git clone https://huggingface.co/THUDM/chatglm3-6b</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">// ModelScope</span></code><code><span class="code-snippet_outer">git lfs install</span></code><code><span class="code-snippet_outer">git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git</span></code></pre></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_6"></span><h4 data-cangjie-key="237" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">下载文本嵌入模型 m3e-base</span></h4></li></ul><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">为什么选择 moka-ai 的 M3E 模型 m3e-base？M3E 向量模型属于小模型，资源使用不高，CPU 也可以运行，使用场景主要是中文，少量英文的情况。用来验证我们的知识库系统足够了</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">官方下载地址：moka-ai/m3e-base，先把所有的模型文件 download 下来，后面使用</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_7"></span><h4 data-cangjie-key="253" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">使用 chatglm.cpp 对 ChatGLM3-6B 进行量化加速</span></h4></li></ul><section style="margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">当我第一次知道 chatglm.cpp，只能说好人一生平安，chatglm.cpp 的出现拯救了纯 MacBook 党，让我们能在（低性能的）果本上基于 CPU 进行推理，也不会损失过多的精度。（其实损失多少我也不知道，不影响我们正常进行工程部署验证就行）</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">Github Repo: https://github.com/li-plus/chatglm.cpp﻿</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">我使用的 Python 版本：3.11，最好单独准备一个 virtualenv</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041738" data-ratio="0.13240740740740742" src="https://oscimg.oschina.net/oscnet/e1860e0e-5e2f-4eaf-8411-dab67f6a95cd.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">安装依赖：</span></section><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="properties"><code><span class="code-snippet_outer">cd /Users/yaolu/AGI/github/chatglm.cpp</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"># 先初始化 git 仓库</span></code><code><span class="code-snippet_outer">git submodule update --init --recursive</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"># 构建可执行文件</span></code><code><span class="code-snippet_outer">cmake -B build</span></code><code><span class="code-snippet_outer">cmake --build build -j</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"># 安装 Python 依赖</span></code><code><span class="code-snippet_outer">pip install .</span></code></pre></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">如果发生 No module named 'chatglm_cpp._C' 的错误，把编译出来的文件 _C.cpython-311-darwin.so 放到 chatglm_cpp 目录下。</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">对 ChatGLM3-6B 进行 8-bit 量化处理：</span></p><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="nginx"><code><span class="code-snippet_outer">python ./chatglm_cpp/convert.py -i /Users/yaolu/AGI/huggingface/THUDM/chatglm3-6b -t q8_0 -o chatglm3-ggml-q8.bin</span></code></pre></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">如果电脑带不动，还可以尝试 4-bit、5-bit 参数量化，完整参数列表见 chatglm.cpp 的 quantization types</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_8"></span><h4 data-cangjie-key="300" data-cangjie-leaf-block="true" data-type="heading-4" style="line-height: 1.75em;margin-bottom: 8px;margin-top: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">验证模型问答效果</span></h4></li></ul><p style="margin-bottom: 8px;margin-top: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></p><p style="line-height: 1.75em;margin-bottom: 8px;margin-top: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">完成模型量化后，就可以在本地把大模型跑起来了，命令如下：</span></p><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js"><code><span class="code-snippet_outer">./build/bin/main -m chatglm3-ggml-q8.bin -i</span></code></pre></section><p style="line-height: 1.75em;margin-bottom: 8px;margin-top: 8px;"><br></p><p style="line-height: 1.75em;margin-bottom: 8px;margin-top: 8px;text-align: center;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041739" data-ratio="0.5796296296296296" src="https://oscimg.oschina.net/oscnet/a783a6e0-4e31-4f72-81ca-8f9e37688b7f.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></section><span id="OSC_h4_9"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">搭建模型 API 服务</span></strong></span></h4><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">我们在这个环节要完成的任务是，按照 ChatGPT 的接口规范、基于 FastAPI 封装 ChatGLM3-6B 的对话和 m3e-base 的嵌入能力；并注册到 One API 接口管理/分发系统中。</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_10"></span><h4 data-cangjie-key="325" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">搭建模型 API</span></h4></li></ul><section style="margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">用 chatglm.cpp 自带的 openai_api.py 魔改了一下，使其支持完成对话和文本 embedding 的两个核心调用：</span></section><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">/v1/chat/completions</span></section></li><li><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">/v1/embeddings</span></section></li></ol><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">代码如下：</span></section><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="python"><code><span class="code-snippet_outer">import asyncio</span></code><code><span class="code-snippet_outer">import logging</span></code><code><span class="code-snippet_outer">import time</span></code><code><span class="code-snippet_outer">from typing import List, Literal, Optional, Union</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">import chatglm_cpp</span></code><code><span class="code-snippet_outer">from fastapi import FastAPI, HTTPException, status, Depends</span></code><code><span class="code-snippet_outer">from fastapi.middleware.cors import CORSMiddleware</span></code><code><span class="code-snippet_outer">from pydantic import BaseModel, Field#, computed_field</span></code><code><span class="code-snippet_outer">#from pydantic_settings import BaseSettings</span></code><code><span class="code-snippet_outer">from sse_starlette.sse import EventSourceResponse</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">from sentence_transformers import SentenceTransformer</span></code><code><span class="code-snippet_outer">from sklearn.preprocessing import PolynomialFeatures</span></code><code><span class="code-snippet_outer">import numpy as np</span></code><code><span class="code-snippet_outer">import tiktoken</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">logging.basicConfig(level=logging.INFO, format=r"%(asctime)s - %(module)s - %(levelname)s - %(message)s")</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class Settings(object):</span></code><code><span class="code-snippet_outer">    model: str = "/Users/yaolu/AGI/github/chatglm.cpp/chatglm3-ggml-q8.bin"</span></code><code><span class="code-snippet_outer">    num_threads: int = 0</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ChatMessage(BaseModel):</span></code><code><span class="code-snippet_outer">    role: Literal["system", "user", "assistant"]</span></code><code><span class="code-snippet_outer">    content: str</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class DeltaMessage(BaseModel):</span></code><code><span class="code-snippet_outer">    role: Optional[Literal["system", "user", "assistant"]] = None</span></code><code><span class="code-snippet_outer">    content: Optional[str] = None</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ChatCompletionRequest(BaseModel):</span></code><code><span class="code-snippet_outer">    model: str = "default-model"</span></code><code><span class="code-snippet_outer">    messages: List[ChatMessage]</span></code><code><span class="code-snippet_outer">    temperature: float = Field(default=0.95, ge=0.0, le=2.0)</span></code><code><span class="code-snippet_outer">    top_p: float = Field(default=0.7, ge=0.0, le=1.0)</span></code><code><span class="code-snippet_outer">    stream: bool = False</span></code><code><span class="code-snippet_outer">    max_tokens: int = Field(default=2048, ge=0)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    model_config = {</span></code><code><span class="code-snippet_outer">        "json_schema_extra": {"examples": [{"model": "default-model", "messages": [{"role": "user", "content": "你好"}]}]}</span></code><code><span class="code-snippet_outer">    }</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ChatCompletionResponseChoice(BaseModel):</span></code><code><span class="code-snippet_outer">    index: int = 0</span></code><code><span class="code-snippet_outer">    message: ChatMessage</span></code><code><span class="code-snippet_outer">    finish_reason: Literal["stop", "length"] = "stop"</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ChatCompletionResponseStreamChoice(BaseModel):</span></code><code><span class="code-snippet_outer">    index: int = 0</span></code><code><span class="code-snippet_outer">    delta: DeltaMessage</span></code><code><span class="code-snippet_outer">    finish_reason: Optional[Literal["stop", "length"]] = None</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ChatCompletionUsage(BaseModel):</span></code><code><span class="code-snippet_outer">    prompt_tokens: int</span></code><code><span class="code-snippet_outer">    completion_tokens: int</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    #@computed_field</span></code><code><span class="code-snippet_outer">    @property</span></code><code><span class="code-snippet_outer">    def total_tokens(self) -&gt; int:</span></code><code><span class="code-snippet_outer">        return self.prompt_tokens + self.completion_tokens</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ChatCompletionResponse(BaseModel):</span></code><code><span class="code-snippet_outer">    id: str = "chatcmpl"</span></code><code><span class="code-snippet_outer">    model: str = "default-model"</span></code><code><span class="code-snippet_outer">    object: Literal["chat.completion", "chat.completion.chunk"]</span></code><code><span class="code-snippet_outer">    created: int = Field(default_factory=lambda: int(time.time()))</span></code><code><span class="code-snippet_outer">    choices: Union[List[ChatCompletionResponseChoice], List[ChatCompletionResponseStreamChoice]]</span></code><code><span class="code-snippet_outer">    usage: Optional[ChatCompletionUsage] = None</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    model_config = {</span></code><code><span class="code-snippet_outer">        "json_schema_extra": {</span></code><code><span class="code-snippet_outer">            "examples": [</span></code><code><span class="code-snippet_outer">                {</span></code><code><span class="code-snippet_outer">                    "id": "chatcmpl",</span></code><code><span class="code-snippet_outer">                    "model": "default-model",</span></code><code><span class="code-snippet_outer">                    "object": "chat.completion",</span></code><code><span class="code-snippet_outer">                    "created": 1691166146,</span></code><code><span class="code-snippet_outer">                    "choices": [</span></code><code><span class="code-snippet_outer">                        {</span></code><code><span class="code-snippet_outer">                            "index": 0,</span></code><code><span class="code-snippet_outer">                            "message": {"role": "assistant", "content": "你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。"},</span></code><code><span class="code-snippet_outer">                            "finish_reason": "stop",</span></code><code><span class="code-snippet_outer">                        }</span></code><code><span class="code-snippet_outer">                    ],</span></code><code><span class="code-snippet_outer">                    "usage": {"prompt_tokens": 17, "completion_tokens": 29, "total_tokens": 46},</span></code><code><span class="code-snippet_outer">                }</span></code><code><span class="code-snippet_outer">            ]</span></code><code><span class="code-snippet_outer">        }</span></code><code><span class="code-snippet_outer">    }</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">settings = Settings()</span></code><code><span class="code-snippet_outer">app = FastAPI()</span></code><code><span class="code-snippet_outer">app.add_middleware(</span></code><code><span class="code-snippet_outer">    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]</span></code><code><span class="code-snippet_outer">)</span></code><code><span class="code-snippet_outer">pipeline = chatglm_cpp.Pipeline(settings.model)</span></code><code><span class="code-snippet_outer">lock = asyncio.Lock()</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">embeddings_model = SentenceTransformer('/Users/yaolu/AGI/huggingface/moka-ai/m3e-base', device='cpu')</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">def stream_chat(history, body):</span></code><code><span class="code-snippet_outer">    yield ChatCompletionResponse(</span></code><code><span class="code-snippet_outer">        object="chat.completion.chunk",</span></code><code><span class="code-snippet_outer">        choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(role="assistant"))],</span></code><code><span class="code-snippet_outer">    )</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    for piece in pipeline.chat(</span></code><code><span class="code-snippet_outer">        history,</span></code><code><span class="code-snippet_outer">        max_length=body.max_tokens,</span></code><code><span class="code-snippet_outer">        do_sample=body.temperature &gt; 0,</span></code><code><span class="code-snippet_outer">        top_p=body.top_p,</span></code><code><span class="code-snippet_outer">        temperature=body.temperature,</span></code><code><span class="code-snippet_outer">        num_threads=settings.num_threads,</span></code><code><span class="code-snippet_outer">        stream=True,</span></code><code><span class="code-snippet_outer">    ):</span></code><code><span class="code-snippet_outer">        yield ChatCompletionResponse(</span></code><code><span class="code-snippet_outer">            object="chat.completion.chunk",</span></code><code><span class="code-snippet_outer">            choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(content=piece))],</span></code><code><span class="code-snippet_outer">        )</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    yield ChatCompletionResponse(</span></code><code><span class="code-snippet_outer">        object="chat.completion.chunk",</span></code><code><span class="code-snippet_outer">        choices=[ChatCompletionResponseStreamChoice(delta=DeltaMessage(), finish_reason="stop")],</span></code><code><span class="code-snippet_outer">    )</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">async def stream_chat_event_publisher(history, body):</span></code><code><span class="code-snippet_outer">    output = ""</span></code><code><span class="code-snippet_outer">    try:</span></code><code><span class="code-snippet_outer">        async with lock:</span></code><code><span class="code-snippet_outer">            for chunk in stream_chat(history, body):</span></code><code><span class="code-snippet_outer">                await asyncio.sleep(0)  # yield control back to event loop for cancellation check</span></code><code><span class="code-snippet_outer">                output += chunk.choices[0].delta.content or ""</span></code><code><span class="code-snippet_outer">                yield chunk.model_dump_json(exclude_unset=True)</span></code><code><span class="code-snippet_outer">        logging.info(f'prompt: "{history[-1]}", stream response: "{output}"')</span></code><code><span class="code-snippet_outer">    except asyncio.CancelledError as e:</span></code><code><span class="code-snippet_outer">        logging.info(f'prompt: "{history[-1]}", stream response (partial): "{output}"')</span></code><code><span class="code-snippet_outer">        raise e</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">@app.post("/v1/chat/completions")</span></code><code><span class="code-snippet_outer">async def create_chat_completion(body: ChatCompletionRequest) -&gt; ChatCompletionResponse:</span></code><code><span class="code-snippet_outer">    # ignore system messages</span></code><code><span class="code-snippet_outer">    history = [msg.content for msg in body.messages if msg.role != "system"]</span></code><code><span class="code-snippet_outer">    if len(history) % 2 != 1:</span></code><code><span class="code-snippet_outer">        raise HTTPException(status.HTTP_400_BAD_REQUEST, "invalid history size")</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    if body.stream:</span></code><code><span class="code-snippet_outer">        generator = stream_chat_event_publisher(history, body)</span></code><code><span class="code-snippet_outer">        return EventSourceResponse(generator)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    max_context_length = 512</span></code><code><span class="code-snippet_outer">    output = pipeline.chat(</span></code><code><span class="code-snippet_outer">        history=history,</span></code><code><span class="code-snippet_outer">        max_length=body.max_tokens,</span></code><code><span class="code-snippet_outer">        max_context_length=max_context_length,</span></code><code><span class="code-snippet_outer">        do_sample=body.temperature &gt; 0,</span></code><code><span class="code-snippet_outer">        top_p=body.top_p,</span></code><code><span class="code-snippet_outer">        temperature=body.temperature,</span></code><code><span class="code-snippet_outer">    )</span></code><code><span class="code-snippet_outer">    logging.info(f'prompt: "{history[-1]}", sync response: "{output}"')</span></code><code><span class="code-snippet_outer">    prompt_tokens = len(pipeline.tokenizer.encode_history(history, max_context_length))</span></code><code><span class="code-snippet_outer">    completion_tokens = len(pipeline.tokenizer.encode(output, body.max_tokens))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    return ChatCompletionResponse(</span></code><code><span class="code-snippet_outer">        object="chat.completion",</span></code><code><span class="code-snippet_outer">        choices=[ChatCompletionResponseChoice(message=ChatMessage(role="assistant", content=output))],</span></code><code><span class="code-snippet_outer">        usage=ChatCompletionUsage(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens),</span></code><code><span class="code-snippet_outer">    )</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class EmbeddingRequest(BaseModel):</span></code><code><span class="code-snippet_outer">    input: List[str]</span></code><code><span class="code-snippet_outer">    model: str</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class EmbeddingResponse(BaseModel):</span></code><code><span class="code-snippet_outer">    data: list</span></code><code><span class="code-snippet_outer">    model: str</span></code><code><span class="code-snippet_outer">    object: str</span></code><code><span class="code-snippet_outer">    usage: dict</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">def num_tokens_from_string(string: str) -&gt; int:</span></code><code><span class="code-snippet_outer">    """Returns the number of tokens in a text string."""</span></code><code><span class="code-snippet_outer">    encoding = tiktoken.get_encoding('cl100k_base')</span></code><code><span class="code-snippet_outer">    num_tokens = len(encoding.encode(string))</span></code><code><span class="code-snippet_outer">    return num_tokens</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">def expand_features(embedding, target_length):</span></code><code><span class="code-snippet_outer">    poly = PolynomialFeatures(degree=2)</span></code><code><span class="code-snippet_outer">    expanded_embedding = poly.fit_transform(embedding.reshape(1, -1))</span></code><code><span class="code-snippet_outer">    expanded_embedding = expanded_embedding.flatten()</span></code><code><span class="code-snippet_outer">    if len(expanded_embedding) &gt; target_length:</span></code><code><span class="code-snippet_outer">        # 如果扩展后的特征超过目标长度，可以通过截断或其他方法来减少维度</span></code><code><span class="code-snippet_outer">        expanded_embedding = expanded_embedding[:target_length]</span></code><code><span class="code-snippet_outer">    elif len(expanded_embedding) &lt; target_length:</span></code><code><span class="code-snippet_outer">        # 如果扩展后的特征少于目标长度，可以通过填充或其他方法来增加维度</span></code><code><span class="code-snippet_outer">        expanded_embedding = np.pad(expanded_embedding, (0, target_length - len(expanded_embedding)))</span></code><code><span class="code-snippet_outer">    return expanded_embedding</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">@app.post("/v1/embeddings", response_model=EmbeddingResponse)</span></code><code><span class="code-snippet_outer">async def get_embeddings(request: EmbeddingRequest):</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    # 计算嵌入向量和 tokens 数量 </span></code><code><span class="code-snippet_outer">    embeddings = [embeddings_model.encode(text) for text in request.input]</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    # 如果嵌入向量的维度不为 1536，则使用插值法扩展至 1536 维度 </span></code><code><span class="code-snippet_outer">    embeddings = [expand_features(embedding, 1536) if len(embedding) &lt; 1536 else embedding for embedding in embeddings]</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    # Min-Max normalization</span></code><code><span class="code-snippet_outer">    embeddings = [embedding / np.linalg.norm(embedding) for embedding in embeddings]</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    # 将 numpy 数组转换为列表</span></code><code><span class="code-snippet_outer">    embeddings = [embedding.tolist() for embedding in embeddings]</span></code><code><span class="code-snippet_outer">    prompt_tokens = sum(len(text.split()) for text in request.input)</span></code><code><span class="code-snippet_outer">    total_tokens = sum(num_tokens_from_string(text) for text in request.input)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    response = {</span></code><code><span class="code-snippet_outer">        "data": [</span></code><code><span class="code-snippet_outer">            {</span></code><code><span class="code-snippet_outer">                "embedding": embedding,</span></code><code><span class="code-snippet_outer">                "index": index,</span></code><code><span class="code-snippet_outer">                "object": "embedding"</span></code><code><span class="code-snippet_outer">            } for index, embedding in enumerate(embeddings)</span></code><code><span class="code-snippet_outer">        ],</span></code><code><span class="code-snippet_outer">        "model": request.model,</span></code><code><span class="code-snippet_outer">        "object": "list",</span></code><code><span class="code-snippet_outer">        "usage": {</span></code><code><span class="code-snippet_outer">            "prompt_tokens": prompt_tokens,</span></code><code><span class="code-snippet_outer">            "total_tokens": total_tokens,</span></code><code><span class="code-snippet_outer">        }</span></code><code><span class="code-snippet_outer">    }</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    return response</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ModelCard(BaseModel):</span></code><code><span class="code-snippet_outer">    id: str</span></code><code><span class="code-snippet_outer">    object: Literal["model"] = "model"</span></code><code><span class="code-snippet_outer">    owned_by: str = "owner"</span></code><code><span class="code-snippet_outer">    permission: List = []</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">class ModelList(BaseModel):</span></code><code><span class="code-snippet_outer">    object: Literal["list"] = "list"</span></code><code><span class="code-snippet_outer">    data: List[ModelCard] = []</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">    model_config = {</span></code><code><span class="code-snippet_outer">        "json_schema_extra": {</span></code><code><span class="code-snippet_outer">            "examples": [</span></code><code><span class="code-snippet_outer">                {</span></code><code><span class="code-snippet_outer">                    "object": "list",</span></code><code><span class="code-snippet_outer">                    "data": [{"id": "gpt-3.5-turbo", "object": "model", "owned_by": "owner", "permission": []}],</span></code><code><span class="code-snippet_outer">                }</span></code><code><span class="code-snippet_outer">            ]</span></code><code><span class="code-snippet_outer">        }</span></code><code><span class="code-snippet_outer">    }</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">@app.get("/v1/models")</span></code><code><span class="code-snippet_outer">async def list_models() -&gt; ModelList:</span></code><code><span class="code-snippet_outer">    return ModelList(data=[ModelCard(id="gpt-3.5-turbo")])</span></code><code><span class="code-snippet_outer"><br></span></code></pre></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">让他跑起来的命令，跑在 8000 端口下：</span></section><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="css"><code><span class="code-snippet_outer">uvicorn chatglm_cpp.openai_api:app --host 127.0.0.1 --port 8000</span></code></pre></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">运行该程序所需的 Python 依赖项：</span></p><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="ini"><code><span class="code-snippet_outer">accelerate==0.24.1</span></code><code><span class="code-snippet_outer">aiofiles==23.2.1</span></code><code><span class="code-snippet_outer">aiohttp==3.8.6</span></code><code><span class="code-snippet_outer">aiosignal==1.3.1</span></code><code><span class="code-snippet_outer">altair==5.1.2</span></code><code><span class="code-snippet_outer">annotated-types==0.6.0</span></code><code><span class="code-snippet_outer">anyio==3.7.1</span></code><code><span class="code-snippet_outer">async-timeout==4.0.3</span></code><code><span class="code-snippet_outer">attrs==23.1.0</span></code><code><span class="code-snippet_outer">blinker==1.7.0</span></code><code><span class="code-snippet_outer">cachetools==5.3.2</span></code><code><span class="code-snippet_outer">certifi==2023.7.22</span></code><code><span class="code-snippet_outer">charset-normalizer==3.3.2</span></code><code><span class="code-snippet_outer">click==8.1.7</span></code><code><span class="code-snippet_outer">contourpy==1.2.0</span></code><code><span class="code-snippet_outer">cpm-kernels==1.0.11</span></code><code><span class="code-snippet_outer">cycler==0.12.1</span></code><code><span class="code-snippet_outer">fastapi==0.103.2</span></code><code><span class="code-snippet_outer">ffmpy==0.3.1</span></code><code><span class="code-snippet_outer">filelock==3.13.1</span></code><code><span class="code-snippet_outer">fonttools==4.44.0</span></code><code><span class="code-snippet_outer">frozenlist==1.4.0</span></code><code><span class="code-snippet_outer">fsspec==2023.10.0</span></code><code><span class="code-snippet_outer">gitdb==4.0.11</span></code><code><span class="code-snippet_outer">GitPython==3.1.40</span></code><code><span class="code-snippet_outer">gradio==3.50.2</span></code><code><span class="code-snippet_outer">gradio_client==0.6.1</span></code><code><span class="code-snippet_outer">h11==0.14.0</span></code><code><span class="code-snippet_outer">httpcore==1.0.2</span></code><code><span class="code-snippet_outer">httpx==0.25.1</span></code><code><span class="code-snippet_outer">huggingface-hub==0.19.1</span></code><code><span class="code-snippet_outer">idna==3.4</span></code><code><span class="code-snippet_outer">importlib-metadata==6.8.0</span></code><code><span class="code-snippet_outer">importlib-resources==6.1.1</span></code><code><span class="code-snippet_outer">Jinja2==3.1.2</span></code><code><span class="code-snippet_outer">joblib==1.3.2</span></code><code><span class="code-snippet_outer">jsonschema==4.19.2</span></code><code><span class="code-snippet_outer">jsonschema-specifications==2023.7.1</span></code><code><span class="code-snippet_outer">kiwisolver==1.4.5</span></code><code><span class="code-snippet_outer">latex2mathml==3.76.0</span></code><code><span class="code-snippet_outer">linkify-it-py==2.0.2</span></code><code><span class="code-snippet_outer">Markdown==3.5.1</span></code><code><span class="code-snippet_outer">markdown-it-py==2.2.0</span></code><code><span class="code-snippet_outer">MarkupSafe==2.1.3</span></code><code><span class="code-snippet_outer">matplotlib==3.8.1</span></code><code><span class="code-snippet_outer">mdit-py-plugins==0.3.3</span></code><code><span class="code-snippet_outer">mdtex2html==1.2.0</span></code><code><span class="code-snippet_outer">mdurl==0.1.2</span></code><code><span class="code-snippet_outer">mpmath==1.3.0</span></code><code><span class="code-snippet_outer">multidict==6.0.4</span></code><code><span class="code-snippet_outer">networkx==3.2.1</span></code><code><span class="code-snippet_outer">nltk==3.8.1</span></code><code><span class="code-snippet_outer">numpy==1.26.2</span></code><code><span class="code-snippet_outer">orjson==3.9.10</span></code><code><span class="code-snippet_outer">packaging==23.2</span></code><code><span class="code-snippet_outer">pandas==2.1.3</span></code><code><span class="code-snippet_outer">Pillow==10.1.0</span></code><code><span class="code-snippet_outer">protobuf==4.25.0</span></code><code><span class="code-snippet_outer">psutil==5.9.6</span></code><code><span class="code-snippet_outer">pyarrow==14.0.1</span></code><code><span class="code-snippet_outer">pydantic==2.1.1</span></code><code><span class="code-snippet_outer">pydantic_core==2.4.0</span></code><code><span class="code-snippet_outer">pydeck==0.8.1b0</span></code><code><span class="code-snippet_outer">pydub==0.25.1</span></code><code><span class="code-snippet_outer">Pygments==2.16.1</span></code><code><span class="code-snippet_outer">pyparsing==3.1.1</span></code><code><span class="code-snippet_outer">python-dateutil==2.8.2</span></code><code><span class="code-snippet_outer">python-multipart==0.0.6</span></code><code><span class="code-snippet_outer">pytz==2023.3.post1</span></code><code><span class="code-snippet_outer">PyYAML==6.0.1</span></code><code><span class="code-snippet_outer">referencing==0.30.2</span></code><code><span class="code-snippet_outer">regex==2023.10.3</span></code><code><span class="code-snippet_outer">requests==2.31.0</span></code><code><span class="code-snippet_outer">rich==13.6.0</span></code><code><span class="code-snippet_outer">rpds-py==0.12.0</span></code><code><span class="code-snippet_outer">safetensors==0.4.0</span></code><code><span class="code-snippet_outer">scikit-learn==1.3.2</span></code><code><span class="code-snippet_outer">scipy==1.11.3</span></code><code><span class="code-snippet_outer">semantic-version==2.10.0</span></code><code><span class="code-snippet_outer">sentence-transformers==2.2.2</span></code><code><span class="code-snippet_outer">sentencepiece==0.1.99</span></code><code><span class="code-snippet_outer">six==1.16.0</span></code><code><span class="code-snippet_outer">smmap==5.0.1</span></code><code><span class="code-snippet_outer">sniffio==1.3.0</span></code><code><span class="code-snippet_outer">sse-starlette==1.6.5</span></code><code><span class="code-snippet_outer">starlette==0.27.0</span></code><code><span class="code-snippet_outer">streamlit==1.28.2</span></code><code><span class="code-snippet_outer">sympy==1.12</span></code><code><span class="code-snippet_outer">tabulate==0.9.0</span></code><code><span class="code-snippet_outer">tenacity==8.2.3</span></code><code><span class="code-snippet_outer">threadpoolctl==3.2.0</span></code><code><span class="code-snippet_outer">tiktoken==0.5.1</span></code><code><span class="code-snippet_outer">tokenizers==0.13.3</span></code><code><span class="code-snippet_outer">toml==0.10.2</span></code><code><span class="code-snippet_outer">toolz==0.12.0</span></code><code><span class="code-snippet_outer">torch==2.1.0</span></code><code><span class="code-snippet_outer">torchvision==0.16.0</span></code><code><span class="code-snippet_outer">tornado==6.3.3</span></code><code><span class="code-snippet_outer">tqdm==4.66.1</span></code><code><span class="code-snippet_outer">transformers==4.30.2</span></code><code><span class="code-snippet_outer">typing_extensions==4.6.1</span></code><code><span class="code-snippet_outer">tzdata==2023.3</span></code><code><span class="code-snippet_outer">tzlocal==5.2</span></code><code><span class="code-snippet_outer">uc-micro-py==1.0.2</span></code><code><span class="code-snippet_outer">urllib3==2.1.0</span></code><code><span class="code-snippet_outer">uvicorn==0.24.0.post1</span></code><code><span class="code-snippet_outer">validators==0.22.0</span></code><code><span class="code-snippet_outer">websockets==11.0.3</span></code><code><span class="code-snippet_outer">yarl==1.9.2</span></code><code><span class="code-snippet_outer">zipp==3.17.0</span></code><code><span class="code-snippet_outer"><br></span></code></pre></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_11"></span><h4 data-cangjie-key="358" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">搭建 One API 接口管理/分发系统</span></h4></li></ul><section style="margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">﻿One API 是一套兼容多种 LLM 接口规范的 API 路由方案，支持限额和计费管理，通过标准的 OpenAI API 格式访问所有的大模型，开箱即用，其多模型渠道接入、多用户管理、费用管理、额度管理、以及集群化部署支持等功能，对商用场景都很友好。项目使用 MIT 协议进行开源。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">One API 基于 Go 和 Node.js 开发，搭建之前准备好，我的版本是：go1.21.4、Node.js v20.9.0，构建命令如下：</span></section><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="properties"><code><span class="code-snippet_outer">git clone https://github.com/songquanpeng/one-api.git</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"># 构建前端</span></code><code><span class="code-snippet_outer">cd one-api/web</span></code><code><span class="code-snippet_outer">npm install</span></code><code><span class="code-snippet_outer">npm run build</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"># 构建后端</span></code><code><span class="code-snippet_outer">cd ..</span></code><code><span class="code-snippet_outer">go mod download</span></code><code><span class="code-snippet_outer">go build -ldflags "-s -w" -o one-api</span></code></pre></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">One API 里面预置了很多市面上的可用模型接口，好处是可以直接使用无需配置，缺点是没有添加自定义（本地）接口的能力。由于我们是自己搭建的 LLM 和 embedding 服务，需要修改其源代码，增加 ChatGLM3 和 m3e-base 的选项。</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">改动涉及两个文件，分别是 common/model-ratio.go 和 controller/model.go，改动内容如下图：</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041746" data-ratio="0.5166666666666667" src="https://oscimg.oschina.net/oscnet/b980c07e-c638-4726-8149-aa6f42f3df6b.jpg" data-type="other" data-w="1080" style="width: 578px;height: auto;" referrerpolicy="no-referrer"></span></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503041745" data-ratio="0.4564814814814815" src="https://oscimg.oschina.net/oscnet/bc153ea4-f8c8-4029-bef4-d61d6562cb64.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;">注意，改完文件后记得重新编译可执行文件。本地的元数据存储我使用了 MySQL，编译+启动命令是：</span></p><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="bash"><code><span class="code-snippet_outer">go build -ldflags "-s -w" -o one-api</span></code><code><span class="code-snippet_outer">export SQL_DSN=oneapi:oneapi@tcp(localhost:3306)/oneapi &amp;&amp; ./one-api --port 3001 --log-dir ./log</span></code></pre></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">初始登录进去，创建一个新令牌用于权限管控和计费：</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041742" data-ratio="0.4074074074074074" src="https://oscimg.oschina.net/oscnet/633c7cae-c8f2-479a-abdd-7ea0b14593d1.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">令牌可以从这里复制，下面有用：</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="letter-spacing: 1px;font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503041743" data-ratio="0.2824074074074074" src="https://oscimg.oschina.net/oscnet/22e2145c-3437-4ad8-8052-d99cf2bed6e6.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">One API 的渠道管理界面如下图，我已经配置了俩渠道，一个 chat 渠道，一个 embedding 渠道：</span></p><p><span style="font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503041744" data-ratio="0.4546296296296296" src="https://oscimg.oschina.net/oscnet/f988f5ad-ffb3-4568-9e4e-12f80431bad8.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">具体的配置值如下图，名称写实际的模型名 ChatGLM3，模型选刚才手动添加上去的 ChatGLM3：</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041751" data-ratio="0.7148148148148148" src="https://oscimg.oschina.net/oscnet/a3ae8bc9-2ce8-466a-8e32-2327dd193a66.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503041747" data-ratio="0.7185185185185186" src="https://oscimg.oschina.net/oscnet/af7b338e-56ca-42e3-873a-f194005c09cb.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">配置完后可以在列表页点一下测试验证，连通无问题就行，但现在似乎一测就会把模型 API 服务弄挂，不过没关系，不影响后面验证。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></section><span id="OSC_h4_12"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">搭建知识库应用</span></strong></span></h4><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">在这个环节里，我们采用类似 Dify.ai （地址：https://dify.ai/）的国产化开源 FastGPT 方案搭建属于自己的本地知识库应用平台。FastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景。FastGPT 遵循 Apache License 2.0 开源协议，我们可以 Fork 之后进行二次开发和发布。</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">FastGPT 的核心流程图如下：</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041748" data-ratio="0.525" src="https://oscimg.oschina.net/oscnet/dbc410c8-6a51-451c-a9d8-7c05661240c8.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">从 FastGPT 官网得知，这套开源系统基于以下几个基本概念进行知识库检索：</span></p><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">向量：将人类直观的语言（文字、图片、视频等）转成计算机可识别的语言（数组）。</span></p></li><li><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">向量相似度：两个向量之间可以进行计算，得到一个相似度，即代表：两个语言相似的程度。</span></p></li><li><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">语言大模型的一些特点：上下文理解、总结和推理。</span></p></li></ol><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">结合上述 3 个概念，便有了 「向量搜索 + 大模型 = 知识库问答」 的公式。下图是 FastGPT V3 中知识库问答功能的完整逻辑：</span></p><section style="margin-top: 8px;line-height: 1.75em;text-align: center;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041749" data-ratio="0.46296296296296297" src="https://oscimg.oschina.net/oscnet/1bed1c79-1bae-44a9-897a-ac6100c37f67.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">﻿</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">FastGPT 的向量存储方案是 PostgreSQL+pgvector，其他数据放在 MongoDB 里面，因此我们先把这两项依赖搞定。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_13"></span><h4 data-cangjie-key="538" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">安装 MongoDB</span></h4></li></ul><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">MacBook 安装 MongoDB 很简单，如果没有特别的安全诉求，可以先不用设置用户名密码</span></p><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="properties"><code><span class="code-snippet_outer">brew install mongodb-community</span></code><code><span class="code-snippet_outer">brew services start mongodb-community</span></code></pre></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">FastGPT 基于 MongoDB 存储知识库索引、会话内容、工作流等管理数据：</span></section><section style="margin-top: 8px;line-height: 1.75em;text-align: center;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041750" data-ratio="1.3333333333333333" src="https://oscimg.oschina.net/oscnet/72381400-16aa-4e0b-b636-70faee6c5c51.jpg" data-type="other" data-w="450" referrerpolicy="no-referrer"></span></section><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_14"></span><h4 data-cangjie-key="560" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">安装 PostgreSQL &amp; pgvector</span></h4></li></ul><section style="margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">FastGPT 采用了 RAG 中的 Embedding 方案构建知识库，PostgresSQL 的 PG Vector 插件作为向量检索器，索引为 HNSW。PostgresSQL 仅用于向量检索，MongoDB 用于其他数据的存取。另外也可以采用第三方模型的 Embedding API，比如 ChatGPT embedding，不过为了实现完整的本地化部署，就没有用外部服务。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">我们可以从 PostgreSQL 的官网下载 PostgreSQL 安装包：https://www.postgresql.org/download/macosx/﻿</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">从源码安装 pgvector：https://github.com/pgvector/pgvector</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="typescript"><code><span class="code-snippet_outer">// 安装 pgvector 前指定 PostgreSQL 位置</span></code><code><span class="code-snippet_outer">export PG_CONFIG=/Library/PostgreSQL/16/bin/pg_config</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">// 如果 pgvector 认错了 MacOS SDK 的位置，还得帮他软链一个</span></code><code><span class="code-snippet_outer">sudo ln -s /Library/Developer/CommandLineTools/SDKs/MacOSX13.sdk /Library/Developer/CommandLineTools/SDKs/MacOSX11.sdk</span></code><code><span class="code-snippet_outer">// 或者用这个命令</span></code><code><span class="code-snippet_outer">export SDKROOT=$(xcrun --sdk macosx --show-sdk-path)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">// 源码编译安装 </span></code><code><span class="code-snippet_outer">make</span></code><code><span class="code-snippet_outer">make install # may need sudo</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">// 确保插件已安装到 PostgreSQL 目录下</span></code><code><span class="code-snippet_outer">cd /Library/PostgreSQL/16/share/postgresql/extension/</span></code><code><span class="code-snippet_outer">ls | grep vector</span></code></pre></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">完成以上步骤后，打开 PostgreSQL 控制枱，随便建立一个连接，运行下面的查询：</span></section><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="sql"><code><span class="code-snippet_outer">CREATE EXTENSION vector;</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">SELECT * FROM pg_extension WHERE extname = 'vector';</span></code></pre></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">如果能出现下图结果，说明 pgvector 已经安装成功：</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041753" data-ratio="0.44166666666666665" src="https://oscimg.oschina.net/oscnet/dac102ea-3331-4cba-9e81-a7bbf955d8ea.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_15"></span><h4 data-cangjie-key="605" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">搭建 FastGPT 知识库问答系统</span></h4></li></ul><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">好，我们的主角终于上场了，下面有请 FastGPT，安装指南见：https://doc.fastgpt.in/docs/development/intro/﻿</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">第一步，按照里面的步骤，配置 .env.local 文件内容，指定 One API、MongoDB 和 PostgreSQL 的访问地址：</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041755" data-ratio="0.3675925925925926" src="https://oscimg.oschina.net/oscnet/ad8bc609-8cdc-4490-9d41-6d0cf05d1086.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">这里的 CHAT_API_KEY 填入上面 OneAPI 创建的令牌</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">第二步，在 config.local.json 里面注册对话模型和向量嵌入模型，注意这里的 model 值要和 One API 里配置的保持一致：</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041752" data-ratio="0.9101123595505618" src="https://oscimg.oschina.net/oscnet/f5654367-e043-4a33-af56-6fa29b073fd5.jpg" data-type="other" data-w="534" referrerpolicy="no-referrer"></span></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503041754" data-ratio="0.5521885521885522" src="https://oscimg.oschina.net/oscnet/cb497e99-311a-46eb-b6fe-bbbf6a4c9531.jpg" data-type="other" data-w="594" style="width: 529px;height: 292px;" referrerpolicy="no-referrer"></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">第三步，安装 Node.js 依赖并以开发模式启动：</span></p><section class="code-snippet__fix code-snippet__js"><pre class="code-snippet__js" data-lang="properties"><code><span class="code-snippet_outer"># 代码根目录下执行，会安装根 package、projects 和 packages 内所有依赖</span></code><code><span class="code-snippet_outer">pnpm i</span></code><code><span class="code-snippet_outer"># 切换到应用目录</span></code><code><span class="code-snippet_outer">cd projects/app</span></code><code><span class="code-snippet_outer"># 开发模式运行</span></code><code><span class="code-snippet_outer">pnpm dev</span></code></pre></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">第四步，访问本地 FastGPT 地址 http://localhost:3000/，如果能顺利登录，则搭建成功。</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_16"></span><h4 data-cangjie-key="673" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">验证模型对话能力</span></h4></li></ul><section style="margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">创建一个应用：</span></section><p style="margin-top: 8px;line-height: 1.75em;text-align: center;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041756" data-ratio="0.7851851851851852" src="https://oscimg.oschina.net/oscnet/4a48d9ed-303b-4116-ad64-db2e3c14ab62.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">应用创建完成，进入对话界面，注意 AI 模型选择我们在 One API 里配置的 ChatGLM3。试着问他两个问题，可以看到推理速度还是很快的，分别是 5.83s、7.52s：</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041759" data-ratio="0.6212962962962963" src="https://oscimg.oschina.net/oscnet/e9a9fe0b-8101-498c-a692-7f17f7d0d35c.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="letter-spacing: 1px;font-size: 15px;">点开单条对话响应，详细的对话参数（消耗 token、响应时长、计费信息）清晰可见：</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="letter-spacing: 1px;font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503041758" data-ratio="1.3190184049079754" src="https://oscimg.oschina.net/oscnet/c1f6a537-1bcf-43e7-b191-0339eba66711.jpg" data-type="other" data-w="978" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="letter-spacing: 1px;font-size: 15px;">查看 MacBook 上的 ChatGLM3 推理资源占用情况，占用了 3.78GB 内存</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><span style="letter-spacing: 1px;font-size: 15px;"><img class="rich_pages wxw-img" data-imgfileid="503041760" data-ratio="0.6240740740740741" src="https://oscimg.oschina.net/oscnet/e9008ab1-d693-4941-bd75-cef91cbe81d1.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><p style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);text-align: center;line-height: normal;"><img class="rich_pages wxw-img" data-imgfileid="503041757" data-ratio="0.328125" data-s="300,640" src="https://oscimg.oschina.net/oscnet/0fd3c7b5-94e1-4236-9399-af4ec7167e22.png" data-type="png" data-w="256" style="outline: 0px;letter-spacing: 0.544px;font-size: 16px;visibility: visible !important;width: 122px !important;" referrerpolicy="no-referrer"></p><p style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);color: rgb(100, 99, 99);font-size: 15px;text-align: center;line-height: normal;"><span style="outline: 0px;font-family: -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 1px;font-size: 18px;color: rgb(0, 17, 255);">知识库问答实战</span></p><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><span id="OSC_h4_17"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">准备知识库语料</span></strong></span></h4><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">在有知识库使用诉求的场景，我们一般都积累了比较多的私有知识数据，比如：语雀文档、钉钉文档、PDF、Office 文件等，视知识识图的建设标准，需要将它们一一结构化整理。数据的梳理、清洗、结构化是一项繁杂而重要的工作，但也有比较成熟的办法和工具，在此不再赘述。</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></p><span id="OSC_h4_18"></span><h4 data-lake-id="ca6fa186228ff7efaa653334195896c8" data-wording="true" style="margin-top: 0.5em;margin-bottom: 8px;text-wrap: wrap;outline: 0px;letter-spacing: 0.544px;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;visibility: visible;line-height: 1.75em;"><span style="outline: 0px;visibility: visible;letter-spacing: 1px;color: rgb(0, 17, 255);"><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">▐</span></strong><strong style="outline: 0px;visibility: visible;"><span style="outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">&nbsp;&nbsp;</span></strong><strong style="outline: 0px;visibility: visible;"><span style="color: rgb(0, 17, 255);outline: 0px;font-family: Optima-Regular, PingFangTC-light;visibility: visible;">导入知识库数据</span></strong></span></h4><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">FastGPT 提供了很多种原始数据导入的办法，并且为了更好地和企业系统集成，FastGPT 支持通过 API 的方式地二次开发导入能力，支持和已有知识管理系统更好地自动化集成。常见的导入方法有：</span></section><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">导入数据方案 1 - 直接分段导入：直接分段会利用句子分词器对文本进行一定长度拆分，可以理解为原始文档 Chunk。</span></section></li><li><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">导入数据方案 2 - QA 导入：导入时会调用大模型对分段进行学习，然后直接生成问题-答案。</span></section></li><li><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">导入数据方案 3 - 手动录入：手动录入问题和答案。</span></section></li><li><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">导入数据方案 4 - CSV 录入：批量快速录入问题答案对。</span></section></li><li><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;text-align: left;"><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 1px;">导入数据方案 5 - API 导入，</span><span style="font-size: 15px;letter-spacing: 1px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;">详见：OpenAPI 接口文档（地址：https://doc.fastgpt.in/docs/development/openapi/#%E7%9F%A5%E8%AF%86%E5%BA%93%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE）</span><span style="font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 1px;">﻿</span></section></li></ol><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">﻿</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">我们先简单地录入几个问题和答案，然后后面快速验证 RAG 效果。</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">新建一个知识库，注意，索引模型一旦选择不可更改。这里我们选择刚部署好的 m3e-base</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041761" data-ratio="0.769620253164557" src="https://oscimg.oschina.net/oscnet/d357e075-29d4-4b82-8032-48ddf63a806a.jpg" data-type="other" data-w="790" referrerpolicy="no-referrer"></span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">进入知识库初始界面，已经默认有了一个「手动录入」文件夹，我们在这里录入几条测试问答</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503041762" data-ratio="0.387037037037037" src="https://oscimg.oschina.net/oscnet/8812cf81-0f6c-4d49-9e3c-900ff61c2b4b.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"><br></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="font-size: 15px;letter-spacing: 1px;">录入内容分两种类型，其中：被搜索的内容指将被向量化的部分，通常是问题，或者精炼扼要的描述，需要准确填写</span></section><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;text-align: center;"><span style="font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041763" data-ratio="0.33425925925925926" src="https://oscimg.oschina.net/oscnet/6d02fcef-1034-404e-8fe3-6269dece3ada.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></p><ul class="list-paddingleft-1" style="list-style-type: disc;"><li style="color: rgb(0, 17, 255);"><span id="OSC_h4_19"></span><h4 data-cangjie-key="820" data-cangjie-leaf-block="true" data-type="heading-4" style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;color: rgb(0, 17, 255);">验证知识库问答效果</span></h4></li></ul><section style="margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="font-size: 15px;letter-spacing: 1px;">重新打开应用，关联刚才创建好的知识库（注意这里一定要保证才会生效），问他一个简单的问题，回答的质量看起来还可以。</span></section><blockquote style="box-sizing: inherit;margin-top: 0px;margin-bottom: 0px;margin-left: 2px;padding-top: 0px;padding-left: 11px;opacity: 0.5;border-left-width: 4px;border-left-color: rgb(217, 219, 221);color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;font-size: 14.6667px;letter-spacing: normal;text-align: start;white-space: pre-wrap;background-color: rgb(255, 255, 255);"><p style="text-align: justify;"><span style="box-sizing: inherit;font-size: 15px;">问：简单说一下商家共振模型是个啥呗？</span></p></blockquote><blockquote style="box-sizing: inherit;margin-top: 0px;margin-bottom: 0px;margin-left: 2px;padding-top: 0px;padding-left: 11px;opacity: 0.5;border-left-width: 4px;border-left-color: rgb(217, 219, 221);color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;font-size: 14.6667px;letter-spacing: normal;text-align: start;white-space: pre-wrap;background-color: rgb(255, 255, 255);"><p style="text-align: justify;"><span style="box-sizing: inherit;font-size: 15px;">答：商家共振模型是一种商业策略和运营模式，旨在促进商家和平台之间的合作关系，同时提高用户的粘性和平台的影响力。这个模式通过激励商家在站内和站外进行投放活动，帮助商家获得更多的流量和销售机会，从而促进商家的生意增长。同时，这个模式也能够扩大平台的规模和影响力，增加商家对平台的依赖性和忠诚度。总的来说，商家共振模型是一种有效的商业策略和运营模式，对于猫超等平台来说是一个不错的选择。</span></p></blockquote><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;text-align: center;"><img class="rich_pages wxw-img" data-imgfileid="503041764" data-ratio="0.6083333333333333" src="https://oscimg.oschina.net/oscnet/ad7951a6-f135-449a-8cc7-0a6081e1a501.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></section><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;text-align: left;white-space: break-spaces;background-color: rgb(255, 255, 255);font-size: 15px;letter-spacing: 1px;">点击答案中的「1 条引用」，还可以看到答案的出处来源：</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;text-align: center;"><span style="color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;text-align: left;white-space: break-spaces;background-color: rgb(255, 255, 255);font-size: 15px;letter-spacing: 1px;"><img class="rich_pages wxw-img" data-imgfileid="503041765" data-ratio="0.6555555555555556" src="https://oscimg.oschina.net/oscnet/af4bb324-f2ed-432d-8dd3-c8c14d6ac63a.jpg" data-type="other" data-w="1080" referrerpolicy="no-referrer"></span></p><section style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><br></section><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;text-align: left;white-space: break-spaces;background-color: rgb(255, 255, 255);font-size: 15px;letter-spacing: 1px;">至此，我们就完成了一个简单的知识库构建和应用的过程，也验证了整套本地知识库方案的可行性。</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;text-align: left;white-space: break-spaces;background-color: rgb(255, 255, 255);font-size: 15px;letter-spacing: 1px;">﻿</span></p><section style="margin-right: 0cm;margin-bottom: 0px;margin-left: 0cm;outline: 0px;text-wrap: wrap;background-color: rgb(255, 255, 255);font-size: 11pt;font-family: DengXian;color: rgb(0, 0, 0);letter-spacing: normal;text-align: center;line-height: normal;"><img class="rich_pages wxw-img" data-imgfileid="503041766" data-ratio="0.328125" data-s="300,640" data-type="png" data-w="256" src="https://oscimg.oschina.net/oscnet/4454b7bc-70eb-48c3-afc5-445ce122d5f1.png" style="outline: 0px;color: rgb(34, 34, 34);font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;visibility: visible !important;width: 133px !important;" referrerpolicy="no-referrer"></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);color: rgb(100, 99, 99);font-size: 15px;text-align: center;line-height: normal;"><span style="outline: 0px;font-family: -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 1px;font-size: 18px;color: rgb(0, 17, 255);">未来展望</span></section><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;text-align: left;white-space: break-spaces;background-color: rgb(255, 255, 255);font-size: 15px;letter-spacing: 1px;"><br></span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">串通了整个知识库应用流程，我们完成了从 0 到 1 的起步。虽然整体应用架构是按实际商用标准来搭建的，但要想使用效果也达到工业级别的标准，还有很多工作值得进一步探索，包括但不限于：</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">1️⃣ 大模型应用层面：</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">1、更好的文档 Chunk、Embedding、多路加权平均搜索召回方案，提升 RAG 整体效能</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">2、更好的 Prompt Engineering，充分挖掘 LLM 的潜力</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">3、工作流编排、CoT、Agent，满足实际的企业应用需求</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">2️⃣ 稳定性层面：如果达到商用级别，需要更高配置的软硬件环境</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">3️⃣ 落地价值层面：从解决身边的问题开始，解决真金白银的商业问题</span></p><p style="margin-top: 8px;margin-bottom: 8px;line-height: 1.75em;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">﻿</span></p><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">希望本文能给每位计划自己搭建大模型知识库应用的工程师一点参考，动手跑通一个程序的乐趣是无穷的，更多的实操作经验分享，我们在评论区交流。</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="color: rgb(64, 64, 64);font-family: &quot;zh quote&quot;, &quot;Helvetica Neue&quot;, -apple-system, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, STHeiti, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;;text-align: left;white-space: break-spaces;background-color: rgb(255, 255, 255);font-size: 15px;letter-spacing: 1px;"><br></span></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;color: rgb(34, 34, 34);background-color: rgb(255, 255, 255);text-align: center;line-height: normal;"><img class="rich_pages wxw-img" data-imgfileid="503041767" data-ratio="0.328125" data-s="300,640" src="https://oscimg.oschina.net/oscnet/1e2db517-aad4-4517-add1-0856c0864c05.png" data-type="png" data-w="256" style="outline: 0px;color: rgb(51, 51, 51);font-size: 20px;font-weight: bold;letter-spacing: 0.578px;visibility: visible !important;width: 134px !important;" referrerpolicy="no-referrer"></section><section style="margin-bottom: 0px;outline: 0px;font-family: system-ui, -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0.544px;text-wrap: wrap;background-color: rgb(255, 255, 255);color: rgb(100, 99, 99);font-size: 15px;text-align: center;line-height: normal;"><span style="outline: 0px;font-family: -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 1px;font-size: 18px;color: rgb(0, 17, 255);">团队介绍</span></section><section style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><br></section><p style="margin-top: 8px;line-height: 1.75em;margin-bottom: 8px;"><span style="box-sizing: inherit;font-size: 15px;letter-spacing: 1px;">我们是淘天渠道分销技术团队，负责淘天全渠道一盘货产品技术研发。我们通过用技术手段解决电商多段销售中的多角色商业往来问题，构建了灵活的新零售供应链分销产品平台，致力于为商家提供多元化的供给和销售渠道、助力商家在全平台取得更高的成交额。<br style="background-clip: padding-box;caret-color: rgb(23, 26, 29);color: rgb(23, 26, 29);font-family: -apple-system, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Segoe UI&quot;, system-ui, Roboto, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;font-size: 14px;letter-spacing: normal;text-align: left;white-space: pre-wrap;text-size-adjust: auto;"><br style="background-clip: padding-box;caret-color: rgb(23, 26, 29);color: rgb(23, 26, 29);font-family: -apple-system, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Segoe UI&quot;, system-ui, Roboto, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;font-size: 14px;letter-spacing: normal;text-align: left;white-space: pre-wrap;text-size-adjust: auto;">长期招募人才，欢迎投递简历：xieyi.xie@alibaba-inc.com</span></p><section style="margin-bottom: 8px;"><br></section><section data-role="outer" label="Powered by 135editor.com" style="outline: 0px;letter-spacing: 0.544px;visibility: visible;"><section style="margin-top: 8px;outline: 0px;letter-spacing: 0.544px;word-break: break-all;color: rgb(0, 0, 0);font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, &quot;PingFang SC&quot;, Cambria, Cochin, Georgia, Times, &quot;Times New Roman&quot;, serif;text-align: center;line-height: 1.75em;margin-bottom: 8px;"><span style="outline: 0px;color: rgb(0, 17, 255);"><strong style="outline: 0px;">¤</strong></span><span style="outline: 0px;"><strong style="outline: 0px;">&nbsp;拓展阅读&nbsp;</strong></span><span style="outline: 0px;color: rgb(0, 17, 255);"><strong style="outline: 0px;">¤</strong></span></section><section style="margin-top: 8px;margin-bottom: 8px;outline: 0px;letter-spacing: 0.544px;word-break: break-all;color: rgb(0, 0, 0);font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, &quot;PingFang SC&quot;, Cambria, Cochin, Georgia, Times, &quot;Times New Roman&quot;, serif;text-align: center;line-height: 1.75em;"><br style="outline: 0px;"></section><section style="margin-top: 8px;margin-bottom: 8px;outline: 0px;font-family: -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.544px;text-indent: 0em;text-align: center;line-height: 1.75em;min-height: 24px;clear: both;visibility: visible;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fappmsgalbum%3F__biz%3DMzAxNDEwNjk5OQ%3D%3D%26action%3Dgetalbum%26album_id%3D2565944923443904512%23wechat_redirect" textvalue="3DXR 技术" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);-webkit-user-drag: none;cursor: pointer;">3DXR 技术</a>&nbsp;|&nbsp; 
   <a target="_blank" href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fappmsgalbum%3F__biz%3DMzAxNDEwNjk5OQ%3D%3D%26action%3Dgetalbum%26album_id%3D1533906991218294785%23wechat_redirect" textvalue="终端技术" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);-webkit-user-drag: none;cursor: pointer;">终端技术</a>&nbsp;|&nbsp; 
   <a target="_blank" href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fappmsgalbum%3F__biz%3DMzAxNDEwNjk5OQ%3D%3D%26action%3Dgetalbum%26album_id%3D1592015847500414978%23wechat_redirect" textvalue="音视频技术" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);-webkit-user-drag: none;cursor: pointer;">音视频技术</a></section><section style="margin-top: 8px;margin-bottom: 8px;outline: 0px;min-height: 24px;font-family: -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.544px;text-indent: 0em;text-align: center;line-height: 1.75em;visibility: visible;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fappmsgalbum%3F__biz%3DMzAxNDEwNjk5OQ%3D%3D%26action%3Dgetalbum%26album_id%3D1539610690070642689%23wechat_redirect" textvalue="服务端技术" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);-webkit-user-drag: none;cursor: pointer;">服务端技术</a><span style="outline: 0px;letter-spacing: 0.544px;">&nbsp;|&nbsp;</span><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fappmsgalbum%3F__biz%3DMzAxNDEwNjk5OQ%3D%3D%26action%3Dgetalbum%26album_id%3D2565883875634397185%23wechat_redirect" textvalue="技术质量" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);-webkit-user-drag: none;cursor: pointer;">技术质量</a>&nbsp;|&nbsp; 
   <a target="_blank" href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fmp%2Fappmsgalbum%3F__biz%3DMzAxNDEwNjk5OQ%3D%3D%26action%3Dgetalbum%26album_id%3D1522425612282494977%23wechat_redirect" textvalue="数据算法" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);-webkit-user-drag: none;cursor: pointer;">数据算法</a></section><p style="margin-top: 8px;margin-bottom: 8px;outline: 0px;min-height: 24px;font-family: -apple-system, &quot;system-ui&quot;, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 0.544px;text-indent: 0em;text-align: center;line-height: 1.75em;visibility: visible;"><br style="outline: 0px;"></p><section class="mp_profile_iframe_wrp" style="margin-bottom: 24px;outline: 0px;"><mp-common-profile class="custom_select_card mp_profile_iframe js_wx_tap_highlight" data-pluginname="mpprofile" data-id="MzAxNDEwNjk5OQ==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/33P2FdAnju8t5nZGhAatCrc4e2iaDfAaoInribRKxc7MOqdTGygfcLqSDxhj0trCHVEh94Sjl7zuWYzwouYtJ0VQ/300?wx_fmt=png&amp;wxfrom=19" data-nickname="大淘宝技术" data-alias="AlibabaMTT" data-signature="大淘宝技术官方账号" data-from="2" data-index="0" data-origin_num="685" data-isban="0" data-weuitheme="light" data-biz_account_status="0" data-is_biz_ban="0"></mp-common-profile><span style="outline: 0px;color: rgb(0, 0, 0);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;font-size: 15px;letter-spacing: 1px;"></span></section></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - 大淘宝技术（AlibabaMTT）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 03:15:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4662964/blog/10448445</guid>
            <link>https://my.oschina.net/u/4662964/blog/10448445</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Mozilla 和 Firefox 的变迁：市场下滑背后的思考]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>最近 Mozilla 基金会发布的 2023 年度报告引发了广泛关注。报告显示，在 Mozilla CEO 薪酬大幅上涨的同时，其旗舰产品 Firefox 浏览器的市场份额却持续下滑。这一现象不仅反映出 Mozilla 的财务状况尚佳，也揭示了其业务重心可能正在发生变化，但同时也让人不免对 Firefox 前景产生担忧。</p><p style="text-align:center"><img height="919" src="https://oscimg.oschina.net/oscnet/up-4547075666f5a7674598dca51bafe5cc568.png" width="1920" referrerpolicy="no-referrer"></p><p style="color:#999999; text-align:center">The State of Mozilla 网站截图</p><p>具体来看，2022 年 Mozilla CEO 的年薪高达 690 万美元，较去年增加了 130 万美元，达到创纪录的新高。与此形成对比的是，Mozilla 的整体收入出现轻微下滑，由 2021 年的 6 亿美元降至 2022 年的 5.93 亿美元。这表明，尽管财务资产总额继续增长，达到高达 13 亿美元，但收入增长出现停滞。</p><p>更值得关注的是，在 Mozilla 财务数据保持乐观的背景下，其核心产品 Firefox 的市场表现却难掩颓势。据统计，2022 年 Firefox 的全球浏览器市场份额已从 2021 年底的 3.79% 下降至 3.04%，跌幅达 20%。考虑到近年移动互联网的快速发展，这一数据更显示出 Firefox 在移动端的表现不佳。</p><p>面对 Firefox 市场份额的下滑，业内分析普遍认为其背后反映的是 Mozilla 业务重心的转变。在财务报告中可以看出，Mozilla 的「版税收入」有所下降，而「订阅和广告收入」则有所增加，似乎显示出其正在加速多元化业务，减少对 Firefox 的依赖。而今年早些时候，Mozilla CEO 就明确表示，公司将重点转向人工智能等新兴领域。</p><p>因此，有分析指出，Mozilla 可能正处在从浏览器向人工智能等新业务转型的关键节点上。这可能也解释了为何在 Firefox 表现疲软的情况下，企业高管的薪酬水平还能大幅提升。很显然，Mozilla 领导层正在根据新的发展战略进行调整。</p><p>但业内也存在担忧的声音。毕竟，Firefox 曾是开源运动的一面旗帜，同时也是少数能与 Chrome 竞争的浏览器之一。一旦 Mozilla 继续减少 Firefox 投入，将可能对浏览器市场格局和网络开放性产生一定影响。</p><p>最近，Mozilla 基金会报告在 Hacker News 社区也引发了热烈讨论。许多社区成员对报告反映出的 Mozilla 业务战略转变表示不解甚至失望。他们普遍认为，Mozilla 不应过度减少对 Firefox 的投入，而应更专注于维护其核心产品。</p><p>一些用户还表示，由于 Firefox 的私密性保护功能，其实际用户量可能高于统计数据。他们希望 Mozilla 能继续致力于提升 Firefox 的核心功能，如密码管理、广告屏蔽等，这对维系用户群至关重要。此外，一些社区用户还呼吁 Mozilla 应该采取行动巩固 Firefox 的市场地位，确保浏览器市场的开放和多样。</p><p>综合来看，Mozilla 当前的市场表现确实反映了一家企业在变革中的两难处境。CEO 薪酬的大幅提高似乎预示着企业正根据新的发展战略进行市场调整，这在商业上也许可理解。但作为曾经开源界的领军产品，Firefox 的持续下滑无疑让人担忧。维护核心产品与开拓新业务之间的平衡，可能是 Mozilla 当前面临的主要难题。</p><p>无论前景如何，Firefox 在开源浏览器市场的地位和作用还将持续受到业内关注。而 Mozilla 也面临着在财务增长和维系开源社区期望之间找到最佳路径的挑战。我们期待 Mozilla 能继续致力于开放和创新，同时维护其社区支持度。毕竟，只有在社区的积极参与下，开源精神才能持续发扬光大。</p><blockquote><p>注：Mozilla 的报告总是会滞后一年，所以文中提到了很多 2022 年的信息</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 02:57:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273248</guid>
            <link>https://www.oschina.net/news/273248</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[英伟达推出特供中国销售的 GeForce RTX 4090 D]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在 GeForce RTX 4090 被列入了出口管控清单之后，英伟达推出了特供中国市场销售的<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.nvidia.cn%2Fgeforce%2Fgraphics-cards%2F40-series%2Frtx-4090-d%2F" target="_blank"> GeForce RTX 4090 D</a></u>，建议零售价 1.3 万人民币，1 月 20 日发售。</p><p>作为英伟达针对美国新出口限制的回应，RTX 4090 D 能满足美国的出口管控要求，旨在为中国游戏玩家提供高性能的游戏体验。</p><p><img src="https://oscimg.oschina.net/oscnet/up-a380dbf4c058357511bfd4b9931862b1751.png" referrerpolicy="no-referrer"></p><p>据介绍，RTX 4090 D 有 14592 个 CUDA Core 核心，频率 2.28GHz-2.52GHz，显存 24 GB GDDR6X。相比下原版 RTX 4090 有 16384 个 CUDA Cores 核心。此外，RTX 4090 D 基础频率高于 RTX 4090，但由于综合运算性能的限制，其 CUDA 和 Tensor 核心数量低于 RTX 4090。因此新版本预计会大幅降低 AI 推理性能，游戏性能可能变化不太显著。</p><p>与 RTX 4090 的主要参数对比：</p><ul><li>CUDA 核心数量：从 16384 个减至 14592 个</li><li>Tensor 核心数量：从 512 个减至 456 个</li><li>RT 核心数量：从 128 个减至 114 个</li><li>基础频率：2280 MHz，高于 RTX 4090 的 2235 MHz</li><li>加速频率：与 RTX 4090 相同，均为 2.52 GHz</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-b9e684e4d8fd2a515232e8171aa8f883882.png" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-68e049a7a0911bd428574a653036dc7f080.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 02:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273243/nvidia-rtx-4090-d</guid>
            <link>https://www.oschina.net/news/273243/nvidia-rtx-4090-d</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[ODC —— 全场景数据库开发和数据管理协同工具]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#1f2328">OceanBase 开发者中心（简称 ODC）是&nbsp;</span>Oceanbase 开源的<span style="background-color:#ffffff; color:#1f2328">全场景数据库开发和数据管理协同工具，通过协同开发解决数据库的变更风险管控、数据管理和数据安全问题。</span></p><p><span style="background-color:#ffffff; color:#1f2328">优点：</span></p><p style="text-align:start"><strong>随时随地高效 SQL 开发</strong></p><ul><li>ODC 基于现代 WEB 架构，随时随地，只要有浏览器就可以访问您的数据库。</li><li>ODC SQL 开发功能全面且易用，桌面开发工具有的功能 ODC 也有，甚至支持 PL 调试。</li></ul><p style="text-align:start"><strong>守护 SQL 开发过程的每一次变更</strong></p><ul><li>在 SQL 开发过程的全部场景，包括可视化对象管理、SQL 查询、数据编辑、数据导入和导出、... ，ODC 都内置了风险控制。</li><li>ODC 提供基于项目的协同和变更审批流程，并且内置了 SQL 检查规则、SQL 窗口规范、风险等级识别。</li></ul><p style="text-align:start"><strong>自动数据生命周期管理</strong></p><ul><li>ODC 支持数据按照保留时长清理或归档，5 分钟构建你的冷热数据分离系统。</li><li>ODC 不仅支持按照数据的时间标记处理，也支持按照分区批量处理。</li><li>还可以通过 ODC SQL 计划任务完成计算任务，为什么还要继续使用你的 CRONTAB ？</li></ul><p style="text-align:start"><strong>全场景敏感数据保护</strong></p><ul><li>ODC 数据脱敏支持静态场景也支持动态场景，结构变更、SQL 查询、结果集导出、数据导出，全部开发场景都会脱敏。</li><li>安全管理员配置敏感数据规则和脱敏算法，DBA 和，开发，都无法接触敏感数据。</li></ul><p style="text-align:start"><strong>无需改变已有系统就可以集成 ODC 到当前工作流程</strong></p><ul><li>无需改变你的系统就可以把 ODC 集成到你当前的数据库开发协同工作流程中.</li><li>SSO、审批集成、SQL 审核集成、堡垒机集成、审计集成，企业管控集成需要的能力全都有。</li></ul><h4 style="text-align:start"><strong>功能特性</strong></h4><p style="color:#1f2328; text-align:start">ODC 产品功能包括 SQL 开发和管控协同 2 个方面，核心功能列举如下。</p><p style="text-align:start"><strong>SQL 开发</strong></p><ul><li>多数据源：支持 OceanBase MySQL 模式/Oracle 模式、ODP Sharing、MySQL 等数据库（不断增加中）。</li><li>数据库对象：表、视图、序列、同义词、触发器、存储过程、函数、程序包、类型，等对象的可视化管理。</li><li>SQL 执行：SQL 编辑和执行、命令行窗口、脚本、代码片段。</li><li>数据查看与编辑：数据查看、数据编辑、结果集 Excel 互操作。</li><li>监控诊断：执行计划、执行剖析、数据库全链路 TRACE。</li><li>数据生成：测试数据生成，场景化数据生成。</li><li>导入和导出：结构导入、结构导出、数据导入、数据导出、整库导入、整库导出。</li><li>PL 生命周期：PL 对象的执行、编译和调试。</li><li>数据库运维：回话管理、变量管理、回收站管理、权限管理（规划中）。</li></ul><p style="text-align:start"><strong>管控协同</strong></p><ul><li>权限管理：用户管理、基于 RBAC 模型的自定义角色。</li><li>团队协同：基于项目的管理员、DBA、开发多角色协同、库级别访问权限管理。</li><li>变更管控：自定义审批流程、基于语法规则的 SQL 检查。</li><li>稳定变更：无锁结构变更、无锁数据变更（规划中）。</li><li>数据生命周期：数据清理、数据归档、自动分区、SQL 定时任务。</li><li>数据安全：敏感列管理、数据导出脱敏、数据查询脱敏、敏感列权限。</li><li>合规审计：操作审计、SQL 审计、审计集成。</li><li>协同效率：批量导入配置、自动授权规则、通知中心。</li><li>系统集成：OAuth2、OIDC 账号集成、堡垒机集成、审批集成、SQL 审核集成。</li><li>体验学习：实验资源、教程和课程、代码库（规划中）。</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 02:17:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/odc</guid>
            <link>https://www.oschina.net/p/odc</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 基于 Kotlin 开发的播放器软件 MXVideo]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-mxvideo" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#mxvideo"></a>MXVideo</h1><h4><a id="user-content-介绍" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#%E4%BB%8B%E7%BB%8D"></a>介绍</h4><p>基于 Kotlin 开发的播放器，默认支持 MediaPlayer 播放器，可扩展 IJK 播放器、EXO 播放器、阿里云播放器、以及任何使用 TextureView 的播放器, 开箱即用，欢迎提 issue 和 pull request</p><blockquote><p>简书相关介绍（待完善）：<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.jianshu.com%2Fnb%2F50294642">https://www.jianshu.com/nb/50294642</a></p></blockquote><p>最新版本：<a href="https://gitee.com/link?target=https%3A%2F%2Fjitpack.io%2F%23zhangmengxiong%2FMXVideo"><img src="https://jitpack.io/v/zhangmengxiong/MXVideo.svg" alt="" referrerpolicy="no-referrer"></a></p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="n">implementation</span><span class="s1">'com.github.zhangmengxiong:MXVideo:1.9.0'</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><img src="https://gitee.com/zhangmengxiong/MXVideo/raw/master/imgs/1.png" alt="Normal" referrerpolicy="no-referrer"><img src="https://gitee.com/zhangmengxiong/MXVideo/raw/master/imgs/2.png" alt="Land Screen" referrerpolicy="no-referrer"><img src="https://gitee.com/zhangmengxiong/MXVideo/raw/master/imgs/3.png" alt="Touch Seek" referrerpolicy="no-referrer"><img src="https://gitee.com/zhangmengxiong/MXVideo/raw/master/imgs/4.png" alt="Pause" referrerpolicy="no-referrer"><img src="https://gitee.com/zhangmengxiong/MXVideo/raw/master/imgs/5.png" alt="Rotation" referrerpolicy="no-referrer"><img src="https://gitee.com/zhangmengxiong/MXVideo/raw/master/imgs/6.png" alt="Light Seek" referrerpolicy="no-referrer"></p><h4><a id="user-content-功能特性" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#%E5%8A%9F%E8%83%BD%E7%89%B9%E6%80%A7"></a>功能特性</h4><ul><li>任意播放器内核（包含开源 IJK、谷歌 Exo、阿里云等等）</li><li>单例播放，只能同时播放一个节目</li><li>0 代码集成全屏功能</li><li>可以调节音量、屏幕亮度</li><li>可以注册播放状态监听回调</li><li>播放器高度可以根据视频高度自动调节</li><li>播放器支持设置宽高比，设置宽高比后，高度固定。</li><li>自动保存与恢复播放进度（可关闭）</li><li>支持循环播放、全屏时竖屏模式、可关闭快进快退功能、可关闭全屏功能、可关闭非 WiFi 环境下流量提醒</li><li>支持播放时获取实时截屏 Bitmap</li></ul><h5><a id="user-content-1 通过-dependence-引入 mxvideo" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#1%E9%80%9A%E8%BF%87-dependence-%E5%BC%95%E5%85%A5mxvideo"></a>1、通过 dependence 引入 MXVideo</h5><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="n">dependencies</span><span class="o">{</span></span><span id="LC2" class="line"><span class="n">implementation</span><span class="s1">'com.github.zhangmengxiong:MXVideo:x.x.x'</span></span><span id="LC3" class="line"><span class="o">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h5><a id="user-content-2 页面集成" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#2%E9%A1%B5%E9%9D%A2%E9%9B%86%E6%88%90"></a>2、页面集成</h5><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nt">&lt;com.mx.video.MXVideoStd</span></span><span id="LC2" class="line"><span class="na">android:id=</span><span class="s">"@+id/mxVideoStd"</span></span><span id="LC3" class="line"><span class="na">android:layout_width=</span><span class="s">"match_parent"</span></span><span id="LC4" class="line"><span class="na">android:layout_height=</span><span class="s">"wrap_content"</span><span class="nt">/&gt;</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// Activity 或者 Fragment 中生命周期变更，处理进入后台/前台时的暂停/续播功能</span></span><span id="LC2" class="line"><span class="k">override</span><span class="k">fun</span><span class="nf">onStart</span><span class="p">()</span><span class="p">{</span></span><span id="LC3" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">onStart</span><span class="p">()</span></span><span id="LC4" class="line"><span class="k">super</span><span class="p">.</span><span class="nf">onStart</span><span class="p">()</span></span><span id="LC5" class="line"><span class="p">}</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="k">override</span><span class="k">fun</span><span class="nf">onStop</span><span class="p">()</span><span class="p">{</span></span><span id="LC8" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">onStop</span><span class="p">()</span></span><span id="LC9" class="line"><span class="k">super</span><span class="p">.</span><span class="nf">onStop</span><span class="p">()</span></span><span id="LC10" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h5><a id="user-content-3 开始播放" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#3%E5%BC%80%E5%A7%8B%E6%92%AD%E6%94%BE"></a>3、开始播放</h5><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 设置播放占位图</span></span><span id="LC2" class="line"><span class="nc">Glide</span><span class="p">.</span><span class="nf">with</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nf">load</span><span class="p">(</span><span class="s">"http://www.xxx.com/xxx.png"</span><span class="p">).</span><span class="nf">into</span><span class="p">(</span><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getPosterImageView</span><span class="p">())</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c1">// 默认从上一次进度播放</span></span><span id="LC5" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setSource</span><span class="p">(</span><span class="nc">MXPlaySource</span><span class="p">(</span><span class="nc">Uri</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="s">"https://aaa.bbb.com/xxx.mp4"</span><span class="p">),</span><span class="s">"标题 1"</span><span class="p">))</span></span><span id="LC6" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">startPlay</span><span class="p">()</span></span><span id="LC7" class="line"></span><span id="LC8" class="line"><span class="c1">// 从头开始播放</span></span><span id="LC9" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setSource</span><span class="p">(</span><span class="nc">MXPlaySource</span><span class="p">(</span><span class="nc">Uri</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="s">"https://aaa.bbb.com/xxx.mp4"</span><span class="p">),</span><span class="s">"标题 1"</span><span class="p">),</span><span class="n">seekTo</span><span class="p">=</span><span class="mi">0</span><span class="p">)</span></span><span id="LC10" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">startPlay</span><span class="p">()</span></span><span id="LC11" class="line"></span><span id="LC12" class="line"><span class="c1">// 从第 10 秒开始播放</span></span><span id="LC13" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setSource</span><span class="p">(</span><span class="nc">MXPlaySource</span><span class="p">(</span><span class="nc">Uri</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="s">"https://aaa.bbb.com/xxx.mp4"</span><span class="p">),</span><span class="s">"标题 1"</span><span class="p">),</span><span class="n">seekTo</span><span class="p">=</span><span class="mi">10</span><span class="p">)</span></span><span id="LC14" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">startPlay</span><span class="p">()</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><blockquote><p>MXPlaySource 可选参数说明：</p></blockquote><table><thead><tr><th align="left">参数</th><th align="center">说明</th><th align="right">默认值</th></tr></thead><tbody><tr><td align="left">title</td><td align="center">标题</td><td align="right">""</td></tr><tr><td align="left">headerMap</td><td align="center">网络请求头部</td><td align="right">null</td></tr><tr><td align="left">isLooping</td><td align="center">是否循环播放</td><td align="right">false</td></tr><tr><td align="left">enableSaveProgress</td><td align="center">是否存储、读取播放进度</td><td align="right">true</td></tr><tr><td align="left">isLiveSource</td><td align="center">是否直播源，当时直播时，不显示进度，无法快进快退暂停</td><td align="right">false</td></tr></tbody></table><h5><a id="user-content-4 监听播放进度" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#4%E7%9B%91%E5%90%AC%E6%92%AD%E6%94%BE%E8%BF%9B%E5%BA%A6"></a>4、监听播放进度</h5><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">addOnVideoListener</span><span class="p">(</span><span class="kd">object</span><span class="err">: </span><span class="nc">MXVideoListener</span><span class="p">()</span><span class="p">{</span></span><span id="LC2" class="line"><span class="c1">// 播放状态变更</span></span><span id="LC3" class="line"><span class="k">override</span><span class="k">fun</span><span class="nf">onStateChange</span><span class="p">(</span><span class="n">state</span><span class="p">:</span><span class="nc">MXState</span><span class="p">)</span><span class="p">{</span></span><span id="LC4" class="line"><span class="p">}</span></span><span id="LC5" class="line"></span><span id="LC6" class="line"><span class="c1">// 播放时间变更</span></span><span id="LC7" class="line"><span class="k">override</span><span class="k">fun</span><span class="nf">onPlayTicket</span><span class="p">(</span><span class="n">position</span><span class="p">:</span><span class="nc">Int</span><span class="p">,</span><span class="n">duration</span><span class="p">:</span><span class="nc">Int</span><span class="p">)</span><span class="p">{</span></span><span id="LC8" class="line"><span class="p">}</span></span><span id="LC9" class="line"><span class="p">})</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h5><a id="user-content-5 全屏返回--释放资源" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#5%E5%85%A8%E5%B1%8F%E8%BF%94%E5%9B%9E--%E9%87%8A%E6%94%BE%E8%B5%84%E6%BA%90"></a>5、全屏返回 + 释放资源</h5><blockquote><p>这里 MXVideo 默认持有当前播放的 MXVideoStd，可以使用静态方法操作退出全屏、释放资源等功能。</p><p>也可以直接使用 viewId：mxVideoStd.isFullScreen()，mxVideoStd.isFullScreen()，mxVideoStd.release() 等方法。</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="k">override</span><span class="k">fun</span><span class="nf">onBackPressed</span><span class="p">()</span><span class="p">{</span></span><span id="LC2" class="line"><span class="k">if</span><span class="p">(</span><span class="nc">MXVideo</span><span class="p">.</span><span class="nf">isFullScreen</span><span class="p">())</span><span class="p">{</span></span><span id="LC3" class="line"><span class="nc">MXVideo</span><span class="p">.</span><span class="nf">gotoNormalScreen</span><span class="p">()</span></span><span id="LC4" class="line"><span class="k">return</span></span><span id="LC5" class="line"><span class="p">}</span></span><span id="LC6" class="line"><span class="k">super</span><span class="p">.</span><span class="nf">onBackPressed</span><span class="p">()</span></span><span id="LC7" class="line"><span class="p">}</span></span><span id="LC8" class="line"></span><span id="LC9" class="line"><span class="k">override</span><span class="k">fun</span><span class="nf">onDestroy</span><span class="p">()</span><span class="p">{</span></span><span id="LC10" class="line"><span class="nc">MXVideo</span><span class="p">.</span><span class="nf">releaseAll</span><span class="p">()</span></span><span id="LC11" class="line"><span class="k">super</span><span class="p">.</span><span class="nf">onDestroy</span><span class="p">()</span></span><span id="LC12" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-功能相关" class="anchor" href="https://gitee.com/zhangmengxiong/MXVideo#%E5%8A%9F%E8%83%BD%E7%9B%B8%E5%85%B3"></a>功能相关</h3><ul><li>切换播放器内核</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 默认 MediaPlayer 播放器，库默认内置</span></span><span id="LC2" class="line"><span class="n">com</span><span class="p">.</span><span class="n">mx</span><span class="p">.</span><span class="n">video</span><span class="p">.</span><span class="n">player</span><span class="p">.</span><span class="nc">MXSystemPlayer</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c1">// 谷歌的 Exo 播放器</span></span><span id="LC5" class="line"><span class="n">com</span><span class="p">.</span><span class="n">mx</span><span class="p">.</span><span class="n">mxvideo_demo</span><span class="p">.</span><span class="n">player</span><span class="p">.</span><span class="n">exo</span><span class="p">.</span><span class="nc">MXExoPlayer</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="c1">// IJK 播放器</span></span><span id="LC8" class="line"><span class="n">com</span><span class="p">.</span><span class="n">mx</span><span class="p">.</span><span class="n">mxvideo_demo</span><span class="p">.</span><span class="n">player</span><span class="p">.</span><span class="nc">MXIJKPlayer</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="c1">// 阿里云播放器，仅支持 」armeabi-v7a「、」arm64-v8a「 两种 CPU</span></span><span id="LC11" class="line"><span class="n">com</span><span class="p">.</span><span class="n">mx</span><span class="p">.</span><span class="n">mxvideo_demo</span><span class="p">.</span><span class="n">player</span><span class="p">.</span><span class="nc">MXAliPlayer</span></span><span id="LC12" class="line"></span><span id="LC13" class="line"><span class="c1">// 设置播放源是可以设置内核，默认 = MXSystemPlayer</span></span><span id="LC14" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setPlayer</span><span class="p">(</span><span class="nc">MXSystemPlayer</span><span class="o">::</span><span class="k">class</span><span class="p">.</span><span class="n">java</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置播放地址，标题，跳转等信息</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setSource</span><span class="p">(</span><span class="nc">MXPlaySource</span><span class="p">(</span><span class="nc">Uri</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="s">"xxx"</span><span class="p">),</span><span class="n">title</span><span class="p">=</span><span class="s">"xxx"</span><span class="p">),</span><span class="n">seekTo</span><span class="p">=</span><span class="mi">0</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>视频渲染旋转角度</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 默认旋转角度 = MXOrientation.DEGREE_0</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setTextureOrientation</span><span class="p">(</span><span class="nc">MXOrientation</span><span class="p">.</span><span class="nc">DEGREE_90</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置当前视频静音,不会影响系统音量</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 默认=false</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setAudioMute</span><span class="p">(</span><span class="k">true</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置播放器音量百分比，实际音量 = (volume * 系统当前音量)</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 默认=1f，当设置=0f 时，视频则静音</span></span><span id="LC2" class="line"><span class="c1">// 取值范围：0f -&gt; 1f</span></span><span id="LC3" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setVolumePercent</span><span class="p">(</span><span class="mf">0.5f</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>视频填充规则</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 强制填充宽高 MXScale.FILL_PARENT</span></span><span id="LC2" class="line"><span class="c1">// 根据视频大小，自适应宽高 MXScale.CENTER_CROP</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c1">// 默认填充规则 = MXScale.CENTER_CROP</span></span><span id="LC5" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setScaleType</span><span class="p">(</span><span class="nc">MXScale</span><span class="p">.</span><span class="nc">CENTER_CROP</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>MXVideoStd 控件宽高约束</li></ul><blockquote><p>在页面 xml 中添加，layout_width 一般设置 match_parent，高度 wrap_content</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nt">&lt;com.mx.video.MXVideoStd</span></span><span id="LC2" class="line"><span class="na">android:id=</span><span class="s">"@+id/mxVideoStd"</span></span><span id="LC3" class="line"><span class="na">android:layout_width=</span><span class="s">"match_parent"</span></span><span id="LC4" class="line"><span class="na">android:layout_height=</span><span class="s">"wrap_content"</span><span class="nt">/&gt;</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><blockquote><p>可以设置任意宽高比，如果设置宽高比，则控件高度需要设置 android:layout_height="wrap_content"，否则不生效。</p><p>当取消约束、MXVideo 高度自适应、填充规则=MXScale.CENTER_CROP 时，控件高度会自动根据视频宽高自动填充高度</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// MXVideoStd 控件设置宽高比= 16：9</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setDimensionRatio</span><span class="p">(</span><span class="mf">16.0</span><span class="p">/</span><span class="mf">9.0</span><span class="p">)</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c1">// MXVideoStd 控件设置宽高比= 4：3</span></span><span id="LC5" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setDimensionRatio</span><span class="p">(</span><span class="mf">4.0</span><span class="p">/</span><span class="mf">3.0</span><span class="p">)</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="c1">// 取消约束</span></span><span id="LC8" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">setDimensionRatio</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>进度跳转</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 进度单位：秒，可以在启动播放后、错误或播完之前调用</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">seekTo</span><span class="p">(</span><span class="mi">55</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置不能快进快退</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">canSeekByUser</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置不能全屏</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">canFullScreen</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置缓冲时显示网速信息</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">canShowNetSpeed</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>没有设置 source 时不显示播放按钮</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=false</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">hidePlayBtnWhenNoSource</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置全屏按钮是否显示</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="c1">// 全屏按钮只有在  canFullScreen=true &amp;&amp; showFullScreenButton=true 时显示</span></span><span id="LC3" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">showFullScreenButton</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置不显示控件右上角时间</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">canShowSystemTime</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置不显示底部 1dp 高度的进度条</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">canShowBottomSeekBar</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置不显示控件右上角电量图</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">canShowBatteryImg</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置关闭 WiFi 环境播放前提醒</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">showTipIfNotWifi</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置打开 TextureView 的水平镜像模式</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=false</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">mirrorMode</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">true</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置播放完成后自动退出全屏</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">gotoNormalScreenWhenComplete</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置播放错误后自动退出全屏</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">gotoNormalScreenWhenError</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置播放时用户不可以暂停</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=true</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">canPauseByUser</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">false</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置播放时如果手机横屏则自动进入全屏播放</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=false</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">autoFullScreenBySensor</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">true</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置全屏播放时屏幕方向自动跟随重力方向</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=MXSensorMode.SENSOR_FIT_VIDEO</span></span><span id="LC2" class="line"><span class="c1">// MXSensorMode.SENSOR_AUTO = 跟随重力方向</span></span><span id="LC3" class="line"><span class="c1">// MXSensorMode.SENSOR_FIT_VIDEO = 跟随视频宽高自动旋转 0 或 180 度</span></span><span id="LC4" class="line"><span class="c1">// MXSensorMode.SENSOR_NO = 根据视频宽高比固定横屏/竖屏，横屏 = 视频宽&gt;=高   --   竖屏 = 视频宽&lt;高</span></span><span id="LC5" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">fullScreenSensorMode</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="nc">MXSensorMode</span><span class="p">.</span><span class="nc">SENSOR_AUTO</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置直播流播放错误时自动重试</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放前设置，默认=false</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">replayLiveSourceWhenError</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">true</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>设置播放器内控件动画效果</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 将动画时长设置为 &lt;=0 时，禁止动画效果</span></span><span id="LC2" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">animatorDuration</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="mi">0L</span><span class="p">)</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c1">// 默认时长 200 毫秒</span></span><span id="LC5" class="line"><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getConfig</span><span class="p">().</span><span class="n">animatorDuration</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="mi">200L</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>播放时截屏</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="k">if</span><span class="p">(</span><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">isPlaying</span><span class="p">())</span><span class="p">{</span></span><span id="LC2" class="line"><span class="kd">val</span><span class="py">bitmap</span><span class="p">:</span><span class="nc">Bitmap</span><span class="p">?</span><span class="p">=</span><span class="n">mxVideoStd</span><span class="p">.</span><span class="nf">getTextureView</span><span class="p">()</span><span class="o">?.</span><span class="n">bitmap</span></span><span id="LC3" class="line"><span class="n">screenCapImg</span><span class="p">.</span><span class="nf">setImageBitmap</span><span class="p">(</span><span class="n">bitmap</span><span class="p">)</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>非全屏时，设置支持滑动快进快退/音量调节/亮度调节功能</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 默认非全屏时不支持滑动相关操作</span></span><span id="LC2" class="line"><span class="n">config</span><span class="p">.</span><span class="n">enableTouchWhenNormalScreen</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="k">true</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li>播放倍数设置</li></ul><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1">// 播放倍数设置，默认 1.0 倍数播放</span></span><span id="LC2" class="line"><span class="n">config</span><span class="p">.</span><span class="n">playSpeed</span><span class="p">.</span><span class="k">set</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div>]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 02:13:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/zhangmengxiong/MXVideo</guid>
            <link>https://gitee.com/zhangmengxiong/MXVideo</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 探索 Seata 项目开源开发之旅]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><em>作者：尹祥琨，清华大学，Seata 开源之夏学生参与者</em></p><p>Seata 是一款开源的分布式事务解决方案，致力于在微服务架构下提供高性能和简单易用的分布式事务服务。在今年的开源之夏活动中，我加入了 Apache Seata (Incubator) 社区，完成了开源之夏的课题，并从此一直积极参与社区。我有幸在云栖大会-开发者秀场上分享了我的开发者经验。在本文中，我将与大家分享我在 Seata 社区中的开发者之旅，以及在这个旅程中积累的经验和见解。希望通过我的故事，能够激励更多人踏上这充满挑战和激励的开源之路，为开源社区的繁荣做出自己的贡献。</p><p><img src="https://oscimg.oschina.net/oscnet/up-aebda794f193baa6a525759178f78ec45b8.png" alt="" referrerpolicy="no-referrer"></p><h2>相关背景</h2><p>在正式介绍我的经历之前，我想先提供一些相关的背景信息，以解释为什么我要参与开源以及如何参与开源。关于参与开源的原因，我相信每个人都有不同的动机。以下是我认为一些主要的原因：</p><ul><li><strong>学习：</strong> 参与开源使我们有机会为不同组织开发的开源项目做出贡献，与行业专家互动，提供了学习的机会。</li><li><strong>技能提升：</strong> 以我为例，我通常使用 Java 和 Python 进行后端开发。但在参与 Seata 项目时，我有机会学习 Go 语言，拓宽了我的后端技术栈。此外作为学生，我很难接触到生产级框架或应用，而开源社区为我提供了这个机会。</li><li><strong>兴趣：</strong> 我身边的朋友都是热衷于开源的，他们享受编程，对开源充满热情。</li><li><strong>求职：</strong> 参与开源可以丰富我们的作品集，为简历增加分量。</li><li><strong>工作需求：</strong> 有时参与开源是为了解决工作中遇到的问题或满足工作需求。</li></ul><p>这些都是参与开源的原因，对我来说，学习、技能提升和兴趣是我参与开源的主要动机。无论你是在校学生还是在职人员，如果你有参与开源的意愿，不要犹豫，任何人都可以为开源项目做出贡献。年龄、性别、工作和所在地都不重要，关键是你的热情和对开源项目的好奇心。</p><p><strong>我参与开源的契机是参加了中科院软件所举办的开源之夏活动。</strong></p><p>开源之夏是一个面向高校开发者的开源活动，社区发布开源项目，学生开发者在导师的指导下完成项目的开发，结项成果贡献给社区，合入社区仓库，获得项目奖金和证书。开源之夏是踏入开源社区的一个绝佳契机，也是我第一次比较正式地接触开源项目，而这个经历为我打开了一扇全新的大门。自此我深刻地认识到参与开源项目的建设，分享自己的技术成果，让更多的开发者能够使用你所贡献的东西，是一件极富乐趣和意义的事情。</p><p>下面我分享的这张图片是开源之夏官方公开的数据，从 2020 年开始参与的社区数量还有学生数量都在逐年增加，活动也是越办越好。可以看到今年的参与的社区项目共有 133 个，每个社区又提供了若干个课题，而每位学生只能选择一个课题。想要在这么多个社区中找到想要参与的社区和适合自己的课题是一个相对复杂的任务。</p><p><img src="https://oscimg.oschina.net/oscnet/up-2cb63ff2fddcde028ba35a97e0ee5b21e2f.png" alt="" referrerpolicy="no-referrer"></p><p><strong>综合考虑社区的活跃程度、技术栈契合度、新人引导情况等，最终我选择加入 Seata 社区。</strong></p><p>Seata 是一款开源的分布式事务框架，提供了完整的分布式事务解决方案，包括 AT、TCC、Saga 和 XA 事务模式，可支持多种编程语言和数据存储方案。从 19 年开源起到今年已经走过了&nbsp;<strong>5</strong>&nbsp;个年头，社区中有超过 <strong>300</strong> 多位贡献者，项目收获了&nbsp;<strong>24k+</strong> &nbsp;星标，是一个非常成熟的社区。同时 Seata 兼容&nbsp;<strong>10</strong>&nbsp;余种主流 RPC 框架和 RDBMS，与&nbsp;<strong>20</strong>&nbsp;多个社区存在集成和被集成的关系，被<strong>几千</strong>家客户应用到业务系统中，可以说是分布式事务解决方案的事实标准。</p><p><img src="https://oscimg.oschina.net/oscnet/up-1322af951d1654325b68bb62db27ac119fe.png" alt="" referrerpolicy="no-referrer"></p><p>2023 年 10 月 29 日，Seata 正式捐赠给了 Apache 软件基金会，成为孵化项目。经过孵化之后，Seata 将有望成为首个 Apache 软件基金会的分布式事务框架顶级项目。这次捐赠也将推动 Seata 更广泛地发展，对生态系统的建设产生深远的影响，从而使更多的开发者受益。这个重要的里程碑也为 Seata 带来更广阔的发展空间。</p><h2>开发之旅</h2><p><strong>介绍完了一些基本情况，后文中我将分享我在 Seata 社区的开发之旅。</strong></p><p>在正式开始开发之前，我进行了许多准备工作。因为 Seata 已经经历了五年的发展，积累了数十万行代码，因此直接参与开发需要一定的上手成本。我分享了一些准备经验，希望能够为大家提供一些启发。</p><h3>1. 文档和博客是第一手材料</h3><p>文档和博客这类的文本材料可以帮助社区新人迅速了解项目背景和代码结构。</p><p>首先，官方文档是最主要的参考资料，从这里可以了解到一切官方认为你需要了解的东西。</p><p><img src="https://oscimg.oschina.net/oscnet/up-00cfd6de008e0436da49cf6e2ced547e0ad.png" alt="" referrerpolicy="no-referrer"></p><p>博客，仅次于官方文档的材料，一般是开发者或者是深度用户编写的，和文档不同的点在于博客可能会更深入到某个专项上去介绍，比如一些项目的理论模型、项目结构、某个模块的源码分析等等。</p><p><img src="https://oscimg.oschina.net/oscnet/up-3fa38a82a0fb5a68516e1f8917a6bcf914e.png" alt="" referrerpolicy="no-referrer"></p><p>公众号，和博客类似，一般是偏技术性的文章，公众号还有个优点是可以订阅推送，利用碎片时间阅读一些技术。</p><p><img src="https://oscimg.oschina.net/oscnet/up-66dba9d75a4371e6cec17b1f6bcde799b7b.png" alt="" referrerpolicy="no-referrer"></p><p>此外，开源社区的一些在线分享或线下 Meetup 公开的幻灯片也是非常有意义的文本资料。</p><p><img src="https://oscimg.oschina.net/oscnet/up-36ffca7b98bc0d65f0567e08e6e57c3913b.png" alt="" referrerpolicy="no-referrer"></p><p>除了官方资料之外，还有许多第三方资料可供学习，比如可以通过用户分享的 use cases 了解项目的具体实施和实践；通过第三方社区的集成文档了解项目的生态；还有就是通过第三方的视频教程来学习。但在所有这些资料中，我认为官方文档和博客是最有帮助的。</p><h3>2. 熟悉使用框架</h3><p>当然刚才说的这些文本资料肯定不需要面面俱到的看完，纸上得来终觉浅，看到感觉差不多明白了就可以去实践了。可以按照官方文档的"Get Started"章节逐步了解项目的基本流程。另一种方法是查找官方提供的示例或演示，构建并运行它们，理解代码和配置的含义，并通过使用项目了解项目的需求、目标以及现有功能和架构。</p><p>例如，Seata 有一个名为 seata-samples 的仓库，其中包含 20 多种用例，比如 Seata 和 Dubbo 集成，和 SCA, Nacos 集成的案例，基本可以覆盖到支持的所有场景。</p><h3>3. 粗略阅读源代码把握主要逻辑</h3><p>在准备阶段，粗略地阅读源代码以把握项目的主要逻辑也很重要。了解如何高效地把握项目的主要内容是一个需要长期积累的技能。首先，通过前述的准备步骤，了解项目的概念、交互和流程模型是很有帮助的。</p><p>以 Seata 为例，通过官方文档和实际操作，可以了解 Seata 事务领域的三个角色：TC（Transaction Coordinator）、TM（Transaction Manager）和 RM（Resource Manager）。TC 作为独立部署的 Server 用于维护全局和分支事务的状态，是 Seata 实现高可用的关键；TM 用于与 TC 交互，定义全局事务的开始、提交或回滚；RM 用于管理分支事务处理的资源，与 TC 交互以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。粗略地了解这些角色之间的交互后，可以更轻松地把握项目的主要逻辑。</p><p><img src="https://oscimg.oschina.net/oscnet/up-5c6f741753e54c13960a277e28738b6a8b6.png" alt="" referrerpolicy="no-referrer"></p><p>脑海里刻下了这些模型的印象，对源码的主干提取就相对得心应手了一些。比如 Seata TC 事务协调者，作为 Server 端，是一个独立于业务部署的单独应用。那为了分析源码，就可以直接在本地把 server 起起来，通过启动类开始追踪。可以分析到一些初始化的逻辑比如服务注册、全局锁的初始化等等。还有可以通过 RPC 的调用来追踪到交互逻辑的代码，比如 TC 是如何对全局事务和分支事务进行持久化，如何驱动全局事务提交或者回滚的。</p><p>然而内嵌客户端的框架代码，没有一个启动类入口可以入手分析。那其实可以从一个 sample 入手，找到其对框架代码的引用从而进行阅读。比如 Seata 一个很重要的注解是 GlobalTransaction，用于标识一个全局事务。想要知道 TM 是如何对这个注解分析的，那我们通过 IDE 的搜索功能，找到 GlobalTransaction 的拦截器即可分析其中的逻辑。</p><p>还有一个小 tips 分享给大家，往往来说单测注重於单一模块的职能，可以通过阅读单测可以了解一个模块的输入输出、逻辑边界，也可以顺着单测的调用链去阅读代码，也是理解源码一个很重要的手段。</p><p><strong>万事俱备只欠东风，做完充足的准备，下一步就是区积极参与到社区之中。</strong></p><p>参与的方式也有很多种，最常见的参与方式是查看项目的 Issues 列表，社区通常会为新贡献者标记一些带有特殊标签的 Issue，如「good-first-issue」、「contributions-welcome」和「help-wanted」等。可以通过这些标签筛选感兴趣的任务。</p><p><img src="https://oscimg.oschina.net/oscnet/up-77fc64ac1ba403624668e51380910130c78.png" alt="" referrerpolicy="no-referrer"></p><p>除了 Issues，GitHub 还提供了讨论的功能，可以参与一些公开的讨论并获取新的想法。</p><p><img src="https://oscimg.oschina.net/oscnet/up-658832d16529452411c236cdae75a8df324.png" alt="" referrerpolicy="no-referrer"></p><p>此外，社区通常会定期举行会议，比如周会或双周会，可以通过参加这些会议来了解社区的最新进展，提出问题以及与其他社区成员交流。</p><h2>总结与心得</h2><p>我加入 Seata 社区最初是通过开源之夏活动。我完成了我的课题，为 Seata Saga 实现了一些新的功能，也做了一系列的优化。但我不止于此，因为在 Seata 的开源经历中我获得了学生生涯中最宝贵的一次开发者体验，在之后的时间我也持续通过上述参与方式持续活跃在社区中。这主要得益于以下几个方面：</p><ul><li><p><strong>沟通与社交</strong></p><p>导师制度为我提供了重要的支持。在开发过程中，我与我的导师亦夏之间的密切合作对我适应社区文化和工作流程起到了关键作用。他不仅帮助我适应了社区，还为我提供了程序设计的思路，也与我分享了一些在工作中的经验和见解，这些都对我的发展非常有帮助。此外，Seata 社区创始人清铭也提供了很多帮助，包括建立了与其他同学的联系，帮助我进行 Code Review，也为我提供了许多机会。</p></li><li><p><strong>正反馈</strong></p><p>在 Seata 的开发过程中，我经历了一个良性的循环。许多细节为我提供了许多正反馈，例如我的贡献能被用户广泛使用和受益，比如开发得到了社区的认可。这些正反馈加强了我继续在 Seata 社区贡献的意愿。</p></li><li><p><strong>技能提升</strong>再就是参与 Seata 开发，对我能力的提升也是巨大的。在这里，我能学习到生产级别的代码，包括性能优化，接口设计，边界判断的技巧。可以直接参与一个开源项目的运作，包括项目计划，安排，沟通等。当然还了解一个分布式事务框架是如何设计并实现的。</p></li></ul><p>除了这些宝贵的开发者体验，我也从这次经历中体悟到了一些关于参与开源的个人心得，为激励其他有兴趣参与开源社区的同学，我做了简单的总结：</p><ul><li><p><strong>了解和学习社区文化和价值观</strong></p><p>每个开源社区都有不同的文化和价值观。了解社区的文化和价值观对于成功参与社区至关重要。观察和了解社区其他成员的日常开发和交流方式是学习社区文化的好方法。在社区中要尊重他人的意见和包容不同的观点。</p></li><li><p><strong>敢于迈出第一步</strong></p><p>不要害怕面对困难，迈出第一步是参与开源社区的关键。可以通过领取标有"good-first-issue"等标签的 Issue，编写文档、单元测试等方式来开始。重要的是要克服畏难情绪，积极尝试并学习。</p></li><li><p><strong>对自己的工作要充满信心</strong></p><p>不要怀疑自己的能力。每个人都是从零开始的，没有人天生就是专家。参与开源社区是一个学习和成长的过程，需要不断的实践和积累经验。</p></li><li><p><strong>积极参与讨论，持续学习不同技术</strong></p><p>不要害怕提出问题，无论是关于项目的具体技术还是开发过程中的挑战。同时也不要局限于一个领域。尝试学习和掌握不同编程语言、框架和工具，这可以拓宽技术视野，为项目提供有价值的洞见。</p></li></ul><hr><p>通过我的开源之旅，我积累了宝贵的经验和技能，这些不仅帮助我成长为一个更有价值的开发者，也让我深刻地了解了开源社区的力量。然而，我不仅仅是个别的参与者，我代表着 Seata 社区的一部分。Seata 作为一个正在不断成长和演变的开源项目，有着巨大的潜力，同时也面临着新的挑战。因此我要强调 Seata 社区的重要性和未来的潜力，它已经进入 Apache 软件基金会的孵化阶段，这个重要的里程碑将为 Seata 带来更广阔的发展空间。Seata 欢迎更多的开发者和贡献者的加入，让我们共同推动这个开源项目的发展，为分布式事务领域的进步贡献一份力量。</p><p>搜索钉钉群号</p><p>加入 Seata Group 开源交流群（群号：32033786）</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 02:05:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/10439711</guid>
            <link>https://my.oschina.net/u/3874284/blog/10439711</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Apideploy 重磅开源，在成熟 API 文档解决方案中又多了一个搅局者]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h3>背景</h3><p style="color:#333333; text-align:start">当前，在很多中、小型的开发团队或创业团队中，依然用着落后的方式进行着 API 接口的交互，他们写完文档来写代码，或者，写完代码来补文档，更甚者，文档全靠一张嘴，接口描述信息在 IM 中沟通飞扬，在联调的过程中前后端相互扯皮、苦不堪言。</p><p style="color:#333333; text-align:start">当然，也有很多优秀的团队，在使用较成熟的解决方案，诸如：swagger、springfox、springdoc、knife4j、apifox、eolink、smart-doc 等等。作者基于自己多年的研发经验，参考和对比了诸如上述国内外多个 API 管理工具和实现，始终觉得一款精美、高效的 API 平台工具，对开发者的意义重大。在苦苦找寻与对比下，各 API 管理工具依旧没有达到作者的期望，于是亲自下场，一怒之下写下了 Apideploy。</p><h3>设计理念</h3><p style="color:#333333; text-align:start">Apideploy 遵从以下设计理念：</p><ul><li><strong>代码即文档。</strong>API 文档应该通过代码自动生成，并能保持与代码的同步性，而不是通过手写文档来与前端、测试等进行协作；</li><li><strong>主流标准友好。</strong>文档应该支持主流的 OpenAPI 2（OAS2.0）、OpenAPI 3（OAS3.0）等协议标准，同时需要支持 HTTP、WebSocket、SSE 等协议；</li><li><strong>版本可追溯。</strong>每一次的版本迭代，可以快速查阅接口变更的明细，支持不同版本的差异对比，并能支持回滚文档版本；</li><li><strong>接口可 mock。</strong>可以直接在该产品上完成接口的测试、联调甚至接口自动化；</li><li><strong>界面要精美。</strong>友好的用户界面与交互；</li><li><strong>第三方兼容。</strong>支持导入常见的 API 协议标准文档，也支持导出常用的文档格式。</li></ul><h3>产品架构</h3><p style="color:#333333; text-align:start"><img alt="en_intro" src="https://doc.apideploy.cn/getting-started/en_intro.jpg" referrerpolicy="no-referrer"></p><p style="color:#333333; text-align:start">Apideploy 核心分为两部分：<strong>API 文档生成 SDK+API 托管与调试平台</strong>。</p><p style="color:#333333; text-align:start"><strong>API 文档生成 SDK</strong>是完全开放源码的，访问<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapideploy-team" target="_blank">https://github.com/apideploy-team</a><span>&nbsp;</span>可以查阅。目前仅支持 Java 语言实现，其他语言社区用户可以贡献，或自行直接通过<strong>API 托管与调试平台</strong>的 RESTFull API 进行对接。Java 语言 SDK 实现基于 javadoc 注释方式自动生成 API 文档（无代码侵入方式），也兼容了基于 swagger 的实现。具体使用请参考：</p><p style="color:#333333; text-align:start"><strong>API 托管与调试平台</strong>主要功能包括：项目管理、团队协作、权限管理、API 文档托管、文档调试、接口数据 mock、版本更新记录、版本对比、个性化文档导出、多格式文档导入等，是一个集 API 全生命周期管理的平台，非常适合团队协作。目前<strong>支持公有云与私有化部署</strong>，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdoc.apideploy.cn%2Fgetting-started%2Fwww.apideploy.com" target="_blank">www.apideploy.com</a>是公有云的解决方案。</p><h3>文档生成原理</h3><p style="color:#333333; text-align:start">Apideploy 推崇<strong>文档即代码</strong>的设计理念，所以作者<strong>强烈推荐基于代码注释的文档生成方式</strong>，它不像<strong>swagger</strong>的实现那么笨重。</p><h4>基于代码注释生成文档</h4><p style="color:#333333; text-align:start">基于代码注释生成文档，Apideploy 的实现参考并依赖了<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FTongchengOpenSource%2Fsmart-doc" target="_blank">smart-doc</a>的开源代码，smart-doc 是一款基于<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fpaul-hammant%2Fqdox" target="_blank">qdox</a>优秀的 Java 文档解析工具，它很方便的实现了基于代码注释到 API 文档的过程。</p><p style="color:#333333; text-align:start">Apideploy 开源了<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapideploy-team%2Fapideploy-jakarta-client" target="_blank">Java 代码注释生成文档</a>的所有代码。</p><h4>基于 Swagger / OpenAPI 的文档生成</h4><p style="color:#333333; text-align:start">在基于 Java Web 项目开发的 Swagger 实现中，目前用的比较多的开源实现是<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fspringfox%2Fspringfox" target="_blank">Springfox</a>(包括国内的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdoc.xiaominfo.com%2F" target="_blank">Knife4j</a>) 与<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fspringdoc%2Fspringdoc-openapi" target="_blank">Springdoc-openapi</a>。Springfox 的用户较多，但貌似已经停止更新，已经不再支持 springboot3.0+的项目。</p><p style="color:#333333; text-align:start">Apideploy 兼容了 springfox 和 springdoc-openapi 的全部实现，所以如果之前的项目使用的是 swagger 项目，也非常方便切换到 Apideploy 进行文档托管和测试。</p><h3>API 托管与调试平台预览</h3><p style="color:#333333; text-align:start"><img src="https://i.v2ex.co/vjQiUAMZl.jpeg" referrerpolicy="no-referrer"></p><p><img height="631" src="https://oscimg.oschina.net/oscnet/up-f3b7e7b16ff2539848c29736048a80f0a60.png" width="1280" referrerpolicy="no-referrer"></p><p><img height="1916" src="https://oscimg.oschina.net/oscnet/up-12ef3871f6c01d514e945c6e367e56b6f84.png" width="3742" referrerpolicy="no-referrer"></p><p><img height="1869" src="https://oscimg.oschina.net/oscnet/up-314606b0148f12694dd771012925d45e33c.png" width="3756" referrerpolicy="no-referrer"></p><p><img height="1600" src="https://oscimg.oschina.net/oscnet/up-ad5b3c3b4d97c98ae6e693a1c749587a94f.png" width="3812" referrerpolicy="no-referrer"></p><p><img height="1893" src="https://oscimg.oschina.net/oscnet/up-e1f3e4b1e7e6abeebdab56dd886314c62ec.png" width="3840" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 28 Dec 2023 15:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273213/apideploy-opensource</guid>
            <link>https://www.oschina.net/news/273213/apideploy-opensource</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[百度 CTO 王海峰：文心一言用户规模破 1 亿]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="margin-left:0.0001pt; margin-right:0px"><span><span><span><span><span><span><span>「文心一言用户规模突破</span></span></span><span><span><span>1</span></span></span><span><span><span>亿。」1</span></span></span><span><span><span>2 月 28 日，</span></span></span><span><span><span>百度首席技术官</span></span></span><span><span><span>、深度学习技术及应用国家工程</span></span></span><span><span><span>研究中心</span></span></span><span><span><span>主任</span></span></span><span><span><span>王海峰在第十届</span></span></span><span><span><span>WAVE SUMMIT 深度学习开发者</span></span></span><span><span><span>大</span></span></span><span><span><span>会</span></span></span><span><span><span>上宣布。会上，王海峰以《文心加飞桨，翩然赴星河》为题作了主旨演讲，分享了飞桨和文心的最新成果。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><img height="284" src="https://static.oschina.net/uploads/space/2023/1228/192946_d8IJ_4252687.png" width="500" referrerpolicy="no-referrer"></p><p style="margin-left:0.0001pt; margin-right:0px"><span><span><span><span><strong><span><span><span><strong>飞桨开发者已达 1</strong></span></span></span></strong><strong><span><span><span><strong>070</strong></span></span></span></strong><strong><span><span><span><strong>万</strong></span></span></span></strong></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span>WAVE SUMMIT 深度学习开发者大会</span></span></span><span><span><span>始于</span></span></span><span><span><span>2019 年 4 月，每年两次与开发者相聚，如今</span></span></span><span><span><span>已是</span></span></span><span><span><span>五载十届</span></span></span><span><span><span>。</span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span><span>回顾五年</span></span></span><span><span><span>，</span></span></span><span><span><span>大会一路</span></span></span><span><span><span>见证了</span></span></span><span><span><span>百度对人工智能技术和产业趋势的前瞻判断，指引了技术创新和产业实践的方向。2</span></span></span><span><span><span>019</span></span></span><span><span><span>年王海峰在</span></span></span><span><span><span>首届大会上</span></span></span><span><span><span>提出，深度学习框架是智能时代的操作系统。深度学习的通用性特点，以及深度学习框架及平台的发展，推动人工智能标准化、自动化和模块化，进入工业大生产阶段。2</span></span></span><span><span><span>020</span></span></span><span><span><span>年，王海峰提出了打造 AI 新型基础设施，云智一体加速产业智能化，将 AI 大生产平台升级为云智一体的新型基础设施，为产业智能化奠定坚实的基础。2</span></span></span><span><span><span>021</span></span></span><span><span><span>年，王海峰表示，人工智能呈现出「融合创新」和「降低门槛」的特点：一方面，AI 技术及产业的融合创新越来越多；另一方面，虽然 AI 技术越来越复杂，但 A</span></span></span><span><span><span>I</span></span></span><span><span><span>开发与应用的门槛却越来越低。2</span></span></span><span><span><span>022</span></span></span><span><span><span>年，王海峰进一步提出，深度学习平台加上大模型，贯通了从硬件适配、模型训练、推理部署，到场景应用的 AI 全产业链，夯实了产业智能化基座。今年，大语言模型的出现，为通用人工智能带来曙光。</span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span>五年来，在持续技术创新和赋能产业的发展历程中，飞桨自身也在不断升级，从深度学习框架，到平台生态，发展成为技术领先、功能丰富的产业级深度学习开源开放平台。飞桨集核心框架、基础模型库、开发套件、工具组件，以及助力开发者成长的星河社区于一体，具有动静统一的深度学习框架、端到端自适应大规模分布式训练、云边端全场景高性能推理等关键核心技术。</span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span><span>飞桨生态愈加繁荣，</span></span></span><span><span><span>2019 年，凝聚在飞桨平台的开发者规模 150 万，到今年 8 月的 Wave Summit，已经达到 800 万，服务的企业数量、基于飞桨创建的模型数量，也都高速增长</span></span></span><span><span><span>。王海峰现场公布了飞桨生态最新成果，截至 2</span></span></span><span><span><span>023</span></span></span><span><span><span>年 1</span></span></span><span><span><span>2</span></span></span><span><span><span>月底，飞桨已凝聚</span></span></span><span><span><span>1070</span></span></span><span><span><span>万开发者，服务 2</span></span></span><span><span><span>3.5</span></span></span><span><span><span>万家企事业单位，基于飞桨创建了</span></span></span><span><span><span>86</span></span></span><span><span><span>万个模型。</span></span></span></span></span></span></span></span></p><p style="margin-left:0.0001pt; margin-right:0px"><span><span><span><span><span><strong><span><span><span><strong>文心一言用户规模破亿，日提问量快速增长</strong></span></span></span></strong></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span><span>据了解，百度自</span></span></span><span><span><span>2019</span></span></span><span><span><span>年起深耕预训练模型研发，发布了文心大模型 1</span></span></span><span><span><span>.0</span></span></span><span><span><span>。经过近四年积累，百度于今年 3 月在全球科技大厂中率先发布了知识增强大语言模型文心一言。1</span></span></span><span><span><span>0</span></span></span><span><span><span>月，文心一言的基础模型升级到 4</span></span></span><span><span><span>.0</span></span></span><span><span><span>，理解、生成、逻辑和记忆四大人工智能基础能力全面提升。文心大模型</span></span></span><span><span><span>4.0</span></span></span><span><span><span>过去两个多月整体效果又提升了</span></span></span><span><span><span>32%</span></span></span><span><span><span>。</span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><img height="283" src="https://static.oschina.net/uploads/space/2023/1228/193026_HXnt_4252687.png" width="500" referrerpolicy="no-referrer"></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span><span>王海峰现场披露，文心一言用户规模已突破</span></span></span><span><span><span>1</span></span></span><span><span><span>亿，自 8 月 3</span></span></span><span><span><span>1</span></span></span><span><span><span>日</span></span></span><span><span><span>获准开放对公众提供服务</span></span></span><span><span><span>以来，文心一言的用户提问量一路上扬，基本与文心大模型的效果提升同步。越来越多的用户在信任和使用文心一言。</span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span><span>王海峰最后表示：「</span></span></span><span><span><span>五</span></span></span><span><span><span>载</span></span></span><span><span><span>十届，</span></span></span><span><span><span>我们与所有开发者一起，踔厉奋发，笃行不怠。愿继续与所有开发者携手并肩，在飞桨和文心的支持下，共赴通用人工智能的星辰大海！」</span></span></span></span></span></span></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 28 Dec 2023 11:30:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273184</guid>
            <link>https://www.oschina.net/news/273184</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[广电运通加入 openKylin，助力社区创新技术发展！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>近日，广电运通集团股份有限公司（以下简称」广电运通「）签署了 openKylin 社区 CLA（Contributor License Agreement 贡献者许可协议），正式加入 openKylin 开源社区。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-cab724551f7e2a4ba37a2d71058d9f83c08.png" referrerpolicy="no-referrer"></p><p><span>广电运通创立于 1999 年，隶属于广州无线电集团，是国有控股的高科技上市企业，主营业务覆盖智能金融、公共安全、智能交通、数字政府、大文旅、新零售及智慧教育等领域，为全球客户提供具有竞争力的智能终端、运营服务及大数据解决方案。</span></p><p><span>广电运通已连续 15 年位列智能金融设备市场第一，是国内最大的金融智能自助设备供应商和服务商，旗下信创软硬件产品已在各大金融机构广泛应用和验证，持续为金融信创繁荣发展输入源源不断的动能。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-e6b2fb1b3ecd1366d41e1da581a52ad6d78.png" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>加入 openKylin 社区后，广电运通将充分发挥自身在金融科技领域的技术和资源优势，联合上下游合作伙伴，建立互利共赢的良性循环，与社区在金融智能终端技术迁移等方面开展合作，丰富产业生态，助力创新技术发展。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 28 Dec 2023 09:12:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273155</guid>
            <link>https://www.oschina.net/news/273155</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[深耕开发者生态，openKylin 入选 2023 中国技术品牌影响力企业榜]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>2023 年 12 月 26 日，中国技术先锋年度评选&nbsp;| 2023 中国技术品牌影响力企业榜单正式发布。作为中国领先的新一代开发者社区，SegmentFault 思否依托数百万开发者用户数据分析，各科技企业在国内技术领域的行为及影响力指标，最终评选出 30 家上榜企业。<strong>openKylin 作为中国领先的开源操作系统根社区，凭借在技术领域和开发者生态领域的持续贡献，入选 30 强之列。</strong></span></p><p style="text-align:center"><img alt="" height="3508" src="https://oscimg.oschina.net/oscnet/up-f71b87d34c96069fbcb15077685dcb8cfdf.png" width="2481" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>相信开发者的力量，一直以来 openKylin 社区都坚持大力推动开发者生态建设。2023 年，openKylin 社区曾组织和参与顶级技术大会、开发者大赛、技术沙龙</span><span>等活动</span><span><strong>70+</strong>场，以领先技术回馈社区。截至目前，openKylin 已累计发布<strong>6</strong>个社区版本，下载量<strong>100 万+</strong>；汇聚<strong>400+</strong>社区会员<strong>、5500+</strong>开发者加入社区，并累计成立<strong>94</strong>个 SIG 组开展技术研究与创新，<span>和开发者共同成长。</span></span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>未来，openKylin 也将保持初心，加强生态建设，服务广大开发者，为营造良好开源生态和技术发展持续努力，也期待越来越多的开发者参与进来，为建设开源、贡献开源添砖加瓦！&nbsp;</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:left"><span style="color:#7f7f7f">附：《2023 中国技术品牌影响力企业》</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-5f8de216bffebbb3744628f4612997a427f.png" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0"><span>openKylin（开放麒麟）社区旨在以「共创」为核心，在开源、自愿、平等、协作的基础上，通过开源、开放的方式与企业构建合作伙伴生态体系，共同打造桌面操作系统顶级社区，推动 Linux 开源技术及其软硬件生态繁荣发展。</span></p><p style="margin-left:0; margin-right:0"><span style="background-color:#ffffff">社区理事成员单位包括麒麟软件、普华基础软件、中科方德、麒麟信安、凝思软件、一铭软件、中兴新支点、元心科技、中国电科 32 所、技德系统、北京麟卓、先进操作系统创新中心、飞腾、兆芯、龙芯中科、景美、京东科技、玄铁、申泰信息、海光等 21 家产业同仁和行业机构。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 28 Dec 2023 09:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273154</guid>
            <link>https://www.oschina.net/news/273154</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
