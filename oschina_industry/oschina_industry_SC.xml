<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 31 Jan 2024 06:07:31 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[Monibuca 2023 年终回顾]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h1>2023 年回顾</h1><ul><li>成立南京模拟不卡科技有限公司</li><li>monibuca.com 域名通过备案</li><li>开通 qq 频道</li><li>开通微信公众号</li><li>推出各产品的 docker 版</li><li>Jessibuca 亮相 LVS 大会</li><li>推出 API 调试工具（基于 ApiFox）</li></ul><h2>Monibuca</h2><ul><li>版本号从<code>v4.3.4</code> 到 <code>v4.7.4</code></li><li>修复大量 bug，稳定性大幅提升</li></ul><h3>重要功能</h3><ul><li>时光回溯 (v4.4.0)</li><li>内存回收 (v4.4.0)</li><li><code>WebRTC</code> 传输 <code>AAC</code>、<code>H265</code>(v4.4.13)</li><li>日志多语言 (v4.5.0)</li><li>配置功能优化，可以用环境变量覆盖配置 (v4.5.3)</li><li>鉴权机制 ( v4.5.5)</li><li>增加 <code>trace</code> 级别日志 (v4.5.5)</li><li>读取 <code>mp4</code> 流 (v4.5.5)</li><li>增加内部订阅者机制 (v4.5.5)</li><li>支持<code>EnhancedRTMP</code>规范 (v4.5.6)</li><li>新增<code>idletimeout</code>配置 (v4.5.7)</li><li>增加全局配置项<code>disableall</code>(v4.5.8)</li><li>配置支持<code>yaml tag name</code>(v4.5.9)</li><li>支持 <code>RTMPS</code> (v4.6.1)</li><li><code>http/ws-h264/h265</code> 裸流支持（v4.6.2)</li><li>引擎引入<code>sync.Cond</code>替代伪自旋锁（v4.6.2)</li><li>单元测试和基准测试（v4.6.5)</li><li>录制支持 mp4 格式（v4.6.8)</li><li>流新增一个状态（v4.7.0）</li><li>远端推拉增加正则表达式（v4.7.0）</li><li>增加 av1 和 opus 支持（v4.7.2)</li></ul><h3>新增插件</h3><ul><li><code>Monitor</code>监控插件 (v4.5.0)</li><li><code>PS</code>接受 PS 流插件 (v4.5.7)</li><li><code>LL-HL</code> llhls 格式插件 (v4.6.1)</li><li><code>CascadeServer/CascadeClient</code>级联插件 (付费)</li><li><code>Transcode</code>转码插件 (付费)</li><li><code>Snappro</code>快速截图插件 (付费)</li><li><code>CDN</code>转推 CDN 插件 (付费)</li><li><code>Crypto</code>加密插件 (付费)</li><li><code>Gb28181pro</code>高级 GB 插件 (付费)</li></ul><h2>Jessibuca</h2><h3>网络层</h3><ul><li>WebRTC 协议流（<code>h26x</code>）对接 <code>m7s</code>、<code>srs</code>、<code>zlm</code>、<code>阿里云</code> 等服务器</li><li>加密流（<code>M7S</code>）(<code>H26x</code>)</li><li><code>fmp4</code>协议</li><li><code>h26x</code>裸流格式</li><li><code>Hls</code>协议流（<code>h26x</code>）</li><li>TF 卡录像流（倍速、保持连接）</li><li>SEI 帧解析</li></ul><h3>解码层</h3><ul><li><code>H.26x</code><code>WASM</code>（<code>SIMD</code>) 多线程视频 (1080P 及以上)</li><li><code>Mpeg-4</code> 视频格式软解码</li><li><code>MP3</code>音频</li></ul><h3>编码层</h3><ul><li>语音通讯（<code>PCM</code>/<code>G711A</code>/<code>G711U</code>/<code>Opus</code>）</li><li>视频录制（<code>flv</code>、<code>mp4</code>）</li></ul><h3>UI 层</h3><ul><li>性能面板</li><li>底部 UI 按钮自定义</li><li>loading 效果</li><li>进度条</li><li>PTZ 云台</li><li>电子放大</li><li>水印（全屏、动态、幽灵）</li><li>AI（人脸识别、物品识别、遮挡物检测、黑/绿/花屏检测）</li><li>NxN 路多屏播放、拖拽排序、局部全屏</li></ul><h2>Rebebuca</h2><ul><li>1 月开发 v1 版本</li><li>11 月 v2 版本开源</li></ul></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 03:48:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/langhuihui/blog/10993892</guid>
            <link>https://my.oschina.net/langhuihui/blog/10993892</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源日报：RISC-V 万兆开源交换机发售；npm 存在大量武林外传视频]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>欢迎阅读 OSCHINA 编辑部出品的开源日报，每天更新一期。</p><p><strong># 2024.1.30&nbsp;</strong></p><h2><span><span><span style="color:#000000"><span><span><span style="color:#00b050">今日要点</span></span></span></span></span></span></h2><p style="text-align:justify"><strong>OpenSource Daily</strong></p><h3><u><a href="https://www.oschina.net/news/276876/rwkv-5-world-7b-opensource" target="_blank">RWKV-5-World 7B 模型开源 —— 最环保、最节能的 AI 模型</a></u></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">2024 年 1 月 28 日，RWKV 开源基金会<strong><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F4hTFA2CMcInhGLZTqa_H4g" target="_blank">宣布</a></u></strong>开源 RWKV-5-World 7B 模型。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">据介绍，「RWKV-5-World 7B」 是 RWKV 第五代架构 7B 参数大模型，也是 RWKV 迄今为止多语言性能最强的开源大模型。根据性能评测数据显示，在 100% attention-free 和只训练 1.1T tokens 的前提下，RWKV-5 7B 模型的多语言性能超过 Mistral，英文性能看齐 LlaMa2。</p><p><img src="https://oscimg.oschina.net/oscnet/up-3d734a934d678a080b7392fed400e7c9c3a.png" referrerpolicy="no-referrer"></p><h3><u><a href="https://www.oschina.net/news/276901" target="_blank">Milk-V Vega: 全球首款 RISC-V 万兆开源交换机正式发售</a></u></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">MilkV 于去年 8 月亮相了 「全球首款开源万兆 RISC-V 网络交换机 Milk-V Vega」。时隔数月，该产品现已上架官方淘宝店开售，定价 799 元。</p><p><span style="background-color:#ffffff; color:#333333">据介绍，Milk-V Vega 是 Milk-V 面向新一代网络架构开发的一款小体积、高密度盒式开源万兆网络交换机，为用户提供宽带、语音、视频、监控等多业务统一承载平台，多网融合覆盖全业务；搭载国产 RISC-V 高可靠网络交换芯片，帮助企业与用户大幅简化网络架构，降低能耗成本与运维成本。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Milk-V Vega 支持完备的二层网络协议处理功能，包括 L2 桥接，L2 组播和风暴抑制等；支持基于流、端口、协议、子网的 VLAN 功能；支持 STP，RSTP 和 QinQ 功能；支持防 DOS 攻击，黑白名单和协议包滤除功能；支持滤镜，链路聚合，OAM 报文发送和端口保护功能；支持入口及出口的 ACL 功能，还支持同步以太网，1588 功能。</p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><img src="https://oscimg.oschina.net/oscnet/up-705210c8717e8741ceda531371652a2176d.png" referrerpolicy="no-referrer"></p><h3><u><a href="https://www.oschina.net/news/276877/meta-code-llama-70b" target="_blank">Meta 发布全新代码生成大模型 Code Llama 70B</a></u></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Meta 今天发布了全新的代码生成大模型 Code Llama 70B。据称其 HumanEval 评分达到 67.8，达到 GPT-4 水准，是目前评分最高的开源大模型。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Code Llama 70B 是此前<span>&nbsp;</span><strong><u><a href="https://www.oschina.net/news/255350/meta-code-llama">Code Llama<span>&nbsp;</span></a></u></strong>的升级版本，包括三个模型：</p><ul style="list-style-type:disc; margin-left:0; margin-right:0"><li>CodeLlama-70B</li><li>CodeLlama-70B-Python</li><li>CodeLlama-70B-Instruct</li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><strong>开源地址：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcodellama%2FCodeLlama-70b-hf" target="_blank">https://huggingface.co/codellama/CodeLlama-70b-hf</a></u></em></strong></p><hr><h2><strong><span><span><span style="color:#000000"><span><span><span style="color:#00b050">用户观点</span></span></span></span></span></span></strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-f545e933088dc337f246103e4b82c375daa.png" referrerpolicy="no-referrer"></p><hr><h2><strong><span><span><span style="color:#000000"><span><span><span style="color:#00b050">媒体观点</span></span></span></span></span></span></strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-e098404dded22f6bad87d9c419141da28f7.png" referrerpolicy="no-referrer"></p><hr><h2><strong><span><span><span style="color:#000000"><span><span><span style="color:#00b050">每日项目榜</span></span></span></span></span></span></strong></h2><p>GitHub 榜单：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a06e7992894706ed3ac49cab5b01c1cc732.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-1be56111bfc96a12468e0e12d2848382697.png" referrerpolicy="no-referrer"></p><blockquote><h4>在线阅读完整日报内容，访问：<strong><u><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC005%E6%9C%9F%EF%BC%9Anpm%20%E5%AD%98%E5%9C%A8%E5%A4%A7%E9%87%8F%E6%AD%A6%E6%9E%97%E5%A4%96%E4%BC%A0%E8%A7%86%E9%A2%91.pdf" target="_blank">开源日报第 005 期：npm 存在大量武林外传视频</a></u></strong></h4></blockquote><hr><p><strong>往期回顾</strong></p><ul><li><u><a href="https://www.oschina.net/news/276864" target="news">开源日报第 004 期：百度输入法在候选词区域植入广告；大神用 Excel 构建 CPU</a></u></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 03:28:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277040</guid>
            <link>https://www.oschina.net/news/277040</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Python HTTP 库 urllib3 支持直接在浏览器运行]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Python HTTP 库 urllib3 最新版本 2.2.0 新增了一项重磅特性——<strong>可直接在浏览器运行</strong>。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-7e0846ecd450824c3e2a7cea0f00651c22c.png" referrerpolicy="no-referrer"></p></blockquote><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Furllib3.readthedocs.io%2Fen%2Fstable%2Freference%2Fcontrib%2Femscripten.html" target="_blank">根据介绍</a>，从 2.2.0 开始，urllib3 支持在 Pyodide 运行时中使用 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FFetch_API" target="_blank">JavaScript fetch API</a>，或者在 fetch API 不可用时（例如当跨源隔离未激活时）返回 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FXMLHttpRequest" target="_blank">XMLHttpRequest</a>。这意味着开发者可以使用该库直接从浏览器发出 HTTP 请求。</p><p>由于 urllib3 的 Emscripten 支持与 API 兼容，这意味着依赖于 urllib3 的库现在也可以在 Emscripten 和 Pyodide 环境中使用。</p><p>此外，urllib3 已初步支持 HTTP/2，与此同时其团队正在筹集更多资金（大约 4 万美元）来完成该功能。他们表示这笔资金能确保项目在 2023 年财务支持急剧下降后获得长期可持续维护。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-eae61d14f918da17c8b18cf92c1ed05432d.png" referrerpolicy="no-referrer"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsethmlarson.dev%2Furllib3-is-fundraising-for-http2-support" target="_blank">https://sethmlarson.dev/urllib3-is-fundraising-for-http2-support</a></em></u></p></blockquote><p><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Furllib3%2Furllib3%2Freleases%2Ftag%2F2.2.0" target="_blank">详情查看 Release Notes</a></u>。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 03:22:54 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277037/urllib3-works-in-the-browser</guid>
            <link>https://www.oschina.net/news/277037/urllib3-works-in-the-browser</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[分析了 1.5 亿行代码发现：AI 编程助手降低代码质量]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h2>摘要</h2><p>2023 年是 GitHub Copilot 大放异彩的一年。在短短不到两年的时间里，这款 AI 编程助手已从一个初步的原型迅速成为众多开发者和企业中不可或缺的重要工具 [1]。它的迅猛发展开启了编写代码的新纪元。</p><p>GitHub 已经发布了数份关于 AI 如何影响软件开发的增长和影响的研究。他们的一项重要发现是，开发者在使用 Copilot 时，编码速度提升了「55%」。面对大量由 LLM 生成的代码，我们不禁要问：这些代码在质量和可维护性上与人工编写的代码相比如何？它们是不是更像经验丰富的高级开发者的精心作品，还是更接近短期合同工的零散拼凑？</p><p>为此，GitClear 收集了从 2020 年 1 月到 2023 年 12 月之间的 153 百万行代码变更记录 [A1]。这是目前已知最大的用于分析代码质量差异的高度结构化代码变更数据集 [A2]。</p><p>我们发现了一些关于代码可维护性的令人担忧的趋势。代码变更率 —— 指在编写后不到两周就被修改或撤销的代码行所占的比例 —— 预计在 2024 年将是 2021 年 AI 出现之前的两倍。我们还发现，「新增代码」和「复制/粘贴代码」的比例相比于「更新的」、「删除的」和「移动的」代码在上升。从这个角度来看，AI 生成的代码更像是一位频繁更换工作的合同工写的临时代码，容易违反他访问的代码库的 DRY（Donot Repeat Yourself，不重复自己）原则。。</p><p>我们以一些针对管理者如何在这种逆流中保持代码高质量的建议作为本文的总结。</p><h2>GitHub: 「使用 AI 编程，提升效率 55%，增加代码量 46%，为 GDP 贡献 1.5 万亿美元」</h2><p>这样惊人的数据背后，GitHub 的 CEO Thomas Dohmke 不仅忙于日常的 CEO 工作，还专门抽时间撰写了关于 AI 革命的博客文章和研究论文。他在 2023 年发布于 GitHub 的作品，详细敍述了 Copilot 快速普及的激动人心的故事。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0d299b229f2006a677274efc1e61252dc26.png" referrerpolicy="no-referrer"></p><p><em>来自 Dohmke 2023 年的博客文章《AI 驱动开发者生命周期的经济影响及从 GitHub Copilot 学到的经验》</em></p><p>Dohmke 在博客中提出，目前已有超过 20,000 家组织在使用 GitHub Copilot for Business。这紧随其在 2023 年 2 月宣布的消息，即在 Copilot for Business 推出时，「已有超过一百万人」在使用个人版 Copilot。GitHub 在提高 AI 质量和公开透明地分享其成果方面取得了令人赞赏的进展。</p><p>到底有多少开发者正在使用 AI 编写代码？在 GitHub 2023 年 6 月与 Wakefield Research 合作的一项独立研究中，他们指出：「在美国大型公司工作的开发者中，有 92% 使用 AI 编程工具。」他们还强调，有 70% 的开发者认为使用 AI 带来了显著好处。不过，O’Reilly Publishing 在 2023 年 8 月的一项调查显示，67% 的受访开发者表示他们还没有使用 ChatGPT 或 Copilot。这暗示了 GitHub 在市场上仍有很大的增长潜力。</p><h2>AI 生成代码存在的问题</h2><p>开发者之所以采用 Copilot，是因为他们相信这款工具能够加快编码速度。GitHub 的研究发现，使用 Copilot 的开发者的满意度提高了 75%。这表明开发者普遍接受了这款产品。但是，这并不意味着那些负责维护代码的人也会感到同样满意。资深代码研究员 Adam Tornhill（著有《Your Code as a Crime Scene》）对此表示怀疑：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-bd20f9adfaf74d1818568d65d3765166e5d.png" referrerpolicy="no-referrer"></p><p><em>开发者研究人员对 AI 辅助编程的影响持担忧态度</em></p><p>GitHub 声称，使用 Copilot 编写代码的速度提高了 55%。但问题是，有些本不应编写的代码怎么办？正如《Clean Code: A Handbook of Agile Software Craftsmanship》的作者 Robert Martin 所说，代码的阅读时间是编写时间的十倍。更快地写出低质量代码，意味着后续阅读代码的人将面临更多困难。</p><p>这只是使用 AI 助手的开发者所面临的众多挑战之一。其他挑战包括：</p><ol><li>频繁收到增加代码的建议，却很少有关于更新、移动或删除代码的建议。这是源于文本环境中代码编写时的界面限制。</li><li>评估代码建议可能耗费大量时间，尤其是在有多个自动建议系统相互竞争的环境中，如流行的 JetBrains IDEs（参见 [11]）。</li><li>代码建议的优化并非基于和代码维护者相同的激励机制。代码建议算法旨在提出最可能被接受的建议，而代码维护者则努力减少需要阅读的代码量（即，理解如何调整现有系统）。</li></ol><p>这些问题可能解释了为什么初级开发者比经验丰富的开发者更倾向于接受代码建议。根据 GitHub 的研究：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-261e68ce302259933f0288c5bd57dc43fc8.png" referrerpolicy="no-referrer"></p><p><em>GitHub 的数据显示，相比经验丰富的开发者，初级开发者使用 Copilot 的频率高出大约 20%</em></p><p>经验丰富的开发者更深刻地理解维护代码的长期成本。如果他们更不愿使用 AI 的代码建议，那么这是否意味着初级开发者现在正以前所未有的速度贡献更多额外的代码呢？</p><h2>代码变更的分类</h2><p>为了探究代码质量如何随着时间变化，我们研究了在 AI 应用日益广泛的 2023 年与之前几年，代码变更类型的不同。GitClear 将代码的变更动作分为七大类别，本研究分析了其中的六种：</p><ol><li>新增代码。指新增加的独立代码行，不包含对现有代码的小幅修改（这类修改被标记为「更新」）。此外，新增代码也不包括那些被添加、删除后又重新加入的代码行（这些行被标记为「更新」和「变动」）。</li><li>删除代码。指被移除、提交并且在随后的至少两周内没有被重新加入的代码行。</li><li>移动代码。指被剪切并粘贴到新文件或同一文件中新的函数位置的代码行。按照定义，「移动」的操作中，提交时代码内容不变，除了代码前的空格部分可能会有所改变。</li><li>更新代码。基于已存在的代码行，通过修改大约三个词或更少的词汇来提交的新代码行。</li><li>查找/替换代码。这种变更模式中，同一字符串在三个或更多位置被替换为统一的新内容。</li><li>复制/粘贴代码。除了编程语言的关键字（例如，<code>end</code>，<code>}</code>，<code>[</code>）外，相同的代码内容被提交到一个提交中的多个文件或函数。</li><li>无效操作代码。微小的代码更改，如空格或同一代码块内的行号变更。这类无效操作的代码变更没有包含在本研究中。</li></ol><p>GitClear 对代码操作的具体实例可以在其「Diff Delta」文档中找到。自 2020 年起，GitClear 开始按这些操作分类 git 仓库。截至 2024 年 1 月，GitClear 已分析并分类了大约十亿行代码，这些代码来自于四年间的各种来源，包括商业客户（例如，NextGen Health, Verizon）和知名的开源项目（例如，Facebook 的 React 项目，谷歌的 Chrome 浏览器）。在这些代码中，有 1.53 亿行是有意义的非无效操作的代码变更，被用于本次研究。</p><p>随着代码变更操作的不断演进，我们也在研究所谓的「变动代码」（Churned code）的变化趋势。它并不算是一种标准的代码操作，因为一行变动代码可能涉及到多种不同的操作，如代码的「新增」（Added）、「删除」（Deleted）或「更新」（Updated）。一行代码要被视作「变动的」，它必须被创建并推送到 git 仓库中，之后在随后的两周内被回退或进行重大修改。可以将 Churn 理解为最初由作者编写、提交并推送到公司 git 仓库时，那些不完整或错误的更改。</p><h2>Copilot 对代码编辑操作趋势的影响</h2><p>为了深入了解 Copilot 如何影响代码质量，我们对 GitClear 观察到的各种代码行操作进行了分析，这些操作按照代码编写的年份来分类（依据 git 提交记录中的 authored_at 日期 [12]）。相关的详细数据可以在附录中找到。下面是各年份的操作百分比：</p><table><tbody><tr><th>&nbsp;</th><th>新增</th><th>删除</th><th>更新</th><th>移动</th><th>复制/粘贴</th><th>查找/替换</th><th>代码波动</th></tr></tbody><tbody><tr><td>2020</td><td>39.2%</td><td>19.5%</td><td>5.2%</td><td>25.0%</td><td>8.3%</td><td>2.9%</td><td>3.3%</td></tr><tr><td>2021</td><td>39.5%</td><td>19.0%</td><td>5.0%</td><td>24.8%</td><td>8.4%</td><td>3.4%</td><td>3.6%</td></tr><tr><td>2022</td><td>41.0%</td><td>20.2%</td><td>5.2%</td><td>20.5%</td><td>9.4%</td><td>3.7%</td><td>4.0%</td></tr><tr><td>2023</td><td>42.3%</td><td>21.1%</td><td>5.5%</td><td>16.9%</td><td>10.5%</td><td>3.6%</td><td>5.5%</td></tr><tr><td>2024</td><td>43.6%</td><td>22.1%</td><td>5.8%</td><td>13.4%</td><td>11.6%</td><td>3.6%</td><td>7.1%</td></tr></tbody></table><p>这些数据在图表中的展现方式是这样的：左轴显示了代码变更操作的比例（这些百分比总和为 100%）。右轴和浅蓝色线条则展示了与之相应的「代码波动」变化。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-5056ad3d85208cd11c97d7cbc2e08f934a3.png" referrerpolicy="no-referrer"></p><p>对于 2024 年的预测，我们利用 OpenAI 的 gpt-4-1106-preview 模型，通过对现有数据进行二次回归分析得出。附录中详细介绍了我们如何使用 OpenAI 模型进行这一分析。鉴于 GitHub 报告的 Copilot 的迅猛发展以及 AI 智能体的普遍应用，可以预见 2024 年的趋势将延续 2022 年开始显现的模式，并在 2023 年得到加速。单从 2022 年到 2023 年各种操作频率的变化来看，我们可以发现三个可能影响代码质量的警示信号：</p><table><tbody><tr><th>操作</th><th>同比变化</th></tr></tbody><tbody><tr><td>添加</td><td>+3.1%</td></tr><tr><td>删除</td><td>+4.8%</td></tr><tr><td>更新</td><td>+5.2%</td></tr><tr><td>移动</td><td>-17.3%</td></tr><tr><td>复制/粘贴</td><td>+11.3%</td></tr><tr><td>查找/替换</td><td>-1.3%</td></tr><tr><td>代码变动率 (Churn)</td><td>+39.2%</td></tr></tbody></table><h2>解读代码操作变化的含义</h2><p>2023 年最显著的代码操作变化发生在「代码变动率 (Churn)」、「移动」和「复制/粘贴」这几个方面。我们在这一节将详细探讨这些变化背后的意义。</p><h3>代码变动率的显著增长</h3><p>所谓的「代码变动率 (Churn)」是指代码被推送到仓库后，接着在两周内被撤销、移除或更新的比例。在开发者亲自编写所有代码的情况下，这种情况相对较少见——2023 年之前，只有 3-4% 的代码会发生这样的变动。不过，在 2022 年就已经出现了这种趋势的上升，当时代码变动率跃升至 9%。值得注意的是，2022 年是人工智能编程助手 Copilot 首次以测试版形式推出，同时也是 ChatGPT 开始被广泛使用的一年。</p><p>在 2022 至 2023 年期间，AI 辅助工具的兴起与推送到代码仓库的「错误代码」密切相关。根据引用资料 [1] 和 [8]，如果我们假设 Copilot 在 2021 年的普及率为 0%，2022 年为 5-10%，2023 年为 30%，那么这些因素之间的相关性极高，皮尔森相关系数高达 0.98（更多关于这一计算的细节，请参见附录中的「代码变动率与 Copilot 的相关性」部分）。这意味着，随着 AI 辅助工具的使用增加，代码变动率也在相应增长。</p><p>随着代码变动率的普遍增加，错误代码被部署到生产环境的风险也随之增大。如果这一趋势持续到 2024 年，那么超过 7% 的代码更改可能会在两周内被撤销，这将是 2021 年的两倍。根据这些数据，我们预计 Google DORA 的「变更失败率」将在今年晚些时候发布的「2024 年 DevOps 状态报告」中有所增加，前提是该研究包含了 2023 年使用 AI 辅助的开发者数据。</p><h3>代码移动越少意味着重构和复用的减少</h3><p>通常在重构现有代码系统时，我们会发现代码的移动。重构的系统，尤其是代码的移动，是实现代码复用的关键。随着产品的不断扩展，开发者往往需要将现有代码重组到新的模块和文件中，以便于新功能的复用。对于经验丰富的开发者来说，代码复用的优势非常明显——与新编写的代码相比，已经在实际环境中被测试并证实稳定的代码显得更加可靠。而且，经过多人修改的代码往往包含了丰富的文档，这大大加快了新手开发者对模块的理解速度。</p><p>结合「复制/粘贴」代码的增加，可以清楚地看到，当前的 AI 助手似乎在一定程度上阻碍了代码的复用。相较于进行重构和遵循 DRY（「不要重复自己」）原则，这些助手更倾向于提供一种快捷方式，让开发者重复使用现有代码。</p><h3>复制/粘贴的代码会导致未来的维护困难</h3><p>复制/粘贴的代码可能是长期维护代码中最大的难题之一。当开发者重复使用非关键字的代码行时，实际上是在暗示他们没有时间去深入研究之前的实现方式。这种重复添加代码的做法，将整合实现重复功能的任务留给了未来的维护者。</p><p>大多数开发者更喜欢「实现新功能」而不是「审视潜在可复用的代码」，因此复制/粘贴的代码往往会过期仍然被使用。尤其在经验不足的团队中，可能缺乏有权威的代码维护者来移除这些重复的代码。即便是资深开发者，也需要付出巨大的努力和决心去充分理解代码，以便将其删除。</p><p>如果没有 CTO 或工程副总裁定期安排时间来减少技术债务，那么由于高层的时间压力，新添加的复制/粘贴代码可能永远不会被整合到支撑长期开发速度的组件库中。</p><p>根据 GitClear 的操作，只有在单次提交中重复的代码才会被计算。因此，2023 年测量到的 11% 的复制/粘贴比例，可能只是 2024 年代码库中悄然增加的总复制量的一小部分。</p><h2>修订代码年龄趋势分析</h2><p>我们使用了一个独立的方法来评估 2023 年相比之前代码质量的变化：分析 GitClear 提供的「代码溯源」数据。所谓「代码溯源」，其实就是要看代码从被写出到最终被更新或删除的时间长度。</p><table><tbody><tr><th>年份</th><th>少于 2 周</th><th>少于一个月</th><th>少于一年</th><th>1-2 年</th></tr></tbody><tbody><tr><td>2020</td><td>65.9%</td><td>8.7%</td><td>21.8%</td><td>3.6%</td></tr><tr><td>2021</td><td>66.7%</td><td>9.0%</td><td>20.5%</td><td>3.8%</td></tr><tr><td>2022</td><td>64.7%</td><td>9.9%</td><td>21.1%</td><td>4.4%</td></tr><tr><td>2023</td><td>71.3%</td><td>9.3%</td><td>16.4%</td><td>3.0%</td></tr><tr><td>2024</td><td>74.4%</td><td>9.1%</td><td>14.1%</td><td>2.4%</td></tr></tbody></table><p>根据这些数据，我们可以看到：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-1d1e4582aa2a1e3988d79b7610eac2f0736.png" referrerpolicy="no-referrer"></p><h2>解读代码年龄的趋势</h2><p>对「代码溯源」的数据分析揭示了一个有趣的现象，即在 2022 年到 2023 年间，代码的更新速度加快了。特别是，不到两周就被替换的代码比例增加了 10%。同时，超过一个月的代码更新频率在 2023 年比 2022 年下降了 24%（2023 年为 19.4%，而 2022 年为 25.5%）。</p><p>这一趋势显示，在 AI 助手普及之前，开发者更可能会选择最近编写的代码进行改进和再利用。根据 Techreport 的一项调查[5]，早在 2020 年代初，大约 70% 的项目采用了敏捷开发方法。在敏捷方法中，每个 Sprint（通常持续 2-3 周）都会规划和执行新的功能。这与我们的数据相符，表明大约在 2020 年左右，团队在每个 Sprint 结束后，可能会聚在一起讨论最近的工作成果，以及如何在接下来的 Sprint 中再次利用这些成果。</p><h2>后续研究的思考题</h2><p>我们能否设定激励措施来应对 2024 年代码建议引擎中普遍的「添加后即忘记」的问题？</p><p>尽管我们可以训练 AI 识别代码整合的机会，关键在于它何时被触发？我们需要一个新的用户界面来复查代码的删除和更新，以及潜在的新增内容。同时，那些导致团队今天无法腾出时间来减轻技术债务的管理层压力，可能也会妨碍他们采用一种假设性的「代码清理」工具。但如果有代码助手的开发者对探索如何整合代码感兴趣，GitClear 愿意与他们合作，详细联系方式见附录。</p><p>另一个值得关注的问题是：额外代码对开发进度的影响速度有多大？特别是对于复制/粘贴的代码，库中代码行数与开发者修改这些代码的速度之间几乎肯定存在负相关关系。现在的疑问是：「累积的复制/粘贴技术债务何时变得不可忽视？」了解这种减速效应发生的速率可以帮助未来的工具指导管理者何时应该减少开发新功能的时间。</p><p>最后一个探索的问题是：与 2020-2022 年相比，现在发生的复制/粘贴代码的总比例是多少？由于 GitClear 目前仅测量单个提交中的复制/粘贴代码，因此整体的复制/粘贴量（文件中重复的所有非关键字、非注释代码行）可能是 GitClear 当前测量值的两倍。2024 年，复制/粘贴代码真的会占到所有代码操作的 20-25% 吗？</p><p>GitClear 将在未来的研究中探讨这些问题，并鼓励该领域的其他研究人员共享他们的数据。如果您有兴趣与 GitClear 合作进行进一步研究，请查阅附录中的联系信息。</p><h2>结论：开发者为何保持谨慎？</h2><p>根据我们分析的两项关键数据，2023 年代码质量面临明显下滑，这与大语言模型 (LLMs) 特别是 AI 代码助手的广泛应用密切相关。</p><p>GitHub 与 Wakefield 研究所在 2023 年的一项调查显示，开发者已经意识到代码质量的降低。当被问及「在没有 AI 的情况下，应以哪些指标评估你的工作？」时，他们最关注的是「协作和沟通」，其次是「代码质量」。</p><p>但当问题转向「在使用 AI 时，应以哪些指标评估你的工作？」时，他们的关注点发生变化，「代码质量」成为了最关注的问题，而「生产事件数量」上升为第三大关注点：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-c3c950cc95effe8fe6f9b6d78a0706b2ea6.png" referrerpolicy="no-referrer"></p><p><em>摘自 GitHub 关于 AI 影响的调查</em></p><p>尽管开发者个人可能没有足够的数据来解释为什么使用 AI 时「代码质量」和「生产事件」成为更加紧迫的问题，我们的数据提供了一个可能的解释：当开发者被大量快速且简单的短期有效建议所淹没时，他们往往会不断增加代码行数，却忽视了检查现有系统是否可以优化重用。</p><p>对于那些通过 Tab 键得到复制/粘贴建议的经验不足的开发者来说，解决这一问题并非易事。工程领导们需要密切关注新数据，并思考这些数据对未来产品维护的潜在影响。开发者分析工具，如 GitClear，能够帮助识别问题代码的累积速度。需要考虑的关键问题有：</p><ol><li>代码复用比例是否在下降？</li><li>代码的移动和复制/粘贴量是否有所变化？</li><li>开发者发现代码复用机会的难易程度如何？</li></ol><p>关于 GitClear 如何应对这些问题的进一步讨论见 [A3]。</p><p>AI 助手和 Copilot 如何重塑开发者的角色？无疑，随着 AI 的普及，我们进入了一个代码行数增加速度空前的时代。2024 年更值得关注的问题是：谁将负责整理这一切留下的烂摊子？</p><h2>引用</h2><ol><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-06-27-the-economic-impact-of-the-ai-powered-developer-lifecycle-and-lessons-from-github-copilot%2F" target="_blank">探索 AI 驱动的开发者生命周期带来的经济效益：GitHub Copilot 的案例分析</a>[GitHub]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-02-14-github-copilot-for-business-is-now-available%2F" target="_blank">GitHub Copilot for Business：企业级智能编程助手</a>[GitHub]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2306.15033" target="_blank">软件开发领域的重大变革：AI 驱动下的开发者生命周期经济与生产力分析</a>[GitHub]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F1_zHZSSU5GsuORb776sRVNkuKoHXS7e2smJWWkn1Kxds%2Fedit%23slide%3Did.g9a6371a948_0_732" target="_blank">深入了解 Diff Delta 和 Commit 组</a>[GitClear]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechreport.com%2Fstatistics%2Fhow-many-companies-use-agile%2F" target="_blank">Techreport 调查显示：超过七成团队采用敏捷开发模式</a>[Techreport]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gitclear.com%2Fhelp%2Ftechnical%2Fcode_provenance" target="_blank">代码溯源：它是什么，为何重要？</a>[GitClear]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-06-13-survey-reveals-ais-impact-on-the-developer-experience%2F" target="_blank">调查显示 AI 如何改变开发者的工作体验</a>[GitHub]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oreilly.com%2Fradar%2Fthe-next-generation-of-developer-productivity%2F" target="_blank">领略下一代开发者生产力的风采</a>[O’reilly]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpragprog.com%2Ftitles%2Fatcrime2%2Fyour-code-as-a-crime-scene-second-edition%2F" target="_blank">当你的代码变成犯罪现场</a>[Pragmatic Programmers]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.goodreads.com%2Fbook%2Fshow%2F3735293-clean-code" target="_blank">敏捷软件工艺中的代码洁净之道：实用手册及其引用</a>[Robert C. Martin, 作者]</li><li>[JetBrains AI：提升你的编程工具，迎接新的自由 (<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.jetbrains.com%2Fai%2F" target="_blank">https://www.jetbrains.com/ai/</a>)[JetBrains]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgit-scm.com%2Fdocs%2Fgit-commit" target="_blank">Git 提交指南：深入了解 git-commit</a>[Git 文档]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gitclear.com%2Fhelp%2Ftech_debt_inspector_list_directories_by_developer_velocity" target="_blank">如何使用 Tech Debt 浏览器优化代码</a>[GitClear]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDon%2527t_repeat_yourself" target="_blank">「不要重复自己」原则的智慧</a>[Wikipedia]</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2FAdamTornhill%2Fstatus%2F1729592297887502611" target="_blank">X：Adam Tornhill 分享的思考</a>[X/Twitter]</li></ol><blockquote><p>本文已获授权转载。</p><p>原文：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitclear-public.s3.us-west-2.amazonaws.com%2FCoding-on-Copilot-2024-Developer-Research.pdf" target="_blank">Coding on Copilot</a><br> 作者：William Harding,Matthew Kloster<br> 译文：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaoyu.io%2Ftranslations%2Fllm%2Fcoding-on-copilot-2024-developer-research" target="_blank">在 Copilot 的协助下编程白皮书——2023 年的数据显示了代码质量面临的挑战</a><br> 译者：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaoyu.io%2F" target="_blank">宝玉</a></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 02:43:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277025/coding-on-copilot-2024-developer-research</guid>
            <link>https://www.oschina.net/news/277025/coding-on-copilot-2024-developer-research</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[GLM-4 模型开放 API 正式上线]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">2024 年 01 月 16 日，智谱 AI 正式推出了新一代基座大模型 GLM-4，并宣布了一系列产品升级。时至今日，GLM-4 模型的开放 API 正式上线。所有开发者均可通过智谱 AI 的开放平台接入 GLM-4 模型开放 API。</span></p><p><strong><span style="color:#000000">网址：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopen.bigmodel.cn%2Fdev%2Fapi%23glm-4" target="_blank">https://open.bigmodel.cn/dev/api#glm-4</a></strong></p><p><span style="color:#000000">根据介绍，新一代基座大模型 GLM-4，支持更长上下文；更强的多模态；支持更快推理速度，更多并发，大大降低推理成本；同时 GLM-4 增强了智能体能力。</span></p><ul><li><p><span style="color:#000000"><strong>基础能力（英文）：</strong>GLM-4 在 MMLU、GSM8K、MATH、BBH、HellaSwag、HumanEval 等数据集上，分别达到 GPT-4 94%、95%、91%、99%、90%、100% 的水平。</span></p></li><li><p><span style="color:#000000"><strong>指令跟随能力：</strong>GLM-4 在 IFEval 的 prompt 级别上中、英分别达到 GPT-4 的 88%、85% 的水平，在 Instruction 级别上中、英分别达到 GPT-4 的 90%、89% 的水平。</span></p></li><li><p><span style="color:#000000"><strong>对齐能力：</strong>GLM-4 在中文对齐能力上整体超过 GPT-4。</span></p></li><li><p><span style="color:#000000"><strong>长文本能力：</strong>在 LongBench（128K）测试集上对多个模型进行评测，GLM-4 性能超过 Claude 2.1；在「大海捞针」（128K）实验中，GLM-4 的测试结果为 128K 以内全绿，做到 100% 精准召回。</span></p></li><li><p><span style="color:#000000"><strong>多模态-文生图：</strong>CogView3 在文生图多个评测指标上，相比 DALLE3 约在 91.4% ~99.3% 的水平之间。</span></p></li></ul><p><img height="380" src="https://oscimg.oschina.net/oscnet/up-4fe971d080041b4d60fc9d6078b26f54d9f.png" width="500" referrerpolicy="no-referrer">&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 02:42:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277024</guid>
            <link>https://www.oschina.net/news/277024</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Opera 将推出全球首款非 WebKit 内核的 iOS 浏览器]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>昆仑万维旗下海外信息分发与元宇宙平台 Opera 近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FsgKWxkIL0pIwCN7x7NXXYw" target="_blank">宣布</a>，将为欧洲的 iPhone 和 iPad 用户带来一款基于自研引擎的全新 AI 浏览器「Opera One」。</p><p><img alt="" height="281" src="https://oscimg.oschina.net/oscnet/up-e479abd2879e446aeb1c15c3023eb129f64.jpg" width="500" referrerpolicy="no-referrer"></p><p>Opera 计划于 3 月份首次亮相这款新浏览器，届时 DMA（欧洲数字市场法案）将生效。由于这些变化仅在欧盟实施，Opera 强烈鼓励苹果将这些增强功能推广至全球 iOS 用户。</p><p>为了遵守 DMA，苹果决定允许开发者提交非基于 WebKit 的浏览器，包括 Web 浏览器应用和在 iOS 应用中显示网页的应用内浏览器。Opera 表示，这些变化将使他们有机会为 iPhone 用户提供 Safari 之外的人工智能浏览器。</p><p>目前，Opera 是全球最大的第三方独立浏览器。此前，Opera 于 2023 年 6 月 20 日推出旗舰浏览器「Opera One」，全面引入 Aria 人工智能助手，用户只需注册 Opera 账户即可免费使用。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 02:37:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277020</guid>
            <link>https://www.oschina.net/news/277020</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[GNOME Project Handbook：面向 GNOME 贡献者的官方指南]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>GNOME 团队上线了新网站：<strong><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhandbook.gnome.org%2Findex.html" target="_blank">GNOME Project Handbook</a></u></em></strong>。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-f14ac879fd6ea4b29cc11ea770593bbf2a3.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblogs.gnome.org%2Faday%2F2024%2F01%2F30%2Fannouncing-the-gnome-project-handbook%2F" target="_blank">https://blogs.gnome.org/aday/2024/01/30/announcing-the-gnome-project-handbook/</a></u></em></p></blockquote><p>官方称这份全新、完全重写的文档专门为 GNOME 贡献者创建，旨在帮助新手贡献者参与 GNOME 桌面环境的开发。因此《GNOME Project Handbook》不会为使用 GNOME 的用户提供技术文档，也不包含用户文档，以及不为应用程序和库提供面向公众的主页。它所包含的是作为 GNOME 贡献者进行操作所需的信息。</p><blockquote><p>该站点包含参与 GNOME 项目所需的所有信息。它适用于新的和现有的 GNOME 贡献者。</p><p>如果您是 GNOME 新手，该手册是了解 GNOME 如何工作的好地方，包括每个团队的工作、如何访问 GNOME 开发平台以及如何与其他项目成员沟通。</p><p>如果您已经在为 GNOME 做出贡献，那么您可以在本手册中找到重要的参考信息，例如开发计划、活动详细信息以及如何申请旅行赞助。</p></blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-ab1d2877fb7465713e1026dde056406759a.png" referrerpolicy="no-referrer"></p><p>团队称《GNOME Project Handbook》目前虽然处于早期阶段，但已包含大部分核心内容。后续该文档会替代 wiki，因此他们建议贡献者从现在开始查阅手册而不是 wiki。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 02:22:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277015/gnome-project-handbook</guid>
            <link>https://www.oschina.net/news/277015/gnome-project-handbook</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[deepin Meetup 回顾丨 deepin V23 Beta3 新特性＆如何参与社区贡献]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span>​</span>内容来源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fdeepin-meetup-2024-beijing%2F" target="_blank">deepin（深度）社区</a></p><hr><p>1 月 27 日，deepin Meetup 北京站圆满落幕！作为 2024 年首场线下沙龙，deepin Meetup 吸引了来自北京及周边地区 40 余名用户的积极参与，各位 deepiner 线下面基，昔日网友揭开神秘面纱，共同探讨最新产品进展、分享使用心得、实操经验，大家也对 deepin 未来发展提出了诸多宝贵意见，现场氛围热烈。</p><p>未能亲临现场的小伙伴们，不妨通过镜头感受一下现场的火热氛围吧！</p><p style="text-align:center"><img alt="" height="577" src="https://storage.deepin.org/thread/202401300902294511_807A0015.jpg" width="866" referrerpolicy="no-referrer"></p><div><div><h1><strong>主题分享</strong></h1><p>首先，deepin（深度）社区运营负责人张可带大家回顾了 deepin 社区的发展历史，展示了社区部分明星产品及技术成果。过去一年是 deepin（深度）社区高速发展的一年，而这些成绩的背后，离不开社区所有用户的支持，张可也为大家详细讲解了社区用户应如何为开源项目和社区做贡献，让各位 deepiner 对如何参与 deepin 社区贡献有了更深入的了解。</p><p style="text-align:center"><img alt="" height="645" src="https://storage.deepin.org/thread/202401300904361053_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240130135117.jpg" width="861" referrerpolicy="no-referrer"></p></div></div><div><div><p>随后，deepin（深度）社区产品经理梅毅翔带来了《deepin V23 Beta3 - X11 环境下新增功能》主题分享，为大家提前揭秘了 deepin V23 Beta3 的部分新增功能，如启动器重构、UOS AI 预装、新增低电阈值设置功能、支持了 steam 原生应用的安装等。</p><p>值得一提的是，备受期待的<strong>deepin V23 Beta3 镜像将在本周正式发布</strong>，欢迎大家体验并提供反馈。</p></div></div><p style="text-align:center"><img alt="" height="641" src="https://storage.deepin.org/thread/202401300920179867_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240130134935.jpg" width="856" referrerpolicy="no-referrer"></p><div><div><h1><strong>交流互动</strong></h1><p>在活动现场，多位用户分享了自己与 deepin 及开源的故事，以及一些实操经验。</p><p>过去一年，AI 应用场景也不断扩展，用户&nbsp;<strong>@王鑫</strong>&nbsp;分享了他作为人像摄影师转型为 AIGC 从业者的经历，并为大家介绍了了两款主流 AI 绘画工具 Midjourney、Stable Diffusion 的优、劣势，以及它们在摄影中的实际应用。</p><p style="text-align:center"><img alt="" height="620" src="https://storage.deepin.org/thread/202401300941388408_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240130134847.jpg" width="827" referrerpolicy="no-referrer"></p><p>用户<strong>&nbsp;@GNUman&nbsp;</strong>则为大家分享了他在自由软件与开源方面的所思所想，并与大家探讨应如何支持自由软件运动。</p><p style="text-align:center"><img alt="" height="629" src="https://storage.deepin.org/thread/202401300942187008_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240130134851.jpg" width="838" referrerpolicy="no-referrer"></p><p>参与开源是用户发挥自己技能和为社区做贡献的良好途径，而如何参与一直备受大家关注，开源爱好者&nbsp;<strong>@凯宝</strong>&nbsp;借此机会，结合自己从小白用户进阶成为「开源大使」的经历，为大家分享了他一路以来的认知与收获。</p><p style="text-align:center"><img alt="" height="628" src="https://storage.deepin.org/thread/202401300943095528_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240130134832.jpg" width="837" referrerpolicy="no-referrer"></p><p>资深社区用户&nbsp;<strong>@拉德利&nbsp;</strong>及&nbsp;<strong>@安树收园</strong>&nbsp;也与大家分享了自己和 deepin 的故事，讲述了他们在使用 deepin 过程中所遇到的趣事、挑战和成长。</p><p style="text-align:center"><img alt="" height="464" src="https://storage.deepin.org/thread/202401300945586774_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240130154416.jpg" width="825" referrerpolicy="no-referrer"></p></div></div><p style="text-align:center"><img alt="" height="625" src="https://storage.deepin.org/thread/202401300945253653_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240130134827.jpg" width="834" referrerpolicy="no-referrer"></p><div><div><h1><strong>下一站，成都</strong></h1><p>本次 deepin Meetup · 北京站活动圆满结束，感谢大家在北京的严寒中不辞远路赶来参加活动，有的同学甚至是从外地专程赶来，也感谢所有参与分享和讨论的同学们。</p><p>那么，让我们准备下一次的沙龙吧！我们计划在春节后于<strong>成都</strong>举办，大家觉得呢？</p><p>如果你对做分享感兴趣，或是有更多的建议给到我们，可以扫描下方二维码，申报议题或提交建议。无论是标准的主题演讲 ，还是 5 分钟简短的闪电演讲，都期待你的报名～ 期待与大家下次相见！</p><p style="text-align:center"><img alt="" height="261" src="https://storage.deepin.org/thread/202401300947008284_1027%E5%8C%97%E4%BA%AC%E6%8A%A5%E5%90%8D.jpg" width="261" referrerpolicy="no-referrer"></p></div></div><p>了解 deepin：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Findex%2Fzh" target="_blank">deepin - 基于 Linux 的开源国产操作系统</a></p><p>了解 deepin Meetup：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fdeepin-meetup%2F" target="_blank">deepin Meetup – 深度科技社区</a></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 01:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277008</guid>
            <link>https://www.oschina.net/news/277008</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[博汇科技加入 openKylin，共推信息技术产业发展]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:left"><span>近日，北京市博汇科技股份有限公司（以下简称「博汇科技」）签署了 openKylin 社区 CLA(Contributor License Agreement 贡献者许可协议)，正式加入 openKylin 开源社区。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="1079" src="https://oscimg.oschina.net/oscnet/up-db27b16108d7c3f452d284318dab5b6bd06.png" width="829" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>博汇科技成立于 1993 年，是一家专注于视听大数据领域的科创板上市企业，历经多年探索与发展，用铿锵的步履见证了我国视听信息技术产业的成长。博汇科技业务涵盖传媒安全、智慧教育、智能显控三个主要领域。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="410" src="https://oscimg.oschina.net/oscnet/up-4c73d08f149214ac335e5f0e52f8db92eea.png" width="940" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>加入 openKylin 社区后，博汇科技将利用自己在传媒安全、智慧教育、智能显控三个领域拥有<span>的</span>深厚技术积累和丰富的行业经验，与社区进行深入合作，包括但不限于博汇科技研发的一系列创新硬件产品与社区系统深度兼容、集成；同时双方在技术、生态融合上与市场活动等方面也将<span>定期</span>进行交流沟通与紧密合作。通过双方合作，希望进一步提升产品性能，更好地服务于视听大数据领域，共同推动我国信息技术产业发展。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 31 Jan 2024 01:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/277006</guid>
            <link>https://www.oschina.net/news/277006</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[爆了！Sealos 三天支持 1000 个帕鲁私服]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhzh.sealos.run%2F%3Fopenapp%3Dsystem-template%253FtemplateName%253Dpalworld%2526uid%3Dkc811077" target="_blank">Sealos 的帕鲁私服模板</a>从第一天发布之后就起了 100 多个私服，第二天直接上到 500 多个，第三天直接上千，还在加速增长中。来讲讲我们只用一个晚上怎么做到上线一个专属可用区的，还有一些帕鲁实践让我对云有的一些新的思考。</p><p><img src="https://oscimg.oschina.net/oscnet/up-490ea01e164188b5dfed8d0ca052a6ac442.jpg" alt="" referrerpolicy="no-referrer"></p><h2>Sealos 凭啥杀出一条血路</h2><p>这次各大云厂商都出了私服教程。</p><p>原因很简单，别的厂商都只是虚拟机上搭建的教程，有的甚至是长长的一篇文档，不过后来都搞一件部署了。</p><p>而 Sealos 直接做成了<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsealos.run%2Fdocs%2Fguides%2Ftemplates%2F" target="_blank">应用模版</a>，<strong>点点鼠标，有手就会，对此菜市场大妈表示体验很好，边买菜都可以边部署起来</strong>。论速度我们吊打一切，只需要 30 秒起服，而其他我大部分测试了都要三分钟，原因很简单：<strong>我们抛弃了虚拟机</strong>。</p><p><strong>玩游戏的很多人并非开发者，或者没那么懂技术的，虚拟机，控制枱这些概念都不能让他们知道，就更别劝玩家去用了</strong>。Sealos 就是一个链接：</p><ul><li><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhzh.sealos.run%2F%3Fopenapp%3Dsystem-template%253FtemplateName%253Dpalworld%2526uid%3Dkc811077" target="_blank">https://hzh.sealos.run/?openapp=system-template%3FtemplateName%3Dpalworld%26uid=kc811077</a></strong></li></ul><p>看，有手就能部署，脑子都不用带。</p><p>我们的视频教程在 B 站获得了非常高的热度：</p><p><img src="https://oscimg.oschina.net/oscnet/up-e1ff512810d70062569e7e07974a64a1010.jpg" alt="" referrerpolicy="no-referrer"></p><p>视频传送门：</p><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1Tw41177n3" target="_blank">【幻兽帕鲁】零门槛服务器搭建配置存档教程免费体验</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1Ab4y1P7Es" target="_blank">【幻兽帕鲁】最小白的服务器搭建教程 32 人低延迟免费体验</a></li></ul><p><strong>Sealos 还可以先体验后付钱，付费转化率夸张的高！</strong></p><p>相比较包年包月，Sealos 采用完全按量计费的方式，玩家睡觉时就可以关了，收一点点磁盘的费用。容器还有个好处就是内存炸了很快就可以自动重启，几乎不影响用户体验，而虚拟机重启下不得分钟级。</p><p>还有就是调配置，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsealos.run%2Fdocs%2Fguides%2Fapplaunchpad%2Fautoscale" target="_blank">Sealos 随便伸缩</a>，虚拟机包年包月伸缩就很尴尬。</p><p>最后是一些细节体验，比如导入存档，配置管理，死亡不掉落等等。</p><p>还有一个要点，就是运营的同学要去玩游戏，为什么？如果我们不玩我们就无法体会死亡不掉落的重要性，无法体会孵蛋时间是个啥，无法理解用户在什么时间点会上头，这直接决定需要赠送多少额度，需要提供什么样的配置，需要新增什么样的痛点特性。</p><h2>事情经过</h2><p>有两个朋友给我发了这个游戏的文章，瞬间我就找到了我们 「奇技淫巧奖」 获得者的米开朗基杨同学，当天就把模版做出来了，上架到了我们的两个集群。随后文章一发，瞬间集群资源就满了。</p><p><img src="https://oscimg.oschina.net/oscnet/up-e8607737eb52990918b33443b1aae7332a4.jpg" alt="" referrerpolicy="no-referrer"></p><p>周四晚上我们立马决定建设一个新的专属可用区，这个可用区选择在阿里云上，原因很简单：需要快速扩容，而且以后可能热度下降要考虑释放问题。<strong>所以大家就不要随便黑公有云了，这是成本最低的打法</strong>，下文细讲。</p><p>我们一半人写代码，一半人买服务器起集群，把 Sealos 中玩家不需要的东西通通裁剪掉，应用市场也就只放一个模版，用 Laf 花了五分钟写一个游戏配置管理的程序并上线到 Sealos 上做默认应用。剩下的时间就在不断优化游戏的模版，包括缓存游戏的镜像让启动速度达到极致。</p><p>晚上就火爆了，不是在加服务器就是在加服务器的路上，我们每个节点可以跑 70 个私服。<strong>第二天团建，全员揹着电脑，几乎一半人在烧烤一半人在支撑。</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-a3be78a611bf5d875e6a6ac46c8b5a66546.jpg" alt="" referrerpolicy="no-referrer"></p><p>到目前为止单集群已经启动上千个。</p><h2>帕鲁事件对云的思考</h2><p>这个事只是一个影子，他映射出未来的云应该是什么样子。</p><h3>第一：应用是一等公民，忘掉云原生，忘掉容器，更要忘掉虚拟机</h3><p>你看安卓/iOS/Windows/macOS 以及各种 Linux 发行版，大部分用户用的是具体的应用，每个应用都有不同的用户画像，而很少有用户关心系统本身，更不会去关心内核。</p><p>把用户的最终需求直接给到用户才是云发展的终极目标，像帕鲁，<strong>用户需要的肯定不是容器，也不是虚拟机</strong>，那让用户进入到一个虚拟机的操作控制枱显然就违揹人性了。</p><p>当然会有很多开发者直接需要的就是虚拟机，那提供一个虚拟的应用也可以，核心逻辑解耦合，尽量不要让用户关心他需求以外的东西。随着应用的丰富，大众市场对更底层的需求就会越来越少，比如有了 RDS 之后，很多人要用数据库的人就不想去折腾虚拟机了，这里务必理解用户最终想要的是什么，直接给。</p><p>所以这必然会衍生出一种更合理的架构，其实这个架构已经有答案了，单机操作系统就是答案，有很好的标准，一堆人开发应用，另外一堆人使用应用，云为什么不这样？</p><p>今天的公有云很多都被框死在了控制枱，这样应用很难有自己的发挥空间，比如帕鲁的用户去登录公有云一通操作，VPC 快照对象存储几个标签就能把他看晕乎，这不是一种极致的产品体验。而手机上 LOL 游戏就不会外面套个微信的聊天框，不同的应用需要有自己不同的风格。markteplace 就更不用说了，缺乏标准，一锅大杂烩而已。</p><h3>第二：云是连接应用的生产者与使用者</h3><p>一旦云足够成熟的时候就会有非常好的灵活性，可以让大量开发者开发云上应用，使用者来使用，而今天云厂商大部分是厂商自己开发云产品，虽然也有第三方上架什么的，但是味道不对，怎么样算对，参考手机生态。</p><p>Docker Hub 就有点这个味道了，什么味道？标准的形成，很好的链接了镜像的制作者和使用者，但是提供的是零件，不是完整的分布式应用。</p><p>K8s 生态提供了很好的标准，是个非常优秀的云内核，一旦发行版盛行，此生态大成！那个时候很多人会忘掉 K8s，我可以给一个非常让人吃惊的数字，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsealos.run" target="_blank">Sealos</a> 云上的用户 80% 没接触过容器，20% 完全不是专业的开发者，他们就只想直接起应用。</p><p>所以先进的云需要给应用非常高的自由度，需要让用户用它的时候完全沉浸在应用本身的使用中，Sealos 就是这样，打开一个应用之后你就看不到 Sealos 的任何东西了，<strong>而目前云厂商都有个大大的 Header 和侧边栏</strong>，所以就出现了这种怪物：</p><p><img src="https://oscimg.oschina.net/oscnet/up-dff4b1c0b72679e2c0f1003dacad4c7f038.png" alt="" referrerpolicy="no-referrer"></p><p>我想写代码，可是嵌套一层又一层。</p><p>云 IDE 应当是一个独立的应用，可能是第三方提供，然后别人来使用。即便是官方自研也应当是这样的架构。</p><h3>第三：云是连接资源的提供者与消费者</h3><p>现在的云厂商基本硬件资源都是自己提供，而 Sealos 的做法比较有意思，未来我们的很多 Region 算力本身也可以由第三方提供，接入到我们平台中，真正成为一个算力平台，从算力视角连接双方。有点像直营和加盟的意思，或者京东和淘宝。若无云操作系统，这种想法也无法落地，而我们 24 年就会提供。</p><h3>第五：Sealos 公有云服务坚定选择云厂商，抛弃自托管硬件方向</h3><p>自建还是选择公有云确实要看场景，Sealos 云服务的业务未来只考虑用公有云。不要只看到价格的贵而忽略效率成本，还有很多其它隐性成本不可不查。</p><p>帕鲁这个场景毫无疑问首选公有云，快速扩容这一个需求就决定了没法在自己的机房跑。而且热点过去了是要缩回来了。</p><p>即便不是帕鲁这个场景，Sealos 还是坚定不移用公有云，我曾经一直在摇摆自建还是上云，我们本身就是做云的，绝对有托管硬件自建的能力，物理机的性能也更好，单纯计算算力价格也低太多。</p><p>但是，最终发现需要花好多倍成本就解决那剩下 10% 的问题，比如高可用要备份，你的备份程序和磁盘都需要额外成本，对应人力成本也来了，最要命的是响应速度，换块盘也需要时间，需要沟通成本等等。这些公有云上一个快照就都可以解决。</p><p>还有个恶心的备案问题，几乎只能人肉迁移备案，开白名单这些。而且机房的带宽费用并不便宜。</p><p>买多少硬件资源也是一个很费事的问题，预留不够上架可就慢了。</p><p>综合下来如果你把云的弹性用好，是可以节省大量成本的。特别有状态和无状态分开运行，夜间释放，根据不同的 SLA 等级划分可用区，举个例子，我们可以针对开发测试场景专门做一个对接竞价实例的集群，把所有无状态全部用竞价去跑。</p><p>使用公有云还可以让我们自己保持轻资产运营，天下武功唯快不破，迭代速度是创业公司最为追求的。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 11:04:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4148359/blog/10992796</guid>
            <link>https://my.oschina.net/u/4148359/blog/10992796</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CodeFuse 新开源模型荣登 Big Code 评测榜首！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p style="margin-left:0px; margin-right:0px; text-align:center"><img alt="jahb.png" src="https://intranetproxy.alipay.com/skylark/lark/0/2024/png/98756342/1706606628857-bd1dbf7f-50d0-4b4e-a704-2f26ae5a3814.png?x-oss-process=image%2Fresize%2Cw_900%2Climit_0" width="900" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0"><strong><span>使用多任务高效微调框架 MFTCoder，以 DeepSeek-Coder-33b 模型为底座，微调获得的 CodeFuse-DeepSeek-33b 模型在</span></strong><strong><span style="color:#1f2937">Big<span>&nbsp;</span></span></strong><strong><span style="color:#e6b800">Code</span></strong><strong><span style="color:#1f2937"><span>&nbsp;</span>Models<span>&nbsp;</span></span></strong><strong><span style="color:#e6b800">Leaderboard</span></strong><strong><span>代码大模型榜单上以 43.58% WinRate 成为新晋榜首，同时模型在 NLP 任务上也取得了很好的表现。本文我们将介绍该模型的得来和使用，包括训练数据、训练超参设置、模型评测效果以及如何获取该模型和基于它继续微调。我们已经在 HuggingFace 和 ModelScope 开放了模型下载（下载地址在文末），并同步提供了 4bit 量化版本供大家直接部署到生产环境。</span></strong></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><img src="https://intranetproxy.alipay.com/skylark/lark/0/2024/png/347737/1706581702847-565b14ab-7236-4bab-b74f-b8ce640e2eb9.png" width="1734" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>图 1: Big Code Models LeaderBoard 榜单截图（截取时间 2024-01-30）。</span></strong><strong><span style="color:#1f2937">Big<span>&nbsp;</span></span></strong><strong><span style="color:#e6b800">Code</span></strong><strong><span style="color:#1f2937"><span>&nbsp;</span>Models<span>&nbsp;</span></span></strong><strong><span style="color:#e6b800">Leaderboard</span></strong><strong><span>（</span></strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fbigcode%2Fbigcode-models-leaderboard" target="_blank"><strong><span>https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard</span></strong></a><strong><span>）是由 HuggingFace BigCode 团队维护的代码大模型榜单，是代码大模型领域比较权威的评测榜单。</span></strong></p><p style="margin-left:0; margin-right:0">&nbsp;</p><span id="OSC_h1_1"></span><h1><span>多任务微调 MFT</span></h1><p style="margin-left:0; margin-right:0"><span>我们选择以 DeepSeek-Coder-33b 模型为底座，使用多任务微调框架 MFTCoder 对 5 个下游任务数据进行微调，得到 CodeFuse-DeepSeek-33b 模型。以下将更为详细地进行介绍。</span></p><span id="OSC_h2_2"></span><h2><span>训练数据</span></h2><p style="margin-left:0; margin-right:0"><span>本次训练我们设置了 5 个下游任务，如下表 1 所示，包括代码补全任务、文本生成代码任务、单测生成任务、自然语言表述对齐任务和代码练习题任务，共约 168 万样本数据。得益于我们开源的多任务微调框架 MFTCoder，这些下游任务能一定程度上相互促进，比直接将所有任务数据混合为一后微调表现更优。</span></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>表 1: 下游任务训练数据统计</span></strong></p><table border="1" cellspacing="0" style="border-collapse:collapse; border:1px solid #d9d9d9; table-layout:fixed; width:700px"><tbody><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span><strong><span>序号</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span><strong><span>MFT 下游任务</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>任务能力</span></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span><strong><span>#Samples<span>&nbsp;</span></span></strong></span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>1</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>单测用例生成</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>给定函数级代码生成单元测试用例</span></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>390,393</span></span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>2</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>代码补全</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>根据前文补全代码（方法级）</span></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>192,547</span></span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>3</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>文本生成代码</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>基于文本描述生成功能代码</span></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>66,862</span></span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>4</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>NLP 表述对齐</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>增强 NLP 理解能力</span></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>951,278</span></span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>5</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>代码练习题 (JAVA/CPP/GO)</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>基于文本描述生成基础功能代码</span></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span>82,603</span></span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px">&nbsp;</td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><strong><span>#Total</span></strong></span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span><span style="color:#000000">1,683,683</span></span></p></td></tr></tbody></table><span id="OSC_h2_3"></span><h2><span>关键超参设置</span></h2><p style="margin-left:0; margin-right:0"><span>本次微调使用的是我们已经开源的多任务微调框架 MFTCoder</span><strong><span>(</span></strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcodefuse-ai%2FMFTCoder%2Ftree%2Fmain%2Fmftcoder_accelerate" target="_blank"><strong><span>https://github.com/codefuse-ai/MFTCoder/tree/main/mftcoder_accelerate</span></strong></a><strong><span>)</span></strong><span>，MFTCoder 支持多模型适配（包括 Llama 1/2、CodeLlama、Qwen、Baichuan 2、ChatGLM 2/3、CodeGeex 2、GPT-NEOX、Mistral、DeepSeek 等）、多任务并行、多种均衡 Loss 设计、PEFT（Lora 和 QLora）高效微调，此前已被采纳为 Qwen Code AI 竞赛初赛推荐微调框架（</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftianchi.aliyun.com%2Fcompetition%2Fentrance%2F532169%2Finformation" target="_blank"><span>https://tianchi.aliyun.com/competition/entrance/532169/information</span></a><span>）。本次训练使用的关键超参设置如下表 2 所示，更多详细的参数说明可参考</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcodefuse-ai%2FMFTCoder%2Ftree%2Fmain%2Fmft_peft_hf%2332-loraqlora" target="_blank"><span>https://github.com/codefuse-ai/MFTCoder/tree/main/mft_peft_hf#32-loraqlora</span></a></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>表 2： MFTCoder 微调关键超参设置及解释</span></strong></p><table border="1" cellspacing="0" style="border-collapse:collapse; border:1px solid #d9d9d9; table-layout:fixed; width:734px"><tbody><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><strong><span>参数名称</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><strong><span>参数值</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><strong><span>简要解释</span></strong></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>data_split</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>"98,2,0"</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>98% 数据用于训练，2% 用于验证</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>padding_mode</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>"padding"</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>使用动态填充模式，即每张卡每个 batch 大小是由每次其中的最长者动态决定而不是固定大小。另一种可选数据模式是"pack"。</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:48px"><p style="margin-left:0; margin-right:0"><span>dynamic_padding</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:48px"><p style="margin-left:0; margin-right:0"><span>True</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>weighted_loss_mode</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>"case3"</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>使用数据均衡 Loss 函数，更多细节可见论文</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2311.02303" target="_blank"><span>https://arxiv.org/abs/2311.02303</span></a></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>peft_type</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>"qlora"</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>采取 QLora 4bit 量化微调模式</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>quantization</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>"4bit"</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>lora_rank</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>192</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>决定可训练参数比例</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>lora_alpha</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>32</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>per_device_train_batch_size</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>4</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>训练时单卡 batch 大小</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>per_device_eval_batch_size</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>4</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>验证时单卡 batch 大小</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>learning_rate</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>5e-5</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>初始学习率</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>min_lr</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>1e-6</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>最小学习率</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>gradient_accumulation_steps</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>1</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>梯度累积步数，如果为 2，则每累积 2 步再更新参数，资源不足是一种间接增加 global batch size 的方式</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>world_size</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>64</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>GPU 卡数，使用 64 张 A100/A100 卡</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>evalation_steps</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>500</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>每 500 步验证一次</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>checkpointing_steps</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>500</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>每 500 步保存一次检查点</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>num_train_epochs</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>10</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>最大训练轮数，最大 10 轮</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>early_stopping</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>True</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>开启 early-stopping 机制，即当连续 3 个检查点的 eval loss 均比倒数第 4 个检查点的 eval loss 大时终止训练</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>early_stopping_stall_num</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>3</span></p></td></tr></tbody></table><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0"><span>使用前述训练数据和配置，经过 156.5 小时，模型在完成 5.09 Epochs 训练后触发 Early-Stopping 策略后终止。</span></p><p style="margin-left:0; margin-right:0">&nbsp;</p><span id="OSC_h1_4"></span><h1><span>模型效果</span></h1><p style="margin-left:0; margin-right:0"><span>我们从代码能力和 NLP 能力两个方面对训练获得的 CodeFuse-DeepSeek-33b 进行了测试，pass@1 测试均采用 greedy 解码模式（即</span><span style="background-color:#d8dad9">doSample=False, num_beams=1, num_return_sequences=1</span><span>）。</span></p><span id="OSC_h2_5"></span><h2><span>代码能力</span></h2><p style="margin-left:0; margin-right:0"><span>我们选取了常用的代码评测集对模型进行评测，首先我们使用自己的 CodeFuse-Evaluation 评测框架（</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcodefuse-ai%2Fcodefuse-evaluation" target="_blank"><span>https://github.com/codefuse-ai/codefuse-evaluation</span></a><span>）对模型在 HumanEval-X（含 HumanEval）和 MBPP 测试集上的表现进行了测试并与 CodeFus 此前微调过的模型进行了比较，如下表 3 和表 4 所示。</span></p><p style="margin-left:0; margin-right:0"><strong><span>CodeFuse-DeepSeek-33b 在 HumanEval 上 pass@1 指标值为 78.65%、在 MBPP 上为 71%（zero-shot），两项平均为 74.83%，略高于 DeepSeek-Coder-Instruct-33B</span></strong><span>。</span></p><p style="margin-left:0; margin-right:0"><strong><span>CodeFuse-DeepSeek-33b 在多语言评测集 HumanEval-X 上 pass@1 指标值平均为 67.07%，比此前我们开放的 CodeFuse-CodeLlama-34b 模型高 6.69%，在具体各种语言上高出 3.48%~12.19% 不等</span></strong><span>。</span></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>表 3: CodeFuse-DeepSeek-33b 模型与其他开源底座模型及微调模型在 HumanEval 和 MBPP 上的对比</span></strong></p><p style="margin-left:0; margin-right:0; text-align:center"><img src="https://intranetproxy.alipay.com/skylark/lark/0/2024/png/347737/1704200172671-e3b1411a-19eb-4305-878d-99f7f95945e5.png" width="636" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>表 4: CodeFuse-DeepSeek-33b 模型与其他开源底座模型及 MFT 微调模型在 HumanEval-X 上的对比</span></strong></p><p style="margin-left:0; margin-right:0; text-align:center"><img src="https://intranetproxy.alipay.com/skylark/lark/0/2024/png/347737/1704200193212-a4061f95-d847-46d6-853b-2f0848c16050.png" width="719" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0"><span>由于不同评测框架在代码后处理和生成终止条件（Stop Words）等方面常存在差异，除了用我们自己的 CodeFuse-Evaluation 评测框架，我们也用代码大模型榜单 Big Code Models LeaderBoard 所用的开源评测框架 bigcode-evaluation-harness (</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fbigcode-project%2Fbigcode-evaluation-harness" target="_blank"><span>https://github.com/bigcode-project/bigcode-evaluation-harness</span></a><span>) 进行了评测，并与榜单上的模型进行了比较。榜单会测试模型在 Python 代码补全测试集 HumenEval 和多语言代码补全测试集 MultiPL-E 共 12 种语言上的表现，并根据各语言表现进行 WinRate 排序。（结果复现代码地址：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ftwelveand0%2Fbigcode-evaluation-harness" target="_blank"><span>https://github.com/twelveand0/bigcode-evaluation-harness</span></a><span>）</span></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>表 5： 采用 bigcode-evaluation-harness 评测 CodeFuse-DeepSeek-33b 模型后的新榜单</span></strong><img src="https://intranetproxy.alipay.com/skylark/lark/0/2024/png/347737/1706582301694-43665f48-da68-422d-ad5a-86454c8f627f.png" width="1400" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0"><span>如表 5 所示，</span><strong><span>CodeFuse-DeepSeek-33b 模型的 WinRate 为 43.58%，超过原榜首 DeepSeek-Coder-33b-instruct。在 HumanEval 评测集上，CodeFuse-DeepSeek-33b 表现不如 DeepSeek-Coder-33b-instruct，但在其他 8 种语言（包括 Java 和 JS 等）上超过后者，均值（Average Score）亦超过后者 1.7%</span></strong><span>。</span></p><span id="OSC_h2_6"></span><h2><span>NLP 通用能力</span></h2><p style="margin-left:0; margin-right:0"><span>对于 NLP 通用能力测试，我们参照 OpenCompass 选择了 18 个评测集，包括语言能力（AFQMC、CHID、Wic、WSC）、推理能力（COPA、CMNLI、OCNLI、Ax-b、Ax-g、RTE）、理解能力（CSL、C3、EPRSTMT）、学科综合能力（MMLU、C-Eval、ARC-c）、代码能力（HumanEval、MBPP）。对于每个模型，我们会使用生成式和 PPL 方式计算每个指标，并在每个维度上选取两种方式中较高的值作为指标值。</span></p><p style="margin-left:0px; margin-right:0px; text-align:center"><img src="https://intranetproxy.alipay.com/skylark/lark/0/2024/png/347737/1704260089384-cb664a24-5d58-4a7d-9081-a61159a469e3.png" width="1230" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>图 2: CodeFuse-DeepSeek-33b NLP 通用能力雷达图</span></strong></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0"><span>CodeFuse-DeepSeek-33b 模型的评测结果如图 3 雷达图所示，我们将其与底座模型 DeepSeek-Coder-33b 和 DeepSeek 通用模型 DeepSeek-67b-Chat 进行了对比。</span><strong><span>从图中可以看出，相较于底座模型 DeepSeek-Coder-33b，CodeFuse-DeepSeek-33b 在所有维度上均有正向提升；相较于我们此前开源的 CodeFuse-CodeLlama-34b，CodeFuse-DeepSeek-33b 在绝大多数维度上表现更优；相较于通用模型 DeepSeek-67b-Chat，CodeFuse-DeepSeek-33b 在语言能力、代码能力和理解能力上整体表现更优，在推理能力上表现稍差，在学科综合能力上差距较大。考虑到模型参数规模差距和底座目标功能类型差异，我们认为 CodeFuse-DeepSeek-33b 已经表现很好。</span></strong></p><p style="margin-left:0; margin-right:0">&nbsp;</p><span id="OSC_h1_7"></span><h1><span>模型 INT4 量化</span></h1><p style="margin-left:0; margin-right:0"><span>为了便于直接部署投入生产，我们同步提供了 CodeFuse-DeepSeek-33b-INT4 量化版本。对于量化后的模型，我们测试了它的代码能力，如表 5 所示，量化后模型在代码补全任务上只有微弱降幅。</span></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>表 5：模型量化前后在 HumanEval-X 和 MBPP 上的指标对比</span></strong></p><table border="1" cellspacing="0" style="border-collapse:collapse; border:1px solid #d9d9d9; table-layout:fixed; width:749px"><tbody><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>Model</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>HumanEval-X</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>MBPP</span></strong></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>Python</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>Java</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>C++</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>JS</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>Go</span></strong></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>CodeFuse-DeepSeek-33b</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>78.65%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>67.68%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>65.85%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>67.07%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>56.10%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0"><span>71.0%</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>CodeFuse-DeepSeek-33b-INT4</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>78.05%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>68.29%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>62.19%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>64.63%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>55.49%</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px">&nbsp;</td></tr></tbody></table><p style="margin-left:0; margin-right:0"><span>此外，我们测试了该模型实际部署后的性能。测试环境为单张 A10（24G 显存）、部署框架为 NVIDIA 开源的 tensorRT。测试结果具体如表 6 所示：</span></p><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>表 6: CodeFuse-DeepSeek-33b-INT4 在单张 A10 的推理性能</span></strong></p><table border="1" cellspacing="0" style="border-collapse:collapse; border:1px solid #d9d9d9; table-layout:fixed; width:640px"><tbody><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span>模型版本</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span>CodeFuse-DeepSeek-33b</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0; text-align:center"><span>推理速度指标</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0; text-align:center"><span>Tokens/s</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span>模型并行/gpu 型号</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center">&nbsp;</p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span>单卡 A10</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span>量化格式</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center">&nbsp;</p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><span>int4 &nbsp;</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0"><span>输入/输出长度</span><br><span>(batch_size=1)</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0; text-align:center"><span>16/8</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:37px"><p style="margin-left:0; margin-right:0; text-align:center"><span>21.7</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:40px"><p style="margin-left:0; margin-right:0; text-align:center"><span>64/32</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:40px"><p style="margin-left:0; margin-right:0; text-align:center"><span>21.5</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:38px"><p style="margin-left:0; margin-right:0; text-align:center"><span>256/128</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:38px"><p style="margin-left:0; margin-right:0; text-align:center"><span>21.1</span></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:36px"><p style="margin-left:0; margin-right:0; text-align:center"><span>1024/512</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:36px"><p style="margin-left:0; margin-right:0; text-align:center"><span>20.5</span></p></td></tr></tbody></table><span id="OSC_h1_8"></span><h1>&nbsp;</h1><span id="OSC_h1_9"></span><h1><span>模型下载试用</span></h1><p style="margin-left:0; margin-right:0"><span>我们开放了量化前后 2 个模型的下载，提供了推理格式和推理示例，并说明了如何在此基础上继续微调。</span></p><span id="OSC_h2_10"></span><h2><span>下载</span></h2><p style="margin-left:0; margin-right:0"><span>我们已经将 2 个模型（CodeFuse-DeepSeek-33b 和 CodeFuse-DeepSeek-33b-INT4）发布到 HuggingFace 和 ModelScope 社区，大家可以选择通过以下链接下载：</span></p><table border="1" cellspacing="0" style="border-collapse:collapse; border:1px solid #d9d9d9; table-layout:fixed; width:726px"><tbody><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>Model</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>HuggingFace</span></strong></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>ModelScope</span></strong></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>CodeFuse-DeepSeek-33b</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcodefuse-ai%2FCodeFuse-DeepSeek-33B" target="_blank"><span>https://huggingface.co/codefuse-ai/CodeFuse-DeepSeek-33B</span></a></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2Fcodefuse-ai%2FCodeFuse-DeepSeek-33B%2Fsummary" target="_blank"><span>https://modelscope.cn/models/codefuse-ai/CodeFuse-DeepSeek-33B/summary</span></a></p></td></tr><tr><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><span>CodeFuse-DeepSeek-33b-4bits</span></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcodefuse-ai%2FCodeFuse-DeepSeek-33B-4bits" target="_blank"><span>https://huggingface.co/codefuse-ai/CodeFuse-DeepSeek-33B-4bits</span></a></p></td><td style="border-color:#d9d9d9; border-style:solid; border-width:1px; height:33px"><p style="margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Fmodels%2Fcodefuse-ai%2FCodeFuse-DeepSeek-33B-4bits%2Fsummary" target="_blank"><span>https://modelscope.cn/models/codefuse-ai/CodeFuse-DeepSeek-33B-4bits/summary</span></a></p></td></tr></tbody></table><span id="OSC_h2_11"></span><h2><span>推理格式与代码</span></h2><p style="margin-left:0; margin-right:0"><span>CodeFuse-DeepSeek-33b 使用的推理格式如下：</span></p><pre><code>&lt;s&gt;system
{THIS IS YOUR SYSTEM PROMPT}
&lt;s&gt;human
{THIS IS YOUR PROMPT of ROUND 1}
&lt;s&gt;bot
{THIS IS MODEL's RESPONSE of ROUND 1}
&lt;s&gt;human
{THIS IS YOUR PROMPT of ROUND 2}
&lt;s&gt;bot</code></pre><p style="margin-left:0; margin-right:0"><span>在这个格式中，sysem 部分是可选的，对话轮数可以是多轮的也可以是单轮的。下面是一些具体的案例展示：</span></p><pre><code>&lt;s&gt;human
请写一个快排程序
#Python
&lt;s&gt;bot
</code></pre><pre><code>&lt;s&gt;system
你是 CodeFuse，你会被给定一个任务，你必须按照用户的要求完成任务。
&lt;s&gt;human
请写一个快排程序
#Python
&lt;s&gt;bot
下面是一个使用 Python 编写的快速排序算法的示例代码：
```python
def quick_sort(arr):
    if len(arr) &lt;= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x &lt; pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x &gt; pivot]
    return quick_sort(left) + middle + quick_sort(right)
# 测试快速排序算法
arr = [3, 6, 8, 10, 1, 2, 1]
print(quick_sort(arr))
```
这段代码定义了一个`quick_sort`函数，它接受一个列表作为参数，并返回排序后的列表。在函数内部，首先判断列表的长度是否小于等于 1，如果是，则直接返回列表本身。否则，选择列表中间的元素作为基准值，并将列表分成三个部分：小于基准值的部分、等于基准值的部分和大于基准值的部分。然后，对这三个部分分别递归调用`quick_sort`函数进行排序，最后将排序后的三个部分合并并返回。
通过这个示例，你可以了解到如何使用 Python 编写快速排序算法，并对其进行测试。
&lt;s&gt;human
请为以上代码生成单测用例
&lt;s&gt;bot
</code></pre><p style="margin-left:0; margin-right:0">&nbsp;</p><p style="margin-left:0; margin-right:0"><span>下面是一段示例的推理代码：</span></p><pre><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

model_dir = "codefuse-ai/CodeFuse-DeepSeek-33B"

def load_model_tokenizer(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    tokenizer.eos_token = "&lt;｜end▁of▁sentence｜&gt;"
    tokenizer.pad_token = "&lt;｜end▁of▁sentence｜&gt;"
    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
    tokenizer.padding_side = "left"
    
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto',torch_dtype=torch.bfloat16, trust_remote_code=True)
    return model, tokenizer


HUMAN_ROLE_START_TAG = "&lt;s&gt;human\n"
BOT_ROLE_START_TAG = "&lt;s&gt;bot\n"

text_list = [f'{HUMAN_ROLE_START_TAG}Write a QuickSort program\n#Python\n{BOT_ROLE_START_TAG}']

model, tokenizer = load_model_tokenizer(model_dir)
inputs = tokenizer(text_list, return_tensors='pt', padding=True, add_special_tokens=False).to('cuda')
input_ids = inputs["input_ids"]
attention_mask = inputs["attention_mask"]
generation_config = GenerationConfig(
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
        temperature=0.1,
        max_new_tokens=512,
        num_return_sequences=1,
        num_beams=1,
        top_p=0.95,
        do_sample=False
)
outputs = model.generate(
        inputs= input_ids,
        attention_mask=attention_mask,
        **generation_config.to_dict()
)
gen_text = tokenizer.batch_decode(outputs[:, input_ids.shape[1]:], skip_special_tokens=True)
print(gen_text[0])</code></pre><span id="OSC_h2_12"></span><h2><span>继续微调</span></h2><p style="margin-left:0; margin-right:0"><span>如果你想在这两个模型基础上继续微调，欢迎使用我们开源的多任务高效微调框架 MFTCoder（</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcodefuse-ai%2FMFTCoder%2Ftree%2Fmain%2Fmftcoder_accelerate" target="_blank"><span>https://github.com/codefuse-ai/MFTCoder/tree/main/mftcoder_accelerate</span></a><span>）。要继续微调，你需要准备好训练数据集（CodeFuse-ChatML 格式）、设置训练配置文件、设置运行配置文件并启动训练。这里提供一个对 Qwen-1.8 模型用 MFTCoder 进行微调的案例供参考：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcodefuse-ai%2FMFTCoder%2Ftree%2Fcodeqwen_competition%2Fmft_peft_hf" target="_blank"><span>https://github.com/codefuse-ai/MFTCoder/tree/codeqwen_competition/mft_peft_hf</span></a><span>。</span></p><p style="margin-left:0; margin-right:0">&nbsp;</p><span id="OSC_h1_13"></span><h1><span>联系我们</span></h1><p style="margin-left:0; margin-right:0"><span>MFTCoder 已经开源，本文中提到的模型和数据集也在陆续开源中，如果您喜欢我们的工作，欢迎试用、指正错误和贡献代码，可以的话请给我们的项目增加 Star 以支持我们。</span></p><ul><li><span>GitHub 项目主页：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcodefuse-ai%2FMFTCoder" target="_blank"><span>https://github.com/codefuse-ai/MFTCoder</span></a></li><li><span>HuggingFace 主页：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fcodefuse-ai" target="_blank"><span>https://huggingface.co/codefuse-ai</span></a></li><li><span>魔搭社区主页：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmodelscope.cn%2Forganization%2Fcodefuse-ai" target="_blank"><span>https://modelscope.cn/organization/codefuse-ai</span></a></li></ul></div></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 11:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6942768/blog/10992906</guid>
            <link>https://my.oschina.net/u/6942768/blog/10992906</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[AI 版权第一案：我用 AI 做的图，版权归我吗？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>最近，一个跟 AI 相关的案子引起了全国同行的围观，原因是一个百家号作者在网上写文章时，用了一张网上的 AI 图片当配图，结果被图片的生产者告侵权。这不禁让人紧张：以后 AI 产的图片，还能随便用吗？会不会侵权？</p><p>事情是这样的——</p><p style="text-align:left">2023 年 2 月，李昀锴使用 Stable Diffusion 模型，通过在模型上输入数十个提示词，设置相关迭代步数、图片高度、提示词引导系数以及随机数种子等，生成了数张人像图片。随后，李昀锴以 「春风送来了温柔」 为名发布在社交平台小红书上。2023 年 3 月 2 日，百家号一自媒体账号发布的名为《三月的爱情，在桃花里》的文章使用了李昀锴制作的图片。</p><p style="text-align:center"><img height="768" src="https://oscimg.oschina.net/oscnet/up-f6cf6dc908fcf7b1c685805400c6c984609.png" width="512" referrerpolicy="no-referrer"></p><p style="text-align:center"><span style="color:#999999"><em>李昀锴用 Stable Diffusion 模型生成的图片</em></span></p><p style="text-align:left">李昀锴认为，对方未获得他的许可就截掉了他在小红书平台的署名水印进行发布，侵犯了署名权及信息网络传播权，应当承担法律责任。</p><p style="text-align:left">2023 年 5 月，李昀锴起诉被告刘某侵害作品署名权、信息网络传播权纠纷一案在北京互联网法院立案。</p><p><strong>这里最关键的争议在于，李昀锴基于自己设定的提示词和参数，使用 Stable Diffusion 模型制作的图片 「春风送来了温柔」 是否构成作品？以及构成哪一种类型的作品？</strong></p><p style="text-align:left"><strong>法院认为，原告李昀锴使用</strong><strong>开源软件</strong><strong> Stable Diffusion 创作的图片由其独立完成，体现了他的个性化表达，因此涉案图片具备 「独创性」 要件。涉案图片符合作品的定义，属于美术作品，受到著作权法的保护。最终判决李昀锴胜诉，被告需在百家号平台公开道歉，并向原告赔偿 500 元</strong>。</p><p style="text-align:left">这个案件，也成为了 AI 生成图片相关领域著作权第一案，让 AI 领域的从业者和使用者都提高了警惕：</p><p>原来 AI 创作的图片也是有著作权的，那以后使用 AI 图片还得注意授权问题了？</p><p>AI 图片很多都是用别人的图片「溶」出来的，相当于「二创」。这种做法放在文艺领域早被当成抄袭了，为啥它还能享有著作权呢？这是不是合理的？</p><p>我用 AI 创作的图，总共有哪些权利和风险？权责如何划分？</p><p>对此，我们邀请了知识产权领域的专业律师邓超进行对谈，一起聊聊究竟怎么回事。</p><p>&nbsp;</p><span id="OSC_h1_1"></span><h1>01 AI 作品，版权并不明晰</h1><p><span style="color:#245bdb">开源中国：首先我们捋清一下概念，版权和著作权是一回事吗？</span></p><p>&nbsp;</p><p>邓超：是的，著作权和版权没有区别。根据相关法律规定，著作权是指作品的作者等相关人员，依法对其已经完成的作品所享有的发表权、署名权、复制权、保护作品完整性权等。<strong>且著作权即版权。</strong></p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：OK，那在现行法律当中，个人或单位使用 </span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 生产的图片，拥有版权吗？</span></p><p>&nbsp;</p><p>邓超：<strong>在现行法律当中，这块还没有明确规定，行业意见也不统一，需要进行个案分析。</strong></p><p>前年北京互联网法院有一个「菲林诉百度案」，当时法院认为，机器生成的内容是没有版权的。</p><p style="text-align:center"><img height="625" src="https://oscimg.oschina.net/oscnet/up-d4964bffd09758af1e3dbb525b78e0b065e.png" width="1421" referrerpolicy="no-referrer"></p><p>但是在深圳的「腾讯新闻案」，法院则认为，机器生成的内容有版权。</p><p style="text-align:center"><img height="687" src="https://oscimg.oschina.net/oscnet/up-9672009ee8539049d0417268a64ca6f9a30.png" width="1437" referrerpolicy="no-referrer"></p><p>一南一北，对这个问题给出了不同的答案，但是这些案子都没有上诉，所以说现在还没有一个统一的意见。今天这个案子里，北京互联网法院就认为，AI 生成的图片是有版权的。但是，北京互联网法院它是个基层法院，判决的效力有限，不好说能不能全国推广。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：那 </span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 生成的图片所拥有的权利很模糊啊？</span></p><p>&nbsp;</p><p>邓超：也不能说很模糊，它有一个判断的过程。判决也强调了，<strong>利用人工智能生成的内容是否构成作品，需要视个案情况而定，不能一概而论。</strong>在这个案子里，这张图片的生成涉及了上百个提示词和参数的调整，在这种情况下，法院会认可这个创作者是付出了一定的创造性劳动的。相反，假设只是跟 AI 说「画一幅秋天的风景画」，那这个在我看来就不能有版权了，因为没付出什么符合著作权法要求的创作。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：可是，这些为创作而输出的参数和设定，多少算多，又多少算少呢？</span></p><p>&nbsp;</p><p>邓超：是的，这里恰恰就是判断的难点，可能在每个案子里，每个法官的判断都会不一样。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：有律师认为，</span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 作图跟传统的美术创作有很大的区别，原告只输出了提示词，并没有动笔去画具体的线条，也没有百分之百地告知 Stable Diffusion 模型怎样去画出具体的线条和色彩，就通过人工智能生成了图片。</span><span style="color:#245bdb"><strong>根据《著作权法》的传统理论，人创作的作品才受版权保护，原告李昀锴的行为并不构成创作。</strong></span><span style="color:#245bdb">你怎么看？</span></p><p>&nbsp;</p><p>邓超：是的，这种反对的观点还挺普遍。做个类比，我让一位画家画一幅秋天的风景画，但我不能在画家画完了之后说「我是这幅画的作者」。这是不合理的。</p><p>还有一种反对观点，它的逻辑是：即使我说了「画一幅秋天的风景画」这句话，不同的人他会画出不一样的东西，不能因为我说了这句指令，我就拥有了画的著作权，这是很奇怪的。像秋天的风景这种东西，本来就是一千个人一千种画法，即使是 AI ，不同的 AI 画出来的也不一样。甚至相同的 AI ，你让它再画一遍，都不一定画得出来。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：即使是相同的参数，它也不一定会生成同样的图。</span></p><p>&nbsp;</p><p>邓超：是的，没错。但是这个类比，只能放在人身上，放在 AI 身上就不太一样，因为 AI 它是一个机器，不是具体的人，是没有权利的。如果这幅图是我让一个人来画的话，那著作权肯定就属于画画的那个人了。但 AI 它不是人，它没法成为作者。<strong>现阶段，生成式人工智能模型不具备自由意志，无论是国内还是国外，都不支持 AI 成为法律上的主体。</strong></p><p>这也是法院在这个判决中做出的考量。目前来讲，无论是认为 AI 作品应该有版权还是不应该有版权，其实都能解释得通，因为 AI 这东西是新兴的，法律比较落后，现在还没有达成一致说这种情况应该怎么办？所以，法院现在判决的时候，更多是从产业或者立法目的的角度，去解释说：这 AI 是把它保护起来比较好，还是不保护更好？</p><p>在这个案子里面，原告为了生成这张图片，输入了大量的指令，调整了上百个参数，如此正向反向的一系列约束下来，才得到这张图。这个过程，在法院看来，是有一定的独创性的。咱们国家法律规定的独创性很低。比如说摄影作品，摄影师等了一天安排了特定角度拍出来的图，大家会认为有独创性；但咱们普通人随便拍的图，一般来讲也是有独创性的，即使你没调参数，也没啥构思。所以说，<strong>独创性，这东西的门槛就很低，不需要达到什么专业高度就能拥有。</strong></p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：可是拍照这事儿是我们自己动手拍的，不是别人告诉我怎么拍的。</span><span style="color:#245bdb"><strong>而且用 </strong></span><span style="color:#245bdb"><strong>AI</strong></span><span style="color:#245bdb"><strong> 的话，即使描述得再具体，它也只是文字创作，不是美术创作，</strong></span><span style="color:#245bdb">它对于创作结果是不能预见也无法控制的，这与人们之前使用画笔、绘图软件去作图有很大的不同。</span><span style="color:#245bdb"><strong>它是「文生图」，不是「手作图」。</strong></span></p><p>&nbsp;</p><p>邓超：所以说，这里其实是一个价值取向的问题，给不给版权保护都说得通。</p><p>有一种反对观点认为，比如说「画一幅秋天的风景」这个指令，可以拥有文字性的版权，但这个文字性的版权，不能延伸到它生成的图片，不然以后所有的秋天风景画都侵犯了我的版权了，因为我最先说了这句指令。</p><p><strong>不过，从历史的角度来看，各种形式的创作，要获得版权保护都是一个时间问题。</strong>比如说摄影技术刚诞生的时候，大家觉得摄影也不应该给版权，因为摄影出现之前，大家都是用油画，可能得花几十年学习，才能把风景画得很像。结果摄影技术出来之后，不需要任何绘画功底、也不用学几十年，只需按一下快门，就能得到一幅比你学了十几年画画还要接近真实的画，所以大家都认为它不是艺术，不应该给版权。可是后来你看，随着技术的发展，摄影慢慢地也成为一门艺术了。</p><p>从历史的视角来看，每当有新的事物出现，它总会面临一些争议，比方说要不要给版权？但是我们可以想办法把版权保护做得更好，在保障权利的同时适应新事物的出现。</p><p>&nbsp;</p><span id="OSC_h1_2"></span><h1>02 新情况可以参照现有法律吗？</h1><p><span style="color:#245bdb">开源中国：现在有一种想法是将 </span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 创作类比成互联网产品。比方说程序员在后台输入代码，通过计算机生成了一个 APP，生成了它的各种功能。但是代码不属于美术作品，谁来写都一样，能实现就行，同一个功能背后的代码可能是一模一样的。在 AI 创作中，提示词就相当于创作的「代码」，生成的图片就类似于前台的 APP，那这个提示词是不是也是谁来写都一样呢？还应该有版权吗？</span></p><p>&nbsp;</p><p>邓超：在智力创作这一块，法律要求的门槛很低，毕加索的画有版权，小学生的画也一样有，并不以智力贡献的高低来衡量，只要是你写的就能有版权。</p><p>所以，代码也是可以享有版权的，像软件著作权登记，它登记的就是这个代码。如果你写了一个代码，别人抄袭你的代码，那也是构成著作权侵权的。只不过代码的著作权仅限于这个文字代码本身。</p><p>最开始，把代码当成文学小说来保护，也是有争议的。毕竟文字它没有任何实用的功能，但是代码，它唯一的目的就是实现功能。但是版权法不保护功能，要想保护功能可以用专利。所以最开始把代码视为文学小说就有这么一个不自洽的地方。那么法律后来就规定了，<strong>实现某个功能的唯一或者非常有限的代码，它是不能受版权保护的。</strong>比如说，单独写一个「弹出提示框」，仅仅这么一点东西，是没法被版权保护的。<strong>但要是整个程序，比如像微信，作为一个整体来讲，它就受到版权保护。</strong></p><p>要是拿音乐来类比的话，单个音节，像 do、rei、mi、fa、so 本身是没法保护的，但是你把它编成一首曲子，那这首曲子就能受到版权保护。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：意思就是它得作为整体的一个作品来呈现才能算。</span></p><p>&nbsp;</p><p>邓超：对。但是要说得呈现到啥程度，这其实并没有一个清晰的线。比方说一首歌算，那其中一段算不算呢？这就得到具体案件里面，进行具体分析了。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：人工智能的软件是可以大规模复制的，只要</span><span style="color:#245bdb">算力</span><span style="color:#245bdb">足够，短时间生成海量的图片完全没有问题。如果这都可以获得著作权保护，对整个社会而言，会不会起不到鼓励创新的效果？</span></p><p>&nbsp;</p><p>邓超：是这样，现在从实践来讲，很多图片没法判断是不是 AI 生成的，除了那些手画不好的之外，其实很难分辨。在这种情况下，<strong>咱们默认所有图片都是有版权的就行了，不要在意它是不是 AI 生产的，这跟 AI 没关系。</strong>咱们在用图的时候，就两个原则：你独创的，或者是合法购买许可的，那就用；如果不是，那就别随便用。不能在网上看到一个图片就随手拿过来，这无论是不是 AI 生产的，风险都很大。</p><p>如果说我用 AI 大模型生成的图，跟别人生成的一样，比方说一样的提示词下 AI 给我们都出了一样的图，那也没关系，因为这是你独立创作的，就可以避免法律风险，哪怕对方来起诉也不怕。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：就是说</span><span style="color:#245bdb"><strong>我们用同一个 </strong></span><span style="color:#245bdb"><strong>AI</strong></span><span style="color:#245bdb"><strong> 同一套提示词生成的两张图，我们各自分别都有著作权，哪怕这两张图一模一样？</strong></span></p><p>&nbsp;</p><p>邓超：是的，因为咱们讲的是独创性，只要这是你独立完成的，那就没有问题。比如说，两个人同时拍天安门广场升旗的图片，可能俩人的角度非常像，拍出来的图片几乎一样，在这种情况下，各自都有著作权，没有任何人侵权。但如果真到了法庭那个阶段，你就得拿出证据证明这图确实是我拍的，不是我扒的。像这个案子里，被告把原告图片水印都截掉了，那肯定是在网络上扒的，肯定侵权。但如果被告的时候，能把原图或提示词都拿出来，证明确实能生成这图，那被告侵权就不成立了。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：但如果要重现的话，万一输入同样的提示词，</span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 生成的图不一样，那不就没法自证了？</span></p><p>&nbsp;</p><p>邓超：理论上是这样，但另一方面来看，这些图片侵权的案子，判赔额都很低，这个案子判赔 500，其他案子更少，一般就几十块钱，风险很小。如果是自创图片的话，虽然不一定保留有创作过程，但肯定会留痕，实在不行还能找证人出庭作证，总之肯定能查清，所以对现状不会有什么颠覆性的影响。</p><p>&nbsp;</p><span id="OSC_h1_3"></span><h1>03 AI 创作，风险大大的有</h1><p><span style="color:#245bdb">开源中国：说到独创性，</span><span style="color:#245bdb"><strong>目前很多的 </strong></span><span style="color:#245bdb"><strong>AI</strong></span><span style="color:#245bdb"><strong> 图片，都是用别人现有的图片「溶」出来的，这样也能算独创性吗？这不算抄袭吗？</strong></span></p><p>&nbsp;</p><p>邓超：当前刚好有一个案子：2023 年 12 月 27 日，《纽约时报》起诉微软和 OpenAI，宣称报社数百万篇文章被用作 AI 的训练数据，这些数据的投喂到底是属于合理使用还是版权侵权，现在也没有定论，可以密切关注一下。</p><blockquote><p>「合理使用原则」是指，虽然你的行为严格来说算侵权，但你的行为是一种可以接受的借用，用来促进创造性的表达。例如，学者可以在自己的作品中引用摘录他人内容；作者可以出版改编图书；普通人可以截取电影片段做影评。换句话说，如果对版权限制过死，文明的创造力将可能停滞。</p><p>科技公司长期利用这一原则来规避版权争议。2013 年，谷歌因为复制数百万册图书并在线上传书里的片段，遭到作家协会的起诉，法官基于合理使用原则，裁决谷歌这一行为合法，因为它为公众创建了可搜索的索引，创造了公共价值。在大模型时代，合理使用原则仍可能发挥关键作用。支持 AI 不侵权的人认为，<strong>大模型生成内容的过程，跟人类创作相差无几——当你尝试画一幅画或拍一支视频，你的脑海里也会有你看过的画或电影。人类的创作在前人的基础上进步，大模型也是如此。</strong></p></blockquote><p><strong>现实总是领先法律一步：现实里出现问题了，法律再进行回应，现在刚好卡在了没达成一致意见的时候。</strong></p><p><strong>从本质上来讲，著作权法、</strong><strong>知识产权</strong><strong>法的目的都是为了促进社会的繁荣。</strong>像远古时代，没有什么知识产品，宋代印刷术出现之前，大家都是用手抄本，但大家都认为这是好事，可以防止文献失传。印刷术出现之后，才慢慢有了著作权。立法者认为，对作品作者给予一定保护，可以激发其他人创作出更多的作品来。现实也的确是这样，发达国家都有着非常完善的知识沉淀。所以本质来讲，还是看怎么选择更能促进产业和整个社会的发展。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：说到促进创作，这里我要讲一下，</span><span style="color:#245bdb"><strong>在美术界，相似度达到 10% 就属于抄袭了，但是 </strong></span><span style="color:#245bdb"><strong>AI</strong></span><span style="color:#245bdb"><strong> 现在是在不声明的情况下抄袭，并且不以任何代价使用，这不就跟促进创作的初衷刚好相反了？</strong></span><span style="color:#245bdb">如果一做出来就会被抄袭，那不正好打击了人们创作的积极性，挤压人的创作空间吗？</span></p><p>&nbsp;</p><p>邓超：对，是这样。<strong>任何一个东西，政策的影响都是两方面的，</strong>你说的是负面的一方。但从正面来讲，以前创作图片，需要美工先学十年美术，美工画一幅图要画一天。但现在，任何一个没有学过美术的人，也可以创作出一幅画了。从这方面来讲，生产力肯定是大大提高了，AI 一天生成的图就能顶 100 个美工的创作。当然，最后肯定还需要微调，但这生产率已经翻倍了。</p><p>假设现在一个公司需要 100 张图，如果找美工，成本是十万块钱；但找一个 AI 的话，可能就 1000 块钱，加上后期人工修改可能只需一万。从公司的角度来讲，它的成本就降低了，变得更有竞争力了，但背后可能就有十个美工失业了。可是从另外一个角度来讲，可能又会有十个 AI 相关的就业岗位增加。</p><p>所以 AI 这个东西，它就像蝴蝶效应一样，一个因素改变带来的社会变化非常复杂，有人受益，就会有人受损，所以这里很难预测，也不是我一个人能想明白的事。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：在风险这块，举个例子：假设我用 </span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 做图的时候，只输了文字</span><span style="color:#245bdb">指令</span><span style="color:#245bdb">，但 AI 在训练的时候被喂了有版权的图片，所以</span><span style="color:#245bdb"><strong>产出给我的图片也侵权了，那是我负责还是 AI 公司负责？</strong></span></p><p>&nbsp;</p><p>邓超：按照咱们讨论的这个案子来讲，肯定是你负责，因为 <strong>AI</strong><strong> 它只是一个工具，就跟画笔或者照相机一样，最后出问题可能还是得人来负责。</strong>除了我们刚才说的「纽约时报诉 OpenAI 案」，因为 OpenAI 是在训练的时候使用了纽约时报的内容，所以要 OpenAI 负责。但是现在 OpenAI 已经把产品软件提供出来了，你用这个 AI 软件生成的图片，如果侵权的话，（虽然现在还没有案例），AI 工具它大概率是没有责任的，责任在你。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：可是我本意没有想要侵权任何人，也不是我让这个 </span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 去抄袭别人的，甚至我都不知道别人作品的存在，是 AI 公司训练的时候喂了有版权的图导致的生成作品侵权，这种情况我是没法控制的，也需要我来负责吗？</span></p><p>&nbsp;</p><p>邓超：确实，你问的这三个问题，是 AI 时代最棘手的三个问题，现在都没有人有答案：</p><p><strong>第一，</strong><strong>大模型</strong><strong>使用海量数据来进行训练，这种行为到底是侵权还是合理使用？</strong></p><p><strong>第二，</strong><strong>AI</strong><strong> 生成的作品到底有没有版权？（今天讨论的这个案子里北京互联网法院已经初步给出了意见）</strong></p><p><strong>第三，用 </strong><strong>AI</strong><strong> 生成的作品侵权了究竟谁来承担责任？</strong></p><p>这三个问题现在没人能有答案，我刚才说的，只是我个人的观点。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：那意思就是说，有好处的话，是使用者获得，如果是风险的话，也是使用者承担。那 </span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 公司它岂不是只负责赚钱不承担风险？</span></p><p>&nbsp;</p><p>邓超：简单来说是这样，但具体的还得结合具体案情来讲。</p><p>企业的风险就是我们说的第一个问题：用别人的内容来训练 AI 模型，属不属于合理使用？这时候的政策是很敏感的。假设这算合理使用，企业风险很小，那一定程度上会促进 AI 产业的发展，因为企业没啥后顾之忧了；但如果让企业承担很大的责任，那一些刚发展起来的小公司搞不好会因为法律诉讼破产，这样就阻碍了行业发展。法律也好，判决也好，肯定都是要考虑产业发展的。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：但是也不能光揽好处，把风险甩给用户去跟原作者扯皮吧？</span></p><p>&nbsp;</p><p>邓超：对，但是有人受益，肯定就有人受损，这个不太可能双赢，就看你价值取向更偏向谁。比如产业早期的时候，可能倾向于促进产业发展；等产业发展成熟了，公司壮大了，还一点责任都不承担就不太好了。不同阶段，产业政策不一样，法律方面的话，至少在我看来，裁判者也好，立法者也好，都是需要平衡的。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：伴随着人工智能、大语言模型的发展，</span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 创作这种生产方式，还可能会触及哪些法律问题？</span></p><p>&nbsp;</p><p>邓超：除了上面讲到的三点，还有就是<strong>可能会侵犯别人的肖像权</strong>，比方说生成的人像跟别人长得一样，可能就会侵犯肖像权，这也属于第三点中侵权的一种形式吧。</p><p>&nbsp;</p><p><span style="color:#245bdb">开源中国：法律届是怎么看待 </span><span style="color:#245bdb">AI</span><span style="color:#245bdb"> 创作这一新的生产模式的？接下来会不会出台相关的法律法规呢？</span></p><p>&nbsp;</p><p>邓超：按照过往的历史经验来看，不会这么早出台法律。以往像互联网之类的新事物，法律界都会先让子弹飞一会儿，先观察一阵，等到时机成熟了，再出台相关法律法规。另外，咱们现在的知识产权法的框架，足以解决目前跟 AI 相关的一些问题，还没有必要单独针对 AI 立一个法。可能等时机成熟了，会完善法律下面的一些解释或规定吧。</p><hr><p>对此，各位怎么看？你认为 AI 作品应该有版权吗？我们评论区见吧~</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><strong>本期嘉宾：</strong>邓超 | 律师（微信号：dengchao）</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img height="560" src="https://oscimg.oschina.net/oscnet/up-0fa35e5b407c4ce45828bd4c6e0c8078989.png" width="400" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">具有理学学士和法学博士学位，有着深厚的法学理论基础；在知识产权行业从业十余年，有着丰富的实践经验。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">长期专注于科技和媒体领域的知识产权前沿问题的研究和实践，代表客户处理相关的诉讼和非诉法律事务。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">在进入律师事务所之前，曾就职于世界 500 强公司的法务部以及国内顶尖的知识产权事务所，为众多跨国公司、国内上市公司等提供知识产权法律服务。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 08:00:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6852546/blog/10992795</guid>
            <link>https://my.oschina.net/u/6852546/blog/10992795</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[重磅！AutoMQ 品牌全新升级]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>2024 年伊始，</p><p>AutoMQ 作为一家专业的消息队列和流存储软件服务供应商，</p><p>正式发布全新的 logo 和官网与大家见面！</p><p>一直以来，</p><p>我们致力于引领消息和流存储走向云原生时代！</p><p>希望通过这次品牌升级，</p><p>不断创新，为大家提供更先进的产品和服务！</p><p><strong>LOGO 升级</strong></p><p><img alt="" src="https://image.automq.com/%E5%AE%98%E7%BD%91/20231214-171408.png" referrerpolicy="no-referrer"></p><p>新 logo 以"M"字母为核心设计元素，融合"A"字母，不仅象征着 AutoMQ 的名字，更代表了我们对核心业务和理念的坚持。AutoMQ 正是以云原生技术理念，对消息队列和流存储领域进行深刻创新，致力于通过自动伸缩、自动运维和自动优化的技术，为客户兑现云的普惠价值。</p><p>我们的品牌形象以流畅的线条和波浪效果为特征，象征着 AutoMQ 助力企业驾驭规模日益庞大的数据流，确保数据传输的高效和稳定。在视觉上，全新品牌选择了充满未来感的科技紫作为主题色彩，它不仅代表了我们追求技术卓越的决心，也反映了我们以产品为驱动，不断前行的发展理念。</p><p><strong>官网升级</strong></p><p><img alt="" src="https://image.automq.com/%E5%AE%98%E7%BD%91/%E5%AE%98%E7%BD%91%E6%88%AA%E5%9B%BE.jpeg" referrerpolicy="no-referrer"></p><p>新官网链接：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.automq.com" target="_blank">https://www.automq.com</a></p><p>新官网坚持技术开发者友好的设计理念，以可量化的数字结论，从<strong>成本、稳定性、弹性和兼容性</strong>等方面阐述了 AutoMQ 带来的技术优势和客户价值。<br> 新官网全新设计了<strong>博客、文档中心、产品、定价详情页面</strong>，帮助开发者快速了解 AutoMQ 的产品功能和参考资料。<br> 最后，值得一提的是官网的构建过程也使用了 Webflow 低代码设计开发平台。整个开发过程无需前端工程师介入，由设计师直接完成设计上线，并且可以做到随时快速迭代。</p><p>欢迎关注我们，学习更多技术干货！同时，也诚邀各位开源爱好者持续关注我们社区，点亮小星星🌟，跟我们一起构建云原生消息中间件！</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 07:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276906</guid>
            <link>https://www.oschina.net/news/276906</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Wok-Server —— 轻量级 Nodejs 后端框架]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="margin-left:0px; margin-right:0px; text-align:left">Wok-Server 是一个简洁易用的 Nodejs 后端框架，使用 Typescript 开发，有完整的类型约束和定义，注释详细，文档齐全，支持国际化。</p><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">主要功能：配置，日志，国际化，校验，缓存，MVC，mysql，mongodb ，周期任务 。</p><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left"><a href="https://gitee.com/tai/wok-server/blob/master/documentation/zh-cn/index.md">查看文档</a></p><h2 style="margin-left:0; margin-right:0; text-align:left">优点</h2><ul><li>学习成本较低，函数式为主，少量面向对象，未使用代理和装饰器等增强技术</li><li>功能简单，使用方便，保持克制，不引入太多特性</li><li>轻量封装，最大限度兼容已有生态，支持集成第三方 http 相关的库来处理请求</li><li>支持国际化，自带多种语言的支持，国际化内容支持扩展</li><li>有完整的类型约束和定义，结合 IDE 的代码补全功能，开发效率高</li><li>注释详细，文档就在代码中，方法和参数都有详细说明，可在 IDE 的辅助下方便查看</li></ul><h2 style="margin-left:0; margin-right:0; text-align:left">Hello world</h2><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">安装：</p><div style="text-align:left"><div><pre><span>npm i wok-server --save</span></pre><div style="text-align:center">&nbsp;</div></div></div><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">入口文件：</p><div style="text-align:left"><div><pre><span><strong style="color:#000000">import</strong><span>{</span><span>startWebServer</span><span>}</span><strong style="color:#000000">from</strong><span>'</span><span style="color:#dd1144">wok-server</span><span>'</span></span><span><span>startWebServer</span><span>({</span></span><span><span style="color:#008080">routers</span><span>:</span><span>{</span></span><span><span>'</span><span style="color:#dd1144">/</span><span>'</span><span>:</span><strong style="color:#000000">async</strong><span>exchange</span><span>=&gt;</span><span>exchange</span><span>.</span><span>respondText</span><span>(</span><span>'</span><span style="color:#dd1144">Hello world !</span><span>'</span><span>)</span></span><span><span>}</span></span><span><span>}).</span><strong style="color:#000000">catch</strong><span>(</span><span>e</span><span>=&gt;</span><span>{</span></span><span><span>console</span><span>.</span><span>error</span><span>(</span><span>'</span><span style="color:#dd1144">Start server failed</span><span>'</span><span>,</span><span>e</span><span>)</span></span><span><span>})</span></span></pre><div style="text-align:center">&nbsp;</div></div></div><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">上面的代码启动了 web 服务，访问路径<span>&nbsp;</span><code>http://localhost:8080</code><span>&nbsp;</span>将输出文本 「Hello world !」。</p><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">相关的设置可以通过环境变量来修改，查看<a href="https://gitee.com/tai/wok-server/blob/master/documentation/zh-cn/index.md">完整文档</a>了解细节。</p></div>
                                                                ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 06:37:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/wok-server</guid>
            <link>https://www.oschina.net/p/wok-server</link>
        </item>
        <item>
            <title>
                <![CDATA[Jina AI 开源中英和英德双语 8K 向量模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">在 Jina Embeddings 英语向量模型突破百万下载后，Jina AI <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FT-2EB1LfZKhueidIGW1x0w" target="_blank">宣布</a>又正式开源了两款双语向量模型：中英双语（Chinese-English）和英德双语（English-German）向量模型，这也是全球首次推出支持 8K 双语文本的开源向量模型。</span></p><h4><span style="color:#000000"><strong>技术亮点</strong></span></h4><p><span style="color:#000000"><strong>8K 输入：长文本处理更得心应手</strong></span></p><p><span style="color:#000000">在 RAG 应用里，文本就像被切成了多个块，通过 Embedding 模型变成向量，然后存进数据库。当你搜索时，系统会把这些文本块的向量和你的搜索词比对，找到最匹配的文本。</span></p><p><span style="color:#000000">传统模型最多只能处理 512 Token 的文本，面对更长文本时，就力不从心，尤其是预测的时候。</span></p><p><span style="color:#000000">但 Jina Embeddings v2 能处理高达 8k 的输入，能够实现从实体、句子、段落到整个篇章的多粒度向量表示。实际应用的时候，可以将这些不同层次的向量结合起来，实现更为精准的匹配。此外，也支持按语义切割，获得更佳的搜索效果。</span></p><blockquote><p><span style="color:#000000">8K token，也就是说一整版人民日报的内容可以压缩成一个向量！</span></p></blockquote><p><span style="color:#000000"><strong>技术创新：JinaBert 架构</strong></span></p><p><span style="color:#000000">Jina Embeddings 系列模型均基于 Jina AI 自研的 JinaBert 架构，这是首次将 ALiBi 应用到编码器上，该架构专为长文本任务优化，直接在 Attention 层处理位置信息，让模型更准确地捕捉词语间的关系。就算是超长文本，也能轻松应对。</span></p><p><span style="color:#000000">这一技术创新让模型在处理长文本时更加强大，也为 RAG 应用带来了更多可能性。无论是解读法律文件、研究医学文献、还是文学分析，Jina Embeddings 系列模型都表现出色，任务的准确率和效率都大大提升。</span></p><p><span style="color:#000000"><strong>双语支持：无缝跨语言交互</strong></span></p><p><span style="color:#000000">双语模型能把中文（或德文）和英文映射到同一个向量空间。即使是不同语言，只要含义相近，它们在向量空间里就很接近。</span></p><p><span style="color:#000000">尽管很多模型也声称支持多种语言，但由于英语在互联网上的主导地位，以及训练阶段大量使用机器翻译文本，这些模型往往对某些人群、主题或话题存在偏见，其影响会在下游任务中进一步放大。Jina AI 则专注于优化 2 种语言的向量表征，严格把关训练数据，力求把偏见降到最低，并确保性能超越那些支持多语言的大型模型。</span></p><p><span style="color:#000000">用 Jina Embeddings，不管是在 RAG 聊天机器人里，还是给文本分类、做摘要、分析情感，都能把不同语言的文本当作同一种语言来处理，让多语言应用的构建变得无比丝滑。用中文搜中文资料，或者中文搜英文、英文搜中文，都能得到又准又顺的结果。Jina Embeddings 助力您打造和全球用户无障碍沟通的新时代！</span></p><p><span style="color:#000000"><strong>MTEB 排行榜：性能领先</strong></span></p><p><span style="color:#000000">Jina Embeddings v2 系列模型在 MTEB 排行榜上，在文本分类、检索、重排、摘要等任务上均有优势。并且输出和 OpenAI 完全一致，是 OpenAI ada 002 模型的理想替代开源解决方案。</span></p><p><span style="color:#000000">jina-embeddings-v2 的双语模型以 322MB 的轻巧体积（包含 1.61 亿参数），输出维度为 768，能够在普通计算机硬件上高效运行，无需依赖 GPU，极大地提升了其实用性和便捷性。</span></p><p><span style="color:#000000">最近，在 Standford HAI 发布的 LoCo 性能测试中，Jina Embeddings 同样名列前茅。</span></p><p><img height="221" src="https://oscimg.oschina.net/oscnet/up-5db7acf49ccda5e5a0aba0d4098ee18b6b6.png" width="500" referrerpolicy="no-referrer"></p><p><img height="135" src="https://oscimg.oschina.net/oscnet/up-789c9393a0fb12fa9b322e8af83f1b0e6c0.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000"><span style="background-color:#ffffff">Jina</span> AI 方面表示，未来将持续扩展多语言 8k 向量模型家族，支持更多语言。</span></p><ul><li><span style="color:#000000">中英双语向量模型：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fjinaai%2Fchinese-english-embeddings" target="_blank">https://huggingface.co/jinaai/chinese-english-embeddings</a></li><li><span style="color:#000000">英德双语向量模型：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fjinaai%2Fenglish-german-embeddings" target="_blank">https://huggingface.co/jinaai/english-german-embeddings</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 06:27:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276902</guid>
            <link>https://www.oschina.net/news/276902</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Milk-V Vega: 全球首款 RISC-V 万兆开源交换机正式发售]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>MilkV 于去年 8 月亮相了「全球首款开源万兆 RISC-V 网络交换机 Milk-V Vega」。时隔数月，该产品现已上架官方淘宝店开售，定价 799 元。</p><p><img src="https://oscimg.oschina.net/oscnet/up-dc8f8d1b6ca91d1342e4484e8e267513a9c.jpg" referrerpolicy="no-referrer"></p><p>据介绍，Milk-V Vega 是 Milk-V 面向新一代网络架构开发的一款小体积、高密度盒式开源万兆网络交换机，为用户提供宽带、语音、视频、监控等多业务统一承载平台，多网融合覆盖全业务；搭载国产 RISC-V 高可靠网络交换芯片，帮助企业与用户大幅简化网络架构，降低能耗成本与运维成本；可广泛应用于数据中心、园区、大中型企业网络、酒店、科研教学等场景。 &nbsp;</p><p>Milk-V Vega 支持完备的二层网络协议处理功能，包括 L2 桥接，L2 组播和风暴抑制等；支持基于流、端口、协议、子网的 VLAN 功能；支持 STP，RSTP 和 QinQ 功能；支持防 DOS 攻击，黑白名单和协议包滤除功能；支持滤镜，链路聚合，OAM 报文发送和端口保护功能；支持入口及出口的 ACL 功能，还支持同步以太网，1588 功能。</p><p>同时 Milk-V Vega 作为全球首款 RISC-V 开源万兆交换机，为开发者提供了以下特性：</p><ul><li>基于开源 Linux 系统，方便二次开发及 DIY</li><li>开放 openSBI，u-boot，Linux kernel 源码，提供交叉编译工具链</li><li>提供 JTAG 接口，方便底层开发</li><li>提供 RS232 及 I2C 接口，方便外接其他传感器</li><li>支持 1U 标准机柜，每一层机柜可安装 2 台 Milk-V Vega</li><li>双万兆光口 + 4 千兆光口 + 8 千兆电口，满足家用 Homelab 需求</li><li>支持远程配置，提供 WEB 和 SSH 访问</li><li>提供命令行业务配置，实现 VLAN 配置，端口镜像，MAC 地址学习，流量控制等功能</li><li>提供接口 SDK 和 API，灵活实现业务配置</li></ul><p><img height="1922" src="https://oscimg.oschina.net/oscnet/up-18941523fbbd18a8e49875ecd1672e1a1a8.jpg" width="1080" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-553d000aeabc9e13f63432542706d102e13.jpg" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-c56ea9be1224ad7658d518aedd13b7c0dea.jpg" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-dd5e9f39bbba69c1b283082faf3484443da.jpg" referrerpolicy="no-referrer"></p><p><strong><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FsFzKJKkRKq3q9DYPtpm2_Q" target="_blank">https://mp.weixin.qq.com/s/sFzKJKkRKq3q9DYPtpm2_Q</a></u></em></strong></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 06:19:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276901</guid>
            <link>https://www.oschina.net/news/276901</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[npm 被滥用 —— 有人上传了 700 多个武林外传切片视频]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Sonatype 安全研究团队近日<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.sonatype.com%2Fnpm-flooded-with-748-packages-that-store-movies" target="_blank">介绍</a></u>了一起滥用 npm 的案例——他们发现托管在 npm 的 748 个软件包实际上是视频文件。</p><p>据介绍，这些软件包每个大小约为 54.5MB，包名以 「wlwz」 作为前缀，并附带了应该是代表日期的数字。时间戳显示，这些包至少自 2023 年 12 月 4 日起就一直存在于 npm，但 GitHub 上周已经开始删除。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-4206ec7f1e46a5d3841a266e37293bd8f7b.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-bccdf9a35b4ae1bd8a8920c57f87e5ceeb7.png" referrerpolicy="no-referrer"></p><p>每个包中都有以「.ts」扩展名结尾的视频剪辑，这表明这些视频剪辑是从 DVD 和蓝光光盘中翻录的。</p><blockquote><p>这里的 ts 不是 TypeScript 文件，而是 transport stream 的缩写，全称为 「MPEG2-TS」：</p><p>MPEG2-TS 传输流（MPEG-2 Transport Stream；又称 MPEG-TS、MTS、TS）是一种标准数字封装格式,用来传输和存储视频、音频与频道、节目信息，应用于数字电视广播系统，如 DVB、ATSC、ISDB[3]:118、IPTV 等。</p></blockquote><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-dd32d5c3ad016f5c5ba20f9ec67e9f04a3f.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-85ee088f5f36f3c27c4bb43e72b3c6271ec.png" referrerpolicy="no-referrer"></p><p>此外，某些包（例如「wlwz-2312」）在 JSON 文件中包含普通话字幕。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-37361519bbcf2b7a95d47a0a2451c0e42aa.png" referrerpolicy="no-referrer"></p><p>虽然这些视频不会像挖矿程序、垃圾邮件包和依赖性恶意软件那样毒害社区，但这种把开源基础设施当 CDN 的操作无疑是破坏了规则，也违反了供应商的服务条款，各位耗子尾汁吧。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 03:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276893/npm-flooded-with-748-packages-wlwz</guid>
            <link>https://www.oschina.net/news/276893/npm-flooded-with-748-packages-wlwz</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源小谈，赠现代卧龙、凤雏；创造三赢的方案！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h3><span>前言</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>在 </span><span><a href="https://www.oschina.net/comment/news/276508"><span>ioGame17 文档或将强制收费</span></a></span><span>文中引起了一些讨论，这些讨论更多的是理念上的分歧。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>文中内容简要如下</span></p><blockquote><p style="margin-left:.8em; margin-right:0"><span>近期 ioGame 将要升级到 21 版本，提醒开发者保存好 ioGame17 的文档。</span></p><p style="margin-left:.8em; margin-right:.8em">&nbsp;</p><p style="margin-left:0; margin-right:.8em"><span>开发者如果不打算升级到 ioGame21 的，请提前私下保存好 ioGame17 相关文档。ioGame21 发布后，在线文档的内容则与新版本对应。而旧的文档（ioGame17）将从自愿付费模式转为强制付费文档。如果不打算支付这部分文档费用，请私下保存一份自留使用，但不得以任何形式传播及公开。</span></p></blockquote><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>整体内容并不多，意外的是在讨论区中惊现各种现代化的卧龙、凤雏、五星评论员、如果哥、指点江山哥、慷他人之慨...等，之流。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>由于精力有限，将不对之前的文档、旧的版本进行维护。旧的文档如有需要，可付费获取；如不想付费，请私下保存一份自留使用，但不得以任何形式传播及公开。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>之所以不提供旧文档的阅读及旧版本的维护，是因为 ioGame 开源作者将精力放到了最新的 ioGame 版本上，并提供了与之对应版本的在线文档阅读（不需要登录，没有辣眼的广告，可阅读所有文档）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>但似乎现代化的卧龙、凤雏、五星评论员、如果哥、指点江山哥、慷他人之慨...等，之流，想让开源作者继续免费提供旧文档，并无条件的维护旧版本。（不知道让 spring 团队维护 1.x 版本的，会得到什么样的回复）</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 开源作者在阅读 </span><span><a href="https://www.oschina.net/comment/news/276508"><span>ioGame17 文档或将强制收费</span></a></span><span>讨论区相关的内容后，找到了能够完美的解决这些矛盾的三赢方案。本篇内容不多，大体如下</span></p><ul><li><p style="margin-left:.5rem; margin-right:0"><span>理念冲突（不同）</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>可持续性发展</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>当前开源环境</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>开源策略</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>用户流失？不，这是三赢！</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>比较优势</span></p></li></ul><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>理念不同</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><img height="398" src="https://oscimg.oschina.net/oscnet/up-336b076ff70f355b497e0040cf1b8a333cb.png" width="700" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>当今，在制度理念上可大致分为两种，GC 主义和资本主义。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 源码使用的是 AGPL 协议，之所以选择该协议是因为，就目前来说其更符合 GC 主义；GC 主义的经济制度类型为</span><span><strong><span>生产资料公有制</span></strong></span><span>，而资本主义则是</span><span><strong><span>生产资料私有制</span></strong></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>使用 ioGame 是免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；同时，ioGame 是允许私有的，前提是支付闭源授权费。所以，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。</span></p><blockquote><p style="margin-left:0; margin-right:0"><span>ioGame 使用的开源策略是：两开一免一自主。即源码完全开放、最新文档阅读完全开放、使用完全免费、开发者可自主选择按需付费。</span></p></blockquote><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>为什么需要闭源授权费</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>注意，这里仅指闭源授权，也就是你的产品不需要对外开源（可将生产资料私有制）；除此之外，并不会提供更多的其他服务。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>之所以需要授权费，是因为我们为产品附加了价值。《资本论》中著名的公式：商品价值 = c+v+m，即商品价值 = 不变资本+可变资本+剩余价值。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>c+v+m 可以表示社会总产值，也可以表示某个商品的价值，或某一单位时间内生产的商品的总价值。简单的说 c 表示的是生产资料价值，v 表示的是劳动力的价值，m 表示的是剩余价值</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>为了易懂，这里举个不恰当，但意思差不多的公式。商品价值 = 你为产品迭代的劳动力 + 你的日常开销（自身教育及小孩教育的支出、维持动者本人及家属生存所必需的生活资料、房租房贷...等）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>现在，关于闭源授权费这部分也说清楚了；之所以有闭源授权费，是因为我们为产品附加了价值。这里再重复一次，使用 ioGame 是免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；同时，ioGame 是允许私有的，前提是支付闭源授权费。所以，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>资本主义理念的小算盘是，即希望免费使用你的产品，又希望你免费维护，同时又不想将自身的产品成果公开出来（只想生产资料私有制）。换句话说就是，又想马儿跑，又不给马吃草，因为这一行为很符合资本主义。这里称其为</span><span><strong><span>自以为聪明的资本家</span></strong></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>自以为聪明的资本家</span></strong></span><span>的基本特征是：你这产品不错、你这框架不错、你这项目不错，应该免费；同时，这群家伙还擅长慷他人之慨，真让他们参与付出时，结局是</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.bing.com%2Fsearch%3Fq%3D%25E7%259C%259F%25E6%259C%2589%25E4%25B8%2580%25E5%25A4%25B4%25E7%2589%259B%26qs%3Dn%26form%3DQBRE%26sp%3D-1%26lq%3D0%26pq%3D%26sc%3D0-0%26sk%3D%26cvid%3D9BE3D9C25BDF45FA8341F1641F100333%26ghsh%3D0%26ghacc%3D0%26ghpl%3D" target="_blank"><span>真有一头牛</span></a></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>可持续性发展</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>经济基础是一切可持续性发展的前提，开源亦是如此。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>如大家所熟悉的 apache 组织、srping 等...各开源巨头，都是有基金会或其他经济支持的；也有一些半开源的产品会寻求一些资本的注入，其目的也是为了得到经济支持。所以，经济基础是一切可持续发展的前提。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>大部分开发者认为开源软件是完全免费的，也应当完全免费。造成这些错觉是因为常年使用开源巨头所提供的软件，但又忽略了背后基金或类似基金的支持。注意，这里说的类似基金指的是间接或直接所获得的收入。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>资本家是恐怖的，国外的资本家们取得收益后，或多或少的会向基金会捐赠，以保持相互的良性循环。而</span><span><strong><span>自以为聪明的资本家</span></strong></span><span>们可不这么想，因为他们并懂可持续性发展的重要性。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>这群</span><span><strong><span>自以为聪明的资本家们</span></strong></span><span>秉持着希望免费使用你的产品，又希望你免费维护，同时又不想将自身的成果公开出来（只想生产资料私有制）。现在，我们知道使用 ioGame 是免费的了，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；当</span><span><strong><span>自以为聪明的资本家们</span></strong></span><span>知道要他们把私有资料公开时，他们就会给你上演</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.bing.com%2Fsearch%3Fq%3D%25E7%259C%259F%25E6%259C%2589%25E4%25B8%2580%25E5%25A4%25B4%25E7%2589%259B%26qs%3Dn%26form%3DQBRE%26sp%3D-1%26lq%3D0%26pq%3D%26sc%3D0-0%26sk%3D%26cvid%3D9BE3D9C25BDF45FA8341F1641F100333%26ghsh%3D0%26ghacc%3D0%26ghpl%3D" target="_blank"><span>真有一头牛</span></a></span><span>的故事。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>到这，已经说清楚了经济基础是一切可持续发展的前提。同时，也区分了真正的资本家与</span><span><strong><span>自以为聪明的资本家</span></strong></span><span>。真正的资本家是恐怖的，他们懂得可持续性发展的重要性，并壮大着这些开源组织；而</span><span><strong><span>自以为聪明的资本家们</span></strong></span><span>并不懂得这些，</span><span><strong><span>滑稽</span></strong></span><span>的是他们还喜欢充当所谓的网络侠客（并不具备事物分析能力的人，大部分的发言是臆测主观性的）；</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>开源环境</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>经济是发展的核心，任何强大的开源组织（开源巨头）都离不开经济基础。注意，这里说的开源组织并不是指你在 github 随意创建一个 organization 后，放入几个项目的那种组织。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>现在，国内有很多人想要往这方面发力，建立国产开源组织；但目前在国内很难实现，具体原因这里就不细说了，因为这部分没有十数万字也讲不清楚，而且大部开发者对这部分也没啥兴趣。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>目前国产开源组织也有一些，但大部分都是几个项目勉强凑合到一起所形成的组织。若想要建立一个可持续性发展的开源组织，就目前的环境来说是不可能的。建立一个可持续性发展的开源组织的基本条件，是需要有基金会或类似基金会的支持。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>在没有得到外部资金的支持时，任何一个开源框架想要有长远的发展，首先需要考虑的是如何盈利。如果没有持续资金的支持，通常很难坚持下去。你可以在 GitHub 上找到很多长时间未更新的开源项目，通常是因为最初的热情慢慢消退，导致不再维护。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>国产项目的开源策略大部分都是采用社区版本和商业版本的方式，其本质目的也只有一个，即维持自身的可持续性发展。同时，这也是开源维护者与开源使用者的矛盾点之一，但这些是符合马克思主义哲学三大规律的。（将来，如非洲、东南亚、西亚、中东 ...等地区的多数国家也会经历这一阶段的发展过程，但还需要数十年。）</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>现阶段，国内开源土壤与环境并不理想，并且还有一群擅长慷他人之慨的网络侠客在添堵。但并无大碍，因为这些小卡拉并不能阻碍，也阻碍不了国产开源的发展。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>ioGame 的开源策略</span></h3><blockquote><p style="margin-left:0; margin-right:0"><span>ioGame 使用的开源策略是：两开一免一自主。即源码完全开放、最新文档阅读完全开放、使用完全免费、开发者可自主选择按需付费。</span></p></blockquote><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>源码：源码完全开放，没有社区版、商业版本之说。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>文档：最新文档阅读完全开放，并采用自愿付费原则（不强制付费）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>使用 ioGame 是免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；同时，ioGame 是允许私有的，前提是支付闭源授权费；所以，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。费用采用年付费制，且费用低（不会超过当年全国最低工资标准的 2 倍，并且可申请盈利后支付）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>或许有开发者会问，国产开源项目大部分都是社区版本（免费）和商业版本（强制收费）的策略。但 ioGame 为什么会采用源码完全开放、文档完全开放、使用完全免费的策略呢？</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>源码完全开放的原因</span></strong></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 推崇</span><span><strong><span>生产资料公有制。</span></strong></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>源码完全开放可以让更好地服务使用者，让使用者更快、更好地完成业务并出产品；同时，这对使用者学习框架源码也比较友好。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>通常（中、小、新）这类公司、团队、独立开发者，在前期是比较缺资金的。我们知道，只有让游戏开发者先盈利，之后想支付费用时才无压力（闭源自身的游戏产品）。（注意，使用 ioGame 是完全免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来）</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>最新文档阅读完全开放，并采用自愿付费的原因</span></strong></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>在你阅读 ioGame 的在线文档时，不会有辣眼的广告，没有登录限制。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 致力于提供高质量的使用文档，在 ioGame21 中，大概有三十多万字（不要认为文档内容多，学习框架成本是困难的，这两者不是对等关系；如果文档只保留一个字，你会认为这是简单的吗？）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>不夸张的说，文档的编写比代码还要困难。如果文档只服务于少部分人，那就有点可惜了。只有完全开放了文档，才能帮助更多的游戏团队、独立开发者及想进入游戏行业的开发者。因为我们知道，只有让开发者先盈利了，他们才更愿意支付费用。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>另一个文档阅读完全开放的原因是，ioGame 推崇</span><span><strong><span>生产资料公有制</span></strong></span><span>（不要理解为版权免费赠送）。需要注意的是，ioGame 的文档不是开源的，以后也不会开源。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>或许有开发者会问到，如果以后你的文档不公开了，又或者说强制收费了怎么办。首先，我们的目标很明确，帮助开发者提升、快速地学习整体框架、加快游戏产品的研发并上线盈利。其次，对基础使用文档收费并无兴趣。再次，我们推崇</span><span><strong><span>生产资料公有制</span></strong></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>此时开发者又会说到，xxx 一开始文档也是公开的，等开发者开始用了，就开始收费割韭菜了，大家都被假开源骗怕了...等之类的话。这里借用凤凰传奇曾毅的一句玩笑话，歌土跟我有什么关系，我才几个词；同样的，你被骗跟我有什么关系，又不是我骗的你。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>另外，要是 ioGame 对基础使用文档有收费的兴趣，也就不会请提前让开发者私下保存一份自留使用了。所以与其他案例是有本质区别的。</span></p><ul><li><p style="margin-left:.5rem; margin-right:0"><span>首先，已经发文提前通知了。</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>其次，允许开发者私下保存一份自留使用，但不得以任何形式传播及公开（ioGame 的文档没有开源，以后也不会开源，著作权归作者所有）。</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>再次，在 ioGame21 发布后，在线文档对应的是当前最新版本；新的在线文档依旧采用自愿付费模式策略，与之前一样，文档是完全开放的（开发者可阅读所有文档，没有任何限制）。</span></p></li></ul><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>使用 ioGame 免费的原因</span></strong></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 推崇</span><span><strong><span>生产资料公有制。</span></strong></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>使用 ioGame 是完全免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；同时，ioGame 是允许私有的，前提是支付闭源授权费（</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fgd5l3b0y0h027kcv" target="_blank"><span>为什么采用授权许可申请？</span></a></span><span>）；所以，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>用户流失？不，这是三赢！</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>或许会有开发者问，为什么使用闭源授权的方式，这样不是会流失用户吗？</span></p><blockquote><p style="margin-left:0; margin-right:0"><span>提示：开源 != 免费。即使是做慈善的组织，组织内工作的人也得吃饭。</span></p></blockquote><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>纯白嫖的用户流失是一定的，但同时也会吸引更多的新用户，因为 ioGame 有了更长远的规划。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGama 是海纳百川的，在文档中我们推荐了很多同行业的游戏服务器框架。因为我们知道，流失的用户与 ioGame 的矛盾点是必然存在的，但 ioGame 对待流失用户是友好的，我们会积极的把这部分流失的用户推荐给竞品框架。这样</span><span><strong><span>既解决了矛盾，又创造了三赢的局面，真是太棒了</span></strong></span><span>！</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>对于用户群体，ioGame 自身有明确的规划，而且也没打算说服所有的开发者来使用 ioGame；所以，我们首先要做的就是排除对自身不适合或者说理念不相同的开发者，只有这样才能抽出更多的时间来服务真实有效使用者；</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>注意，这里的真实有效使用者指的是在项目中使用了 ioGame 的团队、个人、公司...等，并不是特指购买了闭源授权的；因为使用 ioGame 是完全免费的，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>最后，闭源授权的目的是为了社区的长期发展建设，通过闭源授权来营收，后将营收继续投入到社区中，将社区壮大。规划长久的发展，可使双方互利，ioGame 就像一台水果采摘机器，你可以使用这台机器来采摘任何水果，在你取得收获时，希望你能为这台机器加些能量。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>完全开源不怕被参考吗</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 完全开源，就不怕其他开发者参考吗？</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>不怕的，在</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fgd5l3b0y0h027kcv%23le5sm" target="_blank"><span>自研框架的成本分析及面临的问题</span></a></span><span>这一小节中，已经列举了很多案例了，这些案例都是与成本相关的。这些问题大致如下</span></p><ul><li><p style="margin-left:.5rem; margin-right:0"><span>如果负责研发框架的开发人员离职后，公司如何安排？</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>如果负责研发框架的开发人员，或其他开发人员离职后，将框架带到下个公司使用，是否等于用公司的资源帮其他公司做嫁衣？</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>或将现有公司的框架带到下个公司去使用，会给新公司带来风险吗？</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>如果员工的框架是上个公司的，你如何确定没有法律风险？</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>有完整的使用文档吗？如果将你当前使用的框架给到其他团队，在没有你的帮助下，该团队能顺利完成开发吗？</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>打算新招一个编写业务逻辑的人员，可以做到零学习成本吗？</span></p></li></ul><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>参考者在制作自己的框架时，也只敢参考部分，而且后续的维护成本也是恐怖的（需要付出巨大的劳动力）。看似参考的策略可以节省成本，但实际上的成本更高了。实际当中，也有很多开发者在开展这一策略，即参考 ioGame 来制作自己的框架，但其生命周期都是短暂的，最终是不了了之；</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h4><span>有趣的小故事</span></h4><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>这里说个有趣的小故事。现在，有两个开发者分别想要研发一款游戏，游戏类型也大致相同，这里将开发者称为 A、B。 A 和 B 的目标很明确，就是快速的将游戏开发出来并上线。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>技术选型的过程中，他们知道了 ioGame；A 、B 知道 ioGame 的源码完全开放、最新文档阅读完全开放、使用完全免费；只是想将游戏产品闭源时，则需要支付闭源授权费，但可以申请盈利后支付闭源授权费。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>现在，A 直接开始使用 ioGame 来研发游戏产品了；而 B 则是在想，如何规避这部分闭源授权费。于是 B 就开始阅读、参考 ioGame 相关源码，并开始自制框架。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>数月后，B 依然把劳动力分配在自制的框架上；而 A 则早已将游戏开发了出来。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>或许 B 早已忘记了最开始的主要目标了，这也是大多数开发者存在的行为；为了达成次要目标，而放弃或拖延主要目标。这种行为的危害是巨大的，从而导致主要目标无法完成，甚至到最后有可能自制框架都没能完成。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>这种真实的案例每天都在上演，你会发现很多想接触游戏行业的开发者，即使过了数年也没有研发出一个游戏。因为他们忘记了最初的主要目标，如果最开始就把劳动力分配在游戏产品上，或许早已经完成主要目标了。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>而 A 的目标与执行力更加的明确，直接使用 ioGame 来研发产品。这样就能把所有的劳动力分配到自身游戏产品的业务上，从而能快速的开发出游戏。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>在这个小故事中，B 最开始只是为了规避闭源授权费，导致自身付出了更多的成本。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h4><span>比较优势</span></h4><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>上面讲了一个有趣的小故事，这里还有一些</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fgd5l3b0y0h027kcv%23aSk5x" target="_blank"><span>关于成本分析的其他案例</span></a></span><span>（点击我）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>大衞·李嘉图在其代表作《政治经济学及赋税原理》中提出了比较成本贸易理论（后人称为「</span><span><strong><span>比较优势</span></strong></span><span>贸易理论」）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>比较优势可以表述为：在两国之间，劳动生产率的差距并不是在任何产品上都是相等的。每个国家都应集中生产并出口具有比较优势的产品，进口具有比较劣势的产品（即「两优相权取其重，两劣相衡取其轻」），双方均可节省劳动力，获得专业化分工提高劳动生产率的好处。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>在小故事及</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fgd5l3b0y0h027kcv%23aSk5x" target="_blank"><span>关于成本分析的其他案例</span></a></span><span>中，本质上也是在描述成本的比较优势。即两个个体、两个团队、两个公司 ... 等，之间的比较。两者之间的分配策略，基本决定了后续的谁的生存几率更大；把有趣小故事中的 A、B 替换成两个公司、团队之间的博弈亦是如此。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>在有趣小故事中，之所以 A 比 B 更有几率出游戏产品，是因为 A 把劳动力放在游戏产品的研发上，而 B 除了需要研发产品外，还需要兼顾自研框架。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>小结</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>现在我们知道了，ioGame 源码完全开放，没有社区版、商业版本之说，让使用者用得开心，用得放心。ioGame 新版在线文档阅读完全开放，与之前一样，依旧采用自愿付费原则的策略（不强制付费）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>使用 ioGame 是免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；同时，ioGame 是允许私有的，前提是支付闭源授权费；所以，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><blockquote><p style="margin-left:0; margin-right:0"><span>ioGame 使用的开源策略是：两开一免一自主。即源码完全开放、最新文档阅读完全开放、使用完全免费、开发者可自主选择按需付费。</span></p></blockquote><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>当其他开发者、团队还有纠结及时，我们的真实有效用户早已将产品上线并运营了。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 是无私的，能容得下所有竞品框架；甚至我们还在文档中介绍了其他的竞品框架。如果 ioGame 不能很好的满足你的需要，请别担心，这里还有</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fwwvg7z%23EH8Oc" target="_blank"><span>更多的 java 游戏服务器框架推荐</span></a></span><span>给你。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>为什么不在之前的讨论区回应</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>因为讨论区大部分是各种现代化的卧龙、凤雏。滑稽的是他们还喜欢充当所谓的网络侠客（一些不具备事物分析能力的人，大部分的发言是臆测主观性的）；</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>注意，这里不是指讨论区内的所有人，只是部分人符合我心中现代化卧龙、凤雏的形象。所以，个人并不会把时间浪费在现代化卧龙、凤雏身上；因为他们的目的不是辩，而是摆弄。不信？后续在其他版块中，你还会看见这群家伙在不同的讨论区中，周而复始的操弄着带有臆测主观性的庸俗观点来发言。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>现代化的卧龙、凤雏，或者说自以为聪明的资本家</span></strong></span><span>的基本特征是：你这产品不错、你这框架不错、你这项目不错，应该免费；同时，这群家伙还擅长慷他人之慨，真让他们参与付出时，结局是</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.bing.com%2Fsearch%3Fq%3D%25E7%259C%259F%25E6%259C%2589%25E4%25B8%2580%25E5%25A4%25B4%25E7%2589%259B%26qs%3Dn%26form%3DQBRE%26sp%3D-1%26lq%3D0%26pq%3D%26sc%3D0-0%26sk%3D%26cvid%3D9BE3D9C25BDF45FA8341F1641F100333%26ghsh%3D0%26ghacc%3D0%26ghpl%3D" target="_blank"><span>真有一头牛</span></a></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>我们只会把时间投入到真实有效使用者的身上，加强文档的质量，耐心回答与 ioGame 相关的 </span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgame-town%2FioGame%2Fissues" target="_blank"><span>issues</span></a></span><span> 。注意，这里的真实有效使用者指的是在项目中使用了 ioGame 的团队、个人、公司...等，并不是特指购买了闭源授权的；因为使用 ioGame 是完全免费的，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>规避与防范 ioGame</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fgd5l3b0y0h027kcv%23a7jj5" target="_blank"><span>如何规避闭源授权费用？</span></a></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>通常来讲你要防范的并不是 ioGame，因为 ioGame 每年需要支付的闭源授权的费用很少，而且可以申请盈利后支付。在成本分析中已经说明，如果你选择自研框架，自研一个月的成本够使用 ioGame 几十年了（而且需要自己编写、维护文档）。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>更多的时候，取得授权是为了保护自身产品源代码泄露时可以得到法律的保护，所以如果你不担心这点，我想你应该知道如何做了。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>ioGame 的使命</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>改变行业现状，降低使用难度。让游戏开发领域的</span><span><strong><span>生产资料公有制</span></strong></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>ioGame 是什么？</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 学习成本非常低，可以说是</span><span><strong><span>零学习成本</span></strong></span><span>，即使没有游戏编程（网络编程）经验，也能轻松上手。开发者只需掌握普通的 java 方法或 webMVC 相关知识，就能用框架开发业务，通常在 10 分钟内就能掌握。框架不要求开发者改变编码习惯，而是自身适应开发者的需求。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>同时，ioGame 也</span><span><strong><span>适用于网络游戏服务器、物联网、内部系统及各种需要长连接的场景</span></strong></span><span>；框架采用无锁异步化、事件驱动的架构设计；轻量级，无需依赖任何第三方中间件或数据库就能支持集群、分布式。通过 ioGame 可以很容易的搭建出一个集群无中心节点、集群自动化、多进程的分布式游戏服务器。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 是一个非常适合网络游戏开发的框架。可以让你轻松地创建高性能、低延迟、易扩展的游戏服务器，并且节省时间和资源。如果你想要快速地开发出令人惊艳的网络游戏，请不要犹豫，立即选择 ioGame 吧！框架屏蔽了很多复杂且重复性的工作，并可为项目中的功能模块结构、开发流程等进行</span><span><strong><span>清晰的组织定义</span></strong></span><span>，减少了后续的项目维护成本。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>ioGame 的使用趋势</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>关注 ioGame 的游戏服务器开发者持续增多，</span><span><strong><span>2022-09 ~ 至今</span></strong></span><span>各月的统计数据；</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>这里的统计信息是关于开发者关注 ioGame 框架相关的，从统计数据中可以看出，由于 ioGame 上手简单，功能强大等优点，得到了众多开发者的关注。如果你想知道 ioGame 有没有人在使用，可以先到这里看下统计数据、开发者的评价与讨论。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fgpxk93%23TwVa8" target="_blank">https://www.yuque.com/iohao/game/gpxk93#TwVa8</a></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>这里展示了每月的统计数据，统计数据来源于语雀后台，这些数据都是</span><span><strong><span>真实的、客观存在的、活的</span></strong></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>因为成本的原因，某宝某多还没有出现能提供这种服务的商家，所以这样的统计数据也更具真实性。</span></strong></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>通过统计数据，我们可以看到每天会有很多开发者在访问 ioGame 的在线文档，并且这些统计数据不是来源于口嗨的，也不是主观创造的。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>所以，还在犹豫要不要使用 ioGame 的开发者们，更应该讨论的是「为什么这些开发者会选择使用 ioGame」，而不是 ioGame 有没有人在使用的问题。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao" target="_blank"><span>点击我，到语雀后台查看 ioGame 的数据</span></a></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>ioGame 版本命名规则</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>从发布到中央仓库开始，ioGame 就有定义了自身的</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fwsgmba%23zagb5" target="_blank"><span>版本规则</span></a></span><span>。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>ioGame17 版本命名规则 x.y.z</span></strong></span></p><ul><li><p style="margin-left:.5rem; margin-right:0"><span>x 表示当前使用的 JDK 版本</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>y 表示 ioGame API 变更版本 （但基本上不会变动，常规下是变动 x 才会变动 API）</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>z 表示 ioGame 新特性、新功能、新模块、bugfix 相关</span></p></li></ul><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 的 x 会跟着最新的 JDK LTS 版本来走的，目的是确保 ioGame 的 API 不会发生很大的变化。为了保持新活力与接受新鲜事物， ioGame 基本会用上最新的 JDK LTS; 也就是说，下一个 x 将会是 21;</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>x 一般延后 1~2 个季度，给开发者一个缓冲。即下一个 JDK LTS 出来后，那么 ioGame 的 x 会在 1~2 个季度后跟上。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>从 ioGame21 开始，版本命名规则将使用 x.y 的格式，其他照旧</span></p><ul><li><p style="margin-left:.5rem; margin-right:0"><span>x 表示当前使用的 JDK LTS 版本</span></p></li><li><p style="margin-left:.5rem; margin-right:0"><span>y 表示 ioGame 新特性、新功能、新模块、bugfix 相关</span></p></li></ul><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><h3><span>最后</span></h3><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>由于整篇内容较长，阅读到最后难免会忘记前面的。所以，在最后这里，来个二次小结与介绍。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 推崇</span><span><strong><span>生产资料公有制。</span></strong></span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGame 使用的开源策略是：两开一免一自主。即源码完全开放、最新文档阅读完全开放、使用完全免费、开发者可自主选择按需付费。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>使用 ioGame 是完全免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；同时，ioGame 是允许私有的，前提是支付闭源授权费（</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yuque.com%2Fiohao%2Fgame%2Fgd5l3b0y0h027kcv" target="_blank"><span>为什么采用授权许可申请？</span></a></span><span>）；所以，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><div><hr></div><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>或许会有开发者问，为什么使用闭源授权的方式，这样不是会流失用户吗？</span></p><blockquote><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>提示：开源 != 免费。即使是做慈善的组织，组织内工作的人也得吃饭。</span></p></blockquote><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>纯白嫖的用户流失是一定的，但同时也会吸引更多的新用户，因为 ioGame 有了更长远的规划。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>ioGama 是海纳百川的，在文档中我们推荐了很多同行业的游戏服务器框架。因为我们知道，流失的用户与 ioGame 的矛盾点是必然存在的，但 ioGame 对待流失用户是友好的，我们会积极的把这部分流失的用户推荐给竞品框架。这样</span><span><strong><span>既解决了矛盾，又创造了三赢的局面，真是太棒了</span></strong></span><span>！</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>对于用户群体，ioGame 自身有明确的规划，而且也没打算说服所有的开发者来使用 ioGame；所以，我们首先要做的就是排除对自身不适合或者说理念不相同的开发者，只有这样才能抽出更多的时间来服务真实有效使用者；</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>注意，这里的真实有效使用者指的是在项目中使用了 ioGame 的团队、个人、公司...等，并不是特指购买了闭源授权的；因为使用 ioGame 是完全免费的，</span><span><strong><span>付费不是必须的，也不是强制性的</span></strong></span><span>，选择权在使用者手上。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start">&nbsp;</p><div><hr></div><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>真正的资本家是恐怖的，国外的资本家们取得收益后，或多或少的会向基金会捐赠，以保持相互的良性循环。而</span><span><strong><span>自以为聪明的资本家</span></strong></span><span>们可不这么想，因为他们并懂可持续性发展的重要性。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>这群</span><span><strong><span>自以为聪明的资本家们</span></strong></span><span>秉持着希望免费使用你的产品，又希望你免费维护，同时又不想将自身的成果公开出来（只想生产资料私有制）。现在，我们知道使用 ioGame 是免费的了，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来；当</span><span><strong><span>自以为聪明的资本家们</span></strong></span><span>知道要他们把私有资料公开时，他们就会给你上演</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.bing.com%2Fsearch%3Fq%3D%25E7%259C%259F%25E6%259C%2589%25E4%25B8%2580%25E5%25A4%25B4%25E7%2589%259B%26qs%3Dn%26form%3DQBRE%26sp%3D-1%26lq%3D0%26pq%3D%26sc%3D0-0%26sk%3D%26cvid%3D9BE3D9C25BDF45FA8341F1641F100333%26ghsh%3D0%26ghacc%3D0%26ghpl%3D" target="_blank"><span>真有一头牛</span></a></span><span>的故事。</span></p><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span><strong><span>现代化的卧龙、凤雏，或者说自以为聪明的资本家</span></strong></span><span>的基本特征是：你这产品不错、你这框架不错、你这项目不错，应该免费；同时，这群家伙还擅长慷他人之慨，真让他们参与付出时，结局是</span><span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.bing.com%2Fsearch%3Fq%3D%25E7%259C%259F%25E6%259C%2589%25E4%25B8%2580%25E5%25A4%25B4%25E7%2589%259B%26qs%3Dn%26form%3DQBRE%26sp%3D-1%26lq%3D0%26pq%3D%26sc%3D0-0%26sk%3D%26cvid%3D9BE3D9C25BDF45FA8341F1641F100333%26ghsh%3D0%26ghacc%3D0%26ghpl%3D" target="_blank"><span>真有一头牛</span></a></span><span>。</span></p><p>&nbsp;</p><div><hr></div><p style="color:#333333; margin-left:.8em; margin-right:.8em; text-align:start"><span>最后，我们的目的是为了帮助（中、小、新）这类公司、团队、独立开发者，通过双方的比较优势来互补。我们知道，只有让游戏开发者先盈利，之后想支付费用时才无压力（闭源自身的游戏产品）。（注意，使用 ioGame 是完全免费的，前提是要认同</span><span><strong><span>生产资料公有制</span></strong></span><span>，就是把你的产品也完全的开源出来）</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 03:37:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276890</guid>
            <link>https://www.oschina.net/news/276890</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[RHEL 源码访问限制影响 CentOS SIG]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">去年 6 月，红帽发布公告<a href="https://www.oschina.net/news/246331/red-hat-centos-stream-sources">宣布</a>了一个限制 Red Hat Enterprise Linux (RHEL)&nbsp; 源代码访问性的政策。时至今日，这一变更似乎无意中给 CentOS Integration&nbsp;Special&nbsp;Interest&nbsp;Group (&nbsp;(SIG) 小组带来了一些麻烦。</span></p><p><span style="color:#000000">Kmods SIG 旨在为 CentOS Stream 和 Red Hat Enterprise Linux 维护额外内核模块。CentOS 博客上周<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.centos.org%2F2024%2F01%2Fjanuary-2024-news%2F" target="_blank">发布</a>了 Kmods SIG 的最新状态，其中一项值得注意的内容是：</span></p><blockquote><p>由于 Red Hat 发布 Red Hat Enterprise Linux 源代码的方式发生了变化，Kmods SIG 目前由于法律原因无法为 Red Hat Enterprise Linux 制作软件包。我们正与 Red Hat 合作解决这一问题，并希望能尽快再次为企业 Linux 提供软件包。</p></blockquote><p><img height="304" src="https://oscimg.oschina.net/oscnet/up-05fcfbc3e8c1312848b6f99d2e074ce2339.png" width="500" referrerpolicy="no-referrer"></p><p>此外，CentOS Hyperscale SIG 在 2023 年第四季度报告中<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.centos.org%2F2024%2F01%2Fcentos-hyperscale-sig-quarterly-report-for-2023q4%2F" target="_blank">表示</a>：</p><blockquote><p><span style="color:#020008">Hyperscale SIG 的最新版本是 CentOS Stream 9 的内核 6.7.1-0.hs1。这个新内核现在基于上游 Fedora 版本内核，而不是 CentOS/RHEL 内核树。在可预见的未来，Hyperscale SIG 将跟踪 Fedora 内核以构建并发布到 CentOS 中。内核仍然采用类似 RHEL 的配置构建，专门针对 CentOS Hyperscale 进行 modulo 更改。</span></p><p><span style="color:#020008">随着&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.centos.org%2F2023%2F04%2Fend-dates-are-coming-for-centos-stream-8-and-centos-linux-7%2F" target="_blank">EOL</a>&nbsp;<span style="color:#020008">的临近，CentOS Stream 8 的内核已停止使用，并且没有计划围绕 CentOS Stream 8 进行进一步的工作。</span></p></blockquote><p><span style="background-color:#ffffff; color:#333333">Hyperscalers SIG 由 Facebook 和 Twitter 等科技公司倡议<a href="https://www.oschina.net/news/126467/centos-hyperscale-sig">成立</a>，致力于在大型基础架构上（如 Facebook、Twitter 和云服务提供商等其他 "hyperscaler" 组织）启用 CentOS Stream 部署，并促进软件包和工具上的协作。&nbsp;</span>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 30 Jan 2024 03:23:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276886/centos-rhel-kmods-sig</guid>
            <link>https://www.oschina.net/news/276886/centos-rhel-kmods-sig</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[简单一招竟把 nginx 服务器性能提升 50 倍]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/action/visit/ad?id=1576" target="_blank">【直播预告】DBA 会被云淘汰吗？ <img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><h2>需求背景</h2><p>接到重点业务需求要分轮次展示数据，预估最高承接 9w 的 QPS，作为后端工程师下意识的就是把接口写好，分级缓存、机器扩容、线程拉满等等一系列连招准备，再因为数据更新频次两只手都数得过来，我们采取了最稳妥的处理方式，直接生成静态文件拿 CDN 抗量</p><p>架构流程大致如下所示：</p><p><img src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-02-11-59FwLpwnTQRa2sueb..jpg" alt="" referrerpolicy="no-referrer"></p><p>数据更新后会重新生成新一轮次的文件，刷新 CDN 的时候会触发大量回源请求，应用服务器极端情况得 hold 住这 9w 的 QPS</p><h2>第一次压测</h2><p>双机房一共 40 台 4C 的机器，25KB 数据文件，5w 的 QPS 直接把 CPU 打到 90%</p><p>这明显不符合业务需求啊，咋办？先无脑加机器试试呗</p><p>就在这时测试同学反馈压测的数据不对，最后一轮文件最大会有 125KB，雪上加霜</p><p>于是乎文件替换，机器数量整体翻一倍扩到 80 台，服务端 CPU 依然是瓶颈，QPS 加不上去了</p><p><img src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-02-11-59591UDaNqz2ejKgeO.png" alt="" referrerpolicy="no-referrer"></p><p>到底是哪里在消耗 CPU 资源呢，整体架构已经简单到不能再简单了</p><p>这时候我们注意到为了节省网络带宽 nginx 开启了 gzip 压缩，是不是这小子搞的鬼</p><pre><code>server
{
      listen 80;
      
      gzip on;
      gzip_disable "msie6";
      gzip_vary on;
      gzip_proxied any;
      gzip_comp_level 6;
      gzip_buffers 16 8k;
      gzip_http_version 1.1;
      gzip_types text/plain application/css text/css application/xml text/javascript application/javascript application/x-javascript;

......
}


</code></pre><h2>第二次压测</h2><p>为了验证这个猜想，我们把 nginx 中的 gzip 压缩率从 6 调成 2，以减少 CPU 的计算量</p><blockquote><p>gzip_comp_level 2;</p></blockquote><p>这轮压下来 CPU 还是很快被打满，但 QPS 勉强能达到 9w，坐实了确实是 gzip 在耗 CPU</p><p><img src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-02-11-5911xGuMeD522kE2Orh.png" alt="" referrerpolicy="no-referrer"></p><p>nginx 作为家喻户晓的 web 服务器，以高性能高并发著称，区区一个静态数据文件就把应用服务器压的这么高，一定是哪里不对</p><h2>第三次压测</h2><p>明确了 gzip 在耗 CPU 之后我们潜下心来查阅了相关资料，发现了一丝进展</p><p>html/css/js 等静态文件通常包含大量空格、标签等重复字符，重复出现的部分使用「距离加长度」表达可以减少字符数，进而大幅降低带宽，这就是 gzip 无损压缩的基本原理</p><p>作为一种端到端的压缩技术，gzip 约定文件<strong>在服务端压缩完成</strong>，传输中保持不变，直到抵达客户端。这不妥妥的理论依据嘛～</p><p>nginx 中的 gzip 压缩分为动态压缩和静态压缩两种</p><p>•动态压缩</p><p>服务器给客户端返回响应时，消耗自身的资源进行实时压缩，保证客户端拿到 gzip 格式的文件</p><p>这个模块是默认编译的，详情可以查看 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_gzip_module.html" target="_blank">https://nginx.org/en/docs/http/ngx_http_gzip_module.html</a></p><p>•静态压缩</p><p>直接将预先压缩过的 .gz 文件返回给客户端，不再实时压缩文件，如果找不到 .gz 文件，会使用对应的原始文件</p><p>这个模块需要单独编译，详情可以查看 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_gzip_static_module.html" target="_blank">https://nginx.org/en/docs/http/ngx_http_gzip_static_module.html</a></p><p>如果开启了 gzip_static always，而且客户端不支持 gzip，还可以在服务端加装 gunzip 来帮助客户端解压，这里我们就不需要了</p><p>查了一下 jdos 自带的 nginx 已经编译了 ngx_http_gzip_static_module，省去了重新编译的麻烦事</p><p><img src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-14-21-44VgvTdUt25t44PAPy.jpg" alt="" referrerpolicy="no-referrer"></p><p>接下来通过 GZIPOutputStream 在本地额外生成一个 .gz 的文件，nginx 配置上静态压缩再来一次</p><blockquote><p>gzip_static on;</p></blockquote><p><img src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-02-11-59bMIHDM9dOrJ8soD.png" alt="" referrerpolicy="no-referrer"></p><p>面对 9w 的 QPS，40 台机器只用了 7% 的 CPU 使用率完美扛下</p><p>为了探底继续加压，应用服务器 CPU 增长缓慢，直到网络流出速率被拉到了 89MB/s，担心影响宿主机其他容器停止压力，此时 QPS 已经来到 27w</p><p>qps 5w-&gt;27w 提升 5 倍，CPU 90%-&gt;7% 降低 10 倍，整体性能翻了 50 倍不止，这回舒服了～</p><h2>写在最后</h2><p>经过一连串的分析实践，似乎静态压缩存在「压倒性」优势，那什么场景适合动态压缩，什么场景适合静态压缩呢？一番探讨后得出以下结论</p><blockquote><p>纯静态不会变化的文件适合静态压缩，提前使用 gzip 压缩好避免 CPU 和带宽的浪费。动态压缩适合 API 接口返回给前端数据这种动态的场景，数据会发生变化，这时候就需要 nginx 根据返回内容动态压缩，以节省服务器带宽</p></blockquote><p>作为一名后端工程师，nginx 是我们的老相识了，抬头不见低头见。日常工作中配一配转发规则，查一查 header 设置，基本都是把 nginx 作为反向代理使用。这次是直接访问静态资源，调整过程的一系列优化加深了我们对 gzip 的动态压缩和静态压缩的基本认识，这在 NG 老炮儿眼里显得微不足道，但对于我们来说却是一次难得的技能拓展机会</p><p>在之前的职业生涯里，我们一直聚焦于业务架构设计与开发，对性能的优化似乎已经形成思维惯性。面对大数据量长事务请求，减少循环变批量，增大并发，增加缓存，实在不行走异步任务解决，一般瓶颈都出现在 I/O 层面，毕竟磁盘慢嘛，减少与数据库的交互次数往往就有效果，其他大概率不是问题。这回有点儿不一样，CPU 被打起来的原因就是出现了大量数据计算，在高并发请求前，任何一个环节都可能产生性能问题</p><p>作者：京东零售，闫创</p><p>来源：京东云开发者社区，转载请注明来源</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 29 Jan 2024 02:49:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/10979685</guid>
            <link>https://my.oschina.net/u/4090830/blog/10979685</link>
            <author>
                <![CDATA[京东云开发者]]>
            </author>
        </item>
    </channel>
</rss>
