<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Mon, 22 Jan 2024 11:32:16 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[德国程序员因报告漏洞被判罚 2.4 万元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">德国于利希地方法院近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.heise.de%2Fnews%2FWarum-ein-Sicherheitsforscher-im-Fall-Modern-Solution-verurteilt-wurde-9601392.html" target="_blank">宣布</a>了一项最新判决结果，认定一名程序员因未经授权访问第三方计算机系统和刺探数据，违反《德国刑法典》（StGB）中所谓的黑客条款 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gesetze-im-internet.de%2Fstgb%2F__202a.html" target="_blank">202a</a> 而处以 3000 欧元的罚款（约 2.35 万元），同时承担所有的诉讼费用。</span></p><p><img height="203" src="https://oscimg.oschina.net/oscnet/up-1685fc08f7024fc47d741af2e54fc872cf5.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#000000">2021 年 6 月，这位名为 Hendrik H. 的研究人员在为 IT 服务公司 Modern Solution GmbH 的一位客户排除软件故障时发现，Modern Solution 的代码通过 MySQL 连接至一台 MariaDB 数据库服务器。而访问远程服务器的密码则以纯文本形式存储在程序文件 MSConnect.exe 中，任何人使用简单的文本编辑器就能打开该文件查看内容，并找到未加密的硬编码密码。</span></p><p><span style="color:#000000">也正是因为这个唾手可得的密码，导致任何人都可以登录远程服务器访问 Modern Solution 的客户的数据，同时还可以访问存储在该数据库服务器上的供应商所有客户的数据。总的来说，这个数据库漏洞暴露了近 70 万条客户记录，包括姓名、电子邮件地址、电话号码、银行信息、密码以及对话和通话记录等。</span></p><p><span style="color:#000000">在发现这一漏洞后，该程序员在一名技术博客作者 Mark Steier 的帮助下联系了相关公司，后者随后修复了安全漏洞，并报警追究这名程序员的责任。2021 年 9 月，德国警方扣押了 Hendrik H. 的电脑，因为 Modern Solution 指控他是通过内部信息获得的密码，并声称他是竞争对手。</span></p><p><span style="color:#000000">2023 年 6 月，德国于利希地方法院以 Modern Solution 软件保护不力为由，支持了 Hendrik H 的诉讼请求。但亚琛地区法院指令于利希地方法院再次审理此案，原先的裁定被推翻。2024 年 1 月 17 日，于利希地方法院最终宣判对 Hendrik H. 处以罚款，并责令其支付诉讼费用。</span></p><p><span style="color:#000000">这一判决不可避免的在广大网络安全专家和研究人员当中引起了争议。Steier 发帖<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwortfilter.de%2Fentdecker-des-datenlecks-modern-solution-heute-vor-gericht%2F" target="_blank">表示</a>，这一判决从根本上就是错误的。「几乎以纯文本形式保存的密码并不构成第 202 条所要求的'special security'。法官无法对此作出判断是可以理解的，但这样一来就必须就这个问题听取专家的意见。遗憾的是，这并没有发生。」</span></p><p><span style="color:#000000">不过，该判决尚未具有法律约束力。</span><span style="background-color:#ffffff; color:#323232">被告的辩护律师辩称，即使法院判定他有罪，他的当事人的行为也是为了公众利益。</span><span style="color:#000000">被指控的程序员已于 1 月 19 日宣布，正在对判决提出上诉。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 09:41:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276369/germany-programmer-fined-security</guid>
            <link>https://www.oschina.net/news/276369/germany-programmer-fined-security</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[北京获准向公众开放的生成式 AI 大模型产品占全国近半]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>北京市第十六届人民代表大会第二次会议于日前召开，会上透露，2023,年，北京获准向公众开放的生成式人工智能大模型产品占全国近一半。今年，北京将推动人工智能模型对标国际先进水平，加快在政务、医疗、工业、生活服务等领域应用。</p><p>北京市市长殷勇作政府工作报告时指出，2023 年，北京加快建设国际科技创新中心，加强科技领军人才尤其是青年人才培养引进，实施基础研究领先行动和关键核心技术攻坚战行动，推动在京国家实验室高质量运行，支持新型研发机构开展有组织科研，加快构建以企业为主导的产学研深度融合新范式。</p><p>北京巩固提升高精尖产业发展优势，出台通用人工智能、人形机器人等 30 余项细分产业支持政策，新设 4 支政府高精尖产业基金，一批创新药品、医疗器械获批上市，小米智能手机工厂、理想汽车旗舰工厂提前投产。</p><p>北京精心打造全球数字经济标杆城市，率先建成全球性能领先的区块链基础设施，新增 5G 基站 3 万个，获准向公众开放的生成式人工智能大模型产品占全国近一半，「京通」「京办」「京智」三个智慧城市应用终端快速升级拓展，高级别自动驾驶示范区实现 160 平方公里连片运行，全国首个数据基础制度先行区启动建设，数字经济增加值占地区生产总值比重达 42.9%。</p><p>殷勇说，今年，北京将加快发展新质生产力。实施制造业重点产业链高质量发展行动，提升产业链供应链韧性和安全水平。加强原创新药和高端医疗器械研发，培育生物制造等医药健康产业新增长点。推动新能源汽车产业高质量发展，积极布局电机、电池、电控等关键零部件产业链。推进超高清视频全产业链优化升级。促进新能源、新材料、商业航天、低空经济等战略性新兴产业发展，开辟量子、生命科学等未来产业新赛道。优化专精特新企业梯队培育体系，助力更多企业发展壮大。</p><p>殷勇指出，今年，北京将促进平台经济有序竞争、创新发展，推动先进数字技术向中小企业深度普及，构建开放共享、充满活力的创新生态。提升人工智能底层技术和基础底座自主可控能力，推动人工智能模型对标国际先进水平，加快在政务、医疗、工业、生活服务等领域应用，保持人工智能研发应用领先水平。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 05:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276347</guid>
            <link>https://www.oschina.net/news/276347</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[RustDesk 新增 2FA 双重认证功能，增强远程桌面访问安全性]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>RustDesk <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frustdesk%2Frustdesk%2Freleases%2Ftag%2Fnightly" target="_blank">nightly</a>&nbsp;新增 2FA 双重认证功能，增强远程桌面访问安全性，欢迎大家试用反馈。</p><p><img height="1246" src="https://oscimg.oschina.net/oscnet/up-0de6626da796bb7195b23fc861ee98e2f12.jpg" width="1708" referrerpolicy="no-referrer"></p><p><img height="623" src="https://oscimg.oschina.net/oscnet/up-eef5d3bce35b2b05039bb7678d9ebbea95b.jpg" width="854" referrerpolicy="no-referrer"></p><p><img height="1270" src="https://oscimg.oschina.net/oscnet/up-d4017aeb3ba844b73de27c16d258a40944d.jpg" width="1780" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:20:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276342/rustdesk-2fa</guid>
            <link>https://www.oschina.net/news/276342/rustdesk-2fa</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[到底什么样的 Java 项目用 Solon 好？？？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">就像华为讲的，不要因为爱国而特意买华为手机。Solon 也是，<strong>有需要就用不需要就跳过</strong>（按正常的需求选择）：</p><ul><li>信创需要国产化，应该用 Solon 或者 Solon Cloud（有案例）</li><li>军工项目要国产化，应该用 Solon 或者 Solon Cloud（有案例）</li><li>嵌入式设备，内存有限，算力差，可以用 Solon 或者 Solon Native（有案例）</li><li>客户的希望你内存更少，可以用 Solon （有案例）</li><li>别的框架用腻了，可以用 Solon （有案例）</li><li>有新系统开发想尝新的框架，可以用 Solon （有案例）</li><li>老系统要轻量化改造，可以用 Solon（有案例）</li></ul><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">作为后来者，大家的疑或是会多一些。有问题，可以去交流群里多交流。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276341</guid>
            <link>https://www.oschina.net/news/276341</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[如何写好大模型提示词？来自大赛冠军的经验分享（进阶篇）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><blockquote><p><strong>编者按</strong>：近期，如何通过 Prompt Engineering 最大程度发挥大模型的潜力已成为一个热点话题。人们越来越关注如何通过 Prompt Engineering 技术低成本地用好大模型。</p><p>今天我们推荐的这篇文章，作者认为 Prompt Engineering 需要结合艺术与科学，需要在理解技术背景的同时，发挥创造力和战略思维。</p><p>本系列文章详细介绍了作者在新加坡首届 GPT-4 Prompt Engineering 大赛中使用的策略技巧，包括：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">使用 CO-STAR 框架构建提示语、使用分隔符明确语义单元</a>、利用 system prompts 添加行为约束、仅依靠 GPT-4 对数据集进行分析等。这些技巧都得到了实例验证，证明了 Prompt Engineering 的重要作用。</p><p>本文属于该系列文章的第二部分，详细介绍适合进阶使用的 Prompt Engineering 高级策略。</p></blockquote><p><strong>作者 |&nbsp;Sheila Teo</strong></p><p><strong>编译&nbsp;|&nbsp;岳扬</strong></p><p><strong>🚢🚢🚢欢迎小伙伴们加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技术软件及技术交流群</a>，追踪前沿热点，共探技术难题~</strong></p><p>上个月，我有幸获得新加坡首届 GPT-4 提示工程（Prompt Engineering）大赛相关奖项，该比赛由新加坡政府科技署（GovTech）组织，汇聚了超过 400&nbsp;位优秀的参与者。</p><p><strong>提示工程（Prompt Engineering）是一门融合了艺术和科学的学科——这门学科不仅需要理解技术，还需要一定的创造力和战略思维。</strong> 以下是我在学习过程中学到的提示工程（Prompt Engineering）策略汇编，这些策略可以驱动任何大语言模型（LLM）精准执行需求，甚至超常发挥！</p><blockquote><p>作者注：</p><p>在撰写本文时，我力图摒弃已在网上广泛讨论和记录的传统提示工程（Prompt Engineering）技术。相反，撰写本文的目的是给大家介绍我在实验中学到的新见解，以及对某些技术的不同理解。希望你会喜欢阅读这篇文章！</p></blockquote><p>本系列文章包括以下内容，其中🔵指的是适合初学者的提示语（prompt）技巧（<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">见基础篇</a>），而🔴指的是高级策略（本文的重点）：</p><p>1.[🔵] 使用&nbsp;CO-STAR&nbsp;框架构建提示语</p><p>2.[🔵]&nbsp;使用分隔符（delimiters）将提示语分段</p><p><strong>3.[🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;创建&nbsp;system prompts</strong>（译者注："guardrails"&nbsp;指的是一种保护机制或限制，用于确保大语言模型生成的内容符合特定标准或要求，防止产生不准确、不合适或有害的信息。）</p><p><strong>4.[🔴]&nbsp;仅使用&nbsp;LLM（无需插件或代码）分析数据集——将介绍一个使用&nbsp;GPT-4&nbsp;分析真实&nbsp;Kaggle&nbsp;数据集的实践示例</strong></p><h1><strong>01 [🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;创建&nbsp;system prompts</strong></h1><p>在进入正题之前，需要注意的是本节只适用于具有 System Prompt 功能的 LLM，而不像基础篇和本文的其他章节那样适用于任何 LLM。最著名的 LLM 当然是&nbsp;ChatGPT ，因此在本节中我们将以 ChatGPT 作为示例。</p><h2><strong>1.1 围绕&nbsp;System Prompt&nbsp;的术语</strong></h2><p>首先，让我们来理清术语，特别是关于 ChatGPT 的三种术语的使用：这三种术语在 ChatGPT 几乎可以互换使用：&nbsp;"System Prompts"、"System Messages "和&nbsp;"Custom Instructions"。这让很多人（包括我在内！）感到困惑，以至于&nbsp;OpenAI&nbsp;特意发布了一篇文章来解释这些术语。以下是其摘要：</p><ul><li><strong>"System Prompts"和"System Messages"是通过 Chat Completions API 以编程方式与 ChatGPT 进行交互时使用的术语。</strong></li><li><strong>另一方面，"Custom Instructions"是通过 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchat.openai.com%2F" target="_blank">https://chat.openai.com/</a>&nbsp; 用户界面与 ChatGPT 交互时使用的术语。</strong></li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-121921e25f98c551f3af8e6f70943254843.png" alt="" referrerpolicy="no-referrer"></p><p>Image from Enterprise DNA Blog</p><p>不过总的来说，这三个术语指的是同一件事，所以不要被这些术语混淆了！后续部分，本文将使用&nbsp;"System Prompts"一词。现在，让我们进入正题！</p><h2><strong>1.2 什么是 System Prompts ？</strong></h2><p>System Prompts 是一种额外的提示语（prompt），我们可以在其中提供有关 LLM 行为方式的 instructions。它被认为是额外的提示语（prompt），因为它不属于您给 LLM 的&nbsp;"正常"&nbsp;提示语（即 User Prompts）。</p><p>在聊天中，每当您给 LLM 发送新的提示语（prompt）时，System Prompts 都会像过滤器一样，LLM 会在回答您的新提示语（prompt）之前自动应用这些提示语（prompt）。这意味着 System Prompts 在 LLM 做出回答时都会被考虑进去。</p><h2><strong>1.3 何时使用 System Prompt ？</strong></h2><p>您心中可能会想到的第一个问题是：为什么我应该在 System Prompts 中提供 instruction，而不是在我向与 LLM 的新对话的第一个提示语（prompt）中提供 instruction，然后再与 LLM 进行更多的对话呢？</p><p>答案是，因为 LLM 的对话记忆是有限的。在后一种情况下，随着对话的继续，LLM 很可能会"忘记"您在聊天中提供的第一条提示语（prompt），从而使这些 instruction （指令）过时。</p><p>另一方面，如果在 System Prompts 中提供了 instruction （指令），那么这些 System Prompts &nbsp;会与聊天中提供的每个新提示语一起发送。这可以确保 LLM 在聊天过程中继续接收这些 instruction，无论聊天过程变得多长。</p><p>总结：</p><p>在整个聊天过程中，使用 System Prompts 提供您希望 LLM 在回答时记住的 instruction 。</p><h2><strong>1.4 System Prompt 应包括哪些内容？</strong></h2><p>System Prompt 通常应包括以下类别的 instruction ：</p><ul><li><strong>目标任务的定义（Task definition）</strong> ，这样 LLM 在整个对话过程中都会记住它必须做什么。</li><li><strong>输出格式（Output format）</strong> ，这样 LLM 在整个对话过程中都会记住它应该如何做出回答。</li><li><strong>防范措施（Guardrails）</strong> ，这样 LLM 在整个对话过程中都会记住它不应该如何做出回答。Guardrails 是 LLM governance 中的新兴领域，指的是 LLM 被允许操作的行为边界。</li></ul><p>例如，System Prompt 可能是这样的：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p><p>If the text does not contain sufficient information to answer the question,&nbsp;do not make up information and give the answer as&nbsp;「NA」.</p><p>You are only allowed to answer questions related to&nbsp;[insert scope].&nbsp;Never answer any questions related to demographic information such as age,&nbsp;gender,&nbsp;and religion.</p></blockquote><p>各部分内容涉及的类别如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-cab48977b545bba61bf371683a3bdc105af.png" alt="" referrerpolicy="no-referrer"></p><p>Breaking down a System Prompt&nbsp;—&nbsp;Image by author</p><h2><strong>1.5 但是，"正常"的聊天提示语又是什么呢？</strong></h2><p>现在你可能会想：听起来 System Prompt 中已经提供了很多信息。那我应该在聊天的"正常"提示语（即 User Prompts）中写些什么呢？</p><p>System Prompt 概述了当前的一般任务。在上面的 System Prompt 示例中，任务已被定义为只使用一段特定的文本来回答问题，并且 LLM 被指示以{"Question":&nbsp;"Answer"}的格式进行回答。</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p></blockquote><p>在这种情况下，聊天过程中的每个 User Prompt 都将简化为你希望 LLM 用文本回答的问题。例如，某个用户的提问可能是「这段文本是关于什么的？（What is the text about?）」然后 LLM 会回答说&nbsp;{"这段文本是关于什么的？（What is the text about?）":&nbsp;"这段文本是关于……（The text is about..）"}。</p><p>但是，让我们进一步概括这个任务示例。在这种情况下，我们可以将上述 System Prompt &nbsp;的第一行从：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p></blockquote><p>编辑为：</p><blockquote><p>You will answer questions using the provided text.</p></blockquote><p>现在，每个用户在聊天时的提示语（prompt）将包括进行问题回答的文本和要回答的问题，例如：</p><blockquote><p>&lt;text&gt;</p><p>[insert text]</p><p>&lt;/text&gt;</p><p>&lt;question&gt;</p><p>[insert question]</p><p>&lt;/question&gt;</p></blockquote><p>在这里，还将使用 XML 标签作为分隔符，以便以结构化的方式向 LLM 提供所需的两个信息片段。<strong>XML 标签中使用的名词「text」和「question」与 System Prompt 中使用的名词相对应，这样 LLM 就能理解标签与 System Prompt instructions 之间的关系。</strong></p><p>总之， System Prompt 应给出总体任务 instructions，而每个 User Prompt 应提供任务执行的具体细节。例如，在本例中，这些具体的细节是 text 和 question。</p><h2><strong>1.6 LLM guardrails&nbsp;动态化</strong></h2><p>上面通过 System Prompt 中的几句话添加了 guardrails 。这些 guardrails 会被固定下来，在整个聊天过程中都不会改变。但是如果您希望在对话的不同阶段设置不同的 guardrails ，该怎么办？</p><p>对于使用 ChatGPT Web 界面的用户来说，目前还没有直接的方法来做到这一点。不过，如果您正在通过编程方式与 ChatGPT 进行交互，那你就走运了！随着人们对构建有效的 LLM guardrail 的关注度越来越高，一些开源软件包也应运而生，它们可以让你以编程方式设置更详细、更动态的 guardrail。</p><p>其中值得注意的是英伟达团队开发的&nbsp;NeMo Guardrails[1]，它允许您配置用户和 LLM 之间预期的对话流程，从而在聊天的不同时间点设置不同的 guardrail ，实现随着聊天进展而不断演变的动态 guardrails 。我强烈推荐您去了解一下！</p><h1><strong>02 [🔴]&nbsp;仅使用&nbsp;LLM（无需插件或代码）分析数据集</strong></h1><p>您可能已经听说过 OpenAI 在 ChatGPT 的 GPT-4 中推出的高级数据分析插件，该插件仅高级（付费）账户可以使用。它允许用户将数据集上传到 ChatGPT，并直接在数据集上运行代码，从而进行精确的数据分析。</p><p>但你知道吗，使用 LLM 分析数据集并不一定需要这样的插件？让我们先来了解一下单纯使用 LLMs 分析数据集的优势和局限性。</p><h2><strong>2.1 大语言模型不擅长的数据集分析类型</strong></h2><p>正如您可能已经知道的那样，LLM 在进行精确数学计算方面的能力有限，因此它们不适合完成需要对数据集进行精确定量分析的任务，比如：</p><ul><li><strong>描述性统计（Descriptive Statistics）</strong> ：通过诸如均值或方差的测量来定量总结数值列。</li><li><strong>相关性分析（Correlation Analysis）</strong> ：获取列之间精确的相关系数。</li><li><strong>统计分析（Statistical Analysis）</strong> ：比如假设检验（hypothesis testing），以确定各组数据点之间是否存在统计意义上的显著差异。</li><li><strong>机器学习（Machine Learning）</strong> ：在数据集上执行预测建模，比如使用线性回归（linear regressions）、梯度提升树（gradient boosted trees）或神经网络（neural networks）。</li></ul><p>方便在数据集上执行这些定量任务正是 OpenAI 推出高级数据分析插件的原因，这样编程语言就可以在数据集上运行代码来执行此类任务。</p><p>那么，为什么有人只想使用 LLM 而不使用此类插件来分析数据集呢？</p><h2><strong>2.2 大语言模型擅长的数据集分析类型</strong></h2><p>LLM 非常擅长识别模式和趋势（patterns and trends）。这种能力源自它们在多样化和海量数据上的广泛训练，使它们能够识别那些可能无法立即察觉的复杂模式。</p><p>这使它们非常适合执行基于数据集进行模式识别的任务，例如：</p><ul><li><strong>异常检测（Anomaly detection）</strong> ：基于一列或多列的数值，识别偏离常规的异常数据点。</li><li><strong>聚类（Clustering）</strong> ：将具有相似特征的数据点分组。</li><li><strong>跨列关系（Cross-Column Relationships）</strong> ：通过分析不同列之间的关系，可以揭示数据中的复杂模式和趋势。</li><li><strong>文本分析（Textual Analysis）（针对基于文本的列）</strong> ：基于主题或情感进行分类。</li><li><strong>趋势分析（Trend Analysis）（针对具有时间特征的数据集）</strong> ：识别列中跨时间的模式、季节性变化或趋势。</li></ul><p>对于这类基于模式的任务，单独使用 LLM 可能会比使用代码在更短的时间内获得更好的结果！让我们用一个例子来充分说明这一点。</p><h2><strong>2.3 仅使用&nbsp;LLM&nbsp;分析&nbsp;Kaggle&nbsp;数据集</strong></h2><p>我们将使用一个广受欢迎的真实&nbsp;Kaggle&nbsp;数据集[2]，该数据集专为进行客户人格分析而准备，其中一家公司试图对其客户群体进行细分，以便更好地了解其客户。</p><p>为了便于之后验证 LLM 的分析结果，我们取该数据集的 50&nbsp;行为一个子集，并只保留最相关的列。之后，用于分析的数据集将如下所示，其中每一行代表一位客户，每一列描述客户信息：</p><p><img src="https://oscimg.oschina.net/oscnet/up-7c8caf80c5954ae73d0746053b0109b7737.png" alt="" referrerpolicy="no-referrer"></p><p>First 3 rows of dataset&nbsp;—&nbsp;Image by author</p><p>假设你在公司的营销团队工作。你的任务是利用这些客户信息数据集来指导营销工作。这是一项分两步走的任务：首先，利用数据集划分多个具有实际意义的客户细分群体。接下来，提出如何最好地针对每个客户群体进行创意营销。现在，这是一个实际的商业问题，LLM 的模式发现（pattern-finding，对于步骤 1 ）能力在这个问题上确实可以大显身手。</p><p>让我们按照以下方式为这个任务制定提示语（prompt），将使用&nbsp;4&nbsp;种&nbsp;prompt engineering&nbsp;技术（后续会详细介绍[3]）：</p><p><strong>1. 将复杂的任务分解为简单的步骤</strong></p><p><strong>2. 参考每个步骤的中间输出</strong></p><p><strong>3. 格式化 LLM 的回答</strong></p><p><strong>4. 将&nbsp;instructions&nbsp;与数据集分开</strong></p><blockquote><p>System Prompt:</p><p>I want you to act as a data scientist to analyze datasets.&nbsp;Do not make up information that is not in the dataset.&nbsp;For each analysis I ask for,&nbsp;provide me with the exact and definitive answer and do not provide me with code or instructions to do the analysis on other platforms.</p><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>GPT-4 的回答如下，接下来我们将数据集以 CSV 字符串的形式传递给它。</p><p><img src="https://oscimg.oschina.net/oscnet/up-e81a5f6ea27da5b058438541aa821009931.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p>随后，GPT-4 将按照我们要求的 markdown 格式回复分析结果：</p><p><img src="https://oscimg.oschina.net/oscnet/up-72040eca14932d7eb9289119eb13af4d0a8.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-03f08b1a6bfadee91b96716822f50f0c4a4.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-f81c09e878806d7f71c9773c8291d3611d5.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><h2><strong>2.4 验证&nbsp;LLM&nbsp;的分析结果</strong></h2><p>为了简洁起见，我们将挑选 LLM 生成的 2 个客户群体进行验证，比如 Young Families 和 Discerning Enthusiasts。</p><h3><strong>2.4.1 Young Families</strong></h3><ul><li>LLM 总结的该人群特征：1980 年后出生，已婚或同居，收入中等偏低，有孩子，经常进行小额购买。</li><li>LLM 将数据集中的这些行聚类到了 Young Families 这个群体中：3、4、7、10、16、20</li></ul><p>深入数据集，这些行的完整数据如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-01030fc251ada675444cc2b84f87f861ce5.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Young Families&nbsp;—&nbsp;Image by author</p><p>LLM 识别出的这部分客户资料，确实对应于所识别的客户群体。甚至能够在我们事先未经过预处理的情况下对具有空值的资料进行聚类！</p><h3><strong>2.4.2 Discerning Enthusiasts</strong></h3><ul><li>LLM 总结的该人群特征：年龄跨度广，可能是任何婚姻状况，高收入，子女状况各异，购买支出高。</li><li>LLM 认为该人群对应的数据行：2、5、18、29、34、36</li></ul><p>深入数据集，这些行的完整数据如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-2844275d166caf555082adb378ad4e1e7b6.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Discerning Enthusiasts&nbsp;—&nbsp;Image by author</p><p>再次与 LLM 识别出的人群资料非常吻合！</p><p>这个例子展示了 LLM 在发现模式、解释和提炼多维数据集，并将其提炼为有意义的见解方面的能力，同时确保其分析深深扎根于数据集的事实。</p><h2><strong>2.5 如果我们使用 ChatGPT 的高级数据分析插件会怎样呢？</strong></h2><p>为了保证分析的完整性，我尝试使用相同的提示语（prompt），并请求 ChatGPT 使用代码执行相同的分析，这就激活了它的高级数据分析插件。这个想法是让插件直接在数据集上运行 K-Means 等聚类算法的代码，以获得各个用户群体的特征，然后综合每个群体的数据来提供营销策略。</p><p>然而，尽管数据集只有 50&nbsp;行，进行了多次尝试都导致出现以下错误信息而没有任何输出：</p><p><img src="https://oscimg.oschina.net/oscnet/up-10c7526d7a4b7d296cef8b48699d37762a9.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 1&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-fa3318a26ba4ab2800121fb5ab4295345f5.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 2&nbsp;—&nbsp;Image by author</p><p>现在使用高级数据分析插件，在数据集上执行较简单的任务（如计算描述性统计数据或创建图表）似乎很容易实现，但需要某些计算算法的较高级任务有时可能会由于计算限制或其他原因导致错误或无输出。</p><h2><strong>2.6 那么......何时使用 LLM 分析数据集？</strong></h2><p>答案是取决于分析的数据类型。</p><p>对于需要精确数学计算或复杂的、基于规则处理的任务，传统的编程方法仍然更胜一筹。</p><p>对于基于模式识别（pattern-recognition）的任务，使用传统的编程或算法方式可能比较具有挑战性或更耗时。然而，LLM 擅长此类任务，甚至可以提供额外的内容输出，比如用来支持其分析的附件和 Markdown 格式的完整分析报告。</p><blockquote><p><strong>归根结底，是否使用 LLM 取决于手头任务的性质，要在 LLM 在模式识别方面的优势与传统编程技术提供的精确性和特异性之间取得平衡。</strong></p></blockquote><h2><strong>2.7 现在回到提示工程（prompt engineering）！</strong></h2><p><strong>在本节结束之前，让我们回顾一下用于生成此数据集分析的提示语（prompt），并分解所使用的 prompt engineering 关键技术。</strong></p><blockquote><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p><strong>技巧 1：将复杂任务分解为简单步骤</strong></p><p>LLM 擅长执行简单任务，但在复杂任务上表现一般。因此，<strong>对于像这样的复杂任务，重要的是要把任务分解成简单的步骤说明，让 LLM 遵循。</strong> 这样做的目的是向 LLM 提供你自己执行任务时会采取的步骤。</p><p>在本例中，步骤如下：</p><blockquote><p><strong>Use this step-by-step process and do not use code:</strong></p><p><strong>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</strong></p><p><strong>For each cluster found,</strong></p><p><strong>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</strong></p><p><strong>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</strong></p><p><strong>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</strong></p><p><strong>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</strong></p></blockquote><p>与简单地将整体任务交给 LLM 相比，例如「将客户分组，然后提出针对每个群体的营销策略」。通过逐步说明，LLM 更有可能提供正确的结果。</p><p><strong>技巧 2：引用每个步骤的中间输出</strong></p><p>在向 LLM 提供逐步说明时，可将每个步骤的中间输出命名为大写的变量名，例如 CLUSTERS、CLUSTER_INFORMATION、CLUSTER_NAME、MARKETING_IDEAS 和 RATIONALE。</p><p><strong>使用大写字母是为了将这些变量名与给出的 instructions 内容区分开。稍后可以使用方括号引用这些中间输出，如[VARIABLE_NAME]。</strong></p><p><strong>技巧 3：规范大模型回答的格式</strong></p><p>在这里，要求使用 Markdown 报告格式，以美化 LLM 的回答。在这里，中间输出中的变量名又派上了用场，可以决定报告的结构。</p><blockquote><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;<br><strong>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</strong></p></blockquote><p>事实上，您甚至可以随后要求 ChatGPT 将报告提供为可下载文件，这样您就可以根据其回答来撰写最终的报告文档。</p><p><img src="https://oscimg.oschina.net/oscnet/up-792771629e0046fa10609d608023fce4df5.png" alt="" referrerpolicy="no-referrer"></p><p>Saving GPT-4's response as a file&nbsp;—&nbsp;Image by author</p><p><strong>技巧 4：将任务说明与数据集分开</strong></p><p>您会注意到我们在第一个提示语中并没有将数据集提供给 LLM。相反，提示语只包含了数据集分析的任务说明，并在最后加上了以下内容：</p><blockquote><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>ChatGPT 随后回复说它理解了，我们将在下一个提示语中将数据集作为 CSV 字符串传递给它：</p><p><img src="https://oscimg.oschina.net/oscnet/up-3b139d01a6cf26df1f930d6e67dc5b8be24.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><strong>但为什么要将 instructions 与数据集分开呢？</strong></p><p>简单明了的答案是，LLM 的上下文窗口存在限制，即在一句提示语中可以输入的 tokens 数量存在限制。同时包含 instructions 和数据的长提示语（long prompt）可能会超过这个限制，从而导致截断（truncation）和信息丢失（loss of information）。</p><p>更复杂的答案是，<strong>将 instructions 和数据集分开可以帮助 LLM 保持清晰的理解，降低遗漏信息的可能性。</strong> 你可能遇到过这样的情况，即 LLM "不小心忘记了"&nbsp;你发送的较长提示语给出的某个 instruction ——例如，如果你要求给出 100&nbsp;字的回答，而 LLM 却给了您一个较长的段落。通过先接收 instructions ，再接收 instructions 所针对的数据集，LLM 可以先消化它应该执行的任务，然后再对接下来提供的数据集执行 instructions 。</p><p>不过请注意，只有聊天型&nbsp;LLM&nbsp;才能实现&nbsp;instruction&nbsp;和数据集的分离，因为它们会保留对话记忆，而&nbsp;completion LLM&nbsp;则不会（译者注：completion LLM 指的是一种能够根据给定的提示语来生成完整文本或完成特定任务的语言模型。这种模型通常不具备对话记忆，而是专注于根据提示语生成连贯的文本）。</p><p><strong>Thanks for reading!</strong></p><p><strong>END</strong></p><p><strong>🚢🚢🚢欢迎小伙伴们加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技术软件及技术交流群</a>，追踪前沿热点，共探技术难题~</strong></p><p><strong>参考资料</strong></p><p>[1]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FNVIDIA%2FNeMo-Guardrails" target="_blank">https://github.com/NVIDIA/NeMo-Guardrails</a></p><p>[2]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fimakash3011%2Fcustomer-personality-analysis" target="_blank">https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis</a></p><p>[3]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41%23544b" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41#544b</a></p><p><strong>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</strong></p><p><strong>原文链接：</strong></p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/10920438</guid>
            <link>https://my.oschina.net/IDP/blog/10920438</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[🔥 周热点 | 纯血 HarmonyOS NEXT 亮相；云风宣布开源自研游戏引擎；ReiserFS 作者在狱中回应被 Linux 内核弃用.....]]>
            </title>
            <description>
                <![CDATA[回顾一周热门资讯。2024.01.15-2024.01.21]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</guid>
            <link>https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</link>
        </item>
        <item>
            <title>
                <![CDATA[助力 AI 技术共享，蚂蚁开源又一核心技术 「因果学习系统 OpenASCE」]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">当地时间 2023 年 12 月 10 日，机器学习和人工智能领域的顶级国际会议 NeurIPS (Neural Information Processing Systems) 在美国路易斯安那州新奥尔良市开幕，来自全球产业界和学术领域的人工智能专家齐聚一堂。</p><h1>首个分布式全链路因果学习系统 OpenASCE</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">大会第一天，蚂蚁集团在主题为 「知识增强 AI 在垂直行业的应用探索」 的研讨会上正式开源了业界首个分布式全链路因果学习系统 OpenASCE (Open All-Scale Causal Engine) 。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">项目 GitHub：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpen-All-Scale-Causal-Engine%2FOpenASCE" target="_blank">https://github.com/Open-All-Scale-Causal-Engine/OpenASCE</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-8d931ba2b03a853fcf7d06c1a780d171b96.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">因果推断主要研究如何从数据中推断因果关系，是数据科学领域的重要分支，而传统的机器学习则主要依赖数据中的相关关系。融合因果推断和机器学习可以同时发挥两者的强项，我们称之为因果学习。因果学习作为一种深入理解数据和决策背后关系的技术，在数据驱动的运营和决策中扮演着重要的角色。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 根植于蚂蚁集团多年积累的实践经验和技术突破，相较于业界已有的一些开源框架，支持全链路大规模因果学习，包含因果发现、因果效应估计和归因，覆盖了因果各个领域的相应实现。在因果发现上，OpenASCE 支持分布式贝叶斯网络结构搜索，能够处理百节点百万样本数据；同时支持基于连续优化的因果发现，支持万级节点亿级样本数据。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 实现的大规模分布式因果纠偏树可以在 4 小时内完成 1 亿样本的训练任务，是业界唯一的分布式因果提升树实现。此外，OpenASCE 还沉淀了 20 多个工业级因果学习算法，包括 15 个以上因果技术和深度学习结合的因果表征学习方法，有效降低了因果技术的工业应用门槛，在蚂蚁集团内部多个场景实现了规模化应用。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-da7d981d440607195474f083b7a1cbb7862.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">在信贷风控领域，通过 OpenASCE 的因果学习方法，可以更准确地识别出风险因素和客户行为之间的因果关系，大幅提高了风险控制的精度和效率。在营销优化方面，OpenASCE 能够帮助营销人员有效寻找 「营销敏感人群」，提升业务指标。在推荐场景中，因果推断可以帮助机器学习纠正数据中的偏置，去除伪相关，学习更稳定的因果关系。</p><h1>开源开放，共建社区</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">作为一家科技公司，蚂蚁集团将 OpenASCE 开源，为业界提供一套大规模、高性能的因果学习技术，并通过开源吸引全球开发者共同参与项目的建设和完善，促进全链路因果学习系统领域的发展和创新。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">技术开源是蚂蚁集团的重要技术战略，我们希望通过开源建立起开放、包容的技术生态，让更多人共享技术红利。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">截至目前，蚂蚁集团已在数据库、云原生、中间件等基础软件领域开源了 1700 多个仓库、积累了 100 多个社区头部开源项目。《COPU2022 中国开源发展蓝皮书》显示，蚂蚁开源影响力排名国内前三，其中重点开源的 9 大技术均为支撑支付宝的核心技术。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:47:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276338</guid>
            <link>https://www.oschina.net/news/276338</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[因政治滥用，OpenAI 将一家 AI 初创公司拉黑]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>OpenAI 于日前<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yahoo.com%2Fnews%2Fopenai-suspends-developer-over-chatgpt-bot-that-impersonated-a-presidential-candidate-214854456.html" target="_blank">封禁</a>了一家开发 Chatbot 的 AI 初创公司 Delphi。因为该公司基于 GPT-4，出于政治目的设计了一个模仿美国民主党总统候选人 Dean Phillips 的机器人 Dean.Bot；以与潜在支持者互动并传播候选人的信息，帮助其竞选。</p><p><img height="220" src="https://oscimg.oschina.net/oscnet/up-5e13ec80f847c98829315c5bf8ebf46f766.png" width="500" referrerpolicy="no-referrer"></p><p>事实上，美国、英国、印度、巴基斯坦和南非等国都计划在 2024 年进行大选。为了防止其技术被滥用，OpenAI 在本月早些时候曾发表了一篇<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Fblog%2Fhow-openai-is-approaching-2024-worldwide-elections" target="_blank">长文</a>，介绍其计划采取的一些措施；其中明确表示不允许人们开发用于政治活动和游说的应用，并且还特别提到了禁止"冒充候选人的聊天机器人 "。</p><p>虽然 Dean.Bot 网站有提供免责声明，告知访问者所有的交互都将由聊天机器人生成，而不是 Phillips 本人。但这种使用方式还是直接违反了 OpenAI 的政策。公司发言人在给《华盛顿邮报》的一份声明中也证实了被 OpenAI 封禁的消息。Delphi 的 OpenAI 帐户据称于上周五被封禁，随后该公司就停止了对 Dean.Bot 的访问权限。</p><p>现在访问该网站的用户仍然会看到免责声明，但会显示聊天机器人本身已因"技术故障"而宕机："Apologies, DeanBot is away campaigning right now!"&nbsp;</p><p>这也是 OpenAI 首次因开发者违反其 AI 滥用准则而采取审查措施。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:34:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</guid>
            <link>https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CursusDB —— 面向文档的内存数据库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CursusDB 是一种面向文档的快速开源内存数据库，提供安全性、持久性、分布性、可用性和类似 SQL 的查询语言。</p><p>CursusDB 的设想是创建无限可扩展的东西，同时又不会真正减慢速度。假设你有 10 亿个文档存储在分布在 100 个节点的 1 个集合中，当集群同时在所有节点上启动非插入操作时，集群将在查询 1000 万个文档所需的时间内查询 10 亿个文档。这就是并行搜索的力量。Cursus 系统可同时在用户集合的多个部分中进行搜索。一个集群可以同时查询数千个节点。将主节点视为多个或一个集合的碎片。每个集合都会锁定插入、更新和删除，但由于 CursusDB 的分布式设计，它就像一个并发交换机，允许大量并发事务。一个集群或多个集群采取操作，这些操作作为请求同时转发到 1 个或多个节点。一致性和可靠性是设计 CursusDB 时的主要目标之一。</p><p style="text-align:start"><strong><span><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>特性</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></strong></p><ul><li>使用共享密钥和 OR TLS 保护集群和节点通信</li><li>运行时内存中的数据</li><li>并行搜索。同时搜索多个节点内的集合部分。</li><li>自动为所有节点生成唯一的所有文档的 $id 键</li><li>具有基本（R、RW）权限的数据库用户</li><li>集群和节点认证</li><li>专门针对读取的集群节点数据复制和同步</li><li>JSON 对象插入</li><li>非结构化集合</li><li>集群和客户端身份验证</li><li>节点（插入、更新、删除）实时转发给观察者</li><li>如果连接丢失，节点观察者自动重新连接</li><li>类似 SQL 的查询语言（CDQL - Cursus 文档查询语言）</li><li>低延迟</li><li>高可用</li><li>默认情况下使用共享密钥和用户确保安全</li><li>高度可配置</li><li>轻量级核心代码总共不到 6000 行代码</li><li>基于<code>log-max-lines</code>配置的文件日志记录和自动日志截断</li><li>自动重新连接任何丢失的节点或节点副本</li><li>如果 .curodeconfig 中的 automatic-backup 设置为 true，则自动备份节点</li><li>如果 .curodeconfig 中的 automatic-backup-cleanup 设置为 true，则自动清理节点备份。</li><li>如果配置了自动备份，则在数据损坏时自动恢复节点</li><li>节点数据 (.cdat) 和节点备份 (/backups/.cdat.{unixtime}) 是在关机或备份时通过序列化-加密 (chacha20poly1305)-压缩 (DEFLATE) 将内存中的数据序列化、加密并逐块压缩后创建的。</li><li>......</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:33:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/cursusdb</guid>
            <link>https://www.oschina.net/p/cursusdb</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 中文对话 0.2B 小模型 ChatLM-Chinese-0.2B]]>
            </title>
            <description>
                <![CDATA[<div align="center"><h1><a id="user-content-中文对话 02b 小模型-chatlm-chinese-02b" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D02b%E5%B0%8F%E6%A8%A1%E5%9E%8B-chatlm-chinese-02b"></a>中文对话 0.2B 小模型 ChatLM-Chinese-0.2B</h1><p>中文  | <a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/README.en.md">English</a></p></div><h1><a id="user-content-一介绍" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%80%E4%BB%8B%E7%BB%8D"></a>一、👋介绍</h1><p>现在的大语言模型的参数往往较大，消费级电脑单纯做推理都比较慢，更别说想自己从头开始训练一个模型了。本项目的目标是整理生成式语言模型的训练流程，包括数据清洗、tokenizer 训练、模型预训练、SFT 指令微调、RLHF 优化等。</p><p>ChatLM-mini-Chinese 为中文对话小模型，模型参数只有 0.2B（算共享权重约 210M），可以在最低 4GB 显存的机器进行预训练（<code>batch_size=1</code>，<code>fp16</code>或者<code> bf16</code>），<code>float16</code>加载、推理最少只需要 512MB 显存。</p><ul><li>公开所有预训练、SFT 指令微调、DPO 偏好优化数据集来源。</li><li>使用<code>Huggingface</code>NLP 框架，包括<code>transformers</code>、<code>accelerate</code>、<code>trl</code>、<code>peft</code>等。</li><li>自实现<code>trainer</code>，支持单机单卡、单机多卡进行预训练、SFT 微调。训练过程中支持在任意位置停止，及在任意位置继续训练。</li><li>预训练：整合为端到端的<code>Text-to-Text</code>预训练，非<code>mask</code>掩码预测预训练。
<ul><li>开源所有数据清洗（如规范化、基于 mini_hash 的文档去重等）、数据集构造、数据集加载优化等流程；</li><li>tokenizer 多进程词频统计，支持<code>sentencepiece</code>、<code>huggingface tokenizers</code>的 tokenizer 训练；</li><li>预训练支持任意位置断点，可从断点处继续训练;</li><li>大数据集（GB 级别）流式加载、支持缓冲区数据打乱，不利用内存、硬盘作为缓存，有效减少内存、磁盘占用。配置<code>batch_size=1, max_len=320</code>下，最低支持在 16GB 内存+4GB 显存的机器上进行预训练；</li><li>训练日志记录。</li></ul></li><li>SFT 微调：开源 SFT 数据集及数据处理过程。
<ul><li>自实现<code>trainer</code>支持 prompt 指令微调， 支持任意断点继续训练；</li><li>支持<code>Huggingface trainer</code>的<code>sequence to sequence</code>微调；</li><li>支持传统的低学习率，只训练 decoder 层的微调。</li></ul></li><li>偏好优化：使用 DPO 进行全量偏好优化。
<ul><li>支持使用<code>peft lora</code>进行偏好优化；</li><li>支持模型合并，可将<code>Lora adapter</code>合并到原始模型中。</li></ul></li><li>支持下游任务微调：<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract">finetune_examples</a>给出<strong>三元组信息抽取任务</strong>的微调示例，微调后的模型对话能力仍在。</li></ul><p>如果需要做基于小模型的检索增强生成（RAG），可以参考我的另一个项目<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese">Phi2-mini-Chinese</a>，代码见<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese%2Fblob%2Fmain%2Frag_with_langchain.ipynb">rag_with_langchain.ipynb</a></p><p>🟢<strong>最近更新</strong></p><details><summary><b>2024-01-07</b></summary>
- 添加数据清洗过程中基于 mini hash 实现的文档去重（在本项目中其实是数据集的样本去重），防止模型遇到多次重复数据后，在推理时吐出训练数据。<br>
- 添加`DropDatasetDuplicate`类实现对大数据集的文档去重。<br></details><details><summary><b>2023-12-29</b></summary>
- 更新模型代码（权重不变），可以直接使用`AutoModelForSeq2SeqLM.from_pretrained(...)`加载模型使用。<br>
- 更新 readme 文档。<br></details><details><summary><b>2023-12-18</b></summary>
- 补充利用`ChatLM-mini-0.2B`模型微调下游三元组信息抽取任务代码及抽取效果展示 。<br>
- 更新 readme 文档。<br></details><details><summary><b>2023-12-14</b></summary>
- 更新 SFT、DPO 后的模型权重文件。 <br>
- 更新预训练、SFT 及 DPO 脚本。 <br>
- 更新`tokenizer`为`PreTrainedTokenizerFast`。 <br>
- 重构`dataset`代码，支持动态最大长度，每个批次的最大长度由该批次的最长文本决定，节省显存。 <br>
- 补充`tokenizer`训练细节。 <br></details><details><summary><b>2023-12-04</b></summary>
- 更新`generate`参数及模型效果展示。<br>
- 更新 readme 文档。<br></details><details><summary><b>2023-11-28</b></summary>
- 更新 dpo 训练代码及模型权重。<br></details><details><summary><b>2023-10-19</b></summary>
- 项目开源， 开放模型权重供下载。 <br></details><h1><a id="user-content-二️chatlm-02b-chinese 模型训练过程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%8C%EF%B8%8Fchatlm-02b-chinese%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>二、🛠️ChatLM-0.2B-Chinese 模型训练过程</h1><h2><a id="user-content-21-预训练数据集" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#21-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"></a>2.1 预训练数据集</h2><p>所有数据集均来自互联网公开的<strong>单轮对话</strong>数据集，经过数据清洗、格式化后保存为 parquet 文件。数据处理过程见<code>utils/raw_data_process.py</code>。主要数据集包括：</p><ol><li>社区问答 json 版 webtext2019zh-大规模高质量数据集，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbrightmart%2Fnlp_chinese_corpus">nlp_chinese_corpus</a>。共 410 万，清洗后剩余 260 万。</li><li>baike_qa2019 百科类问答，见：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F107726">https://aistudio.baidu.com/datasetdetail/107726</a>，共 140 万，清醒后剩余 130 万。</li><li>中国医药领域问答数据集，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FToyhom%2FChinese-medical-dialogue-data">Chinese-medical-dialogue-data</a>，共 79 万，清洗后剩余 79 万。</li><li><del>金融行业问答数据，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F609821974">https://zhuanlan.zhihu.com/p/609821974</a>，共 77 万，清洗后剩余 52 万。</del><strong>数据质量太差，未采用。</strong></li><li>知乎问答数据，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fwangrui6%2FZhihu-KOL">Zhihu-KOL</a>，共 100 万行，清洗后剩余 97 万行。</li><li>belle 开源的指令训练数据，介绍：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>，下载：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2FBelleGroup">BelleGroup</a>，仅选取<code>Belle_open_source_1M</code>、<code>train_2M_CN</code>、及<code>train_3.5M_CN</code>中部分回答较短、不含复杂表格结构、翻译任务（没做英文词表）的数据，共 370 万行，清洗后剩余 338 万行。</li><li>维基百科（Wikipedia）词条数据，将词条拼凑为提示语，百科的前<code>N</code>个词为回答，使用<code>202309</code>的百科数据，清洗后剩余 119 万的词条提示语和回答。Wiki 下载：<a href="https://gitee.com/link?target=https%3A%2F%2Fdumps.wikimedia.org%2Fzhwiki%2F">zhwiki</a>，将下载的 bz2 文件转换为 wiki.txt 参考：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fapertium%2FWikiExtractor">WikiExtractor</a>。</li></ol><p>数据集总数量 1023 万：Text-to-Text 预训练集：930 万，评估集：2.5 万（因为解码较慢，所以没有把评估集设置太大）。<del>测试集：90 万。</del>
SFT 微调和 DPO 优化数据集见下文。</p><h2><a id="user-content-22-模型" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#22-%E6%A8%A1%E5%9E%8B"></a>2.2 模型</h2><p>T5 模型（Text-to-Text Transfer Transformer），详情见论文: <a href="https://gitee.com/link?target=https%3A%2F%2Farxiv.org%2Fabs%2F1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>。</p><p>模型源码来自 huggingface，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftransformers%2Fblob%2Fmain%2Fsrc%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py%23L1557">T5ForConditionalGeneration</a>。</p><p>模型配置见<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese%2Fblob%2Fmain%2Fconfig.json">model_config.json</a>，官方的<code>T5-base</code>：<code>encoder layer</code>和<code>decoder layer </code>均为为 12 层，本项目这两个参数修改为 10 层。</p><p>模型参数：0.2B。词表大小：29298，仅包含中文和少量英文。</p><h2><a id="user-content-23-训练过程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#23-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>2.3 训练过程</h2><p>硬件：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 预训练阶段：</span></span><span id="LC2" class="line">CPU: 28 vCPU Intel<span class="o">(</span>R<span class="o">)</span> Xeon<span class="o">(</span>R<span class="o">)</span> Gold 6330 CPU @ 2.00GHz</span><span id="LC3" class="line">内存：60 GB</span><span id="LC4" class="line">显卡：RTX A5000<span class="o">(</span>24GB<span class="o">)</span><span class="k">*</span> 2</span><span id="LC5" class="line"></span><span id="LC6" class="line"><span class="c"># sft 及 dpo 阶段：</span></span><span id="LC7" class="line">CPU: Intel<span class="o">(</span>R<span class="o">)</span> i5-13600k @ 5.1GHz</span><span id="LC8" class="line">内存：32 GB</span><span id="LC9" class="line">显卡：NVIDIA GeForce RTX 4060 Ti 16GB <span class="k">*</span> 1</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li><p><strong>tokenizer 训练</strong>： 现有<code>tokenizer</code>训练库遇到大语料时存在 OOM 问题，故全量语料按照类似<code>BPE</code>的方法根据词频合并、构造词库，运行耗时半天。</p></li><li><p><strong>Text-to-Text 预训练</strong>：学习率为<code>1e-4</code>到<code>5e-3</code>的动态学习率，预训练时间为 8 天。训练损失：</p></li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/train_loss.png" alt="traing loss" referrerpolicy="no-referrer"></p><ol start="3"><li><strong>prompt 监督微调（SFT）</strong>：使用<code>belle</code>指令训练数据集（指令和回答长度都在 512 以下），学习率为<code>1e-7</code>到<code>5e-5</code>的动态学习率，微调时间 2 天。微调损失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/sft_loss.png" alt="finetune loss" referrerpolicy="no-referrer"></p><ol start="4"><li><strong>dpo 直接偏好优化</strong>：数据集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>作为<code>chosen</code>文本，步骤<code>2</code>中 SFT 模型对数据集中的 prompt 做批量<code>generate</code>，得到<code>rejected</code>文本，耗时 1 天，dpo 全量偏好优化，学习率<code>le-5</code>，半精度<code>fp16</code>,共<code>2</code>个<code>epoch</code>，耗时 3h。dpo 损失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/dpo_loss.png" alt="dpo loss" referrerpolicy="no-referrer"></p><h2><a id="user-content-24-对话效果展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#24-%E5%AF%B9%E8%AF%9D%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA"></a>2.4 对话效果展示</h2><h3><a id="user-content-241-stream-chat" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#241-stream-chat"></a>2.4.1 stream chat</h3><p>默认使用<code>huggingface transformers</code>的 <code>TextIteratorStreamer</code>实现流式对话，只支持<code>greedy search</code>，如果需要<code>beam sample</code>等其他生成方式，请将<code>cli_demo.py</code>的<code>stream_chat</code>参数修改为<code>False</code>。
<img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/stream_chat.gif" alt="" referrerpolicy="no-referrer"></p><h3><a id="user-content-242-对话展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#242-%E5%AF%B9%E8%AF%9D%E5%B1%95%E7%A4%BA"></a>2.4.2 对话展示</h3><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/show1.png" alt="" referrerpolicy="no-referrer"></p><p>存在问题：预训练数据集只有 900 多万，模型参数也仅 0.2B，不能涵盖所有方面，会有答非所问、废话生成器的情况。</p><h1><a id="user-content-三使用说明" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%89%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"></a>三、📑使用说明</h1><h2><a id="user-content-31-快速开始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#31-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"></a>3.1 快速开始：</h2><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="kn">from</span><span class="nn">transformers</span><span class="kn">import</span><span class="n">AutoTokenizer</span><span class="p">,</span><span class="n">AutoModelForSeq2SeqLM</span></span><span id="LC2" class="line"><span class="kn">import</span><span class="nn">torch</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="n">model_id</span><span class="o">=</span><span class="s">'charent/ChatLM-mini-Chinese'</span></span><span id="LC5" class="line"><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="k">if</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span><span class="k">else</span><span class="s">'cpu'</span><span class="p">)</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="n">tokenizer</span><span class="o">=</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span></span><span id="LC8" class="line"><span class="n">model</span><span class="o">=</span><span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span><span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="n">txt</span><span class="o">=</span><span class="s">'如何评价 Apple 这家公司？'</span></span><span id="LC11" class="line"></span><span id="LC12" class="line"><span class="n">encode_ids</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">([</span><span class="n">txt</span><span class="p">])</span></span><span id="LC13" class="line"><span class="n">input_ids</span><span class="p">,</span><span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">]),</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'attention_mask'</span><span class="p">])</span></span><span id="LC14" class="line"></span><span id="LC15" class="line"><span class="n">outs</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">my_generate</span><span class="p">(</span></span><span id="LC16" class="line"><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC17" class="line"><span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC18" class="line"><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span></span><span id="LC19" class="line"><span class="n">search_type</span><span class="o">=</span><span class="s">'beam'</span><span class="p">,</span></span><span id="LC20" class="line"><span class="p">)</span></span><span id="LC21" class="line"></span><span id="LC22" class="line"><span class="n">outs_txt</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outs</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">(),</span><span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></span><span id="LC23" class="line"><span class="k">print</span><span class="p">(</span><span class="n">outs_txt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Apple 是一家专注于设计和用户体验的公司，其产品在设计上注重简约、流畅和功能性，而在用户体验方面则注重用户的反馈和使用体验。作为一家领先的科技公司，苹果公司一直致力于为用户提供最优质的产品和服务，不断推陈出新，不断创新和改进，以满足不断变化的市场需求。</span><span id="LC2" class="line">在 iPhone、iPad 和 Mac 等产品上，苹果公司一直保持着创新的态度，不断推出新的功能和设计，为用户提供更好的使用体验。在 iPad 上推出的 iPad Pro 和 iPod touch 等产品，也一直保持着优秀的用户体验。</span><span id="LC3" class="line">此外，苹果公司还致力于开发和销售软件和服务，例如 iTunes、iCloud 和 App Store 等，这些产品在市场上也获得了广泛的认可和好评。</span><span id="LC4" class="line">总的来说，苹果公司在设计、用户体验和产品创新方面都做得非常出色，为用户带来了许多便利和惊喜。</span><span id="LC5" class="line"></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-32-从克隆仓库代码开始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#32-%E4%BB%8E%E5%85%8B%E9%9A%86%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E5%BC%80%E5%A7%8B"></a>3.2 从克隆仓库代码开始</h2><p>本项目模型为<code>TextToText</code>模型，在预训练阶段、SFT 阶段、RLFH 阶段的<code>prompt</code>、<code>response</code>等字段，请务必加上<code>[EOS]</code>句子结束标记。<br>
本项目模型为<code>TextToText</code>模型，在预训练阶段、SFT 阶段、RLFH 阶段的<code>prompt</code>、<code>response</code>等字段，请务必加上<code>[EOS]</code>句子结束标记。<br>
本项目模型为<code>TextToText</code>模型，在预训练阶段、SFT 阶段、RLFH 阶段的<code>prompt</code>、<code>response</code>等字段，请务必加上<code>[EOS]</code>句子结束标记。</p><h3><a id="user-content-321-克隆项目" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#321-%E5%85%8B%E9%9A%86%E9%A1%B9%E7%9B%AE"></a>3.2.1 克隆项目：</h3><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone <span class="nt">--depth</span> 1 https://github.com/charent/ChatLM-mini-Chinese.git</span><span id="LC2" class="line"></span><span id="LC3" class="line"><span class="nb">cd </span>ChatLM-mini-Chinese</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-322-安装依赖" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#322-%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"></a>3.2.2 安装依赖</h3><p>本项目推荐使用<code>python 3.10</code>，过老的 python 版本可能不兼容所依赖的第三方库。</p><p>pip 安装：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pip <span class="nb">install</span><span class="nt">-r</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果 pip 安装了 CPU 版本的 pytorch，可以通过下面的命令安装 CUDA 版本的 pytorch：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># pip 安装 torch + cu118</span></span><span id="LC2" class="line">pip3 <span class="nb">install </span>torch <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu118</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>conda 安装：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">conda <span class="nb">install</span><span class="nt">--yes</span><span class="nt">--file</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-323-下载预训练模型及模型配置文件" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#323-%E4%B8%8B%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"></a>3.2.3 下载预训练模型及模型配置文件</h3><p>用<code>git</code>命令从<code>Hugging Face Hub</code>下载模型权重及配置文件，需要先安装<a href="https://gitee.com/link?target=https%3A%2F%2Fdocs.github.com%2Fzh%2Frepositories%2Fworking-with-files%2Fmanaging-large-files%2Finstalling-git-large-file-storage">Git LFS</a>，然后运行:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 使用 git 命令下载 huggingface 模型，先安装[Git LFS]，否则下载的模型文件不可用</span></span><span id="LC2" class="line">git clone <span class="nt">--depth</span> 1 https://huggingface.co/charent/ChatLM-mini-Chinese</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="nb">mv </span>ChatLM-mini-Chinese model_save</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>也可以直接从<code>Hugging Face Hub</code>仓库<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>手工下载，将下载的文件移动到<code>model_save</code>目录下即可。</p><h2><a id="user-content-33-tokenizer 训练" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#33-tokenizer%E8%AE%AD%E7%BB%83"></a>3.3 Tokenizer 训练</h2><p>原本打算直接用现成的<code>tokenizer</code>库训练的（如<code>sentencepiece</code>），但是数据集一大就容易 OOM。另外预训练数据集各个领域的语料不平衡，会产生很多不必要的合并。最后使用<code>jieba</code>分词对所有的预训练语料切词后统计词频，只保留出现 1500 次以上的字、词，参照<code>PreTrainedTokenizerFast</code>的<code>BPE model</code>的保存格式，构造<code>tokenzier</code>，最后转换为<code>PreTrainedTokenizerFast</code>。核心代码如下，详细的处理过程见<code>utils/train_tokenizer.py</code>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1"># 构造 merge 数组</span></span><span id="LC2" class="line"><span class="n">words_merge_list</span><span class="o">=</span><span class="p">[]</span></span><span id="LC3" class="line"><span class="k">for</span><span class="n">word</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span></span><span id="LC4" class="line"><span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span></span><span id="LC5" class="line"><span class="k">if</span><span class="n">n</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">:</span></span><span id="LC6" class="line"><span class="c1"># a, b 切分 12345 示例： 1 2345,  12 345,   123 45,   1234 5</span></span><span id="LC7" class="line"><span class="k">for</span><span class="n">i</span><span class="ow">in</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span></span><span id="LC8" class="line"><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]),</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="p">])</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="k">if</span><span class="n">a</span><span class="ow">in</span><span class="n">words_dict</span><span class="ow">and</span><span class="n">b</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">:</span></span><span id="LC11" class="line"><span class="n">words_merge_list</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>本项目还提供了使用预训练模型自带的<code>tokenizer</code>根据自己的语料重新训练<code>tokenizer</code>的例子，见<code>train_tokenizer.ipynb</code>。注意，重新训练<code>tokenizer</code>后，预训练模型的权重将无法使用，需要重新训练模型权重，因为<code>token</code>对应的<code>id</code>变了。</p><h2><a id="user-content-34-text-to-text-预训练" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#34-text-to-text-%E9%A2%84%E8%AE%AD%E7%BB%83"></a>3.4 Text-to-Text 预训练</h2><ol><li>预训练数据集示例</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"对于花园街，你有什么了解或看法吗？"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"花园街（是香港油尖旺区的一条富有特色的街道，位于九龙旺角东部，北至界限街，南至登打士街，与通菜街及洗衣街等街道平行。现时这条街道是香港著名的购物区之一。位于亚皆老街以南的一段花园街，也就是</span><span class="se">\"</span><span class="s2">波鞋街</span><span class="se">\"</span><span class="s2">整条街约 150 米长，有 50 多间售卖运动鞋和运动用品的店舖。旺角道至太子道西一段则为排档区，售卖成衣、蔬菜和水果等。花园街一共分成三段。明清时代，花园街是芒角村栽种花卉的地方。此外，根据历史专家郑宝鸿的考证：花园街曾是 1910 年代东方殷琴拿烟厂的花园。纵火案。自 2005 年起，花园街一带最少发生 5 宗纵火案，当中 4 宗涉及排档起火。2010 年。2010 年 12 月 6 日，花园街 222 号一个卖鞋的排档于凌晨 5 时许首先起火，浓烟涌往旁边住宅大厦，消防接报 4"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li><p>jupyter-lab 或者 jupyter notebook:</p><p>见文件<code>train.ipynb</code>，推荐使用 jupyter-lab，避免考虑与服务器断开后终端进程被杀的情况。</p></li><li><p>控制枱：</p><p>控制枱训练需要考虑连接断开后进程被杀的，推荐使用进程守护工具<code>Supervisor</code>或者<code>screen</code>建立连接会话。</p><p>首先要配置<code>accelerate</code>，执行以下命令， 根据提示选择即可，参考<code>accelerate.yaml</code>，<em>注意：DeepSpeed 在 Windows 安装比较麻烦</em>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">accelerate config</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>开始训练，如果要使用工程提供的配置请在下面的命令<code>accelerate launch</code>后加上参数<code>--config_file ./accelerate.yaml</code>，<em>该配置按照单机 2xGPU 配置。</em></p><p><em>预训练有两个脚本，本项目实现的 trainer 对应<code>train.py</code>，huggingface 实现的 trainer 对应<code>pre_train.py</code>，用哪个都可以，效果一致。本项目实现的 trainer 训练信息展示更美观、更容易修改训练细节（如损失函数，日志记录等），均支持断点继续训练，本项目实现的 trainer 支持在任意位置断点后继续训练，按<code>ctrl+c</code>退出脚本时会保存断点信息。</em></p><p>单机单卡：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer</span></span><span id="LC2" class="line">accelerate launch ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">python pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>单机多卡：
<code>2</code>为显卡数量，请根据自己的实际情况修改。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>从断点处继续训练：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train <span class="nt">--is_keep_training</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line"><span class="c"># 需要在`pre_train.py`中的`train`函数添加`resume_from_checkpoint=True`</span></span><span id="LC6" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li></ol><h2><a id="user-content-35-sft 微调" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#35-sft%E5%BE%AE%E8%B0%83"></a>3.5 SFT 微调</h2><p>SFT 数据集全部来自<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>大佬的贡献，感谢。SFT 数据集分别为：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Fgenerated_chat_0.4M">generated_chat_0.4M</a>、<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_0.5M_CN">train_0.5M_CN</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_2M_CN">train_2M_CN</a>，清洗后剩余约 137 万行。
sft 指令微调数据集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"解释什么是欧洲启示录"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"欧洲启示录（The Book of Revelation）是新约圣经的最后一卷书，也被称为《启示录》、《默示录》或《约翰默示录》。这本书从宗教的角度描述了世界末日的来临，以及上帝对世界的审判和拯救。 书中的主题包括来临的基督的荣耀，上帝对人性的惩罚和拯救，以及魔鬼和邪恶力量的存在。欧洲启示录是一个充满象征和暗示的文本，对于解读和理解有许多不同的方法和观点。"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>参考<code>data</code>目录下的示例<code>parquet</code>文件制作自己的数据集，数据集格式：<code>parquet</code>文件分两列，一列<code>prompt</code>文本，表示提示语，一列<code>response</code>文本，表示期待的模型输出。
微调细节见<code>model/trainer.py</code>下的<code>train</code>方法, <code>is_finetune</code>设置为<code>True</code>时，将进行微调，微调默认会冻结 embedding 层和 encoder 层，只训练 decoder 层。如需要冻结其他参数，请自行调整代码。</p><p>运行 SFT 微调：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer， 添加参数`--is_finetune=True`即可, 参数`--is_keep_training=True`可从任意断点处继续训练</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py <span class="nt">--is_finetune</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer, 多 GPU 请用 accelerate launch --multi_gpu --num_processes gpu 个数 sft_train.py</span></span><span id="LC5" class="line">python sft_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-36-rlhf 强化学习人类反馈优化方法" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#36-rlhf%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"></a>3.6 RLHF（强化学习人类反馈优化方法）</h2><p>偏好方法这里介绍常见的两种：PPO 和 DPO，具体实现请自行搜索论文及博客。</p><ol><li><p>PPO 方法（近似偏好优化,Proximal Policy Optimization）<br>
步骤 1：使用微调数据集做有监督微调（SFT， Supervised Finetuning）。<br>
步骤 2：使用偏好数据集（一个 prompt 至少包含 2 个回复，一个想要的回复，一个不想要的回复。多个回复可以按照分数排序，最想要的分数最高）训练奖励模型（RM， Reward Model）。可使用<code>peft</code>库快速搭建 Lora 奖励模型。<br>
步骤 3：利用 RM 对 SFT 模型进行有监督 PPO 训练，使得模型满足偏好。</p></li><li><p>使用 DPO（直接偏好优化，Direct Preference Optimization）微调（<strong>本项目采用 DPO 微调方法，比较节省显存</strong>）
在获得 SFT 模型的基础上，无需训练奖励模型，取得正向回答（chosen）和负向回答（rejected）即可开始微调。微调的<code>chosen</code>文本来自原数据集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>，拒绝文本<code>rejected</code>来自 SFT 微调 1 个 epoch 后的模型输出，另外两个数据集：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FSkepsun%2Fhuozi_rlhf_data_json">huozi_rlhf_data_json</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fbeyond%2Frlhf-reward-single-round-trans_chinese">rlhf-reward-single-round-trans_chinese</a>，合并后共 8 万条 dpo 数据。</p><p>dpo 数据集处理过程见<code>utils/dpo_data_process.py</code>。</p></li></ol><p>DPO 偏好优化数据集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="w"></span><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"为给定的产品创建一个创意标语。，输入：可重复使用的水瓶。"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"chosen"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">保护地球，从拥有可重复使用的水瓶开始！</span><span class="se">\"</span><span class="s2">"</span><span class="p">,</span></span><span id="LC4" class="line"><span class="w"></span><span class="nl">"rejected"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">让你的水瓶成为你的生活伴侣，使用可重复使用的水瓶，让你的水瓶成为你的伙伴</span><span class="se">\"</span><span class="s2">"</span></span><span id="LC5" class="line"><span class="w"></span><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>运行偏好优化：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c">#  多 GPU 请用 accelerate launch --multi_gpu --num_processes gpu 个数 dpo_train.py</span></span><span id="LC2" class="line">python dpo_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-37-推理" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#37-%E6%8E%A8%E7%90%86"></a>3.7 推理</h2><p>确保<code>model_save</code>目录下有以下文件，这些文件都可以在<code>Hugging Face Hub</code>仓库<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>中找到：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">ChatLM-mini-Chinese</span><span id="LC2" class="line">├─model_save</span><span id="LC3" class="line">|  ├─config.json</span><span id="LC4" class="line">|  ├─configuration_chat_model.py</span><span id="LC5" class="line">|  ├─generation_config.json</span><span id="LC6" class="line">|  ├─model.safetensors</span><span id="LC7" class="line">|  ├─modeling_chat_model.py</span><span id="LC8" class="line">|  ├─special_tokens_map.json</span><span id="LC9" class="line">|  ├─tokenizer.json</span><span id="LC10" class="line">|  └─tokenizer_config.json</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li>控制枱运行：</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python cli_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li>API 调用</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python api_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>API 调用示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">curl <span class="nt">--location</span><span class="s1">'127.0.0.1:8812/api/chat'</span><span class="se">\</span></span><span id="LC2" class="line"><span class="nt">--header</span><span class="s1">'Content-Type: application/json'</span><span class="se">\</span></span><span id="LC3" class="line"><span class="nt">--header</span><span class="s1">'Authorization: Bearer Bearer'</span><span class="se">\</span></span><span id="LC4" class="line"><span class="nt">--data</span><span class="s1">'{</span></span><span id="LC5" class="line"><span class="s1">    "input_txt": "感冒了要怎么办"</span></span><span id="LC6" class="line"><span class="s1">}'</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/api_example.png" alt="api demo" referrerpolicy="no-referrer"></p><h2><a id="user-content-38-下游任务微调" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#38-%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83"></a>3.8 下游任务微调</h2><p>这里以文本中三元组信息为例，做下游微调。该任务的传统深度学习抽取方法见仓库<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。抽取出一段文本中所有的三元组，如句子<code>《写生随笔》是冶金工业 2006 年出版的图书，作者是张来亮</code>，抽取出三元组<code>(写生随笔,作者,张来亮)</code>和<code>(写生随笔,出版社,冶金工业)</code>。</p><p>原始数据集为：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F11384">百度三元组抽取数据集</a>。加工得到的微调数据集格式示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"请抽取出给定句子中的所有三元组。给定句子：《家乡的月亮》是宋雪莱演唱的一首歌曲，所属专辑是《久违的哥们》"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"[(家乡的月亮,歌手,宋雪莱),(家乡的月亮,所属专辑,久违的哥们)]"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>可以直接使用<code>sft_train.py</code>脚本进行微调，脚本<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract/finetune_IE_task.ipynb">finetune_IE_task.ipynb</a>里面包含详细的解码过程。训练数据集约<code>17000</code>条，学习率<code>5e-5</code>，训练 epoch<code>5</code>。微调后其他任务的对话能力也没有消失。</p><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/ie_task_chat.png" alt="信息抽取任务微调后的对话能力" referrerpolicy="no-referrer"></p><p>微调效果：
将<code>百度三元组抽取数据集</code>公开的<code>dev</code>数据集作为测试集，对比传统方法<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。</p><table><thead><tr><th align="left">模型</th><th align="center">F1 分数</th><th align="center">精确率 P</th><th align="center">召回率 R</th></tr></thead><tbody><tr><td align="left">ChatLM-Chinese-0.2B 微调</td><td align="center">0.74</td><td align="center">0.75</td><td align="center">0.73</td></tr><tr><td align="left">ChatLM-Chinese-0.2B 无预训练</td><td align="center">0.51</td><td align="center">0.53</td><td align="center">0.49</td></tr><tr><td align="left">传统深度学习方法</td><td align="center">0.80</td><td align="center">0.79</td><td align="center">80.1</td></tr></tbody></table><p>备注：<code>ChatLM-Chinese-0.2B 无预训练</code>指直接初始化随机参数，开始训练，学习率<code>1e-4</code>，其他参数和微调一致。</p><h1><a id="user-content-四引用" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E5%9B%9B%E5%BC%95%E7%94%A8"></a>四、🎓引用</h1><p>如果你觉得本项目对你有所帮助，欢迎引用。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">@<span class="n">misc</span>{<span class="n">Charent2023</span>,</span><span id="LC2" class="line"><span class="n">author</span>={<span class="n">Charent</span><span class="n">Chen</span>},</span><span id="LC3" class="line"><span class="n">title</span>={<span class="n">A</span><span class="n">small</span><span class="n">chinese</span><span class="n">chat</span><span class="n">language</span><span class="n">model</span><span class="n">with</span><span class="m">0</span>.<span class="m">2</span><span class="n">B</span><span class="n">parameters</span><span class="n">base</span><span class="n">on</span><span class="n">T5</span>},</span><span id="LC4" class="line"><span class="n">year</span>={<span class="m">2023</span>},</span><span id="LC5" class="line"><span class="n">publisher</span> = {<span class="n">GitHub</span>},</span><span id="LC6" class="line"><span class="n">journal</span> = {<span class="n">GitHub</span><span class="n">repository</span>},</span><span id="LC7" class="line"><span class="n">howpublished</span> = {\<span class="n">url</span>{<span class="n">https</span>://<span class="n">github</span>.<span class="n">com</span>/<span class="n">charent</span>/<span class="n">ChatLM</span>-<span class="n">mini</span>-<span class="n">Chinese</span>}},</span><span id="LC8" class="line">}</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h1><a id="user-content-五其他事项" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%94%E5%85%B6%E4%BB%96%E4%BA%8B%E9%A1%B9"></a>五、🤔其他事项</h1><p>本项目不承担开源模型和代码导致的数据安全、舆情风险或发生任何模型被误导、滥用、传播、不当利用而产生的风险和责任。</p>
]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:15:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/charent/ChatLM-mini-Chinese</guid>
            <link>https://gitee.com/charent/ChatLM-mini-Chinese</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | vivo 海量微服务架构最新实践]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section style="font-size: 15px;line-height: 1.6;"><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(219, 219, 219);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgba(0, 0, 0, 0.5);font-size: 14px;text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">作者：来自 vivo 互联网中间件团队</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="text-align: left;justify-content: flex-start;display: flex;flex-flow: row;margin-bottom: 10px;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;align-self: flex-start;flex: 0 0 auto;background-color: rgb(234, 241, 255);border-style: solid;border-width: 0px 0px 0px 4px;border-color: rgb(48, 97, 207) rgb(48, 97, 207) rgb(48, 97, 207) rgb(21, 151, 239);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">本文根据罗亮老师在「2023 vivo 开发者大会"现场演讲内容整理而成。公众号回复【2023 VDC】获取互联网技术分会场议题相关资料。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: left;" powered-by="xiumi.us"><section style="text-align: justify;line-height: 1.8;padding-right: 5px;padding-left: 5px;color: rgb(160, 160, 160);"><p style="text-wrap: wrap;">vivo 微服务平台为全球 5 亿+用户背后的全网十万级机器、万级微服务提供服务，在高效实践过程中，vivo 中间件平台团队输出了一套业务适用的微服务架构最佳实践--架构能力矩阵、高效的开源中间件组件全生命周期管理策略，走出了一条从开源到开源+自研的技术演进路径，通过微服务引擎升级和统一平台建设较好解决了面临的问题与挑战。</p></section></section><section style="margin-right: 0%;margin-bottom: -5px;margin-left: 0%;text-align: right;line-height: 1;font-size: 5px;transform: translate3d(5px, 0px, 0px);" powered-by="xiumi.us"><section style="width: 0px;display: inline-block;vertical-align: top;border-bottom: 0.6em solid rgb(160, 160, 160);border-right: 0.6em solid rgb(160, 160, 160);border-top: 0.6em solid transparent !important;border-left: 0.6em solid transparent !important;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>一、vivo 从 0 到 1 的微服务架构工程实践</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.1 为什么需要微服务及落地挑战</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">伴随业务的高速发展，业务的复杂度越来越高，用户规模和访问量也越来越大；项目的迭代速度越来越快，交付效率要求也越来越高。与此同时，服务的集群规模越来越大，部署架构越来越复杂，故障范围也越来越不可控。此外，突增的业务流量时刻考验着服务的水平扩容能力，创新业务的快速孵化也对服务的可扩展性提出了更高的要求。想要解决以上问题，业务架构会朝着微服务架构方向演进。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014317" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/0efbe187-74ec-4c6c-9f03-665534c87cb6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">正是在这样的背景下，vivo 于 2015 年开始微服务架构改造，在落地过程中碰到了以下问题：</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>一是</strong>：服务数量多，配置修改生效、服务发布等变更场景效率低下；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>二是</strong>：业务链路长，高可用保障难，问题与故障定位耗时长，服务的维护成本高；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>三是</strong>：大量的跨服务通讯，性能和访问体验优化提升难度大；</p><p style="text-wrap: wrap;"><strong>四是</strong>：一个业务链路涉及大量的上下游团队，对接沟通的协作成本高；</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">为了解决以上落地过程中的开发、运维、团队协作等难题，我们需要建设配套的微服务架构技术体系。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.2 vivo 微服务架构最佳实践-架构能力矩阵</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">建设一套微服务架构技术体系，助力业务又快又好地构建微服务工程，需要哪些技术能力？我们对微服务架构的主要业务场景进行了分析，在业务实践过程中，微服务主要会涉及同步、异步、定时任务三大核心业务场景。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014318" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/b246fe36-cacc-49af-a105-9c44ade187ea.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>同步调用</strong>场景：涉及的技术能力主要是 RPC 框架、注册中心、服务治理；</p><p style="margin-bottom: 10px;">在<strong>异步调用</strong>场景：涉及的技术能力主要是消息中间件；</p><p>在<strong>定时任务</strong>场景：涉及的技术能力主要是分布式任务调度。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了上面介绍的框架和系统，业务在微服务架构改造过程中，需要的能力全貌是怎样的？</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在深度参与业务微服务架构改造过程中，我们对最佳实践能力项进行了抽象，从而形成了 vivo 内部的微服务架构最佳实践总结-架构能力矩阵，总计近 30 项能力。为了更直观的呈现这些能力，我们从接入层、服务层、数据层的三层架构分层，开发、运维等 DevOps 的关键环节对架构能力进行了梳理，如下图所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014319" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d1fb2470-8f85-4233-b3f2-010196469737.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;"><br></p></section><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在开发环节：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>在开发接口时</strong>，我们要实现内外网接口分离，保障接口的安全性，为此我们要接入网关来隔离内外网接口；在接入层和服务层，我们可以通过治理平台来实现限流、熔断、降级等能力，保障业务的高可用。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在构建内部服务时</strong>，我们要尽可能实现服务无状态，通过 RPC 框架实现内部接口的 RPC 相互调用，具备异常重试能力，提升服务的鲁棒性；在编码过程中，我们通过接入配置中心实现代码与配置分离，具备运行时动态调整配置的能力，提高服务的变更效率。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在异步调用场景</strong>，我们可以通过接入消息中间件实现业务间的相互解耦、流量削峰；在定时任务场景，我们可以通过分布式任务调度系统，实现失败任务的自动转移能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，我们可以通过落地存储与计算分离能力，实现服务层和数据层的解耦，便于分层扩容，具备面向未来更大规模业务的扩展能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在数据层</strong>，通过落地读写分离、冷热分离等能力，提升系统性能，节省存储成本；同时将这些能力通过研发框架进行封装，便于业务侧复用。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在运维环节：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">我们可以借助 CDN 实现网站的动静分离访问，减小系统的请求压力；在日常运维过程中，我们要实现服务的可灰度、可回滚；服务节点无单点；同时借助容器技术快速实现弹性伸缩能力；提升系统的故障恢复速度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在部署时</strong>，通过部署与发布分离，可以较好规避发布变更时产生的问题，即服务部署成功，并且健康检查通过后再发布到生产环境，减小故障的影响范围。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在遇到严重的系统故障时</strong>，需要具备使用备份数据从零恢复的能力，同时对所有已知的故障场景要有对应的预案，提升系统的故障应对能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在数据运维上</strong>，我们要确保数据属主唯一，避免多个业务对同一个数据库进行访问；同时也要实现业务数据和大数据的存储隔离，避免相互影响。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了以上能力之外，我们<strong>还要</strong>实现业务的安全合规，建设覆盖 Metric、Trace、Log 的可观测能力体系，便于对故障问题的定位排查；在多机房层面，需要具备同城双活、异地多活等跨机房容灾能力。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.3 vivo 微服务平台能力</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">为了更好落地以上最佳实践，我们构建了一套从接入层、服务层、消息层、框架层到存储层的平台能力，完整的平台能力地图如下图所示：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014320" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/3b0a6fb7-b253-48eb-921d-5056d3d9204a.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>接入层</strong>，我们提供了四层流量网关和七层微服务 API 网关；在服务层提供了服务/流量治理平台、配置中心、注册中心、接口管理平台、分布式任务调度等系统。</p><p style="margin-bottom: 10px;">在<strong>消息层</strong>提供了消息中间件；在框架层提供了脚手架，可快速集成日志、配置、限流/熔断、MySQL/Redis 等 SDK，以及 RPC 框架。</p><p style="margin-bottom: 10px;">在<strong>存储层</strong>提供了 DaaS 平台，包含 MySQL、Redis、ElasticSearch、MongoDB、文件服务等系统能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">为了更好排查故障问题，我们在可观测领域构建了监控中心、日志中心、调用链等系统；此外，还有更好支撑服务构建、变更发布的 CICD 系统和 IT 基础设施的配置管理系统 CMDB。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">截止 2019 年，vivo 基本完成了从 0 到 1 的微服务平台能力烟囱式建设。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">快速构建这些能力的过程，离不开开源组件的赋能。例如微服务 API 网关背后的 zuul，注册中心背后的 ZooKeeper 和 etcd，RPC 框架的 Dubbo 和 bRPC；配置中心的 Apollo 和 Nacos，流量治理的 hystrix 和 sentinel，消息中间件的 RabbitMQ 和 RocketMQ，任务调度的 xxl-job；如下图所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014321" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d29efb2f-0028-42ec-b597-77c064aef74c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">在此，我们也通过 VDC(vivo 开发者大会) 平台，感谢开源社区的赋能，助力 vivo 微服务架构技术体系从 0 到 1 的快速构建。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.4 vivo 微服务现状</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">截止当前，vivo 的微服务平台为全球分布在 60+个国家/地区的 5 亿+用户提供服务；其中 vivo 现有万级的微服务，覆盖全网机器规模十万级，每天处理高达 8000 亿次的 RPC 调用次数，流量的峰值 QPS 达到千万级以上。<span style="text-align: center;letter-spacing: 0.034em;"></span></p><p><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014323" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/89e7af31-f3b1-48ab-8291-f6bdfc728e87.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在支撑如此规模的微服务过程中，特别是在 2020 年以后，我们碰到了较多的问题与挑战，为了解决这些问题，我们使用了微服务引擎升级和统一平台建设的解决方案；下面来一起看看我们碰到了哪些问题与挑战？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>二、微服务引擎升级与统一平台建设</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.1 面临的问题与挑战</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我们知道，注册中心和配置中心是微服务架构领域的技术基石；下面给大家说明下我们在这两个基石系统实践过程中遇到的<strong>问题与挑战</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014324" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/fd67ab5a-9617-4bd1-8251-85c51a7af775.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是注册中心，众所周知，ZK 是 CP 特性，在注册中心场景有较多不可用的问题，此外还有跨机房多活能力缺失，集群故障半径大等问题；写性能无法水平扩展，在大规模 Dubbo 服务场景中，接口级注册模型注册的数据量大，在业务高频变更期间网卡的带宽峰值会超过 1000Gbps。此外还有业务易混用，功能缺失；内部的多个技术栈使用不同的注册中心，跨技术栈调用的研发运维成本高等问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在配置中心场景，存在应用、组件配置的变更通道不统一，故障场景配置回滚慢，变更审计日志分散，业务恢复耗时长等问题；配置变更下发的时效不满足业务要求，内部存在多套配置中心，都需要和业务研发流程打通，存在审批、审计、回滚等功能没有对齐的问题；此外在功能和安全上，还需要实现内部的配置定时生效，配置加解密等需求，配置访问通道符合公司的安全要求。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">从以上的问题与挑战中可以看出，基于开源组件快速构建的微服务底层引擎在 vivo 的内部业务场景中存在较多的可用性、性能&amp;容量、研发运维、功能&amp;安全问题与挑战。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="font-size: 16px;color: rgb(65, 95, 255);" powered-by="xiumi.us"><p style="text-wrap: wrap;">2.2 注册中心引擎升级</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">为了解决以上的问题与挑战，我们需要进行技术升级，首先给大家介绍的是注册中心的<strong>解决方案</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014325" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bc83c41-6a74-4125-9924-22bc26ac900c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">针对 Dubbo 接口级服务发现导致 ZK 注册中心流量过大的问题，业界同行都在往应用级服务发现迁移来构建解决方案；通过 Dubbo 开源社区官网的介绍，我们可以看到，应用级服务发现是适应云原生，支持更大规模的服务发现模型；</p><p style="margin-bottom: 10px;">将 Dubbo 接口级服务发现模型升级为应用级，可降低单机 50% 的内存消耗，降低注册中心集群 90% 的存储与推送压力，从架构上支持百万实例集群规模；</p><p>因此我们需要将 Dubbo 框架服务发现模型从接口级升级为应用级，彻底解决注册数据量大，对注册中心请求压力大的问题，同时具备面向云原生微服务架构的扩展能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，针对注册中心的可用性、性能&amp;容量、研发运维等问题，我们需要建设满足 AP 特性、支持跨机房多活的统一注册中心，使用 Session+Data 分离架构，Data 层持久化数据，Session 层处理和客户端的长连接，无状态 Session 层能较好收敛客户端请求，实现读写流量隔离，具备较好的横向扩展能力，真正解决注册中心的性能、容量和扩展性问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>综上</strong>，我们需要构建 Dubbo 应用级服务发现能力，构建 Session+Data 分离的统一注册中心，内部的项目代号为 vns。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">从上面的技术方案分析中，我们可以看到，通过应用级注册可以彻底解决注册中心的流量突刺问题；通过 Session+Data 双层分离架构可以实现业务无感知的多集群拆分，有效缩小故障半径，那如何来<strong>落地</strong>呢？</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014326" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/444085d9-cd4f-491a-b55b-5ec48ccb6c28.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">我们首先想到的就是上图左侧的技术方案，通过构建暴露 gRPC 协议、支持应用级注册的 vns 系统，海量的 Dubbo 服务通过双注册来实现迁移；但是在经过详细的技术分析之后，我们发现该方案存在明显的<strong>耦合问题：</strong></p><p style="margin-bottom: 10px;">首先是 Dubbo 应用级注册升级的进展依赖 vns 系统的建设进度，Dubbo 框架依赖稳定的 vns SDK，Dubbo 框架和 vns 系统之间存在进度依赖问题；</p><p style="margin-bottom: 10px;">其次还存在回滚依赖问题，当 vns 系统因灰度异常回滚时，Dubbo 应用级注册升级进度也会同步回滚；</p><p style="margin-bottom: 10px;">同理当 Dubbo 流量切换异常回滚时，vns 的业务接入进度也会回退。</p><p style="margin-bottom: 10px;">此外，部分不迭代的业务可能需要继续使用接口级注册，无法实现 ZK 注册中心的完全下线。</p><p>为了解决以上问题，我们对技术方案进行了升级，改用通过 vns 系统暴露和支持 ZK 协议，实现 Dubbo 应用级注册升级和 vns 系统的能力建设解耦；当 vns 系统的能力建设进展还未达到生产环境要求时，我们可以通过引入一套新的 ZK 集群来支持 Dubbo 的应用级注册模型升级；当 vns 的能力成熟度达到生产环境的要求后，可以对引入的 ZK 集群进行替代，整个过程可以根据系统建设进展和可用性保障要求，进行可控的灰度放量和回滚操作，控制变更风险；最终，vns 通过暴露 ZK+gRPC 双协议满足业务的接入诉求。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在整个技术方案落地过程中，我们始终坚持业务导向原则，实现业务升级和迁移的零|低成本；采用稳妥、完善的升级迁移方案，确保过程可灰度、可回滚、可观测；大家可以看到，我们通过兼容 ZK 协议，最大限度的保障 Dubbo 业务的平滑升级，切换方案做到了可灰度可回滚可观测，在减少升级成本的同时，降低项目落地风险，最终实现 ZK 注册中心的完全下线。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.3 配置中心引擎升级</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介绍完注册中心，我们再来看看配置中心的解决方案，配置中心主要解决的是配置通道不统一，性能不达标，无法满足内部的业务需求等问题。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014327" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/4591d606-98ae-4070-823d-8aff13aa8f98.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">上图左侧是我们最新的配置中心技术架构图，右侧是统一配置通道的示意图，我们通过支持应用配置与组件配置的统一配置通道，实现了配置管理能力的收敛统一，在此基础上，建设一键审批/审计/回滚等能力，实现了和内部业务研发流程的打通，减少人力运维投入；此外，在新版配置中心上，我们也实现了较多的高可用、性能、安全、可观测能力增强等业务诉求；在配置中心升级过程中，我们追求业务的无感知升级，通过兼容原有配置中心对外开放的接口，实现了新系统的平滑升级，原有系统优雅下线。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，和注册中心的升级方案类似，在配置中心的技术方案设计中，我们也较好的遵循了业务导向原则。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.4 统一微服务平台建设</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介绍完注册中心和配置中心等微服务引擎的技术升级方案，我们再来看下从 0 到 1 快速构建的烟囱式微服务平台会面临哪些问题和挑战？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014328" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/2dcc08b7-f604-43a4-85d3-b7c2ae6eedf3.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">从上图左侧示意图中可以看到，我们快速构建的微服务平台存在 10 个以上的模块，每个模块都有独立的入口，用户使用平台的易用性很低；此外，这些模块在建设过程中，还需要重复对接云平台、单点登录、权限、工单、监控、CMDB 等公共服务系统；系统审计日志分散，不便于快速定位因变更引起的问题；综上，烟囱式微服务平台存在多入口，功能重复对接，运维、研发成本高，故障排查与恢复效率低，易用性不足等问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">要解决烟囱式微服务平台的问题，需要构建更合理的产品方案，我们对用户的使用现状进行了分析：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014329" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/f3ad12b5-778a-41ec-9496-bd59e3ab55a6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">通过系统埋点数据发现，烟囱式微服务平台中用户使用频率最高的两个系统分别是配置中心、服务治理。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">通过上图左侧的 PV/UV 饼状图数据，大家可以发现：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: center;"><p style="text-align: left;">配置中心的用户访问主要集中在配置的【查询与变更】、【变更记录与审批】和配置变更相关的 2 个页面上，服务治理的用户访问主要集中在【服务概览】、【服务查询】和服务相关的 2 个页面上。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">基于埋点数据，我们可以看到用户的访问集中在少数的几个功能上，通过整合各个系统模块高频使用的功能，建设统一的平台入口，实现系统间联动，这也给我们如何建设统一平台提供了较好的思路。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，在对各个模块的技术架构进行分析时，我们识别到了位于最底层、技术依赖程度最高的两个系统：配置中心、注册中心，这两个系统非常适合作为统一平台建设的技术底座。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014330" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bb1b5bf-7287-4f3d-884c-ea3b5ab7d7eb.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">区别于烟囱式微服务平台的多个系统模块独立对接 CICD 等研发平台，在统一微服务平台建设中，我们升级为统一平台对接 CICD 等研发平台；我们的建设思路是，以配置中心/注册中心为底座来建设统一微服务平台：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>：基于统一的配置通道与 CICD 等研发平台系统进行联动，建设一键审批、回滚能力，整合研发流程，降低对接成本；</p><p><strong>二是</strong>：通过统一平台的建设，实现平台间联动，建设高阶的自动化水平，支撑业务进一步提升持续服务能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.5 引擎升级&amp;统一平台建设总结</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">接下来，对我们前面讲到的内容做一个总结：在大规模、海量业务的微服务架构实践过程中，我们通过引擎升级和统一平台能力建设较好的解决了碰到的问题与挑战。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014331" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/16e89ce4-7fa5-497c-9a09-4b45df61c722.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在升级和建设过程中，我们需要保证现有业务的连续性，保障不发生因底层引擎升级和平台建设导致的可用性问题。因此，引擎升级和统一平台建设的工作需要建立在高可用保障的基础上；换句话来说，可用性是我们所有工作的底座。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在这个基础上，我们实现注册中心和配置中心的引擎升级，完成应用级注册模型升级；在这个过程中，解决底层引擎的扩展性、容量、性能、可维护性和安全性等问题；最后，我们要建设统一的微服务平台能力，实现平台间联动，构建自动/自助化使用能力；赋能业务。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，通过完整的方案介绍，在上图右侧我们呈现了微服务架构实践过程中的价值分层逻辑，即在可用性的基础上，提升系统的扩展性、容量、性能、可维护、安全性等能力；然后再在此基础上，交付更高的研发效率，更好的用户使用体验。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>三、微服务架构升级的总结与展望</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介绍完我们的解决方案后，最后来说明下我们对微服务架构升级的总结与思考，以及对未来的展望。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.1 拥抱开源的实用主义</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在构建微服务架构技术体系的过程中，我们始终坚持拥抱开源，迭代业务适用的技术平台；结合内部业务的实际情况，我们走出了一条从开源到开源+自研的研发路径。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014332" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8f26fca0-069c-4375-850c-26e6d7c79812.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在从 0 到 1 的平台能力建设过程中，我们引入开源组件进行能力快速构建，快速交付满足业务的需求；始终坚持业务适用原则，不过度设计，支撑业务的快速迭代；以上阶段，我们称之为「拿来主义」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在面向更大规模、海量业务实践过程中，为了解决碰到的问题与挑战，我们在开源的基础上进行增强，自研部分能力来解决亿级用户规模下内部业务的功能，性能，容量，研发流程打通等需求；这个阶段，我们称之为「实用主义」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在技术平台迭代过程中，我们始终坚持 2 个原则，一是简单有效原则，坚持用最简单的解决方案来解决问题；二是迭代和演进原则，坚持平台持续迭代和演进的原则；前期基于开源组件快速搭建能力，再基于实际的业务需求和痛点来落地自研架构；在这个过程中，始终坚持业务适用，不为了技术而技术，避免大而全的技术架构。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，也要说明一个常见的误区，我们为什么不完全自研？vivo 的微服务平台建设从开源社区获益良多，坚持不闭门造车，站在巨人肩膀上，持续引入优秀特性来支撑业务的快速发展，同时也会考虑将部分行业适用的通用优秀特性反馈给社区，和社区共同成长。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.2&nbsp;中间件组件全生命周期管理</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">大家可以看到，vivo 的微服务架构技术体系引入了较多的开源组件，在实践过程中，我们摸索出了一套完整的中间件组件全生命周期管理策略。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014333" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e436a13c-2e63-49df-a26a-8784d90e98a0.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我们先来看看业务的诉求和底层技术的<strong>特点</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是业务的诉求：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">业务期望更高的迭代交付效率；</span></p></li><li><p style="margin-bottom: 10px;">快速引入新技术，使用新技术助力业务创新，但很多时候新技术往往意味着成熟度不足，可能存在较多问题；</p></li><li><p style="margin-bottom: 10px;"><span style="letter-spacing: 0.034em;">业务的不断创新与发展，对组件的性能、容量要求越来越高；</span><br></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">对业务来说，高效迭代交付需求是第一位的。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(14, 14, 13);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">然而，底层技术有它自己的特点：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">技术的发展有它的客观规律，需要经历萌芽期 → 膨胀期 → 低谷期→ 复苏期→ 成熟期等多个阶段；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">缺</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">乏约束的技术体系必然随着时间推移而腐化，治理不及时会成为技术债务，阻塞业务发展；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">同类中间件组件的快速引入会有重复建设的效率问题；</span></span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">中间件组件的技术升级周期客观上都比较长。</span></span><span style="letter-spacing: 0.034em;"></span></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">实践证明，只有足够稳健的底层技术能力才能更好支撑业务的高效迭代。在这个过程中，如何兼顾效率与质量？尊重客观规律，确保整个过程都有明确的目标和方向，避免走偏，慢就是快。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我们认为，完善的中间件组件全生命周期管理策略，首先需要在所有的技术团队中形成价值共识；再通过组件扫描和组件地图等手段及时对组件全貌进行洞察；在组件的标准化治理和运营阶段实现有规范，补短板；同时在新技术引入时，通过完善的新技术引入规范，覆盖功能/性能/容量/扩展性/成熟度/使用成本等维度；在组件的版本治理上，使用基线版本治理方案，输出明确的使用标准/版本升级方案/版本收敛策略；最后，在组件的成熟度管理上，我们可以借助 Gartner(高德纳) 技术成熟度说明和组件能力矩阵，不断提升组件的成熟度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">综上，为更高效的支撑业务，在组件管理上我们使用了更加入宽松的引入策略，同时也会对组件的全生命周期进行严格管理，践行宽入严出策略，通过完善的中间件组件全生命周期管理助力业务跑的更快，走的更远。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.3 引擎升级探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">展望未来，我们会坚持和践行引擎升级和平台建设的<strong>持续迭代思路</strong>：</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">首先是对引擎升级的探索，通过引入新技术来解决当前碰到的研发效率、成本等痛点问题：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014334" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8bd511dc-cb3c-4056-a1f1-5fca3b345d4b.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在研发效率方向</strong>，存在的痛点问题如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>，组件 SDK 的升级周期长，碎片化问题严重；</p><p><strong>二是</strong>，当前 vivo 内部主要的是 Java、C++技术栈，新业务形态孵化可能会引入新的技术栈，需能够较好解决跨技术栈的问题。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">想要较好的解决以上问题，需要探索基于 Java Agent/SideCar 技术的标准 ServiceMesh 模式，将 RPC、MQ 等中间件能力下沉，透明化实现微服务治理、高可用等能力增强，同时组件具备热升级能力。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">此外，<strong>在成本方向</strong>，存在的痛点问题如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>， MQ 等重资源型应用的 CPU、存储资源利用率差异大；</p><p><strong>二是</strong>，部分事件驱动场景机器资源利用率低。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">要解决以上问题，我们可以通过升级 MQ 组件，落地存算分离技术，探索计算存储资源利用率优化方案。另外，还可以探索 Serverless 技术，实现平台化托管运维，降低资源成本，天然适合小程序、快应用等事件驱动业务场景。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>综上</strong>，在引擎升级探索上，我们会基于业务需求和痛点问题，探索和落地 ServiceMesh/Serverless/存算分离等云原生技术。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.4 平台建设探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">讲完引擎升级探索，我们再来看看在平台建设上的探索：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014335" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d67670bb-72fa-43e3-93a8-1c5f33dfca51.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">作为技术平台团队，我们在持续积极的探索「平台工程」理念，从现在的 DevOps 实践到平台工程，也是团队协作理念的再次升级。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我们知道，DevOps 于 2009 年出现，2015 年在国内火起来，它是一种文化、方法论，是敏捷理念从开发到运维的延伸。DevOps 的理念是：践行谁构建谁运行，开发运维一体化，实现业务的高效交付。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">但是，DevOps 在实际落地过程中存在以下问题：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">「DevOps 团队」的中心化与去中心化取舍问题</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">【<strong>中心化</strong>】指的是，独立的 DevOps 团队，即不在业务团队中配置 DevOps 能力，而把 DevOps 人员集中起来组建团队，这种完全中心化的模式本质上和 DevOps 文化相矛盾。同时根据康威定律，可能会制造新的效能瓶颈。「独立的 DevOps 团队」在 2014 年被 Thoughtworks「技术雷达」列为 Hold (停止采用)。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">【<strong>去中心化</strong>】指的是，将 DevOps 能力分散在业务团队，这种做法会将大量的和基础设施相关的工作职责划给业务团队；这种方式会随之出现基础设施和服务治理缺失、系统稳定性降低、研发和 DevOps 效能浪费等诸多问题。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">因此，想要践行好 DevOps，必须在中心化与去中心化之间取得平衡。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">此外，从平台能力上讲，DevOps 平台往往更侧重于建设流程和工具链，而在使用这些建设的工具技术平台过程中会大大增加业务开发团队的认知负荷，存在无法较好向业务开发团队屏蔽底层基础设施复杂性的问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">平台工程的概念，是在 2017 年首次出现，于 2022 年在国内兴起。平台工程的定义是，一套用来构建和运营支持软件交付和生命周期管理的自助式内部开发者平台的机制和架构；它的特点是：平台在演进中提供足够的透明度、敏捷性，在建设过程中形成适合业务架构的高效协作模式。在这一过程中逐步将知识体系固化到平台中，从而使得工程方式标准化、流程化和规模化并持续改善；它践行的理念是：一个可用的、高效的平台并非一个技术团队埋头苦干就可以产出的；恰恰相反，一个成功的平台工程需要企业各个组织部门合作、协调、推广并根据实际使用反馈不断迭代。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在具体实践中，平台工程约定了「业务团队」和「平台团队」两个团队，其中「业务团队」负责业务研发，「平台团队」负责平台建设；「平台团队」通过将技术知识沉淀到「平台工程」，隐藏和抽象底层基础设施的复杂性，实现基础设施即代码，为「业务团队」赋能增效；同时，基于「业务团队」在使用「平台工程」的过程中的不断反馈来持续改进平台的自助化产品能力，构建一整套覆盖 DevOps 全链路的简单易用平台产品；可以看到，平台工程是一种最佳实践，和我们当前的团队协作模式匹配度非常高。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014336" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e9b65caa-221d-4d7f-ae0c-c6722aa53455.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在平台建设的整体规划上：</strong></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>当前阶段</strong>：我们构建的统一微服务平台会持续探索「平台工程」理念，沉淀配置中心、注册中心等平台的技术知识与最佳实践，构建和打磨业务自助化使用的平台能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>展望未来</strong>：我们会通过明确的北极星指标，牵引平台提供更高的研发效率和更好的开发者体验。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在研发效率上</strong>，我们追求单位时间内更多的代码产出和需求交付；此外我们也追求更好的开发者体验，通过降低用户使用平台的打断次数和平台问题的人工支撑次数，提升业务团队和平台团队两个团队的开发体验。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在具体的落地路径上</strong>，我们始终以开发者用户为中心，针对研发工作中时间消耗较多的场景进行优化，通过北极星指标牵引，形成覆盖 IDE+PaaS 的平台工程实践路径，持续迭代优化平台能力，提升研发效率与开发者体验。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-right: 0%;margin-bottom: 20px;margin-left: 0%;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section><section style="display: inline-block;vertical-align: middle;width: 20%;align-self: center;flex: 0 0 auto;"><section style="text-align: center;color: rgb(45, 66, 87);font-size: 11px;" powered-by="xiumi.us"><p>END</p></section></section><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: left;" powered-by="xiumi.us"><section style="padding-left: 1em;padding-right: 1em;display: inline-block;text-align: center;"><span style="display: inline-block;padding: 0.3em 0.5em;border-radius: 0.5em;background-color: rgb(65, 94, 255);color: rgb(255, 255, 255);" title="" opera-tn-ra-cell="_$.pages:0.layers:0.comps:161.title1"><p>猜你喜欢</p></span></section><section style="border-width: 1px;border-style: solid;border-color: transparent;margin-top: -1em;padding: 20px 10px 10px;background-color: rgb(239, 239, 239);text-align: center;"><section style="font-size: 14px;text-align: left;" powered-by="xiumi.us"><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p><span style="font-size: 14px;letter-spacing: 0.578px;text-align: left;text-wrap: wrap;background-color: rgb(239, 239, 239);"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247498140%26idx%3D2%26sn%3D66854883c362d9145d89f72f267a7773%26chksm%3Debdb890edcac0018fa02b33bf9eff448548362f5174c829a6e20ee6ab656d9ea12b31c0a1e0f%26scene%3D21%23wechat_redirect" textvalue="Spring 七种事务传播性介绍" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Spring 七种事务传播性介绍</a></span><br></p></li><li><p><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497989%26idx%3D1%26sn%3Da98e270e4612356756966bd9d90d80ee%26chksm%3Debdb8997dcac0081e35a2c9ba681902e703f8c52406ee49fcedaafbba77b7dc3279f56305782%26scene%3D21%23wechat_redirect" textvalue="vivo 数据库备份恢复系统演化" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 数据库备份恢复系统演化</a></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497821%26idx%3D1%26sn%3D80e04511f5a5d5acfee4a44a8a8b3e31%26chksm%3Debdb88cfdcac01d954242fd24907b69c542e43fcb99ebe6a03d66858194e03ad14105281b62f%26scene%3D21%23wechat_redirect" textvalue="vivo 容器平台资源运营实践" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 容器平台资源运营实践</a></span></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497810%26idx%3D1%26sn%3Dfb5334c9637cdde4b5125f69ed32e89f%26chksm%3Debdb88c0dcac01d6faf82e4d44e8421616ec9128f46ea494339a599c346b13212b9f1d774886%26scene%3D21%23wechat_redirect" textvalue="Hudi 在 vivo 湖仓一体的落地实践" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Hudi 在 vivo 湖仓一体的落地实践</a></span></p></li></ul></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzI4NjY4MTU5Nw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/4g5IMGibSxt45QXJZicZ9gaNU2mRSlvqhQd94MJ7oQh4QFj1ibPV66xnUiaKoicSatwaGXepL5sBDSDLEckicX1ttibHg/0?wx_fmt=png" data-nickname="vivo 互联网技术" data-alias="vivoVMIC" data-signature="分享 vivo 互联网技术干货与沙龙活动，推荐最新行业动态与热门会议。" data-from="0" data-is_biz_ban="0"></mp-common-profile></section></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - vivo 互联网技术（vivoVMIC）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/10773883</guid>
            <link>https://my.oschina.net/vivotech/blog/10773883</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Stability AI 推出更小、更高效的 1.6B 语言模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Stability AI <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fintroducing-stable-lm-2" target="_blank">宣布</a>推出迄今为止最强大的小语言模型之一 Stable LM 2 1.6B。以英语、西班牙语、德语、意大利语、法语、葡萄牙语和荷兰语的多语言数据为基础进行了训练，体积小、速度快，降低了硬件门槛；并提供了完全透明的训练细节，旨在让开发人员和模型创建者能够快速进行实验和迭代。</p><p>Stable LM 是一种文本内容生成 LLM，Stability AI 于 2023 年 4 月首次推出了 30 亿和 70 亿参数模型。新的 StableLM 模型实际上是 Stability AI 在 2024 年发布的第二个模型，此前该公司在早些时候还发布了一个 Stable Code 3B。</p><p>Stability AI 声称，Stable LM 2 1.6B 在大多数基准测试中均优于其他参数低于 20 亿个的小语言模型，如微软的 Phi-1.5 (1.3B) 和 Phi-2 (2.7B)、TinyLlama 1.1B 或 Falcon 1B。</p><p><img height="202" src="https://oscimg.oschina.net/oscnet/up-2d9d2c9ec7d678e945e32f4889d703808f8.png" width="500" referrerpolicy="no-referrer"></p><p><img height="188" src="https://oscimg.oschina.net/oscnet/up-b30279a4fcccd6dbb87084aa29fd54c9428.png" width="500" referrerpolicy="no-referrer"></p><p><img alt="" height="198" src="https://oscimg.oschina.net/oscnet/up-a33e04e9eca65dc6b9b1137e97e0f66d4e0.png" width="500" referrerpolicy="no-referrer"></p><p><img alt="" height="193" src="https://oscimg.oschina.net/oscnet/up-9336f78e90afcb31c1ce5bb73afb0d482bb.png" width="500" referrerpolicy="no-referrer"></p><p>不过他们也警告称，由于小型、低容量语言模型的特性，Stable LM 2 1.6B 可能会出现高幻觉率、潜在的有毒语言等类似的常见问题。「我们要求社区在构建应用程序时牢记这一点，并采取适当措施确保以负责任的方式进行开发。」</p><p>Stable LM 2 1.6B 目前可在商业和非商业领域使用，只要拥有 Stability AI 会员资格，即可在 Hugging Face 上测试该模型。</p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 03:57:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276269/stable-lm-2-1-6b</guid>
            <link>https://www.oschina.net/news/276269/stable-lm-2-1-6b</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[X 正面向 Android 推出音频和视频通话]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#333333">埃隆·马斯克旗下的社交网络 X （原 Twitter）正面向 Android 客户端，推出直接从应用程序拨打音频和视频电话的功能。一位负责该项目的 X 工程师发布了关于该功能发布的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2Fenriquebrgn%2Fstatus%2F1748114599856193879" target="_blank">消息</a>，并表示 Android 用户将可在应用更新后使用该功能。</span></p><p><img height="220" src="https://oscimg.oschina.net/oscnet/up-d168ce5f0e35f1157f1ed9a8ba3f0e69408.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#333333">2023 年 8 月，其首席执行官 Linda Yaccarino 首次谈到要在平台上引入视频通话，并最终于 10 月向 iOS 用户推出了这一功能。但值得注意的是，任何用户都可以接听电话，只有付费用户才能拨打电话。不过，X 在本月早些时候取消了高级用户将 NFT 设为个人照片的功能。</span></p><p><span style="color:#333333">用户可以通过 Settingle &gt; Privacy and safety &gt; Direct Messages &gt; Enable audio and video calling 来启用或禁用通话。在同一个菜单中，用户还可以控制谁可以给自己打电话：通讯录中的人、关注的人和验证过的用户。可以从这些选项中选择多个选项。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 03:25:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276266/x-audio-and-video-calls-to-android</guid>
            <link>https://www.oschina.net/news/276266/x-audio-and-video-calls-to-android</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Extism —— WebAssembly 插件实现框架]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Extism 是一个 WebAssembly 插件实现框架，它可以给你的应用开发出各种各样的 WebAssembly 插件，支持多种编程语言。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img src="https://oscimg.oschina.net/oscnet/up-5d37397b3cababa42f8754739726d916b86.png" referrerpolicy="no-referrer"></p><p>Exism 团队称他们致力于构建一个可嵌入的、安全的、性能良好的运行时，为任何规模的软件带去可扩展性。</p></div>
                                                                ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 03:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/extism</guid>
            <link>https://www.oschina.net/p/extism</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 龙蜥社区最佳安全加固实践指南 security-benchmark]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-security-benchmark" class="anchor" href="https://gitee.com/anolis/security-benchmark#security-benchmark"></a>security-benchmark</h1><p>security-benchmark 是龙蜥下游各个厂商结合自己在安全合规/加固领域的大规模产品落地经验和实践打造的龙蜥社区最佳安全加固实践指南，它包括安全基线（benchmark）、扫描脚本、修复脚本、安全合规镜像制作、安全合规监控等多个方面。其中，Anolis OS 8 、Anolis OS 23 及其最佳安全基线已经完成与<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FComplianceAsCode%2Fcontent">OpenSCAP 国际知名社区</a>的映射与适配，并被 OpenSCAP 社区合入，详见：</p><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FComplianceAsCode%2Fcontent%2Fblob%2Fmaster%2Fproducts%2Fanolis8%2Fprofiles%2Fstandard.profile">OpenSCAP 社区 Anolis OS 8 standard.profile</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FComplianceAsCode%2Fcontent%2Fblob%2Fmaster%2Fproducts%2Fanolis23%2Fprofiles%2Fstandard.profile">OpenSCAP 社区 Anolis OS 23 standard.profile</a></li></ul><h4><a id="user-content-介绍" class="anchor" href="https://gitee.com/anolis/security-benchmark#%E4%BB%8B%E7%BB%8D"></a>介绍</h4><p>Gitee 是 OSCHINA 推出的基于 Git 的代码托管平台（同时支持 SVN）。专为开发者提供稳定、高效、安全的云端软件开发协作平台
无论是个人、团队、或是企业，都能够用 Gitee 实现代码托管、项目管理、协作开发。企业项目请看 <a href="https://gitee.com/enterprises">https://gitee.com/enterprises</a></p><h4><a id="user-content-参与贡献" class="anchor" href="https://gitee.com/anolis/security-benchmark#%E5%8F%82%E4%B8%8E%E8%B4%A1%E7%8C%AE"></a>参与贡献</h4><p>请参考<a href="https://gitee.com/anolis/security-benchmark/blob/master/docs/development-guide.md">development-guide</a>和以下 gitee 贡献步骤来贡献您的代码。</p><ol><li>Fork 本仓库</li><li>新建 Feat_xxx 分支</li><li>提交代码</li><li>新建 Pull Request</li></ol><h4><a id="user-content-特技" class="anchor" href="https://gitee.com/anolis/security-benchmark#%E7%89%B9%E6%8A%80"></a>特技</h4><ol><li>使用 Readme_XXX.md 来支持不同的语言，例如 Readme_en.md, Readme_zh.md</li><li>Gitee 官方博客 <a href="https://blog.gitee.com/" rel="nofollow">blog.gitee.com</a></li><li>你可以 <a href="https://gitee.com/explore">https://gitee.com/explore</a> 这个地址来了解 Gitee 上的优秀开源项目</li><li><a href="https://gitee.com/gvp">GVP</a> 全称是 Gitee 最有价值开源项目，是综合评定出的优秀开源项目</li><li>Gitee 官方提供的使用手册 <a href="https://gitee.com/help">https://gitee.com/help</a></li><li>Gitee 封面人物是一档用来展示 Gitee 会员风采的栏目 <a href="https://gitee.com/gitee-stars/">https://gitee.com/gitee-stars/</a></li></ol>]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 02:58:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/anolis/security-benchmark</guid>
            <link>https://gitee.com/anolis/security-benchmark</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 得物云原生容器技术探索与落地实践]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1>一、前言</h1><p style="color:#24292f; text-align:start">得物 App 作为互联网行业的后起之秀，在快速的业务发展过程中基础设施规模不断增长，继而对效率和成本的关注度也越来越高。我们在云原生技术上的推进历程如图所示，整体上节奏还是比较快的。</p><p style="color:#24292f; text-align:start"><img alt="156.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/156.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">从 2021 年 8 月开始，我们以提升资源使用率和资源交付效率为目标，开始基于云原生技术建设整个服务体系的高可用性、可观测性和高运维效率，同时要保证成本可控。在容器化过程中我们遇到了很多的挑战，包括：如何将存量的服务在保持已有研发流程不变的情况下，做到容器化部署和管理；容器化之后如何做到高效地运维；如何针对不同的业务场景，提供不同的容器化方案等等。此外，通过技术手段实现持续的成本优化是我们的长期目标，我们先后建设落地了画像系统、混部方案和调度优化等方案。本文把得物在推进云原生容器技术落地过程中相关方案和实践做一些总结和梳理，欢迎阅读和交流。</p><span id="OSC_h1_2"></span><h1>二、云原生应用管理</h1><span id="OSC_h2_3"></span><h2>云原生应用管理方式</h2><p style="color:#24292f; text-align:start">容器与 ECS 的资源形态是有差异的，所以会造成在管理流程上也会有不同之处。但是为了尽可能降低容器化带来的使用体验上的差异，我们参考业内容器应用 OAM 模型的设计模式，对容器的相关概念做了屏蔽和对等解释。例如：以「应用集群」的概念代表 CloneSet 工作负载（Kruise 提供的一种 Kubernetes 扩展工作负载）；将单个 Pod 约定为一个应用集群的实例；以「应用路由/域名配置」的概念代表针对 Ingress/Service 的设置。</p><p style="color:#24292f; text-align:start">在应用集群的构造上（即如何构造出 Kubernetes 工作负载对象），我们设计了「配置/特征分层」的方案，将一个应用集群所处归属的应用、环境组、环境上的配置进行叠加后，使用 Helm 工具渲染生成 Kubernetes 资源对象，提交给容器平台。</p><p style="color:#24292f; text-align:start"><img alt="679.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/679.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">CI 和 CD 过程均使用这种配置/特征分层的方式，一方面可以解决应用依赖的中间件信息的管理问题（由相应的提供者统一维护）；另一方面，这种管理方式可以让中间件组件/服务变更时按照不同维度进行，整体上降低了配置变更带来的风险。</p><p style="color:#24292f; text-align:start">Sidecar 容器在应用集群实例中除了扮演「协作者」的角色外，我们还基于它做了权限管理，以便对应在 ECS 形态下的不同用户的登陆权限，也算是一举两得。当然，在容器场景下也是可以定义不同的用户，赋予不同的角色，但是强依赖基础镜像的维护。</p><p style="color:#24292f; text-align:start"><img alt="564.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/564.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_4"></span><h2>多集群管理方案</h2><p style="color:#24292f; text-align:start">云原生场景下的解决方案对应用集群而言本身就是高可用的，比如：容器编排引擎 Kubernetes 中支持 Pod 实例的拓扑分布设置、支持可用区设置、副本数设置、 Service 负载均衡的设计等，这些都能保证应用集群的高可用。那如果单个 Kubernetes 集群不可用了，会有什么的影响呢，该如何解决？多集群管理方案就是我们解决 Kubernetes 的可用性问题的思路。</p><p style="color:#24292f; text-align:start">如果 Kubernetes 控制面不可用了，会导致应用发布受损，较严重的情况也会影响容器服务的可用性。所以，为了保证 Kubernetes 的可用性，一方面要保证 Kubernetes 各组件的健壮性，另一方面要适当控制单个 Kubernetes 集群的规模，避免集群过大造成系统性风险升高。我们的解决思路就是「不要把鸡蛋放在一个篮子里」，用联邦的方式管理多个 Kubernetes，将业务分散到不同的 Kubernetes 集群。</p><p style="color:#24292f; text-align:start">联邦的思想在 Kubernetes 诞生不久就被开始讨论，逐步设计实现，从最初社区的 KubeFate V1.0 到 V2.0，再到企业开源的 Karmada、KubeAdmiral 逐渐成熟起来，并实际应用到了生产场景。那如果没有集群联邦，多个 Kubernetes 集群就没法管理了吗？当然不是的，容器管控平台其实也能做这件事情，笔者在几年之前还对此深以为然，但现在已经完全改变看法了。因为在实际的生产落地过程中我们发现，相比在管控中用 if...else/switch 的方式，亦或配置的方式相比，基于 CRD 的方式来管理多集群效率更高、逻辑更清晰。<img alt="435.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/435.png" referrerpolicy="no-referrer"><img alt="342.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/342.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">得物在使用联邦思想管理多 Kubernetes 集群的时候，参考华为开源的 Karmada 解决方案，在此基础之上做了定制开发。容器管控平台负责管理应用集群的原始特征和配置，管理 CICD 流程，向 Host Kubernetes 集群发起容器对象管控请求。Host Kubernetes 集群通过 PropagationPolicies 管理工作负载如何分发到 Member Kubernetes 集群，通过 OverridePolicies 管理差异化的配置。单 Kubernetes 集群下我们使用了分批发布的方式来管理应用集群的发布，在引入联邦管理之后，我们把分批发布的逻辑从容器管控层面下移到了 Host Kubernetes 集群上。为了兼容存量的通过 Kubernetes Service 进行调用的服务，我们在 Member Kubernetes 集群通过自定义的 MCS-Controller 来管理跨集群的 Service/Endpoints 对象，在 Host Kubernetes 层通过 MCS-Validator 做双重校验，确保跨集群的 Service 的一致性。</p><span id="OSC_h1_5"></span><h1>三、容器调度优化与混部</h1><p style="color:#24292f; text-align:start">落地云原生容器技术的目标是期望在敏捷、弹性和可用的基础上，最终实现资源利用率上的提升、成本上的节省。这通常有 2 个实现途径，一个是通过技术的手段，另一个则是通过治理方法。本章重点介绍我们在容器精细化调度和混部实践方面的技术方案设计和落地过程。</p><span id="OSC_h2_6"></span><h2>应用画像</h2><p style="color:#24292f; text-align:start">应用服务的研发人员在部署应用集群实例时，通常会申请超过应用集群本身承载业务流量时所要消耗的资源量，这是可以理解的（要确保系统的资源利用率安全水位，防止过载造成系统夯住），但是不同的研发人员对这个「度」把握是不一样的，因为合理地设置应用集群的资源用量是依赖研发人员经验的，也就是说主观性会更强。</p><p style="color:#24292f; text-align:start">为了解决上述问题，业内的做法通常是通过分析应用集群的过往资源利用率数据，来刻画出应用集群在业务流量下的实际资源利用率曲线，这就是应用画像。如下图所示是我们建设的画像系统的架构框图，该画像系统不仅负责应用的画像分析，也负责宿主机、Kubernetes 集群的画像分析，用来指导整个容器平台对资源的管理。<img alt="140.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/140.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">容器的监控数据通过 Prometheus 方案进行采集和管理，自研的 KubeRM 服务将它作为数据源，周期性计算产出应用画像、宿主机画像和 Kubernetes 集群画像（资源池画像）。容器平台部署在线服务服务时，可参考画像值来配置应用集群的资源规格，这里的画像值就是指 Pod 的 Request 值，计算公式如下：</p><p style="color:#24292f; text-align:start">++Pod Request = 指标周期性利用率 / 安全水位++</p><p style="color:#24292f; text-align:start">公式中「指标周期性利用率」是画像系统通过统计学手段、AI 模型等方法计算分析出的资源指标（CPU/内存/GPU 显存）在实际业务流量下所表现出的周期性的规律。画像值的生效我们通过以下 4 个策略进行实施：</p><ul><li>针对 P3/P4 等级的服务，默认在服务部署时生效画像值。</li><li>针对非 P3/P4 等级的服务，将画像值推荐给用户，由用户决定部署时是否采用画像。</li><li>分资源池设置不同的生效策略（默认生效，或者用户决定生效）。</li><li>GPU 显存的画像不做默认生效，推荐给用户，让用户决定。</li></ul><p style="color:#24292f; text-align:start">交由用户决定画像是否生效时，如何让用户更倾向于去生效画像呢？我们使用差异化计费的策略：生效了画像的应用集群实例按照其 Pod 配置的 Request 值计费，未生效画像的应用集群实例按照其 Pod 配置的 Limit 值计费。用户可以根据自己服务的实际情况选择生效画像，以降低成本；平台也因为画像而拿到了更多可以调度的资源，用于其它更多的场景。</p><p style="color:#24292f; text-align:start">此外，画像系统也接入了 KubeAutoScale 自动伸缩器，在业务低峰期，可以指导自动伸缩器对部分场景在线服务做副本缩容操作，以便释放出更多的资源供给其它场景使用（比如：混部任务场景），后面的章节会详细介绍。</p><span id="OSC_h2_7"></span><h2>资源预占</h2><p style="color:#24292f; text-align:start">当整个容器集群的资源冗余量不是很充足的时候，在以下几种情况下是会出现 「虽然集群层面总量资源是够的，但是业务 Pod 却无法调度」的问题，影响业务发布效率和体验。</p><ul><li>在集群中容器实例变更比较频繁的时候，某个大规格的业务集群在做滚动更新时，释放的旧的实例很可能被小规格的容器实例所抢占，导致无法调度。</li><li>研发同学负责 2 个应用服务 A 和 B，它们的规格都是一样的。为了保证总体成本不变会，选择将 A 服务的实例缩掉一些，然后扩容 B 服务的实例。因为 Kubernetes 默认调度会按照 Pod 创建时间来依次调度新 Pod，当用户缩容完 A 服务的实例再去扩容 B 服务实例的时候，A 服务释放的资源很可能被其他容器实例抢占，导致 B 的实例无法调度。</li><li>在大促、全链路压测等业务需要紧急扩容的情况下，容器平台会新扩宿主机节点以满足业务需求，不曾想新扩容的机器资源却被那些「小而快（拉起频繁，执行时间短）」的任务给见缝插针地抢占了，一方面会导致大规格的服务实例无法调度，另一方面还造成了较多的资源碎片。</li></ul><p style="color:#24292f; text-align:start">为了解决以上问题，我们在调度器中自定义实现了资源预占的调度插件（通过 CRD 定义资源预占期望，影响调度决策），用来提升用户体验和提高调度效率。<img alt="650.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/650.png" referrerpolicy="no-referrer"><img alt="178.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/178.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_8"></span><h2>平衡调度</h2><p style="color:#24292f; text-align:start">为了更好地平衡集群中节点的水位，以避免过热节点的出现、尽量减少碎片资源等为目标来思考和设计，我们基于 Kubernetes 提供的调度器扩展框架，自定义实现了多个调度插件：</p><ul><li>CoolDownHotNode 插件：给最近调度过 Pod 的节点降低优先级，避免热点节点</li><li>HybridUnschedulable 插件：阻止使用弹性资源的 Pod 调度到某些节点上。</li><li>NodeBalance 插件：用于平衡各节点上 CPU Request 值与画像的比值，平衡各节点 CPU 使用率。</li><li>NodeInfoRt 插件：基于画像打分数据和实时打分数据优化 Pod 调度。</li></ul><span id="OSC_h2_9"></span><h2>在实时混部</h2><p style="color:#24292f; text-align:start">从今年 1 月份开始，我们着手做在离线混部的落地，一期的目标着眼于将在线服务与 Flink 任务进行混部。之所以选择 Flink 任务做混部，是因为它与在线服务有一个相似之处，那就是它是一种常驻的离线任务，在它启动之后如果没有特殊情况，一般不会下线，这种特质会使得我们的容器集群调度频次、Pod 的变更程度会低一些，进而对稳定性的挑战也会小一点，整体混部风险也会低一些。</p><p style="color:#24292f; text-align:start">在没有混部的情况下，我们的集群整体利用率较低，即便画像功能能帮助用户尽可能合理的为自己的服务实例设置资源规格，但对容器平台而言这依然很被动。所以为了挖掘出可以用来混部的资源，我们为不同等级的服务设置不同的绑核策略。如下表所示定义了 4 种应用类型（LSX、LSR、LS、BE），适用于 P0~P4 范围和离线任务，绑核策略从完全绑核到部分绑核，再到完全共享。</p><p style="color:#24292f; text-align:start"><img alt="900.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/900.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">离线任务（Flink 任务）属于 BE 类型，可以使用的资源是在宿主机所有 CPU 核心里面单独划分出来的一部分专用 CPU 核心，再加上 LS 的共享 CPU 核心，以及 LSR、LS 类型的应用上共享出来的部分 CPU 核心。<img alt="·.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/%C2%B7.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">LSX、LSR 和 LS 类型的应用服务的容器实例均申请使用 Kubernetes 原生的资源 CPU/Memory 资源；BE 类型的任务需要申请使用我们自定义的资源 BE-CPU/BE-Memory 资源。<img alt=".png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/-.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">基于 Kubernetes 的 Device-Plugin 机制我们自研实现了 Kube-Agent 组件，该组件在集群中的所有节点上以 Damonset 的方式部署，一方面负责根据自定义策略将本节点上可用的 BE 资源上报给 API-Server（通过 Kubelet 组件间接上报），另一方面负责执行 CPU 绑核操作。随着混部的深入，该组件也承担了更多的工作内容（例如：执行 CPU 算力压制操作、参数动态调整操作，执行 VPA 操作等）。</p><span id="OSC_h2_10"></span><h2>在离线混部</h2><p style="color:#24292f; text-align:start">一期的混部在应用级别划分的基础上，应用了 CPU 核划分策略来实现混部。站在 CPU 核心的角度来看，通过 CPU 核划分策略之后，每个 CPU 核心已经有了自己的负载归属，能否充分利用取决于分配到它上面的业务特性。但站在整机的利用率上来看（或者站在整个集群的利用率角度来看），依然有很大的提升的空间。混部二期的时候，我们考虑对 BE 资源进行二次超卖，以另一种新的自定义资源（OT 资源）进行分配使用。使用 OT 资源的任务不独占绑核，而是共享划分给 BE 资源的所有 CPU 核心。<img alt="175.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/175.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">我们使用 OT 资源来混部 AI 训练任务、其它数据处理任务，为了消除训练任务对在线业务的影响，通过以下策略进行保证：</p><ul><li>设置宿主机安全水位，通过调度插件防止过热节点出现。</li><li>通过 CPU Group Identity 进行优先级竞争，保障离线任务的调度优先级绝对低于在线服务。</li><li>对离线任务进行独立挂盘，避免影响在线服务的磁盘 IO。</li><li>夜间时段通过 KubeAutoScaler 进行对在线服务进行弹性缩容，等比例提升内存的空闲率，保障离线任务有足够的内存资源。</li></ul><span id="OSC_h2_11"></span><h2>弹性伸缩</h2><p style="color:#24292f; text-align:start">容器平台的弹性能力相较于传统 IDC 资源管理模式、ECS 资源管理方式的要更上一个台阶，因为它更侧重将弹性伸缩的决策权交给应用服务，而不是资源管理方。云原生技术中常说的弹性伸缩方案通常包含 2 种方式：</p><ul><li>HPA：Horizontal Pod Autoscaling，水平方向的 Pod 副本扩缩容。</li><li>VPA：Vertical Pod Autoscaling，垂直方向的 Pod 规格扩缩容。</li></ul><p style="color:#24292f; text-align:start">Kubernetes 中通过资源对象 HorizontalPodautoScalers 来支持工作负载的水平扩缩容，在较早的版本中只支持 CPU 和内存这两个资源指标，较新版本中也开始支持自定义指标了。针对 VPA 的需求，目前 Kubernetes 层面还没有比较稳定的可用功能，因为对一个 Pod 实例做资源规格的调整，会涉及到宿主机上资源账本的管理问题、监控问题，也会涉及到 Pod 的重建/容器重启动作，影响面会比较大，目前社区中依然在讨论。但企业在 VPA 方面，也都是跃跃欲试，会设计自己的个性化 VPA 方案，本文前述应用画像功能，就是我们得物在 VPA 方案上探索的第一步。</p><p style="color:#24292f; text-align:start">此外，我们在实际的支撑业务云原生化转型过程中发现，与通过服务的资源使用率指标来帮助业务来决策服务实例副本数的调整的方式相比，定时扩缩容反而能让研发同学更有信心，研发同学可以根据自己负责的业务服务的流量特征，来设置定时地缩容或者扩容自己的服务实例数量。</p><p style="color:#24292f; text-align:start">为了满足弹性伸缩场景的所有需求，我们设计实现了 KubeAutoScaler 组件，用来统一管理 HPA、VPA、定时伸缩等弹性伸缩策略配置。此外，如前所述，该组件与画像系统相互协作，在混部场景下可以帮助在夜间对部分低流量的服务做缩容操作，释放更多的资源供离线任务使用。</p><p style="color:#24292f; text-align:start">弹性伸缩方案在 GPU 服务场景帮我们避免了很多的资源浪费，特别是在测试环境。如图所示，我们为 GPU 服务注入了一个名为 Queue-Proxy 的 Sidecar 容器，用来采集服务流量，当流量低于某个阈值时，会按照比例减少实例数；当流量为 0 并持续了一段时间之后，会完全缩零。当冷启动时，请求会经过激活器 Activator，激活器再通知 KubeAutoScaler 进行服务扩容。生产环境的部分服务也开启了这一套机制，但在流量低峰期不会完全缩容至零，会维持一个最小的副本数。<img alt="1070.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/1070.png" referrerpolicy="no-referrer"></p><span id="OSC_h1_12"></span><h1>四、容器资源和成本治理优化</h1><p style="color:#24292f; text-align:start">为了更好地提升整体的资源利用、降低基础设施成本，与技术方案的落地周期长、复杂度高的特点比起来，通过治理方法往往能在较短时间内达到不错的效果，特别是在应用服务容器化部署改造进行到后期的时候。我们通过以下 5 个方面的治理实践，降本效果明显。</p><span id="OSC_h2_13"></span><h2>机型替换</h2><p style="color:#24292f; text-align:start">因为历史原因，我们的模型推理服务在刚开始的时候使用的是 V100 的机型，该机型显存较大、GPU 算力较优，更适合用在训练场景，在推理场景的话有点大材小用了。经过机型对比分析，我们选用了一个性价比较高的 A10 机型，推理服务的成本整体降低了 20% 左右，而且因为 A10 机型配备的 CPU 架构有升级，对前后处理有较高要求的推理服务而言稳定性和性能均有提升。从 V100 切换到 A10，主要的工作在于基础镜像的替换，因为部分模型服务可能使用了较低版本的 CUDA，但是 A10 卡的算力需要配备较高的 CUDA。另外，因为两种卡的 GPU 算力也是有差异的，替换之后需要对推理结果做对比验证才可以上线。我们基于流量回放的思路，设计了 AB 实验功能帮助业务做切换测试。<img alt="155.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/155.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">在使用 CPU 计算资源的场景上，我们对算力要求一般、对 CPU 指令集因无特殊要求、对单核/多核性能无要求的服务，均将其使用的机型从 Intel 的切换到了 AMD 的，整体成本降低 14% 左右。</p><span id="OSC_h2_14"></span><h2>资源池管理</h2><p style="color:#24292f; text-align:start">不得不承认的是容器化初期业务方是占据主动的，业务侧会基于稳定性、资源供给量上的考量要求容器平台独立建集群，或者资源池，容器平台也会选择粗放式管理而应承这些需求。随着容器化的推进，业务侧的信心也会增强，容器平台对资源的把控程度也会更好，我们逐步采取以下几个动作来收敛资源的管理，提高整体的资源分配率：</p><ul><li><strong>冗余量控制</strong>：业务的发布是有周期的，我们会根据发布周期，动态调整容器平台管理的资源冗余量。在保证日常的迭代开发的同时，尽可能缩小冗余量。</li><li><strong>集群合并</strong>：统一规划 Kubernetes 集群，按照区域（上海、杭州、北京等）、网络环境类型（测试、预发、生产）、业务形态（普通业务、中间件、基础设施、管控集群等）等维度讨论和决策，下线不必要的集群。</li><li><strong>资源池合并、规整机型</strong>：合并资源需求特征比较相近的资源池（例如：计算型的、内存型的），选择合适的机型。与业务沟通，下线或者合并利用率过低的小资源池。</li><li><strong>碎片整理</strong>：单靠调度器的优化，在调度时尽可能避免碎片力量有限。加上在线服务的变更频率一般比较低，如果不做重调度，长时间累积下来，集群中会存在大量碎片。所以，针对多副本的应用集群，在健壮的优雅停机机制基础上，我们适当进行了一些碎片整理任务（重建 Pod 自由调度、重调度、宿主机腾挪等），有效地减少了资源碎片。</li></ul><span id="OSC_h2_15"></span><h2>工作负载规格治理</h2><p style="color:#24292f; text-align:start">用户自定义工作负载的规格在云原生场景下也是一个常用的做法，这看似对用户友好的做法却对容器平台造成了一些挑战，因为如果对用户设置规格的自由度不做一些限制，很可能出现一些非常不合理的规格设置（例如：6C120G、20C4G），会产生调度碎片、成本分摊计算标准也难统一。</p><p style="color:#24292f; text-align:start">为了解决规格的问题，我们对在线服务的资源规格做了限制，不允许用户随意指定，而是由平台给出规格列表，由用户选择使用。规格列表可以分资源池设计、也可以分业务场景设置。针对任务型的工作负载，我们定义了 3 种 CPU 类型的资源规格（普通型、计算型、内存型，分别对应不同的 CPU:内存比例）。针对特殊的任务需求，我们约定了资源规格当中 CPU:内存的范围。针对使用 GPU 的任务，因每种 GPU 卡的 CPU/内存/显存规格配比都是不一样的，我们定义了针对每种 GPU 卡的 CU 单位，用户只需要选择相应的 CU，填写 CU 数量即可。规格约定之后，我们针对不同的规格做了差异化计费，保证了规格申请和成本分摊上的合理性。关于规格的定义和计费标准，详见下表。<img alt="133.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/133.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_16"></span><h2>产品自建</h2><p style="color:#24292f; text-align:start">得物的基础设施是在云上，所以在业务发展过程中，部分服务能力我们是会直接选用云上产品的。算法侧的模型训练任务，最开始的时候就是选用云上产品，随着容器化的推进，我们自建的 AI 平台（KubeAI 平台）逐步承接模型训练任务，使得训练任务的成本大幅下降。</p><p style="color:#24292f; text-align:start">自建 KubeAI 平台，使得我们将训练使用的资源与在线服务、其它离线任务场景使用的资源纳入了统一的管理体系，便于从全局的视角去合理地调度分配资源，为 AI 模型训练场景拿到更多的可用资源。本文前述 2.5 小节，我们就是通过混部的方式，将在线服务的资源供给了训练任务使用，当前已经在常态混部。<img alt="3678.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/3678.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_17"></span><h2>多云策略</h2><p style="color:#24292f; text-align:start">作为云上用户，多云策略是我们的长期目标。在多云之间获得议价主动权、符合合规性要求、获得更充足的资源供给。尤其今年 4 月份以来，随着 GPT/AIGC 方面的爆发、政策因素导致单个云商的 GPU 资源对我们供给不足，阻碍业务发展。我们及时采用多云策略，将 GPU 业务分散到不同的云供应商，保障业务正常开展。当然，多云的接入不是一蹴而就的，而是需要分业务场景逐步推进，周期较长，难度较大，我们需要考虑以下问题：</p><ul><li>梳理业务，找到适合多云的业务场景，或者找到适合在多云之间灵活迁移的业务场景。</li><li>因不同云供应上的机房可能在不同的区域，所以需要考虑跨地域服务访问、中间件依赖问题。</li><li>跨云供应商的数据访问和传输问题，涉及到专线建设、成本问题。</li></ul><span id="OSC_h1_18"></span><h1>五、云原生 AI 场景建设</h1><p style="color:#24292f; text-align:start">我们期望云原生容器技术的落地是要覆盖全场景的，要将云原生技术在普通服务、中间件产品和特殊的业务场景上都能发挥其巨大优势。目前 MySQL、Redis、Miluvs、ElasticSearch 等产品都已经在推进容器化。云原生 AI 场景的建设，我们通过 KubeAI 平台的建设在持续推进。<img alt="=.png" src="https://h5cdn.dewu.com/efe/ctoo-open-blog-admin/10569101/=-.png" referrerpolicy="no-referrer"></p><p style="color:#24292f; text-align:start">KubeAI 是得物 AI 平台，是我们将云原生容器技术落地得物全站业务过程中，逐步收集和挖掘公司各业务域在 AI 模型研究和生产迭代过程中的需求，逐步建设而成的一个云原生 AI 平台。KubeAI 以模型为主线提供了从模型开发，到模型训练，再到推理 (模型) 服务管理，以及模型版本持续迭代的整个生命周期内的解决方案。此外，随着 AIGC 的火热发展，我们经过调研公司内部 AI 辅助生产相关需求，上线了 AIGC/GPT 服务，为得物丰富的业务场景提供了 GAI 能力，助力业务效果提升。关于 KubeAI 平台相关解决方案，我们之前发布过一些文章，欢迎大家阅读交流，这里不再赘述。</p><span id="OSC_h1_19"></span><h1>六、展望</h1><p style="color:#24292f; text-align:start">云原生容器技术在得物的落地开展还是比较快的，业务覆盖面也比较广泛。经过 2 年时间的实践落地，已经全面深入资源管理系统、预算/成本管理机制、应用服务发布流程、AI 算法等管理体系和业务场景。接下来：</p><ul><li>在容器化，我们会继续推进中间件产品的容器化，进一步提升基础设施的资源效率。</li><li>我们会继续巩固混部方案，继续探索弹性容量、调度优化等方案，进一步提升资源效率。</li><li>在稳定性方面，我们会继续关注容器平台/Kubernetes 本身的稳定性建设，防范风险，切实保证业务平稳运行。</li><li>与业务场景一起探索快速接入多云，以及多云之间的快速切换能力，保障业务规模在持续增长的情况下，容器基础设施切换灵活、坚如磐石。</li></ul><p style="color:#24292f; text-align:start">*<strong>文/weidong</strong></p><p style="color:#252933; margin-left:0; margin-right:0; text-align:start">本文属得物技术原创，更多精彩文章请看：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftech.dewu.com" target="_blank">得物技术官网</a></p><p style="color:#252933; margin-left:0; margin-right:0; text-align:start">未经得物技术许可严禁转载，否则依法追究法律责任！</p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 21 Jan 2024 02:52:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/5783135/blog/10861087</guid>
            <link>https://my.oschina.net/u/5783135/blog/10861087</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[谷歌华人工程师殴打妻子致死]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>当地时间 1 月 19 日，美国加州圣克拉拉县检察官办公室官网发布新闻稿，称圣克拉拉一名男子被指控谋杀妻子。新闻稿写道，警方在家中发现了身上有溅落血迹的 Liren Chen（27 岁），他妻子的尸体就在附近。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-ff37b9d973ac9b4aa4b6c2bf4abb7253fb0.png" referrerpolicy="no-referrer"></p><p>via&nbsp;<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcountyda.sccgov.org%2Fnews%2Fnews-release%2Fsanta-clara-man-charged-wifes-murder" target="_blank">https://countyda.sccgov.org/news/news-release/santa-clara-man-charged-wifes-murder</a></em></u></p></blockquote><p>当地媒体《The San Francisco Standard》报道，警方在法庭记录中称，Liren Chen 在圣克拉拉 Valley Way 家中多次殴打妻子的头部。再结合圣克拉拉县检察官办公室透露的信息，Liren Chen 被抓获时右手极度肿胀并且发紫，他的身上和屋内到处都是血迹，初步可以判定该男子是用拳头殴打妻子致死。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-acd15e46a086a302acdac8c5d0d5297f7ac.png" referrerpolicy="no-referrer"></p><p>via&nbsp;<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsfstandard.com%2F2024%2F01%2F19%2Fgoogle-engineer-murder-liren-chen-xuanyi-yu%2F" target="_blank">https://sfstandard.com/2024/01/19/google-engineer-murder-liren-chen-xuanyi-yu/</a></u></em></p></blockquote><p>检察官证实 Liren Chen 是谷歌员工，目前还在医院接受治疗。截至美国当地时间 19 日早上，Liren Chen 尚未出庭面临审讯。</p><p>据报道，Liren Chen 于 2018 年毕业于清华大学，获得电子信息工程学位，随后于 2018-2019 年在美国加州大学圣迭戈分校获得计算机科学硕士学位。他在湾区进行了短暂的实习之后，于 2020 年 3 月加入谷歌，担任软件工程师；其妻子 Xuanyi Yu 也是 2018 年毕业于清华大学，获得电子信息工程学位，随后于 2018-2019 年在美国加州大学圣迭戈分校学习，同样获得计算机科学硕士学位。她毕业后先是在亚马逊实习，随后加入亚马逊工作，2021 年 6 月加入谷歌。</p><p><img src="https://oscimg.oschina.net/oscnet/up-7fc2adccf7dd5164f1220c6e14d14a221d5.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">此前有消息称涉案 2 人疑似被谷歌裁员，不过有自称 Liren Chen 同事的知情人士告诉记者，涉案 2 人并未被裁员。</span></p><p>《The San Francisco Standard》的报道还指出，Liren Chen 因涉嫌杀害妻子被指控谋杀重罪，如果罪名成立，他将面临终身监禁，不得假释。公开信息显示，当地检方已对 Liren Chen 初步提起控诉，但由于他正在住院治疗，对他的提审已经两度推迟，目前圣克拉拉县高等法院官网公开显示最新聆讯日期为 1 月 24 日。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0143e73ce092391f1ccb990f17eb1460d69.png" referrerpolicy="no-referrer"></p><p>相关信源：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fm.mp.oeeee.com%2Fa%2FBAAFRD000020240120902417.html" target="_blank">https://m.mp.oeeee.com/a/BAAFRD000020240120902417.html</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 11:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276243</guid>
            <link>https://www.oschina.net/news/276243</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Altman 拟筹集数十亿美元，建立 AI 芯片工厂网络]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>彭博社援引知情人士<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2024-01-19%2Faltman-seeks-to-raise-billions-for-network-of-ai-chip-factories" target="_blank">消息称</a>，OpenAI 首席执行官 Sam Altman 正在计划募集数十亿美元资金，用于<span style="background-color:#ffffff; color:#222222">建立</span>一家芯片合资企业。他的目标是利用这笔资金建立一个工厂网络，生产半导体。</p><p><img alt="" height="281" src="https://oscimg.oschina.net/oscnet/up-e93d79d18a9c32a7a6caae67ce76611abfe.jpg" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">Altman 已与几家大型潜在投资者进行了讨论。</span><span style="background-color:#ffffff; color:#2b2b2b">一些知情人士表示，</span><span style="background-color:#ffffff; color:#222222">与 Altman 进行过讨论的公司包括日本软银集团和阿联酋 AI 企业 G42。</span><span style="background-color:#ffffff; color:#2b2b2b">该项目将涉及与顶级芯片制造商合作，晶圆厂网络将覆盖全球。</span></p><p><span style="background-color:#ffffff; color:#222222">不过谈判尚处于早期阶段，参与的合作伙伴和出资人的完整名单尚未确定。此前也曾有<a href="https://www.oschina.net/news/272549/openai-valuation-100-billion-funding-round">消息称</a>，</span><span style="background-color:#ffffff; color:#000000">OpenAI 与阿布扎比 G42 进行了商讨，为一家新的芯片合资企业募集资金，并已讨论过从 G42 筹集 80 亿至 100 亿美元的资金。</span></p><p><span style="background-color:#ffffff; color:#222222">当下的一些市场预测指出，相关 AI 芯片的生产或将跟不上预期需求。部分知情人士称，Altman 的筹资行动也反映出，他担心随着 AI 的普及，大范围部署会缺少足够的芯片。</span></p><p><span style="background-color:#ffffff; color:#222222">Altman 认为，为了确保到 2030 年有足够的芯片供应，AI 行业需要现在就开始采取行动。此外，</span>OpenAI 的最大投资者<span style="background-color:#ffffff; color:#000000">微软似乎也对该项目表现出了兴趣。</span></p><p><span style="background-color:#ffffff; color:#000000">OpenAI 并没有试图涉足芯片代工领域。相反，OpenAI 的计划似乎是将筹集到的资金注入台积电、三星电子和英特尔等尖端芯片制造商，</span><span style="background-color:#ffffff; color:#222222">这些公司都是 OpenAI 的潜在合作伙伴</span><span style="background-color:#ffffff; color:#000000">。</span></p><p><strong><span style="background-color:#ffffff; color:#000000">相关阅读：</span></strong></p><ul><li><a href="https://www.oschina.net/news/269384/openai-buy-ai-chips-startup-sam-altma" target="news">OpenAI 承诺从 Altman 投资的初创公司购买 AI 芯片</a></li><li><p style="margin-left:0px; margin-right:0px; text-align:start"><a href="https://www.oschina.net/news/272549/openai-valuation-100-billion-funding-round" target="_blank">OpenAI 拟以 1000 亿美元估值开启新一轮融资</a></p></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 04:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276217/altman-raise-billions-ai-chip-factories</guid>
            <link>https://www.oschina.net/news/276217/altman-raise-billions-ai-chip-factories</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[夸克 App 上线搜索问答产品「元知」等大模型应用]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#222222">夸克 App 宣布全面升级多个功能板块和智能工具，推出搜索问答产品「元知」，上线夸克 PC 版以及夸克听记等新产品。将围绕智能助手的定位，为用户提供「内容产品+智能工具」的服务矩阵。</span></p><p><span style="background-color:#ffffff; color:#222222"><img alt="" height="534" src="https://oscimg.oschina.net/oscnet/up-30a3f66aba80ac6e349ddbb7c3ee127c67b.webp" width="300" referrerpolicy="no-referrer"></span></p><div style="text-align:start"><p style="color:#222222; margin-left:0; margin-right:0">据介绍，此次升级主要依托自研大模型能力，搜索问答产品「元知」综合全网优质内容，用户可以在搜索结果中，查看到 AIGC 总结提炼出的回答内容，其中包含图文、视频等多种形式，辅助用户更便捷、高效地获取信息。</p></div><div style="text-align:start"><p style="color:#222222; margin-left:0; margin-right:0">同时，为了更好地满足用户在不同场景中的搜索体验，夸克上线了 Windows 系统 PC 版，集合搜索、网盘、扫描等核心功能，给办公、学习用户更好的大屏幕搜索体验。进一步实现个人数字资产在手机、电脑、iPad 三端的一体化信息服务。</p></div><div style="text-align:start"><p style="color:#222222; margin-left:0; margin-right:0">此前，夸克 App 还升级了健康搜索和学习搜索的服务体验。针对健康领域，通过 AIGC 首答、夸克健康百科、智能筛查和夸克健康助手等产品，打造健康信息查询的新体验。</p></div></div>
                                    ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 03:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276216</guid>
            <link>https://www.oschina.net/news/276216</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[QAnything —— 知识库问答引擎]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#1f2328; text-align:start"><strong>QAnything</strong><span>&nbsp;</span>(<strong>Q</strong>uestion and<span>&nbsp;</span><strong>A</strong>nswer based on<span>&nbsp;</span><strong>Anything</strong>) 是致力于支持任意格式文件或数据库的本地知识库问答系统，可断网安装使用。</p><p style="color:#1f2328; text-align:start">你的任何格式的本地文件都可以往里扔，即可获得准确、快速、靠谱的问答体验。</p><p style="color:#1f2328; text-align:start">目前已支持格式：<strong>PDF</strong>，<strong>Word(doc/docx)</strong>，<strong>PPT</strong>，<strong>Markdown</strong>，<strong>Eml</strong>，<strong>TXT</strong>，<strong>图片（jpg，png 等）</strong>，<strong>网页链接</strong>，更多格式，敬请期待...</p><h3 style="text-align:start">特点</h3><ul><li>数据安全，支持全程拔网线安装使用。</li><li>支持跨语种问答，中英文问答随意切换，无所谓文件是什么语种。</li><li>支持海量数据问答，两阶段向量排序，解决了大规模数据检索退化的问题，数据越多，效果越好。</li><li>高性能生产级系统，可直接部署企业应用。</li><li>易用性，无需繁琐的配置，一键安装部署，拿来就用。</li><li>支持选择多知识库问答。</li></ul><h3 style="text-align:start">架构</h3><p><img alt="" height="443" src="https://oscimg.oschina.net/oscnet/up-dd1c3f39b8916876b15ad0cc25aeaa24bfa.png" width="500" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Sat, 20 Jan 2024 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/qanything</guid>
            <link>https://www.oschina.net/p/qanything</link>
        </item>
    </channel>
</rss>
