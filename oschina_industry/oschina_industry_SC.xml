<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Tue, 31 Oct 2023 05:19:12 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[裁员后，SiFive 发文谈前路发展]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">在确认<a href="https://www.oschina.net/news/263350/sifive-lays-off-hundreds-of-risc-v-developers">裁员</a><span style="background-color:#ffffff">五分之一（约 140 名员工）后，RISC-V 创业公司 SiFive 的</span><span style="background-color:#ffffff">创始人兼首席执行官&nbsp;</span>Patrick<span style="background-color:#ffffff">&nbsp;Little&nbsp;发布了一篇名为「</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sifive.com%2Fblog%2Fthe-road-ahead--" target="_blank">The Road Ahead</a><span style="background-color:#ffffff">」的博文，畅谈该公司的发展之路。</span></span></p><blockquote><p><span style="color:#000000">RISC-V 显然是真实存在的，它发展迅速，并将继续存在！</span></p><p><span style="color:#000000">在 SiFive，我们正在紧急重新发明 computing。自 RISC-V 诞生以来，我们一直在推动其变革，我们的产品正被全球半导体行业中最有才华的设计师集成到最具创新性的产品中。</span></p></blockquote><p><img height="222" src="https://oscimg.oschina.net/oscnet/up-19d15e0e38b13c8d504e35b2f85ed9c59d6.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000"><span style="background-color:#ffffff">针对其裁员在社区中引发的大量讨论和猜测，</span>Patrick 解释称，此举是根据公司的前瞻性业务目标而做的运营调整。</span></p><p><span style="color:#000000"><span style="background-color:#ffffff">SiFive 自认发展状况十分良好：资金充足、</span><span style="background-color:#ffffff">投资者积极参与并大力支持，且正在从许多世界领先的半导体公司获得收入和复利版税。该公司表示，后续将继续大力投资于先进研发，以扩大技术领先地位。</span></span></p><p>「<span style="color:#000000">聪明、快速发展的企业会定期停下来认真审视自己的优先事项，以确保其战略、工作流程、能力和人员完全一致......我们发现需要重新将我们的优先事项和资源集中在最有前途的机会上......我们对我们的开放文化和才华横溢的员工感到非常自豪。进行重组涉及艰难的决策，但从战略上讲，我相信这是我们成长并继续引领行业前进的必要条件。我们祝愿所有同事一切顺利。」</span></p><p style="margin-left:0; margin-right:0; text-align:start"><span style="background-color:#ffffff"><span style="color:#000000">SiFive 表示其将继续提供</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sifive.com%2Frisc-v-core-ip" target="_blank">&nbsp;RISC-V 产品组合。</a>「<span style="color:#000000">我们继续增加我们的行业第一和我们的产品组合，包括我们本月早些时候发布的产品——SiFive&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sifive.com%2Fcores%2Fperformance-p870-p870a" target="_blank">Performance P870</a><span style="color:#000000">和</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sifive.com%2Fcores%2Fintelligence-x390" target="_blank">SiFive Intelligence X390</a><span style="color:#000000">。</span>」</span></p><p style="margin-left:0; margin-right:0; text-align:start"><span style="color:#000000">同时，对四大产品系列的承诺也保持不变：<span style="background-color:#ffffff">即 Essential、Intelligence、Performance 和 Automotive 解决方案。</span>Patrick 认为，<span style="background-color:#ffffff">「</span></span><span style="background-color:#ffffff; color:#212529">SiFive 的增长从未如此强劲，我们的机遇也从未如此美好，而且我从未如此自信，SiFive 将通过 RISC-V 以我们尚未开始想象的方式彻底改变计算。</span><span style="color:#000000"><span style="background-color:#ffffff">」</span></span></p><p style="margin-left:0; margin-right:0; text-align:start"><span style="color:#000000"><span style="background-color:#ffffff">但大众对此貌似并不买账，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.ycombinator.com%2Fitem%3Fid%3D38066588%26ref%3Dupstract.com" target="_blank">Hacker News</a> 上有人嘲讽道：</span></span></p><blockquote><p style="margin-left:0; margin-right:0; text-align:start"><span style="color:#000000">「撇开这篇帖子对被解雇员工的麻木不仁不谈（这已经够糟糕了），它实际上并没有告诉现有/潜在客户，SiFive 的业务和产品发生了什么变化。显然是有一些变化发生了，但其并未提供任何实质性的线索。完全适得其反。不过，一切都好，因为 SiFive 将以我们尚未开始想象的方式彻底改变计算！」</span></p></blockquote><p style="margin-left:0; margin-right:0; text-align:start"><span style="color:#000000"><span style="background-color:#ffffff">更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sifive.com%2Fblog%2Fthe-road-ahead--" target="_blank">查看官方博客</a>。</span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 04:00:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264245/sifive-the-road-ahead</guid>
            <link>https://www.oschina.net/news/264245/sifive-the-road-ahead</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[小米免费可商用字体 MiSans L3 发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">随着小米澎湃 OS（Xiaomi HyperOS）的发布，其设计团队也对原先提供的可免费商用字体 MiSans&nbsp;进行了更新。本次更新带来了大量生僻字支持，并符合最新 L3 级别&nbsp;GB18030-2022 国标。</span></p><p><span style="color:#000000">根据介绍，GB18030-2022 强制规范三个实现级别，于 2023 年 8 月 1 日起开始执行。实现级别 1 共 27,584 个汉字；实现级别 2 包含实现级别 1，此外，实现级别 2 还支持《通用规范汉字表》中的没有包含在实现级别 1 之内的编码汉字，共计 27,780 个汉字；实现级别 3 包含实现级别 2，此外，实现级别 3 还支持新标准件规定的全部汉字及表 3 中的康熙部首，总计 87,887 个汉字，用于政务服务和公共服务的产品应满足实现级别 3 的要求。</span></p><p><span style="color:#000000">MiSans 包含级别 1+级别 2，MiSans L3 为级别 3 字库（该字库不包含级别 1 和级别 2）。目前，小米提供的多种字体中只有 MiSans L3 满足新国标要求并增加了大量生僻字支持。</span></p><p><span style="color:#000000"><img height="1081" src="https://oscimg.oschina.net/oscnet/up-9d9e4a6db81c509dd3ee1d3fcdcda093c61.png" width="500" referrerpolicy="no-referrer"></span></p><p><strong><span style="color:#000000">下载地址：</span></strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhyperos.mi.com%2Ffont%2Fdownload" target="_blank">https://hyperos.mi.com/font/download</a></p><p><span style="color:#000000">附字体许可协议：</span></p><blockquote><p><span style="color:#000000">本《MiSans 字体知识产权许可协议》 (以下简称「协议」) 是您与小米科技有限责任公司 (以下简称「小米」或「许可方」) 之间有关安装、使用 MiSans 字体 (以下简称「MiSans」或「MiSans 字体」) 的法律协议。您在使用 MiSans 的所有或任何部分前，应接受本协议中规定的所有条款和条件安装、使用 MiSans 的行为表示您同意接受本协议所有条款的约束。否则，请不要安装或使用 MiSans，并应立即销毁和删除所有 MiSans 字体包。</span></p><p><span style="color:#000000">根据本协议的条款和条件，许可方在此授予您一份不可转让的、非独占的、免版税的、可撤销的、全球性的版权许可，使您依照本协议约定使用 MiSans 字体，前提是符合下列条件：</span></p><ol><li><span style="color:#000000">您应在软件中特别注明使用了 MiSans 字体。</span></li><li><span style="color:#000000">您不得对 MiSans 字体或其任何单独组件进行改编或二次开发。</span></li><li><span style="color:#000000">您不得单独将 MiSans 字体或其组件对外租赁、再许可、给予、出借或进一步分发字体软件或其任何副本以及重新分发或售卖。此限制不适用于您使用 MiSans 字体创作的任何其他作品。如您使用 MiSans 字体创作宣传素材、logo、应用 App 等，您有权分发或出售该作品。</span></li></ol></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 03:12:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264234</guid>
            <link>https://www.oschina.net/news/264234</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[苹果发布 M3 系列芯片，采用 3nm 工艺、支持「动态缓存」技术]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>苹果今天在「来势迅猛」发布会上正式官宣 M3、M3 Pro、M3 Max 芯片，是首款采用 3 纳米工艺技术的 PC 芯片。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-71f73fe69b327c0080613e563857425d4b4.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a5ffd09ef952faf193b698712952791bef6.png" referrerpolicy="no-referrer"></p><p>苹果介绍称，M3 系列芯片搭载的新一代图形处理器实现了 Apple 芯片史上最大幅的图形处理器架构飞跃。这款图形处理器不仅速度更快、能效更高，还引入一项全新技术 —— <strong>动态缓存</strong>，同时带来首次登陆 Mac 的硬件加速光线追踪和网格着色等全新渲染功能。渲染速度与 M1 系列芯片相比最快可达 2.5 倍。中央处理器搭载的高性能核心和高能效核心比 M1 中的相应核心分别快 30% 和 50%，神经网络引擎也比 M1 系列芯片上的快 60%。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-dc04539ba6a94757f1c3fab246b75cd579a.png" referrerpolicy="no-referrer"></p><blockquote><p>M3 配备 8 核 CPU，10 核 GPU，24GB 统一内存，速度最高比 M2 提升 20%；</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-623eac3535af9db850bfb2d64ea6ab5b3dd.png" referrerpolicy="no-referrer"></p><p>M3 Pro 配备 12 核 CPU，18 核 GPU，36GB 统一内存，速度最高比 M2 Pro 提升 10%；</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-28da4c8dc357151c24fb01ea6b31a70e3e5.png" referrerpolicy="no-referrer"></p><p>M3 Max 配备 16 核 CPU，40 核 GPU，128GB 统一内存，速度最高比 M2 Max 提升 20%。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-25db54ce704b69c46fa5dc54117b8d2ca78.png" referrerpolicy="no-referrer"></p></blockquote><p>据介绍，M3 系列芯片中的新一代图形处理器实现了 Apple 芯片史上最大幅的图形处理器架构飞跃。不同于传统图形处理器，它具备动态缓存功能，因而可对硬件中本地内存的使用进行实时分配。在动态缓存功能的加持下，每项任务对内存的消耗精准符合所需。</p><p>此项业界首创技术对开发者透明，为打造全新图形处理器架构提供了基石。它大幅提高了图形处理器的平均利用率，进而给要求更苛刻的专业级 App 及游戏的表现带来显著提升。</p><p>在 M3 系列芯片的支持下，硬件加速光线追踪功能首度登陆 Mac。光线追踪技术能够模拟光线在场景中的表现，从而帮助 App 创造出栩栩如生、逼真的画面。通过这一功能和全新图形处理器架构的加成，专业级 App 的运行速度最高可达到 M1 系列芯片的 2.5 倍。</p><p>此外，全新图形处理器还给 Mac 带来硬件加速网格着色功能，实现图形处理能力和能效的双重提升，更可支持游戏和对图形处理要求高的 App 呈现视觉效果更复杂的场景。官方称，M3 图形处理器在功耗减半的情况下，即可达到与 M1 相当的性能，而在峰值功耗下更可实现高达 65% 的性能提升。</p><p>M3 家族中的所有芯片均搭载 Apple 芯片标志性的统一内存架构。这带来了高带宽、低延迟，以及无出其右的高能效。此外，M3 芯片支持的内存容量最高达 128GB，这使过去无法在笔记本电脑上处理的工作流成为可能，例如 AI 开发者现可运行包含数十亿个参数的规模更大的 Transformer 模型。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-4b873d2a11a318754f6315c42b0e6c81f6f.png" referrerpolicy="no-referrer"></p><p>M3、M3 Pro 和 M3 Max 芯片还引入增强型神经网络引擎，用于加速强大的机器学习（ML）模型。与 M1 系列芯片相比，新的神经网络引擎带来最高达 60% 的速度提升，在进一步加速 AI / ML 工作流的同时，还可将数据保留在设备上，以保护用户隐私。</p><p>此外，M3、M3 Pro 和 M3 Max 还支持多种编解码器，例如 H.264、HEVC、ProRes 和 ProRes RAW 以及 AV1。</p><hr><p>除了 M3 系列芯片，苹果还公布了新款 MacBook Pro、iMac。</p><ul><li><strong>MacBook Pro：芯片升级，全新黑色亮相</strong></li></ul><p>作为首条搭载 M3 系列芯片的产品线，全新的 14 英寸和 16 英寸 MacBook Pro 在外形方面并没有任何变化，依然是我们熟悉的「刘海屏」，传闻中的灵动岛并没有到来。</p><p>不过颜色方面新增了「深空黑色」。深空黑色版 MacBook Pro 不仅有着颜色更深的黑色铝金属外壳，外壳还采用了更加先进的化学工艺，外壳表面的阳极氧化层能够有效减少指纹的产生。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-f4507207761ea7bc0dffbdc2631b18d3bb9.png" referrerpolicy="no-referrer"></p><p>需要注意的是，<strong>深空黑色版 MacBook Pro 仅在搭载 M3 Pro 和 M3 Max 的型号中提供，如果购买 M3 芯片的机型，依然只能选到深空灰色的版本</strong>。</p><p>价格方面，14 英寸版本中 M3 芯片机型起售价格 12999 元起、M3 Pro 芯片机型起售价格 16999 元起、M3 Max 芯片机型起售价格 26999 元起；16 英寸版本中 M3 Pro 芯片机型起售价格 19999 元起，M3 Max 芯片机型起售价格 27999 元起。</p><p><img src="https://static.oschina.net/uploads/space/2023/1031/115309_of9w_2720166.png" referrerpolicy="no-referrer"></p><ul><li><strong>iMac：采用 M3 芯片，时隔两年终更新</strong></li></ul><p>时隔两年半，24 英寸的 iMac 芯片由 M1 升级最新的 M3 芯片，其芯片运算性能相较上一代 iMac M1 提升了一倍，并且最高可选配 24 GB 统一内存。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-98a671e3f39f136b9795179314aaeb3d2b7.png" referrerpolicy="no-referrer"></p><p>本代 iMac 四个端口的版本依然配有蓝、绿、粉、银、黄、橙和紫七种颜色可选，两个端口的版本仅有蓝、绿、粉、银可选。</p><p>售价方面，iMac 两个端口 (2 个雷雳 / USB 4) 的版本起步价为 10999 元、四个端口 (2 个雷雳 / USB 4 + 2 个 USB 3) 的版本起步价为 12499，相比上代均提高了 1000 元。11 月 1 日上午 9 点接受订购。11 月 7 日发售。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 03:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264233/apple-m3-silicon</guid>
            <link>https://www.oschina.net/news/264233/apple-m3-silicon</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[【直播预告】关于开源创业的 15 件小事]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>只要软件开源了，就会有人用？</p><p>开源软件有漏洞，跟作者没关系？</p><p>开源软件协议应当选择最宽松的？</p><p style="text-align:left">应该努力地将软件捐献给基金会？</p><p>开源后，会有很多人来完善项目？</p><p>开源不是为了钱？</p><p>开源软件靠服务和捐助就可以赚钱？</p><p><strong>以上七个问题，禅道创始人王春生的回答都是「 NO 」。</strong>他用自己的亲身经历告诉大家，很多我们想当然的事情，其实并非如此。</p><p>11 月 2 日 19:00，OSCHINA 直播——【开源漫谈】第 5 期，邀请了三位大咖，请他们来聊一聊开源创业遇到的一些难题。他们分别是：</p><ul><li><strong>高春辉</strong>，中国第一个人站长，卓越网、手机之家、ECSHOP 软件、《爱壁纸 HD》应用创始人，全球领先级 ip 库 http://ipip.net 创始人</li><li><strong>王春生</strong>，禅道软件公司的创始人，二十年的 IT 老兵，14 年的创业者</li><li><strong>朱峰</strong>，津津乐道播客网络创始人、主播。连续创业者，商业经验丰富；有多年社区运营经验；资深开发者</li></ul><p>这次直播，不讲大道理，就讲讲开源创业实务，话题不设限，怎么选开源协议，要不要把开源项目捐给基金会，出现了负面舆论怎么「公关」，公司没钱了去哪里找钱，怎么给员工福利，等等，都拿出来讲一讲。</p><p><strong>直播主题：</strong>关于开源创业的 15 件小事</p><p style="text-align:left"><strong>直播时间：</strong>11 月 2 日（周四） 19:00-20:00</p><p style="text-align:left"><strong>直播平台：</strong>「OSC 开源社区」 视频号</p><p><strong>主办方：</strong>开源中国</p><p><span style="background-color:#ffffff; color:#333333">微信扫码预约直播，欢迎加入 OSC 直播交流群，一起唠嗑～</span></p><p><img height="2542" src="https://oscimg.oschina.net/oscnet/up-cb4840eb78bf9005dfd68ff7cb9ce43c4eb.jpg" width="750" referrerpolicy="no-referrer"></p><p><strong>直播福利</strong></p><ul><li><p style="margin-left:0; margin-right:0">互动抽奖：在直播评论区提问，被直播嘉宾回复的用户可获 OSC T 恤 1 件，名额不限。</p></li><li><p style="margin-left:0; margin-right:0">福袋抽奖：直播中将有多轮抽奖，参与就有机会获得 OSC T 恤、笔记本、马克杯 、前沿技术书籍等。</p></li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">我们直播间见吧～</p><div><hr></div><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><strong>另外，本次直播得到了诸多社区或组织的大力支持，在此特别表示感谢：</strong></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span><strong>渠成开源社区</strong></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>渠成开源社区由禅道项目管理软件团队发起，社区的经营主体为青岛渠成开源计算机网络技术研究中心，是非营利性社会服务活动的社会组织。 渠成开源社区主要面向一线开源软件生产者、贡献者、组织者、赞助商和用户，以解决具体实际问题为宗旨，旨在打造以开源软件为核心纽带的开源生态系统，真正做到让每一个优秀的开源软件都能实现商业化。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.qucheng.cc" target="_blank">www.qucheng.cc</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span><strong>禅道</strong></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>禅道是一款开源的全生命周期项目管理软件，基于敏捷和 CMMI 管理理念进行设计，集产品管理、项目管理、质量管理、文档管理、组织管理和事务管理于一体，完整地覆盖了项目管理的核心流程。 禅道自 2009 年发布至今，累计为国内数十万计的公司或团队提供了专业的项目管理工具。</span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.zentao.net%2F" target="_blank">https://www.zentao.net/</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span><strong>津津乐道博客</strong></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>津津乐道播客成立于 2016 年 2 月，是天津猿行天下科技有限公司旗下的播客品牌。津津乐道播客主创团队由多位行业资深人士组成，本着分享体验、传播经验的原则，团队在 IT、科技、旅游、教育等领域，制作了多档播客节目，并获得市场好评。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdao.fm%2F" target="_blank">https://dao.fm/</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><strong><span><span><span><span><span><span><span style="color:#000000"><span><span>IPIP.net &nbsp;</span></span></span></span></span></span></span></span></span></strong></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>IPIP 专注 IP 地理位置以及 IP 画像数据的研究、整理与发行，我们的主力产品 IP 地理位置数据库主要基于 BGP/ASN 数据以及遍布全球的网络监测点进行城市级 IP 地域数据标注，准确度远高于国内国外同类产品。 &nbsp;</span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.ipip.net%2F" target="_blank">https://www.ipip.net/</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><strong><span><span><span><span><span><span><span style="color:#000000"><span><span>GreatSQL 社区 &nbsp;</span></span></span></span></span></span></span></span></span></strong></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>GreatSQL 社区成立于 2021 年，由万里数据库发起，致力于通过开放的社区合作，构建国内自主开源数据库版本及开源数据库技术，推动中国开源数据库及应用生态繁荣发展。GreatSQL 是适用于金融级应用的国内自主开源数据库，具备高性能、高可靠、高易用性、高安全等多个核心特性，可以作为 MySQL 或 Percona Server 的可选替换，用于线上生产环境，且完全免费并兼容 MySQL 或 Percona Server。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网链接：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreatsql.cn%2F" target="_blank">https://greatsql.cn/ </a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>Gitee 仓库：<a href="https://gitee.com/GreatSQL">https://gitee.com/GreatSQL</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span><strong>爱可生开源社区</strong></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>爱可生开源社区，一个有深度的 MySQL 开源社区。社区成立于 2017 年，以开源高质量的运维工具、日常分享技术干货内容、数据库技术布道为己任；目前开源的产品有：SQL 审核工具 SQLE、分布式中间件 DBLE 和数据传输组件 DTLE。在这里，你将收获：高质量的技术内容，企业级数据库工具及服务，丰富的社区活动。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>链接： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopensource.actionsky.com%2F" target="_blank">https://opensource.actionsky.com/</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span><strong>PG 中文社区</strong></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>PostgreSQL 中文社区是一个非盈利的民间组织，目前成员都以志愿者身份加入，成立的目的在于构建 PG 数据库技术生态圈子 (内核、用户培训机构、厂商、服务商、软件开发商、高校形成 「业务与利益双向驱动」 的良性发展生态圈)；帮助企业解决人才培养和企业商用数据库成本问题，社区会在各运营平台发布 PostgreSQL 最新信息和 PostgreSQL 相关技术文章，推动 PG 技术在中国的发展。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网链接：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.postgres.cn%2Findex.php%2Fv2%2Fhome" target="_blank">http://www.postgres.cn/index.php/v2/home</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span><strong>凹语言</strong></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>凹语言是一个面向 WebAssembly 设计的静态类型编译型语言，目标是简化 WASM 应用的开发。目前已经发布 MVP 版本，并提供了在线的纯浏览器 Playground 和贪吃蛇案例实现。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>主页： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwa-lang.org" target="_blank">https://wa-lang.org</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><strong><span><span><span><span><span><span><span style="color:#000000"><span><span>KCL 社区 &nbsp;</span></span></span></span></span></span></span></span></span></strong></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>KCL&nbsp;是一个开源的基于约束的记录及函数语言，作为沙盒项目托管在 CNCF 基金会。KCL 通过成熟的编程语言技术和实践来改进对大量繁杂配置比如云原生 Kubernetes 配置场景的编写，致力于构建围绕配置的更好的模块化、扩展性和稳定性，更简单的逻辑编写，以及更简单的自动化和生态工具集成。 &nbsp;</span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网链接：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fkcl-lang.io+GitHub" target="_blank">https://kcl-lang.io GitHub </a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>仓库：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkcl-lang" target="_blank">https://github.com/kcl-lang</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span><strong>AllData</strong></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>AllData 大数据产品是可定义数据中台，以数据平台为底座，以数据中台为桥梁，以机器学习平台，GPT 平台为框架，提供全链路数字化解决方案。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>项目地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falldatacenter%2Falldata" target="_blank">https://github.com/alldatacenter/alldata </a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>社区官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Falldata.readthedocs.io%2Fzh%2Fmaster%2F" target="_blank">https://alldata.readthedocs.io/zh/master/</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><strong><span><span><span><span><span><span><span style="color:#000000"><span><span>得物技术</span></span></span></span></span></span></span></span></span></strong></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>得物技术一直以"上海最好的技术团队"为目标，现已建立上海、北京、杭州三地研发协同与管理机制，实现研发过程数据化、自动化；覆盖供应链、业务支撑、算法、前端等领域，是得物业务背后强有力的技术力量支撑。 &nbsp;&nbsp;</span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网链接： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftech.dewu.com%2F" target="_blank">https://tech.dewu.com/</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><strong><span><span><span><span><span><span><span style="color:#000000"><span><span>重庆软件园 &nbsp;</span></span></span></span></span></span></span></span></span></strong></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>重庆软件园位于重庆经开区，占地 110 万平方米，布局四大组团，是重庆市首批软件产业园 (综合型)、A 区入选重庆市软件和信息服务业「满天星」示范楼宇 (首批)，于 2019 年 9 月 16 日正式开园，坚持「做生态=做产业，做人才=做产业，做服务=做产业」的发展理念，建设集科技、人文、生态、智慧为一体的领军型软件园区。聚焦「3+2」产业布局，实现新一代信息技术产业集群发展。</span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>园区聚集软件类企业近 2000 家，软件人才近 3 万人，已登记 4000 多项软件著作权，研发投入超 50 亿，40 余项专利将获得科技奖，营收上亿企业近 20 家。立足南岸区、重庆经开区优质产业资源，聚焦软件信息服务业、智能制造、绿色环保 、汽车软件汽车电子、大健康等产业，推动软件产业高质量发展，重庆软件园将全面贯彻落实「满天星」计划，力争到 2026 年成功建成中国软件名园。 &nbsp;</span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>园区官网：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.chongqingpark.com%2F" target="_blank">http://www.chongqingpark.com/</a></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><strong><span><span><span><span><span><span><span style="color:#000000"><span><span>东方瑞通 &nbsp;</span></span></span></span></span></span></span></span></span></strong></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>东方瑞通成立于 1998 年，是国内较早的 IT 高级技术培训企业之一，拥有华为、红帽、微软、PMI、VMware、Oracle 等 33 余家国际厂商授权资质，以培养 it 人才为主，目前覆盖领域：虚拟化、操作系统、网络、安全、数据库、IT 管理、软件开发等细分领域，提供线上，线下交流培训课程与活动。 </span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#000000"><span><span>官网链接：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.easthome.com" target="_blank">www.easthome.com</a></span></span></span></span></span></span></span></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 02:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/10139755</guid>
            <link>https://my.oschina.net/u/3859945/blog/10139755</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[企业部署 Elasticsearch 因漏洞导致数据泄露，被罚款 5 万]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify">北京市网信办依据《中华人民共和国数据安全法》对属地三家企业<strong>涉嫌存在网络数据安全违法行为</strong>进行立案调查并作出行政处罚。</p><p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify">据称，三家企业违反《中华人民共和国数据安全法》第二十七条规定，<strong>未履行数据安全保护义务，<span style="color:#e67e22">部署的 ElasticSearch 数据库存在未授权访问漏洞，造成部分数据泄露</span></strong>。</p><p style="color:#3e3e3e; margin-left:0; margin-right:0; text-align:justify"><img src="https://static.oschina.net/uploads/space/2023/1031/104106_Il49_2720166.png" referrerpolicy="no-referrer"></p><p>北京市网信办依据《中华人民共和国数据安全法》第四十五条第一款规定，对三家企业分别作出责令改正，给予警告，<strong>并处 5 万元罚款的行政处罚，对直接主管人员和其他责任人员处以 1 万元罚款处罚</strong>。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 02:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264219</guid>
            <link>https://www.oschina.net/news/264219</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Ollama —— 在本地启动并运行大语言模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">Ollama 是一款命令行工具，可在 macOS 和 Linux 上本地运行 Llama 2、Code Llama 和其他模型。目前适用于 macOS 和 Linux，并计划支持 Windows。</span></p><p><span style="color:#000000">Ollama 目前支持近二十多个语言模型系列，每个模型系列都有许多可用的"tags"。Tags&nbsp;是模型的变体，这些模型使用不同的微调方法以不同的规模进行训练，并以不同的级别进行量化，以便在本地良好运行。量化级别越高，模型越精确，但运行速度越慢，所需的内存也越大。</span></p><p><span style="background-color:#ffffff; color:#1f2328">以下是一些可以下载的开源模型示例：</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; border-spacing:0px; box-sizing:border-box; color:#1f2328; display:block; font-family:-apple-system,BlinkMacSystemFont,&quot;Segoe UI&quot;,&quot;Noto Sans&quot;,Helvetica,Arial,sans-serif,&quot;Apple Color Emoji&quot;,&quot;Segoe UI Emoji&quot;; font-size:16px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:normal; margin-bottom:16px; margin-top:0px; max-width:100%; orphans:2; overflow:auto; text-align:start; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:max-content; word-spacing:0px"><thead><tr><th>Model</th><th>Parameters</th><th>Size</th><th>Download</th></tr></thead><tbody><tr><td style="border-style:solid; border-width:1px">Mistral</td><td style="border-style:solid; border-width:1px">7B</td><td style="border-style:solid; border-width:1px">4.1GB</td><td style="border-style:solid; border-width:1px"><code>ollama run mistral</code></td></tr><tr><td style="border-style:solid; border-width:1px">Llama 2</td><td style="border-style:solid; border-width:1px">7B</td><td style="border-style:solid; border-width:1px">3.8GB</td><td style="border-style:solid; border-width:1px"><code>ollama run llama2</code></td></tr><tr><td style="border-style:solid; border-width:1px">Code Llama</td><td style="border-style:solid; border-width:1px">7B</td><td style="border-style:solid; border-width:1px">3.8GB</td><td style="border-style:solid; border-width:1px"><code>ollama run codellama</code></td></tr><tr><td style="border-style:solid; border-width:1px">Llama 2 Uncensored</td><td style="border-style:solid; border-width:1px">7B</td><td style="border-style:solid; border-width:1px">3.8GB</td><td style="border-style:solid; border-width:1px"><code>ollama run llama2-uncensored</code></td></tr><tr><td style="border-style:solid; border-width:1px">Llama 2 13B</td><td style="border-style:solid; border-width:1px">13B</td><td style="border-style:solid; border-width:1px">7.3GB</td><td style="border-style:solid; border-width:1px"><code>ollama run llama2:13b</code></td></tr><tr><td style="border-style:solid; border-width:1px">Llama 2 70B</td><td style="border-style:solid; border-width:1px">70B</td><td style="border-style:solid; border-width:1px">39GB</td><td style="border-style:solid; border-width:1px"><code>ollama run llama2:70b</code></td></tr><tr><td style="border-style:solid; border-width:1px">Orca Mini</td><td style="border-style:solid; border-width:1px">3B</td><td style="border-style:solid; border-width:1px">1.9GB</td><td style="border-style:solid; border-width:1px"><code>ollama run orca-mini</code></td></tr><tr><td style="border-style:solid; border-width:1px">Vicuna</td><td style="border-style:solid; border-width:1px">7B</td><td style="border-style:solid; border-width:1px">3.8GB</td><td style="border-style:solid; border-width:1px"><code>ollama run vicuna</code></td></tr></tbody></table><blockquote><p><span style="color:#000000">注意：需要至少有 8 GB 的 RAM 来运行 3B 模型，16 GB 的 RAM 来运行 7B 模型，32 GB 的 RAM 来运行 13B 模型。</span></p></blockquote></div>
                                                                ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 02:28:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/ollama</guid>
            <link>https://www.oschina.net/p/ollama</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 工作流引擎 FlowLong]]>
            </title>
            <description>
                <![CDATA[<img src="https://foruda.gitee.com/images/1693470775312764207/27440c57_12260.png" alt="flowlong" width="100px" height="113px" referrerpolicy="no-referrer"><h1><a id="user-content-项目介绍" class="anchor" href="https://gitee.com/aizuda/flowlong#%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D"></a>项目介绍</h1><p>FlowLong🐉飞龙工作流</p><ul><li>项目说明  <code>flowlong</code> 中文名 <code>飞龙</code> 在天美好愿景！</li></ul><blockquote><p>⭕本项目采用 <code>AGPL</code> 开源协议（抄袭牟利索赔 100 万），</p></blockquote><blockquote><p>使用必须遵守国家法律法规，⛔不允许非法项目使用，后果自负❗</p></blockquote><p><a href="https://gitee.com/aizuda/flowlong/issues/I7XGP5">使用源码登记入口</a></p><p><a href="https://flowlong.gitee.io/" rel="nofollow">打开官方开发文档</a></p><p><a href="https://flowlong.gitee.io/flowlong-designer" rel="nofollow">点击设计器在线演示</a></p><p><a href="https://gitee.com/flowlong/flowlong-designer">点击设计器源码下载</a></p><p>英文字母 <code>flw</code> 为 <code>flowlong workflow</code> 飞龙工作流的缩写</p><p>🚩中国特色流程操作概念</p><table><thead><tr><th>支持功能</th><th>功能描述</th><th>完成程度</th></tr></thead><tbody><tr><td>顺序会签</td><td>指同一个审批节点设置多个人，如 A、B、C 三人，三人按顺序依次收到待办，即 A 先审批，A 提交后 B 才能审批，需全部同意之后，审批才可到下一审批节点。</td><td>✅</td></tr><tr><td>并行会签</td><td>指同一个审批节点设置多个人，如 A、B、C 三人，三人会同时收到待办任务，需全部同意之后，审批才可到下一审批节点。</td><td>✅</td></tr><tr><td>或签</td><td>一个流程审批节点里有多个处理人，任意一个人处理后就能进入下一个节点</td><td>✅</td></tr><tr><td>抄送</td><td>将审批结果通知给抄送列表对应的人</td><td>✅</td></tr><tr><td>驳回</td><td>将审批重置发送给某节点，重新审批。驳回也叫退回，也可以分退回申请人、退回上一步、任意退回等</td><td>✅</td></tr><tr><td>分配</td><td>允许用户自行决定任务转办、委派、主办，及其它</td><td>✅</td></tr><tr><td>转办</td><td>A 转给其 B 审批，B 审批后，进入下一节点</td><td>✅</td></tr><tr><td>委派</td><td>A 转给其 B 审批，B 审批后，转给 A，A 审批后进入下一节点</td><td>✅</td></tr><tr><td>跳转</td><td>可以将当前流程实例跳转到任意办理节点</td><td>✅</td></tr><tr><td>拿回</td><td>在当前办理人尚未处理文件前，允许上一节点提交人员执行拿回</td><td>✅</td></tr><tr><td>撤销</td><td>流程发起者可以对流程进行撤销处理</td><td>✅</td></tr><tr><td>加签</td><td>允许当前办理人根据需要自行增加当前办理节点的办理人员</td><td>✅</td></tr><tr><td>减签</td><td>在当前办理人操作之前减少办理人</td><td>✅</td></tr><tr><td>认领</td><td>公共任务认领</td><td>✅</td></tr><tr><td>已阅</td><td>任务是否查看状态显示</td><td>✅</td></tr><tr><td>催办</td><td>通知当前活动任务处理人办理任务</td><td>✅</td></tr><tr><td>沟通</td><td>与当前活动任务处理人沟通</td><td>✅</td></tr><tr><td>终止</td><td>在任意节点终止流程实例</td><td>✅</td></tr></tbody></table><h1><a id="user-content-贡献力量" class="anchor" href="https://gitee.com/aizuda/flowlong#%E8%B4%A1%E7%8C%AE%E5%8A%9B%E9%87%8F"></a>贡献力量</h1><ul><li><a href="https://gitee.com/aizuda/flowlong/wikis/%E8%BF%90%E8%A1%8C%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95">运行单元测试</a></li><li>PR 请参考现在代码规范注释说明</li></ul><h1><a id="user-content-使用文档" class="anchor" href="https://gitee.com/aizuda/flowlong#%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3"></a>使用文档</h1><ul><li>设计器源码 <a href="https://gitee.com/flowlong/flowlong-designer">https://gitee.com/flowlong/flowlong-designer</a></li></ul><img src="https://foruda.gitee.com/images/1683680723972384655/f957e75d_12260.png" alt="flowlong" width="500px" height="262px" referrerpolicy="no-referrer"><h1><a id="user-content-其它说明" class="anchor" href="https://gitee.com/aizuda/flowlong#%E5%85%B6%E5%AE%83%E8%AF%B4%E6%98%8E"></a>其它说明</h1><ul><li>基于 <a href="https://gitee.com/link?target=https%3A%2F%2Fbaomidou.com">MybatisPlus</a> 为 <code>ORM</code> 层实现</li><li>后端设计参考了 <a href="https://gitee.com/yuqs/snakerflow">snakerflow</a> 开源工作流实体划分</li></ul>]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 02:23:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/aizuda/flowlong</guid>
            <link>https://gitee.com/aizuda/flowlong</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 浅析 Redis 大 Key]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1>一、背景</h1><p><span style="color:#393c5a">在京东到家购物车系统中，用户基于门店能够对商品进行加车操作。用户与门店商品使用 Redis 的 Hash 类型存储，如下代码块所示。不知细心的你有没有发现，如果单门店加车商品过多，或者门店过多时，此 Key 就会越来越大，从而影响线上业务。</span></p><pre><code>userPin:{
      storeId:{门店下加车的所有商品基本信息},
      storeId:{门店下加车的所有商品基本信息},
      ......
}

</code></pre><span id="OSC_h1_2"></span><h1>二、BigKey 的界定和如何产生</h1><span id="OSC_h3_3"></span><h3>2.1、BigKey 的界定</h3><p><span style="color:#393c5a">BigKey 称为大 Key，通常以 Key 对应 Value 的存储大小，或者 Key 对应 Value 的数量来进行综合判断。对于大 Key 也没有严格的定义区分，针对 String 与非 String 结构，给出如下定义：</span></p><ul><li><span style="color:#333333">String：String 类型的 Key 对应的 Value 超过 10KB</span></li><li><span style="color:#333333">非 String 结构（Hash</span><span style="color:#777777">，</span>Set<span style="color:#777777">，</span>ZSet<span style="color:#777777">，</span>List）：Value 的数量达到 10000 个，或者 Vaule 的总大小为 100KB</li><li><span style="color:#333333">集群中 Key 的总数超过 1 亿</span></li></ul><span id="OSC_h3_4"></span><h3>2.2、如何产生</h3><p><span style="color:#393c5a">1、数据结构设置不合理，例如集合中元素唯一时，应该使用 Set 替换 List；</span></p><p><span style="color:#393c5a">2、针对业务缺少预估性，没有预见 Value 动态增长；</span></p><p><span style="color:#393c5a">3、Key 没有设置过期时间，把缓存当成垃圾桶，一直再往里面扔，但是从不处理。</span></p><span id="OSC_h1_5"></span><h1>三、BigKey 的危害</h1><span id="OSC_h3_6"></span><h3>3.1、数据倾斜</h3><p><span style="color:#393c5a">redis 数据倾斜分为</span><strong><span style="color:#393c5a">数据访问倾斜</span></strong><span style="color:#393c5a">和</span><strong><span style="color:#393c5a">数据量倾斜，</span></strong><span style="color:#393c5a">会导致该 Key 所在的数据分片节点 CPU 使用率、带宽使用率升高，从而影响该分片上所有 Key 的处理。</span></p><p><strong><span style="color:#393c5a">数据访问倾斜：</span></strong><span style="color:#393c5a">某节点中 key 的 QPS 高于其他节点中的 Key</span></p><p><strong><span style="color:#393c5a">数据量倾斜：</span></strong><span style="color:#393c5a">某节点中 key 的大小高于其他节点中的 Key，如下图，实例 1 中的 Key1 存储高于其他实例。</span></p><div><img src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=MTY3YzQ1ODcwZjZkOGYzYTc0YzljYjg4ZmVkMTZlNTEsMTY5ODcxODQzNDA3Nw==" referrerpolicy="no-referrer"></div><span id="OSC_h3_7"></span><h3>﻿3.2、网络阻塞</h3><p><span style="color:#393c5a">Redis 服务器是一个事件驱动程序，有文件事件和时间事件，文件事件和时间事件都是主线程完成。其中文件事件就是服务器对套接字操作的抽象，客户端与服务端的通信会产生相应的文件事件，服务器通过监听并处理这些事件来完成一系列网络通信操作。</span></p><p><span style="color:#393c5a">Redis 基于 Reactor 模式开发了自己的网络事件处理器，即文件事件处理器，该处理器内部使用 I/O 多路复用程序，可同时监听多个套接字，并根据套接字执行的任务来关联不同的事件处理器。文件事件处理器以单线程的方式运行，但是通过 I/O 多路复用程序来监听多个套接字，既实现了高性能网络通信模型，又保持了内部单线程设计的简单性。文件事件处理器构成如下图：</span></p><div><img src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=OWJmZjdlMWNjNDIyMmJjMWYwNDU4YjI4NTM0YmQwYmIsMTY5ODcxODQzNDA3Nw==" referrerpolicy="no-referrer"></div><p>﻿﻿<span style="color:#393c5a">文件事件是对套接字操作的抽象，包括连接应答，写入，读取，关闭，因为一个服务器会连接多个套接字，所以文件事件可能并发出现，即使文件事件并发的出现，但是</span><strong><span style="color:#f5222d">I/O 多路复用程序会将套接字放入一个队列，通过队列有序的，同步的每次一个套接字的方式</span></strong>向文件事件分派器传送套接字，当让一个套接字产生的事件被<strong><span style="color:#f5222d">处理完毕</span></strong>后，I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字，当有大 key 时，单次操作时间延长，导致网络阻塞。</p><span id="OSC_h3_8"></span><h3>3.3、慢查询</h3><p><span style="color:#393c5a">严重影响 QPS 、TP99 等指标，对大 Key 进行的慢操作会导致后续的命令被阻塞，从而导致一系列慢查询。</span></p><span id="OSC_h3_9"></span><h3>3.4、CPU 压力</h3><p><span style="color:#393c5a">当单 Key 过大时，每一次访问此 Key 都可能会造成 Redis 阻塞，其他请求只能等待了。如果应用中设置了超时等，那么上层就会抛出异常信息。最后删除的时候也会造成 redis 阻塞，到时候内存中数据量过大，就会造成 CPU 负载过高。单个分片 cpu 占用率过高，其他分片无法拥有 cpu 资源，从而被影响。此外，大 key 对持久化也有些影响。fork 操作会拷贝父进程的页表项，如果过大，会占用更多页表，主线程阻塞拷贝需要一定的时间。</span></p><span id="OSC_h1_10"></span><h1>四、如何检测 BigKey</h1><span id="OSC_h3_11"></span><h3>4.1、redis-cli --bigkeys</h3><p><span style="color:#393c5a">首先我们从运行结果出发。首先通过脚本插入一些数据到 redis 中，然后执行 redis-cli 的--bigkeys 选项</span></p><pre><code>$ redis-cli --bigkeys

# Scanning the entire keyspace to find biggest keys as well as
# average sizes per key type.  You can use -i 0.01 to sleep 0.01 sec
# per SCAN command (not usually needed).
-------- 第一部分 start -------
[00.00%] Biggest string found so far 'key-419' with 3 bytes
[05.14%] Biggest list   found so far 'mylist' with 100004 items
[35.77%] Biggest string found so far 'counter:__rand_int__' with 6 bytes
[73.91%] Biggest hash   found so far 'myobject' with 3 fields

-------- 第一部分 end -------

-------- summary -------

-------- 第二部分 start -------
Sampled 506 keys in the keyspace!
Total key length in bytes is 3452 (avg len 6.82)

Biggest string found 'counter:__rand_int__' has 6 bytes
Biggest   list found 'mylist' has 100004 items
Biggest   hash found 'myobject' has 3 fields
-------- 第二部分 end -------

-------- 第三部分 start -------
504 strings with 1403 bytes (99.60% of keys, avg size 2.78)
1 lists with 100004 items (00.20% of keys, avg size 100004.00)
0 sets with 0 members (00.00% of keys, avg size 0.00)
1 hashs with 3 fields (00.20% of keys, avg size 3.00)
0 zsets with 0 members (00.00% of keys, avg size 0.00)
-------- 第三部分 end -------

</code></pre><p><span style="color:#393c5a">以下我们分三步对 bigkeys 选项源码原理进行解析，简要流程如下图：</span></p><div><img src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=NjgyZWM5MDg1MDBlNDc0MzQwZDc5NDAwMjUxYWJiMGIsMTY5ODcxODQzNDA3Nw==" referrerpolicy="no-referrer"></div><span id="OSC_h4_12"></span><h4>﻿﻿4.1.1、第一部分是如何进行找 key 的呢？</h4><p><span style="color:#393c5a">Redis 找 bigkey 的函数是 static void findBigKeys(int memkeys, unsigned memkeys_samples)，因为--memkeys 选项和--bigkeys 选项是公用同一个函数，所以使用 memkeys 时会有额外两个参数 memkeys、memkeys_sample，但这和--bigkeys 选项没关系，所以不用理会。findBigKeys 具体函数框架为：</span></p><p><span style="color:#393c5a">1.申请 6 个变量用以统计 6 种数据类型的信息（每个变量记录该数据类型的 key 的总数量、bigkey 是哪个等信息）</span></p><pre><code>typedef struct {
    char *name;//数据类型，如 string
    char *sizecmd;//查询大小命令，如 string 会调用 STRLEN
    char *sizeunit;//单位，string 类型为 bytes，而 hash 为 field
    unsigned long long biggest;//最大 key 信息域，此数据类型最大 key 的大小，如 string 类型是多少 bytes，hash 为多少 field
    unsigned long long count;//统计信息域，此数据类型的 key 的总数
    unsigned long long totalsize;//统计信息域，此数据类型的 key 的总大小，如 string 类型是全部 string 总共多少 bytes，hash 为全部 hash 总共多少 field
    sds biggest_key;//最大 key 信息域，此数据类型最大 key 的键名，之所以在数据结构末尾是考虑字节对齐
} typeinfo;

    dict *types_dict = dictCreate(&amp;typeinfoDictType);
    typeinfo_add(types_dict, "string", &amp;type_string);
    typeinfo_add(types_dict, "list", &amp;type_list);
    typeinfo_add(types_dict, "set", &amp;type_set);
    typeinfo_add(types_dict, "hash", &amp;type_hash);
    typeinfo_add(types_dict, "zset", &amp;type_zset);
    typeinfo_add(types_dict, "stream", &amp;type_stream);

</code></pre><p><span style="color:#393c5a">2.调用 scan 命令迭代地获取一批 key（注意只是 key 的名称，类型和大小 scan 命令不返回）</span></p><pre><code>/* scan 循环扫描 */
do {
    /* 计算完成的百分比情况 */
    pct = 100 * (double)sampled/total_keys;//这里记录下扫描的进度

    /* 获取一些键并指向键数组 */
    reply = sendScan(&amp;it);//这里发送 SCAN 命令，结果保存在 reply 中
    keys  = reply-&gt;element[1];//keys 来保存这次 scan 获取的所有键名，注意只是键名，每个键的数据类型是不知道的。
    ......

} while(it != 0); 

</code></pre><p><span style="color:#393c5a">3.对每个 key 获取它的数据类型（type）和 key 的大小（size）</span></p><pre><code>/* 检索类型，然后检索大小*/
getKeyTypes(types_dict, keys, types);
getKeySizes(keys, types, sizes, memkeys, memkeys_samples);

</code></pre><p><span style="color:#393c5a">4.如果 key 的大小大于已记录的最大值的 key，则更新最大 key 的信息</span></p><pre><code>/* Now update our stats */
for(i=0;i&lt;keys-&gt;elements;i++) {
    ......//前面已解析

    //如果遍历到比记录值更大的 key 时
    if(type-&gt;biggest&lt;sizes[i]) {
        /* Keep track of biggest key name for this type */
        if (type-&gt;biggest_key)
            sdsfree(type-&gt;biggest_key);
        //更新最大 key 的键名
        type-&gt;biggest_key = sdscatrepr(sdsempty(), keys-&gt;element[i]-&gt;str, keys-&gt;element[i]-&gt;len);
        if(!type-&gt;biggest_key) {
            fprintf(stderr, "Failed to allocate memory for key!\n");
            exit(1);
        }

        //每当找到一个更大的 key 时则输出该 key 信息
        printf(
            "[%05.2f%%] Biggest %-6s found so far '%s' with %llu %s\n",
            pct, type-&gt;name, type-&gt;biggest_key, sizes[i],
            !memkeys? type-&gt;sizeunit: "bytes");

        /* Keep track of the biggest size for this type */
        //更新最大 key 的大小
        type-&gt;biggest = sizes[i];
    }

    ......//前面已解析
}

</code></pre><p><span style="color:#393c5a">5.对每个 key 更新对应数据类型的统计信息</span></p><pre><code>/* 现在更新统计数据 */
for(i=0;i&lt;keys-&gt;elements;i++) {
    typeinfo *type = types[i];
    /* 跳过在 SCAN 和 TYPE 之间消失的键 */
    if(!type)
        continue;

    //对每个 key 更新每种数据类型的统计信息
    type-&gt;totalsize += sizes[i];//某数据类型（如 string）的总大小增加
    type-&gt;count++;//某数据类型的 key 数量增加
    totlen += keys-&gt;element[i]-&gt;len;//totlen 不针对某个具体数据类型，将所有 key 的键名的长度进行统计，注意只统计键名长度。
    sampled++;//已经遍历的 key 数量

    ......//后续解析

    /* 更新整体进度 */
    if(sampled % 1000000 == 0) {
        printf("[%05.2f%%] Sampled %llu keys so far\n", pct, sampled);
    }
}

</code></pre><span id="OSC_h4_13"></span><h4>4.1.2、第二部分是如何执行的？</h4><p><span style="color:#393c5a">1.输出统计信息、最大 key 信息</span></p><pre><code>   /* We're done */
    printf("\n-------- summary -------\n\n");
    if (force_cancel_loop) printf("[%05.2f%%] ", pct);
    printf("Sampled %llu keys in the keyspace!\n", sampled);
    printf("Total key length in bytes is %llu (avg len %.2f)\n\n",
       totlen, totlen ? (double)totlen/sampled : 0);

</code></pre><p><span style="color:#393c5a">2.首先输出总共扫描了多少个 key、所有 key 的总长度是多少。</span></p><pre><code>/* Output the biggest keys we found, for types we did find */
    di = dictGetIterator(types_dict);
    while ((de = dictNext(di))) {
        typeinfo *type = dictGetVal(de);
        if(type-&gt;biggest_key) {
            printf("Biggest %6s found '%s' has %llu %s\n", type-&gt;name, type-&gt;biggest_key,
               type-&gt;biggest, !memkeys? type-&gt;sizeunit: "bytes");
        }
    }
    dictReleaseIterator(di);

</code></pre><span id="OSC_h4_14"></span><h4>4.1.3、第三部分是如何执行的？</h4><p><span style="color:#393c5a">di 为字典迭代器，用以遍历 types_dict 里面的所有 dictEntry。de = dictNext(di) 则可以获取下一个 dictEntry，de 是指向 dictEntry 的指针。又因为 typeinfo 结构体保存在 dictEntry 的 v 域中，所以用 dictGetVal 获取。然后就是输出 typeinfo 结构体里面保存的最大 key 相关的数据，包括最大 key 的键名和大小。</span></p><pre><code>  di = dictGetIterator(types_dict);
    while ((de = dictNext(di))) {
        typeinfo *type = dictGetVal(de);
        printf("%llu %ss with %llu %s (%05.2f%% of keys, avg size %.2f)\n",
           type-&gt;count, type-&gt;name, type-&gt;totalsize, !memkeys? type-&gt;sizeunit: "bytes",
           sampled ? 100 * (double)type-&gt;count/sampled : 0,
           type-&gt;count ? (double)type-&gt;totalsize/type-&gt;count : 0);
    }
    dictReleaseIterator(di);

</code></pre><span id="OSC_h3_15"></span><h3>4.2、使用开源工具发现大 Key</h3><p><span style="color:#393c5a">在不影响线上服务的同时得到精确的分析报告。使用 redis-rdb-tools 工具以定制化方式找出大 Key，该工具能够对 Redis 的 RDB 文件进行定制化的分析，但由于分析 RDB 文件为离线工作，因此对线上服务不会有任何影响，这是它的最大优点但同时也是它的最大缺点：离线分析代表着分析结果的较差时效性。对于一个较大的 RDB 文件，它的分析可能会持续很久很久。</span></p><p><span style="color:#393c5a">redis-rdb-tools 的项目地址为：https://github.com/sripathikrishnan/redis-rdb-tools﻿</span></p><span id="OSC_h1_16"></span><h1>五、如何解决 Bigkey</h1><span id="OSC_h3_17"></span><h3>5.1、提前预防</h3><ul><li><span style="color:#333333">设置过期时间，尽量过期时间分散，防止同一时间过期；</span></li><li><span style="color:#333333">存储为 String 类型的 JSON，可以删除不使用的 Filed；</span></li></ul><p><span style="color:#393c5a">例如对象为</span><span style="color:#2ea121">{"userName":"京东到家","ciyt":"北京"}</span>，如果只需要用到 userName 属性，那就定义新对象，只具有 userName 属性，精简缓存中数据</p><ul><li><span style="color:#333333">存储为 String 类型的 JSON，利用@JsonProperty 注解让 FiledName 字符集缩小，代码例子如下。但是存在缓存数据识别性低的缺点；</span></li></ul><pre><code>import org.codehaus.jackson.annotate.JsonProperty;
import org.codehaus.jackson.map.ObjectMapper;
import java.io.IOException;
public class JsonTest {
    @JsonProperty("u")
    private String userName;

    public String getUserName() {
        return userName;
    }
    public void setUserName(String userName) {
        this.userName = userName;
    }
    public static void main(String[] args) throws IOException {
        JsonTest output = new JsonTest();
        output.setUserName("京东到家");
        System.out.println(new ObjectMapper().writeValueAsString(output));

        String json = "{\"u\":\"京东到家\"}";
        JsonTest r1 = new ObjectMapper().readValue(json, JsonTest.class);
        System.out.println(r1.getUserName());
    }
}

{"u":"京东到家"}
京东到家

</code></pre><ul><li><span style="color:#333333">采用压缩算法，利用时间换空间，进行序列化与反序列化。同时也存在缓存数据识别性低的缺点；</span></li><li><span style="color:#333333">在业务上进行干预，设置阈值。比如用户购物车的商品数量，或者领券的数量，不能无限的增大；</span></li></ul><span id="OSC_h3_18"></span><h3>5.2、如何优雅删除 BigKey</h3><span id="OSC_h4_19"></span><h4>5.2.1、DEL</h4><p><span style="color:#393c5a">此命令在 Redis 不同版本中删除的机制并不相同，以下分别进行分析：</span></p><p><strong><span style="color:#393c5a">redis_version &lt; 4.0 版本</span></strong><span style="color:#393c5a">：在主线程中同步删除，删除大 Key 会阻塞主线程，见如下源码基于 redis 3.0 版本。那针对非 String 结构数据，可以先通过 SCAN 命令读取部分数据，然后逐步进行删除，避免一次性删除大 key 导致 Redis 阻塞。</span></p><pre><code>// 从数据库中删除给定的键，键的值，以及键的过期时间。
// 删除成功返回 1，因为键不存在而导致删除失败时，返回 0 
int dbDelete(redisDb *db, robj *key) {
    // 删除键的过期时间
    if (dictSize(db-&gt;expires) &gt; 0) dictDelete(db-&gt;expires,key-&gt;ptr);

    // 删除键值对
    if (dictDelete(db-&gt;dict,key-&gt;ptr) == DICT_OK) {
        // 如果开启了集群模式，那么从槽中删除给定的键
        if (server.cluster_enabled) slotToKeyDel(key);
        return 1;
    } else {
        // 键不存在
        return 0;
    }
}

</code></pre><p><strong><span style="color:#393c5a">4.0 版本 &lt; redis_version &lt; 6.0 版本</span></strong><span style="color:#393c5a">：引入 lazy-free</span><strong><span style="color:#393c5a">，</span></strong><span style="color:#393c5a">手动开启 lazy-free 时，有 4 个选项可以控制，分别对应不同场景下，是否开启异步释放内存机制：</span></p><ul><li><span style="color:#333333">lazyfree-lazy-expire：key 在过期删除时尝试异步释放内存</span></li><li><span style="color:#333333">lazyfree-lazy-eviction：内存达到 maxmemory 并设置了淘汰策略时尝试异步释放内存</span></li><li><span style="color:#333333">lazyfree-lazy-server-del：执行 RENAME/MOVE 等命令或需要覆盖一个 key 时，删除旧 key 尝试异步释放内存</span></li><li><span style="color:#333333">replica-lazy-flush：主从全量同步，从库清空数据库时异步释放内存</span></li></ul><p><span style="color:#4a4a4a">开启 lazy-free 后，Redis 在释放一个 key 的内存时，首先会评估代价，如果释放内存的代价很小，那么就直接在主线程中操作了，没必要放到异步线程中执行</span></p><p><strong><span style="color:#393c5a">redis_version &gt;= 6.0 版本</span></strong><span style="color:#393c5a">：引入 lazyfree-lazy-user-del</span><span style="color:#4a4a4a">，只要开启了，del 直接可以异步删除 key，不会阻塞主线程。具体是为什么呢，现在先卖个关子，在下面进行解析。</span></p><span id="OSC_h4_20"></span><h4>5.2.2、SCAN</h4><p><span style="color:#393c5a">SCAN 命令可以帮助在不阻塞主线程的情况下逐步遍历大量的键，以及避免对数据库的阻塞。以下代码是利用 scan 来扫描集群中的 Key。</span></p><pre><code>public void scanRedis(String cursor,String endCursor) {
        ReloadableJimClientFactory factory = new ReloadableJimClientFactory();
        String jimUrl = "jim://xxx/546";
        factory.setJimUrl(jimUrl);
        Cluster client = factory.getClient();
        ScanOptions.ScanOptionsBuilder scanOptions = ScanOptions.scanOptions();
        scanOptions.count(100);
 
        Boolean end = false;
        int k = 0;
        while (!end) {
            KeyScanResult&lt; String &gt; result = client.scan(cursor, scanOptions.build());
            for (String key :result.getResult()){
                if (client.ttl(key) == -1){
                    logger.info("永久 key 为:{}" , key);
                }
            }
            k++;
            cursor = result.getCursor();
            if (endCursor.equals(cursor)){
                break;
            }
        }
    }

</code></pre><span id="OSC_h4_21"></span><h4>5.2.3、UNLINK</h4><p><span style="color:#393c5a">Redis 4.0 提供了 lazy delete (unlink 命令) ，下面基于源码（redis_version:7.2 版本）分析下实现原理</span></p><ul><li><span style="color:#333333">del 与 unlink 命令底层都调用了 delGenericCommand() 方法；</span></li></ul><pre><code>void delCommand(client *c) {
    delGenericCommand(c,server.lazyfree_lazy_user_del);
}
void unlinkCommand(client *c) {
    delGenericCommand(c,1);
}

</code></pre><ul><li><span style="color:#333333">lazyfree-lazy-user-del 支持 yes 或者 no。默认是 no；</span></li><li><span style="color:#333333">如果设置为 yes，那么 del 命令就等价于 unlink，也是异步删除，这也同时解释了之前咱们的问题，为什么设置了 lazyfree-lazy-user-del 后，del 命令就为异步删除。</span></li></ul><pre><code>void delGenericCommand(client *c, int lazy) {
    int numdel = 0, j;
    // 遍历所有输入键
    for (j = 1; j &lt; c-&gt;argc; j++) {
        // 先删除过期的键
        expireIfNeeded(c-&gt;db,c-&gt;argv[j],0);
        int deleted  = lazy ? dbAsyncDelete(c-&gt;db,c-&gt;argv[j]) :
                              dbSyncDelete(c-&gt;db,c-&gt;argv[j]);
        // 尝试删除键
        if (deleted) {
            // 删除键成功，发送通知
            signalModifiedKey(c,c-&gt;db,c-&gt;argv[j]);
            notifyKeyspaceEvent(NOTIFY_GENERIC,"del",c-&gt;argv[j],c-&gt;db-&gt;id);
            server.dirty++;
            // 成功删除才增加 deleted 计数器的值
            numdel++;
        }
    }
    // 返回被删除键的数量
    addReplyLongLong(c,numdel);
}

</code></pre><p><span style="color:#393c5a">下面分析异步删除 dbAsyncDelete() 与同步删除 dbSyncDelete()，底层同时也是调用 dbGenericDelete() 方法</span></p><pre><code>int dbSyncDelete(redisDb *db, robj *key) {
    return dbGenericDelete(db, key, 0, DB_FLAG_KEY_DELETED);
}

int dbAsyncDelete(redisDb *db, robj *key) {
    return dbGenericDelete(db, key, 1, DB_FLAG_KEY_DELETED);
}

int dbGenericDelete(redisDb *db, robj *key, int async, int flags) {
    dictEntry **plink;
    int table;
    dictEntry *de = dictTwoPhaseUnlinkFind(db-&gt;dict,key-&gt;ptr,&amp;plink,&amp;table);
    if (de) {
        robj *val = dictGetVal(de);
        /* RM_StringDMA may call dbUnshareStringValue which may free val, so we need to incr to retain val */
        incrRefCount(val);
        /* Tells the module that the key has been unlinked from the database. */
        moduleNotifyKeyUnlink(key,val,db-&gt;id,flags);
        /* We want to try to unblock any module clients or clients using a blocking XREADGROUP */
        signalDeletedKeyAsReady(db,key,val-&gt;type);
        // 在调用用 freeObjAsync 之前，我们应该先调用 decrRefCount。否则，引用计数可能大于 1，导致 freeObjAsync 无法正常工作。
        decrRefCount(val);
        // 如果是异步删除，则会调用 freeObjAsync 异步释放 value 占用的内存。同时，将 key 对应的 value 设置为 NULL。
        if (async) {
            /* Because of dbUnshareStringValue, the val in de may change. */
            freeObjAsync(key, dictGetVal(de), db-&gt;id);
            dictSetVal(db-&gt;dict, de, NULL);
        }
        // 如果是集群模式，还会更新对应 slot 的相关信息
        if (server.cluster_enabled) slotToKeyDelEntry(de, db);

        /* Deleting an entry from the expires dict will not free the sds of the key, because it is shared with the main dictionary. */
        if (dictSize(db-&gt;expires) &gt; 0) dictDelete(db-&gt;expires,key-&gt;ptr);
        // 释放内存
        dictTwoPhaseUnlinkFree(db-&gt;dict,de,plink,table);
        return 1;
    } else {
        return 0;
    }
}

</code></pre><p><span style="color:#393c5a">如果为异步删除，调用 freeObjAsync() 方法，根据以下代码分析：</span></p><pre><code>#define LAZYFREE_THRESHOLD 64

/* Free an object, if the object is huge enough, free it in async way. */
void freeObjAsync(robj *key, robj *obj, int dbid) {
    size_t free_effort = lazyfreeGetFreeEffort(key,obj,dbid);
    if (free_effort &gt; LAZYFREE_THRESHOLD &amp;&amp; obj-&gt;refcount == 1) {
        atomicIncr(lazyfree_objects,1);
        bioCreateLazyFreeJob(lazyfreeFreeObject,1,obj);
    } else {
        decrRefCount(obj);
    }
}

size_t lazyfreeGetFreeEffort(robj *key, robj *obj, int dbid) {
    if (obj-&gt;type == OBJ_LIST &amp;&amp; obj-&gt;encoding == OBJ_ENCODING_QUICKLIST) {
        quicklist *ql = obj-&gt;ptr;
        return ql-&gt;len;
    } else if (obj-&gt;type == OBJ_SET &amp;&amp; obj-&gt;encoding == OBJ_ENCODING_HT) {
        dict *ht = obj-&gt;ptr;
        return dictSize(ht);
    } else if (obj-&gt;type == OBJ_ZSET &amp;&amp; obj-&gt;encoding == OBJ_ENCODING_SKIPLIST){
        zset *zs = obj-&gt;ptr;
        return zs-&gt;zsl-&gt;length;
    } else if (obj-&gt;type == OBJ_HASH &amp;&amp; obj-&gt;encoding == OBJ_ENCODING_HT) {
        dict *ht = obj-&gt;ptr;
        return dictSize(ht);
    } else if (obj-&gt;type == OBJ_STREAM) {
        ...
        return effort;
    } else if (obj-&gt;type == OBJ_MODULE) {
        size_t effort = moduleGetFreeEffort(key, obj, dbid);
        /* If the module's free_effort returns 0, we will use asynchronous free
         * memory by default. */
        return effort == 0 ? ULONG_MAX : effort;
    } else {
        return 1; /* Everything else is a single allocation. */
    }
}

</code></pre><p><span style="color:#393c5a">分析后咱们可以得出如下结论：</span></p><ul><li><span style="color:#333333">当 Hash/Set 底层采用哈希表存储（非 ziplist/int 编码存储）时，并且元素数量超过 64 个</span></li><li><span style="color:#333333">当 ZSet 底层采用跳表存储（非 ziplist 编码存储）时，并且元素数量超过 64 个</span></li><li><span style="color:#333333">当 List 链表节点数量超过 64 个（注意，不是元素数量，而是链表节点的数量，List 的实现是在每个节点包含了若干个元素的数据，这些元素采用 ziplist 存储）</span></li><li><span style="color:#333333">refcount == 1 就是在没有引用这个 Key 时</span></li></ul><p><span style="color:#393c5a">只有以上这些情况，在删除 key 释放内存时，才会真正放到异步线程中执行，其他情况一律还是在主线程操作。也就是说 String（不管内存占用多大）、List（少量元素）、Set（int 编码存储）、Hash/ZSet（ziplist 编码存储）这些情况下的 key 在释放内存时，依旧在主线程中操作。</span></p><span id="OSC_h3_22"></span><h3>5.3、分而治之</h3><p><span style="color:#393c5a">采用经典算法「分治法」，将大而化小。针对 String 和集合类型的 Key，可以采用如下方式：</span></p><ul><li><span style="color:#333333">String 类型的大 Key：可以尝试将对象分拆成几个 Key-Value， 使用 MGET 或者多个 GET 组成的 pipeline 获取值，分拆单次操作的压力，对于集群来说可以将操作压力平摊到多个分片上，降低对单个分片的影响。</span></li><li><span style="color:#333333">集合类型的大 Key，并且需要整存整取要在设计上严格禁止这种场景的出现，如无法拆分，有效的方法是将该大 Key 从 JIMDB 去除，单独放到其他存储介质上。</span></li><li><span style="color:#333333">集合类型的大 Key，每次只需操作部分元素：将集合类型中的元素分拆。以 Hash 类型为例，可以在客户端定义一个分拆 Key 的数量 N，每次对 HGET 和 HSET 操作的 field 计算哈希值并取模 N，确定该 field 落在哪个 Key 上。</span></li></ul><p><span style="color:#393c5a">如果线上服务强依赖 Redis，需要考虑到如何做到「无感」，并保证数据一致性。咱们基本上可以采用三步走策略，如下图所示。分别是进行双写，双读校验，最后读新 Key。在此基础上可以设置开关，做到上线后的平稳迁移。</span></p><div><img src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ZGE3YWQ4ZDZiZDE4ZjM5MzU1YTRiMTExOWZjY2UxYmMsMTY5ODcxODQzNDA3Nw==" referrerpolicy="no-referrer"></div><span id="OSC_h1_23"></span><h1>﻿﻿六、总结</h1><p><span style="color:#393c5a">综上所述，针对文章开头咱们购物车大 Key 问题，相信你已经有了答案。咱们可以限制门店数，限制门店中的商品数。如果不作限制，咱们也能进行拆分，将大 Key 分散存储。例如。将 Redis 中 Key 类型改为 List，key 为用户与门店唯一键，Value 为用户在此门店下的商品。</span></p><pre><code>存储结构拆分成两种：
第一种：
    userPin：storeId 的集合
第二种：
    userPin_storeId1:{门店下加车的所有商品基本信息}；
    userPin_storeId2:{门店下加车的所有商品基本信息}     

</code></pre><p><span style="color:#393c5a">以上介绍了大 key 的产生、识别、处理，以及如何使用合理策略和技术来应对。在使用 Redis 过程中，防范大于治理，在治理过程中也要做到业务无感。</span></p><span id="OSC_h1_24"></span><h1>七、参考</h1><p><span style="color:#393c5a">﻿https://github.com/redis/redis.git﻿</span></p><p><span style="color:#393c5a">﻿http://redisbook.com/﻿</span></p><p><span style="color:#393c5a">﻿https://github.com/huangz1990/redis-3.0-annotated.git﻿</span></p><p><span style="color:#393c5a">﻿https://blog.csdn.net/ldw201510803006/article/details/124790121﻿</span></p><p><span style="color:#393c5a">﻿https://blog.csdn.net/kuangd_1992/article/details/130451679﻿</span></p><p><span style="color:#393c5a">﻿http://sd.jd.com/article/4930?shareId=119428&amp;isHideShareButton=1﻿</span></p><p><span style="color:#393c5a">﻿https://www.liujiajia.me/2023/3/28/redis-bigkeys﻿</span></p><p><span style="color:#393c5a">﻿https://www.51cto.com/article/701990.html﻿</span></p><p><span style="color:#393c5a">﻿https://help.aliyun.com/document_detail/353223.html﻿</span></p><p><span style="color:#393c5a">﻿https://juejin.cn/post/7167015025154981895﻿</span></p><p><span style="color:#393c5a">﻿https://www.jianshu.com/p/9e150d72ffc9﻿</span></p><p><span style="color:#393c5a">﻿https://zhuanlan.zhihu.com/p/449648332</span></p><blockquote><p>作者：京东零售&nbsp;高凯</p><p>来源：京东云开发者社区，转载请注明来源</p></blockquote><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 02:21:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/10139889</guid>
            <link>https://my.oschina.net/u/4090830/blog/10139889</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[基于模式挖掘的可靠性治理探索与实践]]>
            </title>
            <description>
                <![CDATA[<div class="content"><blockquote><p>本文整理自美团技术沙龙第 77 期《美团亿级流量系统的质量风险防控和稳定性治理实践》。本文介绍了基于模式挖掘的可靠性治理探索，为通过技术手段解决该领域代表性问题开启了新的思路。文章第一部分介绍可靠性治理的痛点；第二部分引入模式的概念；第三部分讨论新基建下的新尝试；第四部分分享三个典型的实践案例。</p></blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-df29f8b032fe5944a8d7b649041f72acc43.jpg" alt="" referrerpolicy="no-referrer"></p><h2>1 可靠性治理的痛点</h2><p>对于亿级流量的线上系统来说，可靠性是至关重要的。从字面上理解，可靠性要求故障少、可信赖。与安全性一样，它们都是信息系统的固有属性之一，也是保障产品质量的关键因素。</p><p>对照 Google 的可靠性模型来看，测试同学会投入很多精力在用例设计、测试执行、持续交付等环节上，研发同学则会更多关注监控、应急和故障分析等。但往往由于项目进度和人力因素，在设计和编码阶段对可靠性的投入和关注不足，导致后续需要付出更高的成本发现和解决潜在隐患。有鉴于此，我们希望能找到更低成本且以更有效的方式发现和治理这些隐患，从而提升系统整体的可靠性。</p><p><img src="https://p0.meituan.net/travelcube/d94c2805fc7216f7058b60b25dee9c2d521185.png" alt="" referrerpolicy="no-referrer"></p><p>在研发设计阶段，我们需要关注系统弹性，考虑潜在故障风险、适应流量变化等，其中相关治理涉及幂等性、健壮性、一致性、超时、限流、熔断等场景。与一般功能测试相比，可靠性治理需要面对不同的服务和系统，发现并治理技术问题，在模糊度上有较大的提升和挑战。就目前而言，质量问题非常明确，但潜在风险策略和解决路径比较模糊。因此，我们希望能找到办法识别并解决这些问题。</p><p><img src="https://p0.meituan.net/travelcube/9fde4b0cae296d02895a1d03351043e3159517.png" alt="" referrerpolicy="no-referrer"></p><p>模糊度的提升会带来两种最常见的现象：</p><ul><li>一种是过于具体，Case by Case 解决问题，类似算法的过拟合，过拟合的问题在于对更广泛范围内的问题缺乏有效性。以幂等性为例，想验证一个接口是否幂等可以很快完成并很快补充接口幂等相关的测试用例，但是对不同的接口、服务、系统以及不同的幂等性设计，还有哪些问题和风险，我们没有办法关注到并控制这些风险。</li><li>另一种是过于泛化，类似算法的欠拟合，欠拟合的问题在于过度虚化导致没有抓住问题的共性特征。以主从延迟为例，主从延迟会给系统带来一致性风险，需要针对性做保护，并进行相关验证，因此我们可以制定规范、梳理 Check List 和测试模板，虽然这样可以最大程度在产研各环节提醒大家关注到这类问题，但并没有找到彻底解决问题的方法。</li></ul><p><img src="https://p0.meituan.net/travelcube/fa0db198a1641d7c6fff31be835b8753341300.png" alt="" referrerpolicy="no-referrer"></p><h2>2 模式的定义</h2><p>类似这些问题如何找到更好的解决办法？我们重点看一下模式对可靠性治理的启发。模式在维基百科的定义是：揭示了这个世界上人工设计和抽象思想中的规律。</p><p>例如下图所示，计算机图形学中的经典分形图案柯赫雪花，是 1904 年瑞典数学家科赫提出。可以看到它有明显的规律，这样的分形规律在自然界无处不在。</p><p><img src="https://p0.meituan.net/travelcube/650b75a49ab89c3f5c5204c5f712e0f6435079.png" alt="" referrerpolicy="no-referrer"></p><p>技术场景的模式会更加丰富些，这类模式和可靠性治理想找到的模式非常接近。</p><p>举例缓存设计的两种常见模式：</p><ul><li>第一种是 Cache-Aside（旁路缓存），也是使用比较广泛的一种方式，它只有在缓存没有命中时，才会查询数据库并更新缓存。</li><li>另外一种是 Write-throught（只写模式），这种模式在每次数据库变更时都会同步更新缓存。</li></ul><p>对比第一种模式，第二种模式的优点是逻辑更清晰、操作简单、缓存命中率更高；缺点是不常请求的数据会被写到缓存中，导致缓存更大。</p><p><img src="https://p1.meituan.net/travelcube/fe10478804776789b284dc94e48f97e1215075.png" alt="" referrerpolicy="no-referrer"></p><p>那么，我们如何找到这些潜在模式并应用到可靠性治理呢？我们现有的业务测试数据、专业知识积累、相关问题分析和覆盘经验，都可以帮助我们找到治理这些通用技术场景的规律。在这里，很重要的一部分是真实的业务数据，我们可以从最基础的数据提取信息，找到解决共性问题的思路。</p><h2>3 大数据下的尝试</h2><p>随着 JVM 非侵入式 AOP 解决方案的成熟，现在我们已经可以采集任意环境下的任意协议流量，以及这些流量依赖的数据关系，也可以在测试环境回放这些流量，包括线上真实采集的流量。这里依赖两个关键点：一是流量采集，这里涉及很多技术方案，这里分享主要是作为一个基础设施；二是有了全链路 Mock 能力，我们才能在测试环境进行各种流量的回放和验证。</p><p><img src="https://p0.meituan.net/travelcube/3df2f4a8531888d62d0b81cc965ebdba289384.png" alt="" referrerpolicy="no-referrer"></p><p>另一个重要基础设施是环境隔离技术，经过快速发展，它已经具备了泳道级别的数据复制隔离，也支持一站式数据消息和部署环境的即拿即用。从最开始通过泳道降低人工测试相互影响，到现在一站式拉出一套环境，能支持各类专项检查独立运行，使用线下数据且不污染主干环境。</p><p><img src="https://p0.meituan.net/travelcube/2fb9dcf93574ab8e46e16237551501ad496283.png" alt="" referrerpolicy="no-referrer"></p><p>基于流量采集和环境隔离这两个能力的成熟，给自动化领域带来了很多新可能。流量采集信息包含了请求参数、返回值和调用链路等信息；环境隔离技术支持数据隔离、消息隔离、各种协议以及部署版本隔离。</p><p>在这种情况下，海量的业务流量可以直接转化成基于规则验证的接口自动化用例，也可以应用到基于业务模型的场景级用例，模式在这里更像是两者之间的「折中」，我们希望通过这种「折中」来解决可靠性治理的难题。</p><h2>4 典型实践分享</h2><p>接下来，我们重点介绍基于模式的三个可靠性治理的典型实践，主要包括幂等性治理、依赖治理和越权治理三个方向。</p><h3>4.1 幂等性治理</h3><p>维基百科中，幂等的定义是数学和计算机科学中某些运算的性质，可以被多次应用，而不会改变最初应用之外的结果。在数学中也有相关的定义，就以一元运算为例，当 f(f(x))=f(x) 时，可以认为这个运算符 f 是幂等的；在计算机科学领域，HTTP 规范中也有对幂等的定义，即多个相同请求的副作用与单个请求相同，例如 GET、PUT 和 DELETE 是幂等的。</p><p><img src="https://p1.meituan.net/travelcube/902fbbee65d75ac506adbbf9db8ebb28204176.png" alt="" referrerpolicy="no-referrer"></p><p>在大部分分布式系统中，请求超时、网络抖动等在线上环境中随时可能发生。幂等性设计是保证服务在高并发情况下高可用的关键。对于每天产生海量订单的线上业务，比如库存、交易、支付和财务等系统都需要通过幂等性避免超卖、重复支付、重复打款等问题的发生；同时幂等性也是消息队列、定时任务、分布式事务等技术类场景稳定运行的基础。</p><p>如下图举例，当一次调用部分成功的情况下，系统会触发重试，而幂等性可以保证在重试时，成功部分不再被重复执行。</p><p><img src="https://p1.meituan.net/travelcube/22c38c3a5234bf04d0059508d0278c17289045.png" alt="" referrerpolicy="no-referrer"></p><p>我们要挖掘通用模式，就需要分析幂等性所有可能的实现方案。</p><p>如下图是几种常见的幂等性实现方案：数据库层面的唯一索引；通过版本或其他状态、条件来限制操作执行的悲观锁和乐观锁；通过具体业务属性参数，构造具有唯一性的 Token 以及分布式系统中广泛使用的分布式锁等。</p><p><img src="https://p0.meituan.net/travelcube/20c2de394399fddbb90f8348429f4020332786.png" alt="" referrerpolicy="no-referrer"></p><p>尽管幂等性的实现方案有多种，但回到幂等性的本质，我们希望多次调用不会产生新的副作用，而系统中副作用的产生往往是通过「写」操作发生。</p><p>分析调用链路发现，当链路上某个节点不幂等而对资源产生副作用后，其后的多个节点都可以检测到相关变化。例如，前序节点通过数据库的写入生成了新的单号，后序节点的参数和返回值很可能会出现这个新单号。这样，我们就可以构造多次同样的请求，之后检查链路上的这些变化来验证幂等性。</p><p><img src="https://p0.meituan.net/travelcube/feded9ba45e7cc6113b8f3c123d5d707559000.png" alt="" referrerpolicy="no-referrer"></p><p>调用链路节点的类型包括了 MYBATIS、RPC、HTTP、MAFKA、CRANE 等，不同幂等性方案在不同类型的节点上有相应的表现，例如唯一索引，更多在 MYBATIS 节点上，不同类型节点的检查策略和优先级也不同。</p><p><img src="https://p0.meituan.net/travelcube/076a87994f48ccad52ebee631b292ba2547874.png" alt="" referrerpolicy="no-referrer"></p><p>如下图，列举了部分节点检查策略和降噪策略。以 MYBATIS 为例，我们会关注到写 SQL 的内容和返回结果，结合索引冲突、锁失败等节点的异常返回进行降噪。比如 THRIFT 节点，我们会关注接口的参数和返回值变化，考虑随机 ID 的生成、时间戳字段等进行相关降噪，最终针对不同幂等性方案和不同节点类型形成通用整体策略。</p><p><img src="https://p0.meituan.net/travelcube/8c2706b169067b4e53e9fa1d6b362cd0229473.png" alt="" referrerpolicy="no-referrer"></p><p>基于通用检查能力，我们可以应用在场景用例编写、流量用例生成和离线流量的自动分析上，通过分析每天线上、线下环境产生的真实流量，我们可以对增量和存量问题进行差异化治理和跟进。</p><p><img src="https://p0.meituan.net/travelcube/2bb82708ca65eccf9942c5515998e20b151408.png" alt="" referrerpolicy="no-referrer"></p><h3>4.2 依赖治理</h3><p>随着微服务的发展，我们的系统变得越来越复杂，调用链路越来越长，例如单接口的下游依赖多达上百个，任何外部依赖的抖动都会成为核心业务的威胁，很多时候系统内部或外部的一些错误被激活，没有得到正确处理，就会在服务内部不断传播，导致系统偏离正确的服务状态，造成服务失败，最终导致业务失败，引起用户可以感知的故障。</p><p><img src="https://p0.meituan.net/travelcube/7aa84a6c96587c952494f351b41bd834559746.png" alt="" referrerpolicy="no-referrer"></p><p>在业务上可以通过依赖分级和熔断策略，保障弱依赖发生故障时，核心流程依然可用。因此我们需要进行依赖治理，而依赖的治理关键在于如何自动化完成分级合理性以及熔断策略有效性的验证。</p><p>类似前面，我们会采用回放业务流量的方式，但基于依赖治理，我们的策略是修改依赖的 Mock 结构，构造依赖故障场景，进行相关验证。</p><p>我们的预期是如果命中了弱依赖，我们期望业务主流程不被阻塞，调用链路也没有阻塞，日志打印和返回信息都符合预期，没有异常表现；如果命中强依赖，验证策略则相反。</p><p><img src="https://p0.meituan.net/travelcube/ea65f83998fd281afb61f0210ffc3115679867.png" alt="" referrerpolicy="no-referrer"></p><p>具体的策略是我们依据接口和依赖关系构造指定依赖故障场景，注入异常后，分析这个异常是否被捕获。如果直接抛到了外层或者接口返回值有相关的异常信息，那当前是强依赖；如果注入依赖后，后续的调用链路被阻断，认为当前依赖是强依赖。反之，则是弱依赖。</p><p>具备这样闭环依赖分级识别以及熔断有效性的治理能力后，我们就可以周期性地对核心服务进行下游依赖等级治理和对熔断策略有效性进行自动验证。</p><p>运营内容主要包括两方面：配置检查和业务验证。</p><p><img src="https://p1.meituan.net/travelcube/21cacef713e1c515e75ad0be73c48182459214.pngv" alt="" referrerpolicy="no-referrer"></p><ul><li>对于依赖等级正确与否的检查，每周运行，发现依赖等级与熔断策略不相符的情况，推动治理。</li><li>对于业务验证，每天运行，持续产生每天增量报告，针对强依赖业务未被阻断、弱依赖业务未被处理，对应的异常等问题推动修改。</li></ul><p><img src="https://p0.meituan.net/travelcube/46852b87e0993671f0f3dd3c788ce537337387.png" alt="" referrerpolicy="no-referrer"></p><h3>4.3 越权治理</h3><p>越权访问是 Web 应用程序中一种常见的漏洞，它存在范围广、危害大，被 OWASP 应用程序安全项目列为 Web 应用十大安全隐患第二名。对于这种商户、用户规模大，交易频繁的线上业务来说，更是存在比较大的安全和合规风险。</p><p>越权就是两个同等级用户，一个可以操作另一个数据；垂直越权则涉及到不同等级用户，例如普通用户可以操作管理员才有的权限数据。</p><p><img src="https://p0.meituan.net/travelcube/bd1d73c092a90fd955b014315529d5fb435342.png" alt="" referrerpolicy="no-referrer"></p><p>典型的有越权处理接口在收到请求后经历以下三个阶段：</p><ul><li>第一步是身份认证，让系统明确当前登录的用户是谁，是后续进行鉴权的基础条件，每家公司和业务可能会有多套鉴权系统。</li><li>第二步是系统决策判断，基于当前登录用户信息，根据身份权限判断是否可以继续操作。</li><li>第三步是数据权限验证，判断当前数据是否是该用户所属，即数据归属判断。</li></ul><p>当系统未做角色判断时，容易发生垂直越权问题；当系统未做数据归属判断时，容易发生水平越权问题。</p><p><img src="https://p0.meituan.net/travelcube/445e4b25c3642b3285bc4c6610720183385165.png" alt="" referrerpolicy="no-referrer"></p><p>我们可以通过回放业务流量构造对应场景，验证接口是否有做权限控制。</p><p>第一次回放，会结合识别到当前流量鉴权方式，构造一个无权限账户进行回放，其余的依赖数据保持不变；第二次回放与第一次类似，只不过需要构造一个有权限账户信息进行回放；比对两次回放结果以及调用链路，验证这个接口是否有相关的鉴权逻辑；再结合调用链路对比以及原始流量的调用链路，比较有效地识别当前的鉴权场景，兼容一些场景通过返回值没有办法完全识别到是否有做鉴权的情况。</p><p><img src="https://p1.meituan.net/travelcube/987a046ffe53e98a2b3a372e5e9470a6616863.png" alt="" referrerpolicy="no-referrer"></p><p>在实际应用中，要考虑我们所使用的流量质量、有效性以及鉴权方式等进行筛选。目前越权检查经过优化和适配不同业务，已经可以自动化、常态化对新增流量进行检测，并将结果同步到报告中，进行日常运营，也支持问题确认、加白和工单创建等。</p><p><img src="https://p0.meituan.net/travelcube/fc1ad4d32886ca9e572b9289ec7fecaa309813.png" alt="" referrerpolicy="no-referrer"></p><p>以上三个治理能力，已经对美团优选部分核心服务默认开启，可以低成本自动化实现相关问题的常态治理及运营。目前覆盖了 500+服务、2 万+接口和 8000+下游、累计发现并治理问题有 1000+。近期已经开始陆续接入到公司内其他业务线进行应用。</p><p><img src="https://p0.meituan.net/travelcube/1ed53bd18db0f07796de4255709e5406164876.png" alt="" referrerpolicy="no-referrer"></p><p>通过以上 3 个案例，我们可以看到共性能力和解法，因此后续的规划主要是建设通用基础设施，包含线上、线下以及不同来源的流量积累、流量分析，在其上进行模式挖掘、结果跟进和运营，在这样体系基础上，不断迭代底层能力，构建更丰富的可靠性治理场景。</p><p><img src="https://p0.meituan.net/travelcube/559a1aaa265cec97745263e1d470c0ef548540.png" alt="" referrerpolicy="no-referrer"></p><h2>5 Q&amp;A</h2><p><strong>Q1：怎样预防配置异常造成的故障？</strong></p><p><strong>A</strong>：用相关事件举例，我们的一些配置平台包含一些规则，可以字符串形式或者一些类型形式存储，系统对这些配置的兼容性或表现，我们可以构造这些配置的异常场景，比如它当前是一个数值类型的配置，那我们可以构造这个配置的异常值、边界值以及空值，比如它包含分隔符的字符串，我们可以用特殊分隔符以及特殊字符，构造异常配置的获取，验证一个配置的兼容性和可靠性的规则是当读取到这样的异常配置后，我们本来能正确回放的流量，在返回值、抛出异常、日志和调用链路层面有哪些相应表现。很多线上配置变更，因为人为原因和系统默认规则没有兜住的情况下，会引入这样的异常配置健壮性验证，有比较好的保障。</p><p><strong>Q2：越权场景检查，链路对比是指走过的链路对比吗？还是每个调用点数据对比？</strong></p><p><strong>A</strong>：对于越权场景，我们可以识别到它在哪个环节进行了鉴权相关调用，不同的鉴权体系，有对应的权限相关服务和基础接口，构造有权限和没权限的场景后，它会识别没权限后的链路调用变化，比如链路节点数量以及哪个节点返回可以发现大部分问题，如果在这层没有识别到是否做鉴权，我们会关注每个节点的请求和返回，通过其他维度信息增强发现的有效性，降低噪声。</p><p><strong>Q3：怎么自动构造接口里没有权限的用户？</strong></p><p><strong>A</strong>：在原始流量里，我们可以识别到它当前的鉴权方式以及它使用了哪些鉴权服务。这样，我们可以基于这个鉴权方式和服务构造有权限或者没有权限的用户。</p><p><strong>Q4：可靠性治理系统是自研的，还是开源的，研发工作量多少人月？</strong></p><p><strong>A</strong>：可靠性系统建设的思路，目前是自研，它基于美团的基础设施和能力，展开上层解决可靠性问题的实践；研发工作量其实还好，它的点在于我们能找到哪些可以进行治理，快速迭代相关能力，并且在这一过程中不断补全新能力加进去，因此它是一个持续的过程，整体规划上，我们会考虑可靠性，从服务和代码的每一层，从机器、资源、基础组件到服务自身、上游流量和网关分层，拆分不同节点，构建不同策略，这样会有一个整体投入。</p><p><strong>Q5：流量限流和服务降级如何实现？</strong></p><p><strong>A</strong>：美团有服务限流和降级能力的基础设施 Rhino 平台，服务降级是研发根据当前依赖等级，结合具体业务分析它是否是一个可降级的依赖，再配置对应的熔断策略，当降级时，是绕过当前故障进行降级还是在故障恢复后 Fallback，这样的相关规则和策略都可以配置化，自动化验证这些策略是否生效、是否符合预期。</p><p><strong>Q6：在有了这些能力基础上，基于模式的可靠性治理用例占比多少？价值怎样评价？</strong></p><p><strong>A</strong>：我们想基于模式找到一个通用的治理策略和能力，这样的话，我们就可以将线上、线下海量流量数据都应用到这里，不需要 QA 同学投入成本，编写对应的用例，对于研发来说，我们希望直接确认和解决一些已识别到的问题，只需要花费问题确认和修复的成本。对于可识别用例占比，因为它是基于全量流量，所以随着时间的积累，历史场景以及新场景会相应覆盖到，它的用例有多少，取决于流量池以及流量质量和代表性。</p><p><strong>Q7：流量回放 Mock，使用字节码，还是沙箱模式？</strong></p><p><strong>A</strong>：这里用到美团的基础设施能力，它在采集过程中，基于字节码增强采集，回放能力也是使用到了同样的能力。</p><p><strong>|</strong> 在美团公众号菜单栏对话框回复【2022 年货】、【2021 年货】、【2020 年货】、【2019 年货】、【2018 年货】、【2017 年货】等关键词，可查看美团技术团队历年技术文章合集。</p><p><img src="https://p1.meituan.net/travelcube/b0364d579285ab22aa6235bd100d7c22178175.png" alt="" referrerpolicy="no-referrer"></p><p>| <a href="https://www.oschina.net/action/GoToLink?url=mailto%3A%E6%9C%AC%E6%96%87%E7%B3%BB%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E5%87%BA%E5%93%81%EF%BC%8C%E8%91%97%E4%BD%9C%E6%9D%83%E5%BD%92%E5%B1%9E%E7%BE%8E%E5%9B%A2%E3%80%82%E6%AC%A2%E8%BF%8E%E5%87%BA%E4%BA%8E%E5%88%86%E4%BA%AB%E5%92%8C%E4%BA%A4%E6%B5%81%E7%AD%89%E9%9D%9E%E5%95%86%E4%B8%9A%E7%9B%AE%E7%9A%84%E8%BD%AC%E8%BD%BD%E6%88%96%E4%BD%BF%E7%94%A8%E6%9C%AC%E6%96%87%E5%86%85%E5%AE%B9%EF%BC%8C%E6%95%AC%E8%AF%B7%E6%B3%A8%E6%98%8E%E2%80%9C%E5%86%85%E5%AE%B9%E8%BD%AC%E8%BD%BD%E8%87%AA%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E2%80%9D%E3%80%82%E6%9C%AC%E6%96%87%E6%9C%AA%E7%BB%8F%E8%AE%B8%E5%8F%AF%EF%BC%8C%E4%B8%8D%E5%BE%97%E8%BF%9B%E8%A1%8C%E5%95%86%E4%B8%9A%E6%80%A7%E8%BD%AC%E8%BD%BD%E6%88%96%E8%80%85%E4%BD%BF%E7%94%A8%E3%80%82%E4%BB%BB%E4%BD%95%E5%95%86%E7%94%A8%E8%A1%8C%E4%B8%BA%EF%BC%8C%E8%AF%B7%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E8%87%B3tech%40meituan.com%E7%94%B3%E8%AF%B7%E6%8E%88%E6%9D%83%E3%80%82" target="_blank">本文系美团技术团队出品，著作权归属美团。欢迎出于分享和交流等非商业目的转载或使用本文内容，敬请注明「内容转载自美团技术团队」。本文未经许可，不得进行商业性转载或者使用。任何商用行为，请发送邮件至 tech@meituan.com 申请授权。</a></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 02:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/meituantech/blog/10117396</guid>
            <link>https://my.oschina.net/meituantech/blog/10117396</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Hugging Face 分词器新增聊天模板属性]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 编辑器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;" data-mpa-powered-by="yiban.io"><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);"><em style="color: black;">一个幽灵，格式不正确的幽灵，在聊天模型中游荡！</em></p></blockquote><span id="OSC_h2_1"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">太长不看版</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">现存的聊天模型使用的训练数据格式各各不同，我们需要用这些格式将对话转换为单个字符串并传给分词器。如果我们在微调或推理时使用的格式与模型训练时使用的格式不同，通常会导致严重的、无声的性能下降，因此匹配训练期间使用的格式极其重要！Hugging Face 分词器新增了 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">chat_template</code> 属性，可用于保存模型训练时使用的聊天格式。此属性包含一个 Jinja 模板，可将对话历史记录格式化为正确的字符串。请参阅，技术文档，以了解有关如何在代码中编写和应用聊天模板。</p><span id="OSC_h2_2"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">引言</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你熟悉 🤗 transformers 库，你可能写过如下代码:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">tokenizer&nbsp;=&nbsp;AutoTokenizer.from_pretrained(checkpoint)<br>model&nbsp;=&nbsp;AutoModel.from_pretrained(checkpoint)<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">通过从同一个 checkpoint 中加载分词器和模型，可以确保对输入字符串使用的分词方法符合模型预期。如果你从另一个模型中选择分词器，则其分词结果很可能会完全不同，此时模型的性能就会受到严重损害。这种现象叫 <strong style="color: black;">分布漂移 (distribution shift)</strong>: 模型一直从一种分布学习 (即训练分词器)，突然，数据分布变成了另一个不同的分布。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">无论你是微调模型还是直接用它进行推理，让这种分布上的变化尽可能小，并保持提供的输入尽可能与训练时的输入一致总是一个好主意。对于常规语言模型，做到这一点相对容易 - 只需从同一检查点加载分词器和模型，就可以了。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">然而，对于聊天模型来说，情况有点不同。这是因为「聊天」不仅仅是直接对单个文本字符串进行分词 - 它需要对一系列消息进行分词。每个消息都包含一个 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">角色</code> 及其 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">内容</code> ，其内容是消息的实际文本。最常见的，角色是「用户」(用于用户发送的消息) 、「助理」(用于模型生成的响应)，以及可选的「系统」(指在对话开始时给出的高级指令)。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">干讲可能有点抽象，下面我们给出一个示例聊天，把问题具象化:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">[<br>&nbsp;&nbsp;&nbsp;&nbsp;{<span style="color: #d14;line-height: 26px;">"role"</span>:&nbsp;<span style="color: #d14;line-height: 26px;">"user"</span>,&nbsp;<span style="color: #d14;line-height: 26px;">"content"</span>:&nbsp;<span style="color: #d14;line-height: 26px;">"Hi&nbsp;there!"</span>},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<span style="color: #d14;line-height: 26px;">"role"</span>:&nbsp;<span style="color: #d14;line-height: 26px;">"assistant"</span>,&nbsp;<span style="color: #d14;line-height: 26px;">"content"</span>:&nbsp;<span style="color: #d14;line-height: 26px;">"Nice&nbsp;to&nbsp;meet&nbsp;you!"</span>}<br>]<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">此消息序列需要先转换为一个文本字符串，然后才能对其进行分词以输入给模型。但问题是，转换方法有很多！例如，你可以将消息列表转换为「即时消息」格式:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">User:&nbsp;Hey&nbsp;there!<br>Bot:&nbsp;Nice&nbsp;to&nbsp;meet&nbsp;you!<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">或者你可以添加特殊词元来指示角色:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">[USER]&nbsp;Hey&nbsp;there!&nbsp;[/USER]<br>[ASST]&nbsp;Nice&nbsp;to&nbsp;meet&nbsp;you!&nbsp;[/ASST]<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">抑或你可以添加词元以指示消息之间的边界，而将角色信息作为字符串插入:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">&lt;|im_start|&gt;user<br>Hey&nbsp;there!&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>Nice&nbsp;to&nbsp;meet&nbsp;you!&lt;|im_end|&gt;<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">方法多种多样，但没有哪种方法是最好的或是最正确的。因此，不同的模型会采用截然不同的格式进行训练。上面这些例子不是我编造的，它们都是真实的，并且至少被一个现存模型使用过！但是，一旦模型接受了某种格式的训练，你需要确保未来的输入使用相同的格式，否则就可能会出现损害性能的分布漂移。</p><span id="OSC_h2_3"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">模板: 一种保存格式信息的方式</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">当前的状况是: 如果幸运的话，你需要的格式已被正确记录在模型卡中的某个位置; 如果不幸的话，它不在，那如果你想用这个模型的话，只能祝你好运了; 在极端情况下，我们甚至会将整个提示格式放在，相应模型的博文，中，以确保用户不会错过它！但即使在最好的情况下，你也必须找到模板信息并在微调或推理流水线中手动将其写进代码。我们认为这是一个特别危险的做法，因为使用错误的聊天格式是一个 <strong style="color: black;">静默错误</strong> - 一旦出了错，不会有显式的失败或 Python 异常来告诉你出了什么问题，模型的表现只会比用正确格式时差多了，但很难调试其原因！</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这正是 <strong style="color: black;">聊天模板</strong> 旨在解决的问题。聊天模板是一个 Jinja 模板字符串，你可以使用分词器保存和加载它。聊天模板包含了将聊天消息列表转换为模型所需的、格式正确的输入字符串所需要的全部信息。下面是三个聊天模板字符串，分别对应上文所述的三种消息格式:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">{% for message in messages %}<br>    {% if message['role'] == 'user' %}<br>        {{ "User : " }}<br>    {% else %}<br>        {{ "Bot : " }}<br>    {{ message['content'] + '\n' }}<br>{% endfor %}<br></code></pre><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">{% for message in messages %}<br>    {% if message['role'] == 'user' %}<br>        {{ "[USER]" + message['content'] + " [/USER]" }}<br>    {% else %}<br>        {{ "[ASST]" + message['content'] + " [/ASST]" }}<br>    {{ message['content'] + '\n' }}<br>{% endfor %}<br></code></pre><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">"{% for message in messages %}"<br>    "{{'&lt;|im_start|&gt;' + message['role'] + '\n' + message['content'] + '&lt;|im_end|&gt;' + '\n'}}"<br>"{% endfor %}"<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你不熟悉 Jinja，我强烈建议你花点时间研究下这些模板字符串及其相应的模板输出，看看你是否可以弄清楚这些模板如何将消息列表转换为格式化的消息字符串！其语法在很多方面与 Python 非常相似。</p><span id="OSC_h2_4"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">为什么要使用模板？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你不熟悉 Jinja，一开始上手可能会有点困惑，但我们在实践中发现 Python 程序员可以很快上手它。在开发此功能的过程中，我们考虑了其他方法，例如允许用户按角色指定消息的前缀和后缀。我们发现该方法会变得令人困惑且笨重，而且它非常不灵活，以至于对一些模型而言，我们得需要一些巧妙的变通才行。而另一方面，模板功能强大到足以完全支持我们所知的所有消息格式。</p><span id="OSC_h2_5"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">为什么要这样做呢？为什么大家不统一到一个标准格式呢？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">好主意！不幸的是，为时已晚，因为现有的多个重要模型已经基于迥异的聊天格式进行了训练。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">然而，我们仍然可以稍微缓解下这个问题。我们认为最接近「标准」的格式是 OpenAI 创建的 ChatML 格式。如果你正在训练新的聊天模型，并且此格式适合你，我们建议你使用它并给分词器添加特殊的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">&lt;|im_start|&gt;</code> 和 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">&lt;|im_end|&gt;</code> 词元。它的优点是角色非常灵活，因为角色只是作为字符串插入，而不是特定的角色词元。如果你想使用这个，它是上面的第三个模板，你可以简单地使用一行代码进行设置:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">tokenizer.chat_template&nbsp;=&nbsp;<span style="color: #d14;line-height: 26px;">"{%&nbsp;for&nbsp;message&nbsp;in&nbsp;messages&nbsp;%}{{'&lt;|im_start|&gt;'&nbsp;+&nbsp;message['role']&nbsp;+&nbsp;'\n'&nbsp;+&nbsp;message['content']&nbsp;+&nbsp;'&lt;|im_end|&gt;'&nbsp;+&nbsp;'\n'}}{%&nbsp;endfor&nbsp;%}"</span><br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">不过，除了格式林立的现状之外，还有第二个不硬设标准格式的原因 - 我们预计模板将广泛用于多种类型模型的预处理，包括那些可能与标准聊天操作迥异的模型。硬设标准格式限制了模型开发人员使用此功能完成我们尚未想到的任务的能力，而模板则为用户和开发人员提供了最大的自由度。甚至可以在模板中加入逻辑检查和判断，这是目前任何默认模板中都没有深入使用的功能，但我们希望它能成为喜欢冒险的用户手中的利刃。我们坚信，开源生态系统应该让你能够做你想做的事，而不是命令你做什么。</p><span id="OSC_h2_6"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">模板如何工作？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">聊天模板是 <strong style="color: black;">分词器</strong> 的一部分，因为它们履行与分词器相同的角色: 存储有关如何预处理数据的信息，以确保你以与训练时相同的格式将数据提供给模型。我们的设计使得用户非常容易将模板信息添加到现有分词器并将其保存或上传到 Hub。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在有聊天模板这个功能之前，聊天格式信息都存储在 <strong style="color: black;">类级别</strong> - 这意味着，例如，所有 LLaMA checkpoint 都将使用同一个硬设在 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">transformers</code> 的 LLaMA 模型类代码中的聊天格式。为了向后兼容，目前具有自定义聊天格式方法的模型类也已被赋予了 <strong style="color: black;">默认聊天模板</strong>。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在类级别设置默认聊天模板，用于告诉 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">ConversationPipeline</code> 等类在模型没有聊天模板时如何格式化输入，这样做 <strong style="color: black;">纯粹是为了向后兼容</strong>。我们强烈建议你在任何聊天模型上显式设置聊天模板，即使默认聊天模板是合适的。这可以确保默认聊天模板中的任何未来的更改或弃用都不会破坏你的模型。尽管我们将在可预见的将来保留默认聊天模板，但我们希望随着时间的推移将所有模型转换为显式聊天模板，届时默认聊天模板可能会被完全删除。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">有关如何设置和应用聊天模板的详细信息，请参阅，技术文档。</p><span id="OSC_h2_7"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">我该如何开始使用模板？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">很简单！如果分词器设置了 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">chat_template</code> 属性，则它已准备就绪。你可以在 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">ConversationPipeline</code> 中使用该模型和分词器，也可以调用 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">tokenizer.apply_chat_template()</code> 来格式化聊天以进行推理或训练。请参阅我们的，开发者指南，或 如何应用聊天模板的文档，以了解更多！</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果分词器没有 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">chat_template</code> 属性，它可能仍然可以工作，但它将使用该模型类的默认聊天模板。正如我们上面提到的，这是脆弱的，并且当类模板与模型实际训练的内容不匹配时，它同样会导致静默错误。如果你想使用没有 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">chat_template</code> 的 checkpoint，我们建议检查模型卡等文档以确保使用正确的格式，然后为该格式添加正确的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">chat_template</code> 。即使默认聊天模板是正确的，我们也建议这样做 - 它可以使模型面向未来，并且还可以清楚地表明该模板是存在的且是适用的。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">即使不是你的 checkpoint，你也可以通过提交，合并请求 (pull request) &nbsp;的方式为其添加 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">chat_template</code> 。仅需将 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">tokenizer.chat_template</code> 属性设置为 Jinja 模板字符串。完成后，推送更改就可以了！</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你想在你的聊天应用中使用某 checkpoint，但找不到有关其使用的聊天格式的任何文档，你可能应该在 checkpoint 上提出问题或联系其所有者！一旦你弄清楚模型使用的格式，请提交一个 PR 以添加合适的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">chat_template</code> 。其他用户将会非常感激你的贡献！</p><span id="OSC_h2_8"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">总结: 模板理念</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">我们认为模板是一个非常令人兴奋的新特性。除了解决大量无声的、影响性能的错误之外，我们认为它们还开辟了全新的方法和数据模式。但最重要的也许是，它们还代表了一种理念转变: 从核心 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">transformers</code> 代码库中挪出一个重要功能，并将其转移到各自模型的仓库中，用户可以自由地做各种奇怪、狂野抑或奇妙的事情。我们迫不及待想看看你会发现哪些用途！</p><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);">🤗 宝子们可以戳 <strong style="color: black;">阅读原文</strong> 查看文中所有的外部链接哟！</p></blockquote><hr data-tool="mdnice 编辑器" style="height: 1px;border-right: none;border-bottom: none;border-left: none;border-top-style: solid;border-top-color: rgb(249, 191, 69);margin-top: 20px;margin-bottom: 20px;"><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;color: rgb(91, 91, 91);background: rgba(158, 158, 158, 0.1);padding-top: 1px;padding-bottom: 1px;padding-left: 5px;margin-top: 0px;margin-bottom: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">英文原文:&nbsp;<span style="color: rgb(136, 136, 136);letter-spacing: 0px;">https://hf.co/blog/chat-templates</span></p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">原文作者: Matthew Carrigan</p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">译者: Matrix Yao (姚伟峰)，英特尔深度学习工程师，工作方向为 transformer-family 模型在各模态数据上的应用及大规模模型的训练推理。</p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">审校/排版: zhongdongy (阿东)</p></blockquote></blockquote></blockquote></blockquote></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - Hugging Face（gh_504339124f0f）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 31 Oct 2023 02:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10120361</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10120361</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[对标 FAISS，百度开源自研高性能检索引擎 Puck]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">近日，百度宣布在 Apache 2.0 协议下开源自研检索引擎 Puck，这也是国内首个适用于超大规模数据集的开源向量检索引擎。向量检索算法在个性化推荐系统、多模态检索、自然语言处理等应用场景中都发挥着重要作用，特别是在处理大规模数据和高维特征数据时。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">名称「Puck」取自经典 MOBA 游戏 DOTA 中的智力英雄 Puck，象征着飘逸和灵动。这个项目经过多年在百度内部的精心打磨，而且在 2021 年底 Nerulps 举办的全球首届向量检索大赛 BIGANN 比赛中，Puck 参与的四个项目均获得第一名。InfoQ 采访了百度搜索内容技术部主任架构师 Ben，以了解该项目的发展历程和核心优势。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：是否方便介绍一下您的工作经历，以及目前的主要职责？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：我从毕业即加入百度，最初在移动搜索部门，负责基础检索和相关性方面工作，经历了移动高速发展的过程。之后作为创始成员协助组建了多模搜索部负责视觉搜索，属于百度最早一批进入 AI 领域的员工。目前在搜索内容技术部，负责内容相关技术，包括内容获取、内容理解、内容计算、内容加工与生成等。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：您从什么时候开始关注开源？是什么让您决定 Puck 要开源？选择这个时候开源的原因是什么？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：我们很早就在思考开源，看到 FAISS（由 Facebook AI Research 开发的大规模向量检索库）开源之后获得了广泛的业界关注和应用，我们也希望开源 Puck 后，可以促进社区的发展，并借助社区的力量提高代码质量，加速技术创新，更好的适应市场需求。自研开源市场变得越来越成熟和规范，可能会带来更多的商业模式和合作机会。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">对外开源，我们其实筹备了很久，做了大量的准备工作。大模型的爆火，导致向量检索技术获得广泛关注，我们认为，这是一个合适的开源契机。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：您能具体讲一下 Puck 在百度的发展史，以及从您角度来看，它对于百度搜索的价值主要体现在哪里？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：Puck 的想法最早来自视觉搜索业务，我们需要一个能支撑数百亿相似图片检索的 ANN 引擎，同时要能支持高吞吐、低延时、高准确、低内存、高灵活性等要求，当时业内没有能满足我们需要的引擎，于是启动了自研的过程。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">2017 年 Puck 完成首次上线，在百亿图片库上成本和效果都取得了极其显著的提升；之后随着 Transformer 模型在 nlp 领域的大放异彩，基于 embedding 的语义检索越来越凸现价值，Puck 的应用也越来越广，2019 年 Puck 在百度内部开源，支撑的业务数快速增长，目前已广泛应用于百度搜索、推荐、网盘、知识图谱等内部多条产品线，支持规模突破万亿。目前 ANN 已经成为互联网底层基础技术之一，是 AI 时代的基石，搜索最重要的支撑技术之一。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：期间经过了几次优化，优化重点是什么，您能具体讲述一下吗？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：到今天 Puck 已经是一个打磨多年的产品，中间的优化数不胜数，大体来说可以分成以下几个阶段：</p><ol><li>2016 年到 2019 年，打磨核心算法和实现，重点在基础性能优化上，不断调整细节，在自有场景上做极致优化，Puck 的核心框架在这一时期建立并沿用至今。</li><li>2019 年到 2021 年，以公司内开源为标志，随着业务接入的增多，Puck 需要适配各种各样的应用场景和诉求，易用性、扩展性、功能多样性成为主要目标，像高性能的实时插入、多条件检索、分布式建库等等功能都是在这一时期完成。</li><li>2021 年到 2022 年，以大规模内容关系计算应用为契机，Puck 重点优化在单实例超大规模数据下的性能，通过大尺度量化和索引结构的优化在十亿规模数据集上大幅提升性能降低成本。以参加全球首届向量检索大赛 BIGANN 并获得四项第一为标志，证明了 Puck 在这部分的竞争优势。</li><li>2022 年至今，核心算法创新，提出了新的算法来适配不同数据场景，新增更多的 feature，同时完善配套设施，做外部开源准备。</li></ol><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">这只是一个粗略的划分。实际上，Puck 的优化更多地由许多微小的优化点组成。我们在讨论中提出了大量有趣的想法，进行了大量的实验和尝试。总的来说，十个想法中最终只有一到两个能成为正式的功能。这些优化最终汇聚在一起，形成了我们今天看到的 Puck。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：您能否详细介绍下 Puck 的核心优势和应用场景？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：Puck 开源项目包含了两种百度自研的检索算法和一系列的附加功能，核心优势首先就是性能，经过多年的打磨和调优，在 benchmark 的千万、亿、十亿等多个数据集上，Puck 性能优势明显，均显著超过竞品，在 2021 年底 Nerulps 举办的全球首届向量检索大赛 BIGANN 比赛中，Puck 参加的四个项目均获得第一。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">其次，易用性上，Puck 提供了一系列的适用于各种场景的功能，比如，同时提供简单易用的 API 接入，尽量少的暴露参数，大部分参数使用默认设置即可达到良好性能。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">最后，Puck 是一个久经考验的引擎，经过多年在实际大规模场景下的验证打磨，广泛应用于百度内部包括搜索、推荐等三十余条产品线，支撑万亿级索引数据和海量检索请求，可靠性上有非常高的保障。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">Puck 引擎这次开源了两种检索算法 Puck 和 Tinker，分别更适用于超大规模数据集和中小规模数据集，几乎可以覆盖绝大部分的检索应用场景。目前已广泛应用于百度内部搜索、推荐等多条产品线，覆盖数据规模从百万至万亿。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：面对 AI 新浪潮，大模型在业内已越来越卷，在您看来未来开源市场会不会更卷？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：AI 大模型的出现确实使得业内竞争更加激烈，但这并不是坏事。首先，大模型的发展推动了 AI 技术的进步，提高了 AI 的性能和效率。其次，大模型为业内带来了更多的创新空间和可能性，推动了开源市场的发展。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">以后业内在自研开源市场的竞争会更加激烈，但这并不意味着会更卷，相反是带来了无限的可能。因为开源市场的特性是开放和共享，企业和个人可以通过开源市场获取最新的 AI 技术和模型，而无需自己从零开始开发。这有助于整个行业降低研发成本和提高研发效率。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">此外，开源市场也是技术交流和创新的平台，业内人士可以在这里分享自己的研究成果，吸收他人的经验和知识，共同推动 AI 技术的发展。所以，虽然竞争会更激烈，但只要我们能适应这种趋势，积极参与交流和创新，就可以从中获益。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：那您认为互联网公司开源项目的未来发展趋势是什么样的？会往哪方面发展？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：</p><ol><li>深度专业化：随着技术的细分，开源项目可能会更加专业化和深度化，解决更具体、更深入的问题，会更多永远专注于某一特定问题的开源项目，Puck 就是其中之一。</li><li>多元化：互联网公司自研的开源项目可能会涉及更多的行业和领域，实现技术的跨界整合，形成各种行业解决方案的开源项目，这种跨界融合将有助于推动技术在各行业的广泛应用。</li><li>更强的实用性：未来的开源项目可能会更注重实战和应用，而不仅仅是理论研究。开源项目会提供更多实用的工具和框架，帮助开发者更好地将理论应用到实际工作中。</li><li>注重数据和算法的开源：随着数据和算法的重要性日益凸显，未来可能会有更多的数据和算法开源，以加速 AI 等领域的发展。</li></ol><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">这些变化都将为推动科技发展和解决实际问题提供更强大的动力。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：您提到 Puck 在内部已广泛应用，有哪些大家熟悉的产品或场景吗？能否举个例子。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：大家熟悉的百度搜索和手机百度内的信息流推荐都有使用 Puck 技术。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：请问开源后是否收到了社区的一些反馈，对您有怎样的启发？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：自从 Puck 开源以来，我们已经收到了不少来自社区的反馈和建议。这些反馈和建议对我们来说是非常宝贵的，它们不仅帮助我们发现了 Puck 的一些问题和不足，也为我们提供了改进和优化的方向。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">对我个人来说，这些反馈启发我认识到，虽然我们在内部使用 Puck 有着丰富的经验，但在面对更广泛的用户群体时，我们还需要不断学习和提高。每个用户的需求都可能不同，我们需要更加深入地理解用户的需求，才能更好地优化 Puck，使其更加适应不同的使用场景。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">同时，这些反馈也让我深切地感受到了开源社区的活力和创新精神。许多社区成员不仅提出了问题，还积极地提供了解决方案，这种积极参与和贡献的精神让我深感鼓舞。我希望在未来，我们能够更紧密地与社区合作，共同推动 Puck 的发展。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">InfoQ：Puck 对您个人的意义，您对 Puck 的未来有什么期待？</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>：Puck 是团队长时间研究和努力的成果，作为 Puck 的负责人，我对这个项目有着深深的热爱和执着，对我个人来说，它不仅仅是一个检索引擎，而是代表团队付出的心血和智慧的结晶，它是我们对技术的追求，对创新的执着，也是我们对未来的期待和憧憬，Puck 的每一次升级和优化都记录着我们的成长和进步。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify">对于 Puck 的未来，我有着很高的期待。首先，我希望 Puck 能在开发者社区中得到广泛的使用，同时也能得到社区的反馈，不断优化和改进。我期待看到更多的人参与到 Puck 的开发和使用中来，通过大家的共同努力，让 Puck 成为 AI 领域有影响力的一款工具。其次，我希望 Puck 能够持续创新，不断优化，保持其技术领先地位，不仅能适应现有的技术需求，还能预见并引领未来的技术趋势。最后，我希望 Puck 能在更多实际应用中发挥出它的价值，为人工智能在各个行业的应用提供强大支撑，推动科技的发展。</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:left">采访嘉宾简介</p><p style="color:#191919; margin-left:1.8em; margin-right:.63em; text-align:justify"><strong>Ben</strong>，百度搜索内容技术部主任架构师，负责多模态内容理解、超大规模内容关系计算、内容加工与生成、模型优化等方向。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 14:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264174</guid>
            <link>https://www.oschina.net/news/264174</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[ChatGPT 可以做 WebRTC 音视频质量性能优化，惊艳到我了]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><strong>摘要</strong></p><blockquote><p>随着 GPT-4 的发布，AI 的风越吹越旺。GPT-4 可以回答问题，可以写作，甚至可以基于一张草图生成 html 代码搭建一个网站。即构社区的一位开发者@倪同学就基于目前在研究的 WebRTC QOS 技术点对 GPT-3.5 跟 GPT-4 进行一场实验，ChatGPT 会取代程序员还是成为最强辅助？</p></blockquote><p><strong>以下为@倪同学的博文。</strong></p><hr><h1>ChatGPT 取代程序员还是给程序员加 Buff？</h1><p>这两周，AI 新闻一个接着一个，3 月 23 日，Google 开放了内测已久的 AI 对话服务 Bard，Google 强调，这是一款定位为用户提供创意之源的产品，可生成写作草稿或生活中的聊天机器人。早在一周前 3 月 15 日凌晨，OpenAi 距发布 GPT-3.5 后四个月发布了升级版模型 GPT-4，据发布会说，GPT-4 可支持图片输入，角色扮演，写作能力更强了。紧接着 3 月 16 日百度发布了文心一言，一共有五大功能：文学创作、商业文案创作、数理逻辑推算、中文理解、多模态生成。</p><p>随着近日各大厂商 AI 产品的接连发布，<strong>AI 取代人工</strong>这个话题持续在发酵。AI 大幅解放人的生产力或是将冲击一大批职业？</p><p>博主近期在输出 WebRTC 相关的技术博客，不如向 AI 提问看他有什么见解。</p><p>和大部分人一样，博主都还没拿到 Bard 跟文心一言的内测资格。得知 NewBing 用的是 GPT-4 的模型，下面就着<strong>WebRTC 通过哪些 QOS 技术提升音视频通话质量</strong>，向 GPT-3.5 和 Newbing（GPT-4）分别提问，看看他们的答案有何差异。</p><p>如下图，技术科普类问题都难不倒 GPT-3.5 和 GPT-4，我就该问题继续深挖让它们举实例说明：</p><p>NewBing(GPT-4)</p><p><img src="https://oscimg.oschina.net/oscnet/up-4629da845197985993c293d94127cc0c271.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-3.5 给出的结果</p><p><img src="https://oscimg.oschina.net/oscnet/up-54f9e77266206f54ed5b4f0946988bcca36.png" alt="" referrerpolicy="no-referrer"></p><p>NewBing(GPT-4) 直接给出了具体操作实例</p><p><img src="https://oscimg.oschina.net/oscnet/up-d7d0918e1048c337e7d4649564b3858f3f3.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-3.5 给出的结果（有些空泛）</p><p><img src="https://oscimg.oschina.net/oscnet/up-e9dc01bfdceb3cc698a4093d5b1d46da426.png" alt="" referrerpolicy="no-referrer"></p><h1>GPT-4 和 GPT-3.5 对比结论</h1><p>通过实验，我们比较了同一问题两个版本的回答。在普通的文本处理当中，GPT-4 和 GPT-3.5 的区别可能比较小，但是当问题足够具体和复杂时，GPT-4 就会比 GPT-3.5 更精准、更有创意，而且能够处理用户更细微的指令。</p><p>当然，本篇内容不是要讨论 GPT-3.5 跟 GPT-4 的具体差别，而是程序员如何利用 ChatGPT 提升工作效率，加上最强 Buff。以下我将以个人开发经验为音视频开发者分享《<strong>WebRTC 的 QOS 如何提升音视频质量》。</strong></p><h1><strong>WebRTC 技术概述</strong></h1><p>WebRTC 通过一系列的 QOS 技术来提升音视频通话质量: 抗丢包策略 (NACK、 FEC), 拥塞控制策略 (TWCC/REMB), SVC 或多视轨, 视频质量自适应策略， Pacer、JitterBuffer 等.</p><p>总体 QOS 架构如下图所示：</p><p><img src="https://oscimg.oschina.net/oscnet/up-ce883c8c8e5fbb12f3bd9320f13c85cb0aa.png" alt="" referrerpolicy="no-referrer"></p><p>图 1</p><h1><strong>1</strong><strong>丢包恢复策略</strong></h1><h2><strong>1.1 NACK</strong></h2><p>NACK(Negative Acknowledgment) 相较于 ACK 是通过"非到达确认"进行选择性重传的机制。基本原理是发送端对数据进行缓存，接收端通过到达包连续性检测丢包，结合 rtt 和乱序情况在合适的时机向发送端发起重传请求。</p><p><img src="https://oscimg.oschina.net/oscnet/up-1d2480e0c83a78a74feed657cd91b53ff67.png" alt="" referrerpolicy="no-referrer"></p><p>图 2</p><p>如图所示,Receiver 在收到报文 4 之后发现报文 2、3 未到达，暂时将报文 2、3 放入丢失 nack 列表。在超过一定乱序阈值 (通过乱序直方图计算得到，假设这里是 2，那么收到包 4 可认为包 2 丢失)，或者超过一定抖动时间 (根据 rtt 计算)，向 Sender 请求重传丢失的报文 2、3。 Receiver 的请求通过 RTP FB 发送给 Sender, 具体 NACK 请求格式参考 RFC4585。Sender 在收到 NACK 请求后重新发送报文 2、3。</p><p><strong>值得注意的是</strong>，NACK 策略丢包恢复效果取决于重传请求时机。一是 rtt 的计算 (webrtc 默认 rtt 是 100ms)，一是乱序阈值计算。重传请求节奏控制不好容易造成重传风暴，加重拥塞导致拉流出现卡顿。</p><p>参考：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rfc-editor.org%2Frfc%2Frfc4585.html%23page-34%3Fsource%3Doschina%26article64" target="_blank">https://www.rfc-editor.org/rfc/rfc4585.html#page-34</a></p><h2><strong>1.2</strong><strong>FEC</strong></h2><p>FEC(Forward Error Correction),前向纠错, 在数据传输和存储中普遍用于数据纠错。WebRTC 中也使用了该技术进行丢包恢复。</p><p>webrtc 实现该冗余功能，有三种方式：</p><h3><strong>1.2.1、RED</strong></h3><p>将前面的报文直接打入到新包里面，在接收端解析主包和冗余包。</p><p><img src="https://oscimg.oschina.net/oscnet/up-dcd63ecbf52cf04e410e589f927271b375a.png" alt="" referrerpolicy="no-referrer"></p><p>图 3</p><p>如图，后面的报文直接包含前面报文，所以当其中某个报文丢失了，可以通过其相邻报文直接恢复。这种方式缺点是抗连续丢包效果差，但是实现简单。</p><p>Opus In-band FEC 正是使用这种方式进行纠错： 将重要信息以较低的比特率再次编码之后添加到后续数据包中，opsu 解码器根据收包情况决定是否利用当前包携带的冗余包进行丢包恢复。</p><p>Opus In-band FEC 详细参考：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdatatracker.ietf.org%2Fdoc%2Fhtml%2Frfc6716%23section-2.1.7" target="_blank">https://datatracker.ietf.org/doc/html/rfc6716#section-2.1.7</a></p><p>RED 详细介绍参考：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rfc-editor.org%2Frfc%2Frfc2198.html%3Fsource%3Doschina%26article64" target="_blank">https://www.rfc-editor.org/rfc/rfc2198.html</a></p><h3><strong>1.2.2、ULPFEC</strong></h3><p>在多个数据包之间使用 XOR 来生成此冗余信息，并能够在需要时在接收方恢复丢失的数据包。 ULPFEC 能够通过选择受保护的字节数并应用 XOR 的先前数据包的数量，为不同的数据包提供不同级别的保护。</p><p><img src="https://oscimg.oschina.net/oscnet/up-dbc67f90d60129a5ac409477503478c5928.png" alt="" referrerpolicy="no-referrer"></p><p>图 4</p><p>如图，FEC packet 1 保护 L0 级报文 A、B。 FEC packet 2 及保护 L0 级的 A、B, 也保护 L1 级报文 C、D。</p><p>参考：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rfc-editor.org%2Frfc%2Frfc5109.html%3Fsource%3Doschina%26article64" target="_blank">https://www.rfc-editor.org/rfc/rfc5109.html</a></p><h3><strong>1.2.3、FLEXFEC</strong></h3><p>较 ULPFEC，FLEXFEC 可以灵活选择 1D 行异或、列异或以及 2D 行列异或，增加网络抗丢包能力。</p><p>1-D 行异或纠错</p><p><img src="https://oscimg.oschina.net/oscnet/up-10096bc8991b4dd01f54dfc56124b023479.png" alt="" referrerpolicy="no-referrer"></p><p>图 5</p><p>1-D 列异或纠错</p><p><img src="https://oscimg.oschina.net/oscnet/up-b5c40a4c18f39836dbb9e92fc3017670964.png" alt="" referrerpolicy="no-referrer"></p><p>图 6</p><p>2-D 行列异或纠错</p><p><img src="https://oscimg.oschina.net/oscnet/up-36d1716da02b29d6139ec01f2c1ca7a4845.png" alt="" referrerpolicy="no-referrer"></p><p>图 7</p><p>FLEXFEC 虽然相比前面两个有更强的恢复能力，行列交错丢包比如图 7 中 (1、2、5、6) 丢失就会出现无法纠错的情况。</p><p>WebRTC 用到 FEC 策略整体丢包恢复能力都偏弱，业界普遍应用 Reed-Solomon FEC 进行丢包恢复，Reed-Solomon FEC(K + N : K 个数据包 N 个 FEC 包) 可以真正恢复分组内任意 &lt;=N 个丢包。</p><p>FLEXFEC 详细实现可以参考：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rfc-editor.org%2Frfc%2Frfc8627.html%3Fsource%3Doschina%26article64" target="_blank">https://www.rfc-editor.org/rfc/rfc8627.html</a></p><h1><strong>2 带宽评估及码率控制</strong></h1><h2><strong>2.1 REMB-GCC</strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-0651b1a1da43906fa2e59639953688b12dc.png" alt="" referrerpolicy="no-referrer"></p><p>图 8</p><p>图 8 是 REMB-GCC 架构图，基本思想是通过接收端评估带宽， 然后通过 RTCP REMB 将带宽反馈给发送端。 发送端结合丢包率计算一个带宽结果 As,和 RMEB 的结果 Ar, 取 min(As, Ar) 作为最终带宽结果。</p><h2><strong>2.2 SendSide BWE</strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-c4f0e4a20b5e2e2bc5801517dce2e63525b.png" alt="" referrerpolicy="no-referrer"></p><p>图 9</p><p>跟<strong>REMB-GCC</strong> 相比，TFB-GCC 主要区别在于大部分带宽计算都转移到发端计算，滤波器的实现不再用 Kalman 滤波，而是变成<strong>TrendLine 滤波器</strong>。</p><p>发送端发送的包需在扩展头带： Transport-wide sequence number.</p><p>接收端定期发送 Transport-wide feedback 报文，通知发送端和接收端接收报文的相关信息，包括报文到达时间、报文到达时间、报文格式等信息。发送端收到 Transport-wide feedback 报文之后，根据报文携带的信息进行延迟滤波计算 (Trandline).</p><p>Transport-wide feedback 报文格式参考：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdatatracker.ietf.org%2Fdoc%2Fhtml%2Fdraft-holmer-rmcat-transport-wide-cc-extensions-01%3Fsource%3Doschina%26article64" target="_blank">https://datatracker.ietf.org/doc/html/draft-holmer-rmcat-transport-wide-cc-extensions-01</a></p><h2><strong>2.3 速率控制</strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-cf155320998fb6f015ca55496f9b6716354.png" alt="" referrerpolicy="no-referrer"></p><p>图 10</p><p><img src="https://oscimg.oschina.net/oscnet/up-6c862c3581fe6d7a57ae15e961f84f7db78.png" alt="" referrerpolicy="no-referrer"></p><p>图 11</p><p>根据过载检测器产生的信号 s，驱动如图 10 所示的有限状态机来调整码率。</p><p>GCC 算法原理详细参考：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fc3lab.poliba.it%2Fimages%2F6%2F65%2FGcc-analysis.pdf%3Fsource%3Doschina%26article64" target="_blank">https://c3lab.poliba.it/images/6/65/Gcc-analysis.pdf</a></p><h1><strong>3</strong><strong>SVC</strong><strong>、多视轨</strong></h1><h2><strong>3.1</strong><strong>SVC</strong></h2><p>SVC (Scalable Video Coding，可适性视频编码或可分级视频编码) 是传统 H.264/MPEG-4 AVC 编码的延伸，可提升更大的编码弹性，并具有时间可适性 (Temporal Scalability)、空间可适性 (Spatial Scalability) 及质量可适性 (SNR/Quality/Fidelity Scalability) 三大特性。</p><p>WebRTC 中 h264 不支持 svc 编码，Vp8 仅支持 Temporal Scalability, VP9 和 AV1 支持时间可适性 (Temporal Scalability)、空间可适性 (Spatial Scalability)。</p><p><img src="https://oscimg.oschina.net/oscnet/up-973028f5afa2ef31a24851f224e9e74bd0e.png" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-e52d6c53588cf1a352975f823e2cef803dd.png" alt="" referrerpolicy="no-referrer"></p><p>图 12</p><p>上面是时间可适应示意图。假设图例中显示的图层以 30 fps 的帧速率显示。如果我们移除所有 L2 层的图片，剩下层（L0 和 L1）仍然可以成功解码，并且产生一个 15fps 的视频。如果我们进一步删除所有的 L1 图像，那么剩下的 L0 层依然可以被解码并产生一个 7.5fps 的视频, 所以即便是出现丢包，相比不可分级编码可明显提升弱网视频流畅度。</p><p><img src="https://oscimg.oschina.net/oscnet/up-b2dfcbe88b9bfb1b8f3f1f22ad9267f43be.png" alt="" referrerpolicy="no-referrer"></p><p>图 13</p><p>如图 12，L0 基层为分辨率最小编码数据，级别越高，分辨率越高。当实际应用中需要较低分辨率时，只需丢弃高 Level 层级数据进行解码。</p><p>针对不同的带宽条件用户和以及不同设备性能的用户可以灵活调整分辨。</p><p>SVC 扩展参考： <a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fip.hhi.de%2Fimagecom_G1%2Fassets%2Fpdfs%2FOverview_SVC_IEEE07.pdf%3Fsource%3Doschina%26article64" target="_blank">http://ip.hhi.de/imagecom_G1/assets/pdfs/Overview_SVC_IEEE07.pdf</a></p><p>SVC 与 H264 结合参考： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.itu.int%2Frec%2FT-REC-H.264-201704-I%3Fsource%3Doschina%26article64" target="_blank">https://www.itu.int/rec/T-REC-H.264-201704-I</a></p><h2><strong>3.2 多视轨</strong></h2><p>目前主流浏览器都支持 unified-plan sdp, 我们可以在 sdp 协商的时候添加多个视轨，业务上比较常见的就是添加两条视轨 (类似于 SVC 的 Spatial Scalability)，复用相同 DTLS 传输通道。</p><p><img src="https://oscimg.oschina.net/oscnet/up-e11f7f6b5c3c71a6cc86c76a69182dc2ddf.png" alt="" referrerpolicy="no-referrer"></p><p>图 14</p><p>图 12 典型利用 WebRTC 支持多视轨特性编码一大一小两条流的出帧示意图。</p><p>支持多视轨 (大小流) 可以让接收端在下行带宽受限的情况下动态切换到可以支持的分辨率，提升弱网体验。</p><p>多视轨 (大小流) 在对网络丢包及带宽受限情况的适应不如 SVC 灵活，但是多视轨实现简单，编码、解码性能消耗较低，在实际的业务场景中得到广泛应用。</p><p>多视轨需要支持 Unified Plan SDP 协商, 参考 WebRTC 相关说明：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwebrtc.github.io%2Fwebrtc-org%2Fweb-apis%2Fchrome%2Funified-plan%2F%3Fsource%3Doschina%26article64" target="_blank">https://webrtc.github.io/webrtc-org/web-apis/chrome/unified-plan/</a></p><h1><strong>4 视频质量调整策略</strong></h1><p>在网络传输质量变差 (上行带宽不足)、CPU 占有率过高，编码器编码质量 QP 值过大等情况下，WebRTC 会通过降质量来保障视频通话。降质量策略主要分降帧率 (即清晰优先模式) 和降分辨率 (即流畅优先模式)，通过 MediaStreamTrack Content Hints 来设置。</p><p><strong>清晰优先模式</strong> WebRTC 在编码的时候更注重视频细节，在出现上述情况需要降质量时，会通过降低帧率、保持分辨率不变来保障拉流用户的主观感受。对于推流端做屏幕分享内容是 PPT 或者拉流用户大屏显示的业务场景尤为重要。</p><p><strong>流畅优先模式</strong> 推流端在需要降质量的时候优先降低分辨率、保持一定的帧率来保障拉流用户的流畅体验。</p><p>在带宽或 CPU 资源等不再受限时，WebRTC 会根据降质量偏好设置逆向提升视频质量。</p><p>使用者应该根据自己的业务场景进行适当设置，才能在极端情况下保证主观体验不至于太差。</p><h1><strong>5 Pacer</strong></h1><p>WebRTC 的 Pacer 模块主要是让需要发送的包根据评估的网络带宽尽量均匀的分布在每个发送时间窗口发出，起到平滑发包、避免网络拥塞的作用。</p><p>假设有一条 5Mbps 和 30fps 的视频流。 在理想情况下，每个帧大小约为 21kB，打包成 18 个 RTP 数据包。 按照一秒时间窗口统计的平均比特率是 5Mbps，但在更短的时间范围内，它可以被视为每 33 毫秒突发 167Mbps。 此外，视频编码器在突然移动的情况下会超过目标帧率，尤其是在处理屏幕共享时，帧比目标尺寸大 10 倍甚至 100 倍很常见。 这些数据包如果编码完成马上发出去会导致几个问题: 网络拥塞、缓冲区膨胀、甚至数据包丢失。 大多数会话都有不止一条媒体流，可能同时包含音频流、视频流、数据流。 如果你一次性将一个帧放在一条传输通道发送，这些数据包需要 100 毫秒才能发出，这可能阻止了任何音频数据包及时发送出去。 Pacer 通过有一个缓冲区来解决这个问题。 媒体包在其中排队，然后使用漏桶算法将它们调整到网络上。 缓冲区包含所有媒体轨道的独立 fifo 流，例如，音频可以优先于视频 - 可以以循环方式发送相同优先级的流，以避免任何一个流阻塞其他流。</p><p><img src="https://oscimg.oschina.net/oscnet/up-3b894e7d0824cc00b76bb2b75a11c932177.png" alt="" referrerpolicy="no-referrer"></p><p>图 15</p><h1><strong>6 JitterBuffer</strong></h1><p><img src="https://oscimg.oschina.net/oscnet/up-0a330d475cd478eb5ad7158389b8d6b3362.png" alt="" referrerpolicy="no-referrer"></p><p>图 16</p><p>WebRTC 接收端收到 RTP 包后，放到 PacketBuffer 进行缓存和排序。如上图，在收到 Mark(帧结束) 标志之后，从后往前开始组帧。组完一帧会放到该帧所在 GOP 的缓存里面，根据帧间参考顺序进行调整，当帧间参考关系建立好之后就会放到解码器进行解码。可以认为 Jitter 主要先后做包排序、帧排序、GOP 排序。之所以要进行着一系工作是因为网络本身存在一定的抖动、甚至有丢包，如果有丢包还得等丢包恢复才能完整组帧，所以导致帧到达时间跟发送时间存在一定抖动。Jitter buffer 的存在就很好的解决这个问题，能够在拉流端对待解码数据进行平滑处理，保证我们渲染出来视频是平滑、流畅的。</p><h1><strong>7 关键帧请求</strong></h1><p>视频流通常是以 1 个关键帧+ N 个增量帧的方式发送，这些增量帧依赖于先前的帧进行解码和显示。如果因为一些原因导致 sps/pps 丢失、 组包错误等，如果不采取任何补救措施，就很难继续解码视频流，视频就会卡主, 直到下个关键帧。很多时候为了编码稳定 GOP 设置很大，这个时候意味着长时间卡顿或者黑屏。</p><p><img src="https://oscimg.oschina.net/oscnet/up-a58f2b74a6631610904cf29d65b3ff8ec98.png" alt="" referrerpolicy="no-referrer"></p><p>图 17</p><p>如图接收端因为丢包不能恢复导致 Frame 9 组帧失败，后面即使能组帧成功也无法解码，此时需要从发送端请求一个 I 帧解码刷新当前视频流。</p><p>WebRTC 通过 RTCP 报文向发送端请求发送关键帧，关键帧请求 RTCP 报文格式比较简单，在<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftools.ietf.org%2Fhtml%2Frfc4585" target="_blank">RFC4585</a>（RTP/AVPF）以及<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftools.ietf.org%2Fhtml%2Frfc5104" target="_blank">RFC5104</a>（AVPF）规定了两种不同的关键帧请求报文格式：Picture Loss Indication (PLI)、Full Intra Request (FIR)。从目前的实现看 WebRTC 在收到 PLI 或者 FIR 之后，都是让编码器编码输出关键帧，然后发送给接收端。</p><p>PLI 报文格式参考： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rfc-editor.org%2Frfc%2Frfc4585.html%23page-36%3Fsource%3Doschina%26article64" target="_blank">https://www.rfc-editor.org/rfc/rfc4585.html#page-36</a></p><p>FIR 参考： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.rfc-editor.org%2Frfc%2Frfc5104.html%3Fsource%3Doschina%26article64" target="_blank">https://www.rfc-editor.org/rfc/rfc5104.html</a></p><h1><strong>QOS 技术总结：</strong></h1><p>本文简单介绍了 WebRTC 中所使用到的 Qos 技术，这些技术从不同的角度去提升 Qos 质量。包括通过<strong>NACK、FEC</strong>技术对丢包进行恢复，解决丢包导致的音、视频卡顿。通过<strong>带宽评估和拥塞控制</strong>技术调整编码和发送码率来自动适应网络带宽的变化情况。通过 SVC、多视轨技术保障不同网络质量的拉流的用户差异的视频质量。 而<strong>Pacer、JitterBuffer</strong>分别在发送端和接收端提升音视频的平滑、流畅度。<strong>关键帧请求</strong>对极端网络抖动之后的快速视频恢复起了重要作用。WebRTC 利用这些技术协同作用，提升整体的 Qos 质量，需要了解技术细节最好的方式还是去阅读 WebRTC 源码。</p><p>WebRTC 的 Qos 技术对提升整体音视频质量效果显著、但 WebRTC 的这些技术还是存在有很多可以优化的地方。音视频厂商 ZEGO 即构自研的 WebRTC 网关对这些策略都做了一定的优化：包括自研带宽评估算法、NACK 算法、大小流等。</p><p>所以，如果你的业务需要一款稳定可靠的音视频服务，可以试试即构实时音视频 RTC 服务。</p><p><strong>点击跳转<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdoc-zh.zego.im%2Farticle%2F9675%3Fsource%3Doschina%26article64" target="_blank">ZEGO 即构实时音视频服务</a>了解更多 WebRTC 最佳实践内容。</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 12:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/5818436/blog/8590740</guid>
            <link>https://my.oschina.net/u/5818436/blog/8590740</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[国家广电总局公示《云游戏总体技术要求》等行业标准]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>国家广播电视总局科技司<u><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.nrta.gov.cn%2Fart%2F2023%2F10%2F30%2Fart_113_65962.html" target="_blank">发布公告称</a></u>，按照广播电视和网络视听行业标准制定程序要求和计划安排，国家广播电视总局组织相关单位编制《沉浸式终端通用技术要求》《云游戏总体技术要求》《自由视角视频系统技术要求》行业标准，现对已通过全国广播电影电视标准化技术委员会审查的报批稿予以公示。</p><p><img height="1820" src="https://oscimg.oschina.net/oscnet/up-ad5c1b15dcc8e8faf28a1caa2fb63e4aefb.png" width="2436" referrerpolicy="no-referrer"></p><p>根据《云游戏总体技术要求》行业标准的报批稿，<strong>该文件规定了云游戏的总体技术架构，以及云游戏平台、网络、云游戏终端和云游戏安全的技术要求</strong>，并针对未成年人用户对云游戏平台和云游戏终端提出了要求。</p><p>该标准的起草单位包括：</p><blockquote><p>国家广播电视总局广播电视科学研究院、腾讯科技（上海）有限公司、中国广播电视网络集团有限公司、咪咕互动娱乐有限公司、元境生生（北京）科技有限公司、浙江华数广电网络股份有限公司、江苏省广电有线信息网络股份有限公司、中国广电湖南网络股份有限公司、国广东方网络（北京）有限公司、青岛西发广电传媒科技有限公司、互影科技（北京）有限公司、北京决策数科技有限公司、北京和创摩尔科技有限公司。</p></blockquote><p>云游戏总体技术架构包括云游戏平台、网络、云游戏终端和云游戏安全四个部分：</p><ul><li><p>云游戏平台接收用户操作指令，完成游戏画面的渲染、音视频编解码和游戏推流等操作，将游戏内容以音视频流的形式通过网络传输到用户侧的终端进行呈现；</p></li><li><p>用户使用终端通过网络发送操作指令到云游戏平台进行下一步游戏画面的渲染、游戏推流等；</p></li><li><p>云游戏安全贯穿云游戏的各个环节，实现对用户信息、账号信息、游戏行为信息等的保护。</p></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-c3dafe5b6f038392293411fb97f761a26cb.png" referrerpolicy="no-referrer"></p><p>文件显示，云游戏平台应具备基于独立 GPU 的画面渲染能力，应符合 GY / T 353—2021 中的视频格式要求，<strong>应具备 1080P / 50fps 或 1080P / 60fps 的画面渲染能力</strong>，宜支持 2K / 60fps、4K / 50fps、4K / 60fps 等的画面渲染。</p><p>网络方面，根据游戏画面质量的不同，对网络的要求也有所不同，<strong>比如 1080p 50/60fps 要求下行带宽为 20Mbps。</strong></p><p><img alt="" src="https://img.ithome.com/newsuploadfiles/2023/10/a00f64bc-0450-4e75-bd10-557e31022a27.png?x-bce-process=image/format,f_avif" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-3d6e9bf9cf65cd2abe5e3c9e79a1f6eb0b9.png" referrerpolicy="no-referrer"></p><p>云游戏终端方面，文件要求设备应满足以下硬件配置要求：</p><ul><li><p>CPU：不低于 4 个处理核心，最高频率不低于 1.5GHz；</p></li><li><p>内存：至少 1GB，建议 2GB 或以上；</p></li><li><p>存储：4GB 或以上，高速 eMMC 或者 UFS。</p></li></ul><p>云游戏终端的解码时延应满足帧率为 30fps 时，解码时延在 20ms 以内；帧率为 50/60fps 时，解码时延在 10ms 以内。终端设备渲染的每一帧画面和播放的声音应严格同步，音画同步时间宜不超过 + 90ms 和-185ms，其中，正值表示声音超前于图像，负值表示声音滞后于图像。<strong>额外操作时延应不大于 150ms，宜不大于 100ms</strong>。</p><p>对未满 18 周岁的未成年人用户，云游戏平台应提供以下保护功能：</p><ul><li><p>具备对用户账号进行实名管理的能力；</p></li><li><p>具备控制未成年人使用游戏时段和时长的能力；</p></li><li><p>具备对未成年人用户游戏消费管理的能力；</p></li><li><p>具备至少通过一种方式向监护者进行提醒通知的能力。</p></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 10:53:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264148</guid>
            <link>https://www.oschina.net/news/264148</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[昆仑万维开源「天工」Skywork-13B 系列大模型，0 门槛商用]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">昆仑万维宣布开源百亿级大语言模型「天工」Skywork-13B 系列，并配套开源了 600GB、150B Tokens 的超大高质量开源中文数据集。昆仑万维「天工」Skywork-13B 系列目前包括 130 亿参数的两大模型：Skywork-13B-Base 模型、Skywork-13B-Math 模型。</span></p><p><span style="color:#000000">除模型开源外，Skywork-13B 系列大模型还将开源 600GB、150B Tokens 的高质量中文语料数据集 Skypile/Chinese-Web-Text-150B。公告称，这是目前最大的开源中文数据集之一。同时，昆仑万维「天工」Skywork-13B 系列大模型即将全面开放商用；开发者无需申请，即可商用。</span></p><p><span style="background-color:#ffffff; color:#000000">「此次 Skywork-13B 系列大模型将全面开放商用许可，用户在下载模型并同意并遵守《Skywork 模型社区许可协议》后，无需再次申请授权即可将大模型进行商业用途。希望用户能够更便捷地探索 Skywork-13B 系列大模型技术能力，探索在不同场景下的商业化应用。」</span></p><p><strong><span style="color:#000000">Skywork-13B-Base 模型</span></strong></p><blockquote><p><span style="color:#000000">Skywork-13B-Base 模型是 Skywork-13B 的基础模型，其经由 3.2 万亿个多语言高质量数据训练，在 CEVAL、CMMLU、MMLUGSM8K 等评测与基准测试上都展现了同等规模模型的最佳效果。</span></p></blockquote><p><strong><span style="color:#000000">Skywork-13B-Math 模型</span></strong>&nbsp;</p><blockquote><p><span style="color:#000000">Skywork-13B-Math 模型经过专门的数学能力强化训练，在 GSM8K 等数据集上取得了同等规模模型的最佳效果。&nbsp;</span></p></blockquote><p><strong><span style="color:#000000">Skypile/Chinese-Web-Text-150B 数据集</span></strong>&nbsp;</p><blockquote><p><span style="color:#000000">该数据集是根据昆仑天工团队方面经过精心过滤的数据处理流程从中文网页中筛选出的高质量数据。本次开源的数据集大小约为 600GB，总 token 数量约为 150B，目前开源最大的中文数据集之一。</span></p></blockquote><p>一些评测结果如下所示：</p><p><img height="232" src="https://oscimg.oschina.net/oscnet/up-1db28014754e9cdff9ea99cd4870f7d3ee1.png" width="500" referrerpolicy="no-referrer">&nbsp;</p><p><img height="267" src="https://oscimg.oschina.net/oscnet/up-091dc8c601db00caf525c2b1517e82fda18.png" width="500" referrerpolicy="no-referrer"></p><p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FQTe6pILo6jehgC7fiZBmmQ" target="_blank">查看官方公告</a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 09:50:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264137</guid>
            <link>https://www.oschina.net/news/264137</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[为什么好好的一个开源项目，商业化却往往扑街？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p>数字化产品如何做商业化？为什么有些开源项目这么优秀，商业化却老是扑街？第四期《开源漫谈》，我们邀请了<strong>王晔倞（头哥）</strong>和<strong>厉启鹏（寈峰）</strong>，一起来聊聊，<strong>开源项目的商业化变现，到底该怎么做？</strong></p><ul><li><p><strong>王晔倞（头哥）</strong>，「头哥侃码」主理人，专注分享技术、创业与产品创新等主题内容。</p></li><li><p><strong>厉启鹏（寈峰）</strong>，现为 vanus.ai CEO，曾就职于阿里云，Apache RocketMQ PMC ，长期专注于 AI 基础设施软件及中间件。</p></li></ul><p>&nbsp;</p><p><strong>头哥：</strong>技术，和一坨代码，和一个好的产品，中间没有直接的关系，只有间接的关系。所以你会发现很多人，他开源做得很好，但商业化做得很差。反过来有的人他商业化做得很好，但社区不会做。现在的 AI 类产品、大模型产品、开源产品，其实都是数字化产品。那么第一个问题来了：</p><p><strong>数字化产品如何实现商业化？中间有什么样的途径吗？</strong></p><p>&nbsp;</p><p><strong>厉启鹏：</strong>我先分享一下开源产品吧。一般来说，我们会先把产品在 Github 上开源，吸引一些人气。这样一来可以找到最初的用户，（毕竟开源是一种很好的推广方式），二来可以通过开源快速地打磨这个产品，在开发者们的帮助下让产品快速迭代，迅速成熟。第二步就是商业化了，在国内的话，像我们做基础设施类的，一般都是以 license 的方式去售卖，或者去拿一些大项目。我们之前服务过大客户，像银行、Saas 公司等等。除此之外，还有一种方式就是提供 Saas 服务，我们会先把产品托管到公有云上去，这样用户在使用产品的时候就不需要自己去部署、自己去运维，需要的时候开箱即用即可。</p><p>不过，不同的项目，它的形态和在商业化的时候需要考虑的东西是不一样的。举个例子，我们现在 vanus 产品的用户有国内的也有国外的，但以我们的经验来说，不同点在于：在国内，提供软件服务的公司，很难避免去做私有化的交付方式。在中国现在的环境下去运营一个纯产品类的公司还是蛮难的。我自己在甲方待过，也在乙方待过，我的感受就是：在中国的软件市场里边，甲方是非常强势的，软件用户的边界感也比较差，如果买了你的产品，就会希望你给他解决所有的问题，无论是软件的问题还是周边的问题，你都要给他解决。现在连我们国内用户做招聘，都要问我们给建议，我还帮我的甲方去面试过（笑~）</p><p>不过话说回来，这种方式还是有它的好处的。首先呢你会跟客户建立一个很强的连接，因为你服务时间长所以他会很信任你，很多东西都给到你，可能你会更容易拿到单子。像中国政企的一些客户，他们的项目都是一年到三年的，这对你的企业来讲，可能毛利不是特别高，但是会让你有持续的现金流输入，对整个企业的发展都有好处。</p><p>当然，这种方式的弊端也很明显。像我现在的产品服务了几十个客户，每个客户都有一个代码分支，每个客户手里的都不一样，这对于我们产品的维护、运维的压力就蛮大的。你会发现，一个软件公司到后期会越做越大，这不是说产品或生意越做越大，而是人员越来越多了，尤其是实施侧的人员、维护的人员，越来越多。这样下来，整体的毛利就会比较低，产品本身也比较割裂，最后可能形成了好几个产品而不是一个产品了。这也算是个有趣的中国特色吧。</p><p>从市场来看，中国纯产品型的软件公司还是比较少的，更多的是项目型的公司。但海外就不一样，就我们接触的经验而言，海外的客户他的边界感非常强，续费率又高，这就很有利于你费心思去打磨自己的产品。不过这种方式也有弊端，那就是他不愿意跟你建立太多的连接。我们之前想做个用户访谈，想问问产品的使用体验，看看你还有哪些场景是我们的产品可以满足的，但就遇到了困难。他可能会觉得，这个产品我用着没事，我也付钱了，你干嘛老找我？</p><p>哈哈，总之，这个市场的差别就非常明显。我个人觉得这两种形态都是比较典型的，也各有利弊，如果要做一个创业项目的话，还要结合具体的产品、不同的团队风格，甚至是创始人的风格，来做选择。</p><p>&nbsp;</p><p><strong>头哥：</strong>刚刚启鹏说的，我真是感同身受啊。我分享一下我的经验吧，我工作比较早，2001 年开始接触 Java，2004 年开始接触 IOE 架构，上海有个电视购物叫东方购物就是我们做的。七八年前，中国是没有基础软件厂商的，更早一点，十五年前，你说要找数据库，那只能想到 Oracle，这是一方面；另一方面，以神州数码为代表的基于标准厂商上面的第三方服务公司大行其道，我们的甲方之所以这么强势，就是被这些人给哄出来的。</p><p>当年我在东方购物的时候，我们一开始买了 Oracle 原厂的服务，Oracle 的工程师来这里支持，1 天 8 小时就要给 1 万块钱，加班另算。你想想 04 年 1 天 1 万是什么水平？后面我们把原厂的服务包给神州数码，服务非常好，价格还只要一半，加班不要钱。就这样，甲方慢慢地就被捧出来了。如果你不能全包，那我就不选你。反正我只是花钱解决问题，却还要我分清问题分开花钱，那我为什么不选一个全包的呢？我自己要是运维这么强还用找你吗？</p><p>所以，现在的甲方都很喜欢把项目总包给阿里云、华为云，这些云厂商都有行业解决方案架构师，他们做的都是解决方案，下面的产品都是模糊的，能用就行。内卷就是这么出来的。不过话说回来，市场没有好坏，关键是你要去适应它，而不是从自己的技术经验出发来判定这个市场好不好，这个思维要不得。</p><p>&nbsp;</p><p><strong>厉启鹏：</strong>是的，你在不同的市场做，就是要尊重不同市场的规则。像国内就是用这种方式去驱动产业的发展的。你也很难用好坏来评判一个市场。我上个月去考察了日本的市场，发现日本市场特别难进入，当时同行有个公司，花了一年的时间，才从日本市场拿了一个 80 万的单子。因为它那里有 POC 、安全认证等等的审核，门槛比较高。但是呢，一旦你吃下这个市场，你就可以一直吃下去，因为他们很少会主动更换供应商。所以，这也是日本市场的一个特点，我们要是想做的话也一样要尊重它的规则，国内国外都一样。</p><p>&nbsp;</p><p><strong>头哥：</strong>说得很好。刚刚启鹏也提到了一个点，就是国内的集成商比较厉害，<strong>很多甲方也会把你当外包看待，压根不管你的产品标不标准，还提一堆有关无关的需求，每个项目按人头算钱，对于这种现象，你怎么看呢？</strong></p><p>&nbsp;</p><p><strong>厉启鹏：</strong>我个人感觉，在国内做项目，要是想做得比较大，那对于这个项目负责人的要求还蛮高的，负责人他可能需要考虑很多方面。我见过一种情况就是，因为一个项目孵化了一个产品，通过这个产品打下了一个行业。之前有个做监控的公司就是这样，刚开始的时候是从建行做起来，后来把产品完善之后推到了浦发等等别的银行去了。那我觉得这种类型还是蛮有价值的，因为你解决的这个问题是一些通用的问题，你把它抽象成了产品，实现了规模化。</p><p>当然，更多的是头哥你说的那种，根本不管你产品如何是不是要做商业化，他只想解决他的问题。当然，这也没有问题，毕竟人家是甲方，出了钱的嘛。对于这种情况，可能这个项目负责人就要考虑下投入的问题，或者是怎么投入这个项目。一种是直接让自己的研发上，all in 到这个项目里边，还有一种方式是去找一些合作伙伴一起把这个项目拿下。甚至很多公司可能会先临时找些外包，因为你如果只要人头，那我就只给你人头，然后我只赚人头的钱。但有的公司呢可能会说对不起这人头钱我不赚，这个项目我不做了。</p><p>我觉得还是要想清楚吧，因为你要是没想清楚的话，可能会对公司的影响比较大，它会冲击你整个研发体系，甚至会影响整个产品的正常迭代。所以去做项目的时候一定要想清楚，你要做什么样的项目，你要服务什么样的用户，你要以什么样的方式去服务他。这个用户画像一定要清楚，不然就会把你的节奏带乱。</p><p>&nbsp;</p><p><strong>头哥：还有一个问题，作为我们这种普通的技术人，如何把我们手上的技术变现呢？</strong></p><p>&nbsp;</p><p><strong>厉启鹏：</strong>加入一家大公司，我觉得可能是一个比较好的方式。因为我自己的体会就是这样的，当时我在阿里做社区，看到不少小伙伴当时还在一家小公司，但是他在我们这个社区里边比较活跃，贡献了很多代码，这给他的经验、经历做了很大的加持，后来他们就都跳槽到大公司去了。从 ROI 的角度，或者从收益的角度来看，这是一个收益很高的事情，通过在知名项目做贡献，提升自己的技术和影响力，从而提升自己的职业生涯，我觉得这是最直接的一种方式。</p><p>第二种方式我也见过好多，就是技术经验丰富之后，去做咨询，或者是指导别人写代码，出书，出课程，做培训等等，也都做得挺不错的。</p><p>最后一种就是创业了，不过如果纯技术人想要创业的话，我建议你可以先加入一家创业公司，如果你能适应的话。假如公司靠谱，那之后它发达了你也就财富自由了。加入一家创业公司工作跟在大公司做一个具体细分的工作差异肯定非常大，在创业公司，假如说要做一辆劳斯莱斯，说不定得先从一辆自行车做起，然后做一辆电动车，再做一辆奥拓，再到宝马，最后才到劳斯莱斯，他是这么一个过程。绝对不是说我给你几年时间，让你做一辆劳斯莱斯，那样公司在市场上很难活下去的。但是在这种逐渐发展的过程中，对于一个技术人的技术视野，甚至商业的视野都会打开很多。我也见过很多这种人，之前是纯做技术的，加入这家创业公司之后，可能刚开始是做技术，后来他可能要做产品经理，再后来他可能是负责整个的售前，这对他个人能力就会有非常大的提升。</p><p>当然，选择也跟年龄段有关系。比如说工作五年以前的，我觉得还是可以去大厂看一看，体验一下。但是如果工作 5 年到 10 年甚至更久的时间了，我觉得加入一家创业公司还是一个不错的选择。但是，如果你说你要作为一个合伙人甚至是创始人去创办一家商业公司，那说实话我个人不是特别推荐。因为如果你是作为合伙人的身份的话，你会发现到后面你首先关注的不是技术了，而是用户，是融资，是市场，你要花很大的精力去做这些事情，可能有些人不一定喜欢。可能你后面做着做着发现自己成了一个销售，当然不是说销售不好，但他可能之前不喜欢，但是后边需要他做这个工作。所以第二个要考虑的就是，这是不是你能力范围内的事情，有些人他快速成长，快速改变，他的适应力非常强，那就没有问题。</p><p>&nbsp;</p><p><strong>头哥：最后一个问题，作为一个优秀的开源项目，如果想尝试商业化，有哪些方式呢？</strong></p><p>&nbsp;</p><p><strong>厉启鹏：</strong>如果是大厂想通过开源实现商业化的话，现在最典型的路径就是捐到基金会去，然后再通过运营社区的方式去获客。这种方式尤其适合基础软件，如果是一个特别垂类的软件倒不一定合适了。个人更赞同的一种方式是通过开源树立一个标杆，获得某些标杆企业的开发者的认可，这时候你再去复制可能就会非常快。比方说我这个软件，如果大厂采用了，那下面的二三线厂商可能也会跟进。影响力打出去了之后，再寻求付费可能就会容易一点了。</p><p>但是在当下，2023 年，这个节点，你建立一个项目去创业，那是比较难的。第一，开源商业化的路径比较长，你得先有项目，然后通过运营社区把这个影响力做起来，再出商业版去变现，那可能意味着这家创业公司要一年两年甚至三年没有收入，或者养活不了自己。在目前的这种就业环境包括投融资环境下，你能不能活两三年，这是一个非常大的问题，很多人撑不住的。</p><p>还有就是，以我自己的感受来讲，很多用户他只用开源版本，他从来就没有想过要用你的商业版，或者你出了商业版之后他就直接走了，这种就很难转化。有些开源公司做商业化成功了，并不是因为转化了开源用户为商业化用户，而是因为这个项目影响力起来之后，影响了那些不使用开源项目的用户，从而实现了商业化。这属于间接影响，你不好量化，从顶上来看的话，你都不好去制定一个考核机制，让大家知道哪些事有价值和引导他们做事。所以我觉得，如果是 Saas 的话，会比开源更能解决你打磨产品的问题，因为上面有数据，他们用了多少你看得到。</p><p>最后，我觉得开源最大的价值就是标准化，比如 Conflict ，它是构建大数据平台的一个标准，不管用它的开源还是买它的商业版，只要构建大数据平台都会想到它。其次就是开源有助于国际化，给了中国的企业出海或者是服务海外客户的一个机会，这是本土闭源的软件公司很难做到的。我一度认为开源加上云，是一个蛮好的方式，能够助力中国的软件企业成为一个服务全球的企业。以前没有云，想服务海外客户还得建本地团队，现在托 AWS 就可以了，大大节省了成本。</p><p>当然，开源好处多多，明天也很美好，现在的挑战就是看你能不能活到明天了。（笑~）</p><p>&nbsp;</p><p>本期直播回放如下，大家快扫码查看吧~</p><p><img height="355" src="https://oscimg.oschina.net/oscnet/up-41198acd2e49349768fe28449ea945e7227.png" width="385" referrerpolicy="no-referrer"></p></div></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 09:34:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6852546/blog/10139809</guid>
            <link>https://my.oschina.net/u/6852546/blog/10139809</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[谷歌承诺 20 亿美元投资 OpenAI 对手 Anthropic]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">谷歌发言人日前表示，该公司已同意向 OpenAI 的强力竞争对手 Anthropic 投资最多 20 亿美元。目前已预先投资了 5 亿美元，随着时间的推移将再追加 15 亿美元。</span></p><p><span style="color:#000000">Anthropic 成立于 2021 年，是一家由前 OpenAI 团队成员创立的人工智能初创公司。其在 ChatGPT 发布两个月后，就推出了 GPT-4 的重要竞品 Claude，并在 7 月初推出了升级版的 Claude 2。在今年上半年，Anthropic 的估值已达到了约 41 亿美元。</span></p><p><img height="277" src="https://oscimg.oschina.net/oscnet/up-cdc73121940d2bf6a2632504928ecf5790a.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">Anthropic 的一份<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2023%2F04%2F06%2Fanthropics-5b-4-year-plan-to-take-on-openai%2F" target="_blank">内部文件透露</a>，该公司计划筹集 50 亿美元或更多以直接与 OpenAI 较量。并预计花费 10 亿美元，在 2024 年底推出自己的新一代大语言模型「Claude-Next」。</span></p><p><span style="color:#000000">Anthropic 首席执行官兼联合创始人 Dario Amodei 曾在上个月的一次谈话中称，「我们只成立了两年半多一点……在这段时间里，我们已经筹集了 15 亿美元，这是一个很大的数字。我们的团队规模相较来说要小得多，但我们已经成功地保持了自己的地位。我们真正做到了少花钱多办事，我认为很快我们就能用更多的资源做更多的事。」</span></p><p><span style="color:#000000">而除谷歌外，Anthropic 还获得了 Salesforce 和 Zoom 的融资。亚马逊也已经向 Anthropic 投资了 12.5 亿美元；并在 9 月份承诺，后续计划共向 Anthropic 投资高达 40 亿美元。</span></p><p><strong><span style="color:#000000">相关阅读：</span></strong></p><ul><li><a href="https://www.oschina.net/news/232921/claude-ai" target="_blank">Anthropic 推出 「更理性的 Claude」，正面硬刚 ChatGPT</a></li><li><p style="margin-left:0px; margin-right:0px; text-align:start"><a href="https://www.oschina.net/news/263552/frontier-model-forum-ai-safety" target="_blank">OpenAI、谷歌微软等设立 1000 万美元 AI 安全基金</a></p></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 08:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264126/google-invest-2-billion-anthropic</guid>
            <link>https://www.oschina.net/news/264126/google-invest-2-billion-anthropic</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[MetaGPT 实现多智能体通信，智能体也能轻松狼人杀]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h1>概述</h1><p>狼人杀游戏是一种受欢迎的多人沟通策略游戏。在 Xu 等人所作的 《Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf》（以下简称「论文」）为题的论文中，展示了大型语言模型（LLM）在游戏中的潜力。考虑到 MetaGPT 作为一个智能体框架，我们提出了这个挑战：我们能否使用 MetaGPT 来快速复制生动的游戏体验？我们非常高兴地宣布，我们成功完成了这个挑战。</p><p>遵循论文的思路，我们成功地通过 MetaGPT 实现了狼人杀游戏智能体的开发。我们展示了以下内容：</p><ol><li>当需要构建多智能体文本游戏，其中智能体之间需要进行精细化沟通时，MetaGPT 框架是极佳选择。</li><li>MetaGPT 提供了直观和自然的抽象，当恰当地使用时，有助于将强大的功能集成到智能体中，如反思、经验学习等。</li><li>在初步实验中，通过调整反思和经验学习机制，我们观察到了智能体性能的明显提高。</li></ol><p>有关更多详细信息，将在本文中剩下部分进行探讨。完整的代码可在<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgeekan%2FMetaGPT%2Ftree%2Fwerewolf_game" target="_blank">MetaGPT 代码库</a>上获得。有关运行代码的指南，请参阅「代码运行指南」部分。关于 MetaGPT 的总体介绍，请参阅我们的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2308.00352" target="_blank">论文</a>。</p><h1>演示</h1><h2>狼人杀智能体游玩演示</h2><p>在深入实现细节探讨之前，让我们先看一下智能体在狼人杀游戏中的精彩瞬间。我们在<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwerewolf.deepwisdom.ai%2F" target="_blank">网页</a>上展示了 5 个具有代表性的游戏过程，并提供完整的 30 个运行的转录，供您探索和娱乐！</p><p>友情提示：</p><ul><li>关于这个游戏的完整介绍可以在论文中找到。游戏的设置是两个村民，一个预言家，一个女巫，一个守衞和两个狼人。我们采用了游戏社区中更为常见的规则，即当狼人屠边时（消除了所有特殊角色或所有村民），狼人就获胜。</li><li>为了促进更加精彩的游戏效果，我们在狼人智能体游玩之前引入相关策略，以此来引导他们积极地模仿特殊角色。</li><li>当然为了实验的简易化的进行：当两个狼人在晚上瞄准两个不同的玩家时，目标默认为第二个选择。</li><li>我们使用 GPT-4 进行游戏运行。</li></ul><h2>智能体的精彩瞬间</h2><p>我们观察到了各种情况，其中我们的智能体表现出逻辑甚至战略行为。以下是一些精彩瞬间：</p><h3>合作 / 共谋</h3><p>Player5（守衞）推理出预言家，分析出当晚选择守衞预言家可以最大化价值，因此守衞了 Player6。</p><p><img alt="" height="580" src="https://oscimg.oschina.net/oscnet/up-b5c5420da74986208482aa943dfb4e9edae.png" width="800" referrerpolicy="no-referrer"></p><p>Player1（狼人）控告 Player2 时，Player5（狼人）果断进行了支持。</p><p><img alt="" height="369" src="https://oscimg.oschina.net/oscnet/up-10bd3f15226c0a7313c90b0442eb76537c1.png" width="800" referrerpolicy="no-referrer"></p><h2>对抗</h2><p>当 Player1（狼人）悍跳预言家时，真正的预言家 Player4 站出来反对狼人。</p><p><img alt="" height="515" src="https://oscimg.oschina.net/oscnet/up-b96b60fe0d3de2d712af304587a84838f61.png" width="800" referrerpolicy="no-referrer"></p><h2>卖队友</h2><p>当大多数玩家对 Player3（狼人）产生怀疑时，Player6（狼人）仔细权衡了利弊，决定开始卖队友。</p><p><img alt="" height="324" src="https://oscimg.oschina.net/oscnet/up-2fd8b8dfc02bdafdc3106c485b83b6bdd22.png" width="800" referrerpolicy="no-referrer"></p><h2>观望</h2><p>当 Player2（狼人）声称自己是预言家时，Player6（村民）从过去的经验中吸取了教训，保留了自己的判断，要求在采取立场之前进行更多的观察。</p><p><img alt="" height="509" src="https://oscimg.oschina.net/oscnet/up-86bc6d3fa8dcc23a474a833b5e7b47a313e.png" width="800" referrerpolicy="no-referrer"></p><p>Player5（女巫）通过分析选择留药。</p><p><img alt="" height="413" src="https://oscimg.oschina.net/oscnet/up-272b1ba2775ddb394a1a2837d4623c7ee6c.png" width="800" referrerpolicy="no-referrer"></p><h2>复杂推理</h2><p>基于先前对 Player2 是狼人的判断，Player6（村民）分析了票面等盘面情况，清楚地区分了 Player3（狼人）和其他玩家。</p><p><img alt="" height="548" src="https://oscimg.oschina.net/oscnet/up-ccaaf1318b54858160f79c4021a2b2bd6fb.png" width="800" referrerpolicy="no-referrer"></p><p>Player6（预言家）准确地通过反思辨别了每个玩家的角色，并由于其对经过验证的村民的敌意检测到了狼人。</p><p><img alt="" height="559" src="https://oscimg.oschina.net/oscnet/up-a5cded422bedf58bedab8bde0924bf33dda.png" width="800" referrerpolicy="no-referrer"></p><h1>实施方案</h1><h2>多智能体通信</h2><p>实现狼人游戏的一个重要元素在于促进智能体之间的精确、细粒度的通信。让我们考虑三种类型的消息：</p><ol><li>从主持人发送给预言家的私聊中，通知预言家或其他有关特定玩家的身份（一对一）。</li><li>从一名狼人发送的私聊中，通知狼人伙伴和主持人所选择的袭击目标（一对多）。</li><li>从主持人发出的公开消息，指示所有玩家醒来（一对所有）。</li></ol><p>MetaGPT 支持所有三种通信，这要归功于关键的抽象概念：<code>Environment</code>（环境）和 <code>Message</code>（消息），以及 agent’s（智能体角色）通过 <code>_publish</code>（发布）和 <code>_observe</code>（观察）两个函数来作为处理消息的方法。每当一个 agents 发布一个 <code>Message</code>，它都会将 <code>Message</code><code>_publish</code> 到 <code>Environment</code> 中。反过来，接收 <code>Message</code> 的 agents 会从 <code>Environment</code> 中_observe <code>Message</code>。而我们需要做的就是填充 <code>Message</code> 属性，如 send_to 和 restricted_to，包括预期的接收者（agents）。然后 MetaGPT 会处理剩下的工作。</p><p>综合考虑，我们建立了一个复杂的智能体之间<strong>通信拓扑结构</strong>。有关详细的实施信息，请随时查看我们的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fgeekan%2FMetaGPT%2Ftree%2Fwerewolf_game" target="_blank">代码</a>。我们正在积极努力完善这一机制，并将很快发布一个全面的指南。</p><p><img alt="" height="500" src="https://oscimg.oschina.net/oscnet/up-b7bf83b4832178c7cdbb4e5b0a39824e732.png" width="800" referrerpolicy="no-referrer"></p><h2>智能体能力</h2><p>在论文中，生成最终响应需要多个组件（下图）。在这一部分，我们展示了从智能体的角度出发，构建包含所有这些组件的高效智能体是非常直接和简单的。</p><p><img alt="" height="1378" src="https://oscimg.oschina.net/oscnet/up-b6d94f144c2238dbb480dcb2f9c5f90a25b.png" width="800" referrerpolicy="no-referrer"></p><p>我们采用的方法是利用 MetaGPT 的 <code>Role</code>抽象来定义一个智能体，然后为其配备适当的 <code>Action</code>（动作）。我们定义 <code>Speak</code>和 <code>NighttimeWhisper</code> 作为返回响应的最终 <code>Action</code>（动作）。关于每个准备组件发送到最终响应生成方式，如论文中所概述，请参见下表了解各自的实现。</p><p><img alt="" height="247" src="https://oscimg.oschina.net/oscnet/up-a191e5e9247b78baf9ce767ec61e7442877.png" width="1120" referrerpolicy="no-referrer"></p><p>我们将所有这些 <code>Action</code>（行动） 组合在 <code>Role</code>’s（智能体角色） 的 <code>_observe</code>（观察）、 <code>_think</code>（思考） 和 <code>_act</code>（行动） 中，从而形成了一个清晰的智能体思考和行动流程（下图）。此外，流程中的每个步骤都被模块化，意味着在其他游戏中更容易重用。<strong>通过这种方式，我们构建了一个拥有各种能力的智能体，能够进行复杂的推理和言辞表达。</strong></p><p><strong><img alt="" height="1130" src="https://oscimg.oschina.net/oscnet/up-f465474544ce499d9f77b8b677866113ec7.png" width="800" referrerpolicy="no-referrer"></strong></p><h1>关于新方法的实验</h1><p>在遵循论文的主要程序的同时，我们基于试错的方式修改了反思和经验学习组件的内部工作方式。我们修订后的方法是：</p><ol><li>让智能体展示它游戏过程中反思的状态，并以结构化的方式用语言进行总结。</li><li>记录一个包含四个元素的元组（反思，静态动作指令，来自反思和指令的动作，游戏的最终结果），作为一种经验，并积累成一个经验池。</li><li>当智能体下次遇到相似的情况时，提供相关的过去经验。这里的相似性是根据反思嵌入的语义接近度来定义的。通过回顾类似的经验，智能体如果在过去因为某些操作失败了，将会改变他们的操作；如果成功了，将会增强他们对这一步操作的信心。</li></ol><p>在实践中，我们发现这种方法相当有效。遵循论文的实验设置，我们进行了 30 轮的实验。在前 10 轮中，村民方没有过去的经验；在第 11 轮到第 20 轮中，村民方可以接触到前 10 轮的经验；在第 21 轮到第 30 轮中，村民方可以接触到前 20 轮的经验。当然，本次实验中，我们停止了狼人获得经验的能力，以此更好地来观察村民方的结果。</p><p>下面是性能提升的图表。随着经验的积累，村民方对抗狼人的胜率逐渐增加。我们还检查了村民在识别狼人方面的平均投票准确率。上升的趋势表明，当村民拥有经验时，他们的判断更为准确，从而证实了他们提高的胜率不仅仅是偶然事件的结果。此外，由于投票准确率还取决于投票的难度，因此我们还评估了在一个固定情景下的准确率：在一组 6 名幸存玩家中识别 2 名狼人，这通常是在投票的第一天面临的情况。这一趋势与平均投票准确率的趋势相吻合。</p><p><img alt="" height="358" src="https://oscimg.oschina.net/oscnet/up-8ffd401a7790a720a5fbaea0c72ad900e50.png" width="565" referrerpolicy="no-referrer"></p><h1>代码运行指南</h1><pre><code class="language-python">python examples/werewolf_game/start_game.py # use default arguments</code></pre><pre><code class="language-python">python examples/werewolf_game/start_game.py \\\\
    --use_reflection True \\\\
    --use_experience False \\\\
    --use_memory_selection False \\\\
    --new_experience_version "01-10" \\\\
    --add_human False

# use_reflection: switch to False to disable reflection, this can reduce token costs 
# use_experience: switch to True to supply agents with experience, this requires recording experiences first
# use_memory_selection: switch to True to select only recent and informative messages from memory
# new_experience_version: specify a version to record the current run as experience 
# add_human: switch to True to participate in the game</code></pre><p>我们建议使用 GPT-4 运行代码。平均而言，如果不使用反思，每次运行大约需要 1.5 美元，如果使用反思，则需要 4 美元，如果使用反思和经验学习，则需要 7 美元。</p><h1>致谢</h1><p>这是由来自 MetaGPT 社区的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fmannaandpoem" target="_blank">mannaandpoem</a>、<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdavidlee21" target="_blank">davidlee21</a>和<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fariafyy" target="_blank">ariayyy</a>作为核心贡献者合作努力的成果。当然，我们还要十分感谢<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2F1766left" target="_blank">Elfe</a>、<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fchaleeluo" target="_blank">chaleeluo</a>、<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkevin-meng" target="_blank">kevin-meng</a>和<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Flinkedin.com%2Fin%2Fshutian-xiao-b29649241" target="_blank">Shutian</a>也提供了宝贵的见解。我们对他们的奉献心存感激。我们热烈邀请更多社区成员加入并为我们的 MetaGPT 项目做出贡献！</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 07:52:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264108</guid>
            <link>https://www.oschina.net/news/264108</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[百川智能发布 Baichuan2-192K 大模型，上下文窗口全球最长]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>10 月 30 日，百川智能发布<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FlAJh6qGG27u_qCl0kI-0lA" target="_blank">Baichuan2-192K 大模型</a></u>，其上下文窗口长度高达 192K，是目前全球最长的上下文窗口。</p><blockquote><p>上下文窗口长度是大模型的核心技术之一，通过更大的上下文窗口，模型能够结合更多上下文内容获得更丰富的语义信息，更好的捕捉上下文的相关性、消除歧义，进而更加准确、流畅的生成内容，提升模型能力。</p></blockquote><p>据介绍，<strong>Baichuan2-192K 能够处理约 35 万个汉字</strong>，是目前支持长上下文窗口最优秀大模型 Claude2（支持 100K 上下文窗口，实测约 8 万字）的 4.4 倍，更是 GPT-4（支持 32K 上下文窗口，实测约 2.5 万字）的 14 倍。Baichuan2-192K 不仅在上下文窗口长度上超越 Claude2，在长窗口文本生成质量、长上下文理解以及长文本问答、摘要等方面的表现也全面领先 Claude2。</p><p><img height="708" src="https://static.oschina.net/uploads/space/2023/1030/143754_9Lc3_2720166.png" width="1280" referrerpolicy="no-referrer"></p><p>Baichuan2-192K 在 Dureader、NarrativeQA、LSHT、TriviaQA 等 10 项中英文长文本问答、摘要的评测集上表现优异，有 7 项取得 SOTA，显著超过其他长窗口模型。</p><p><img src="https://static.oschina.net/uploads/space/2023/1030/143926_8N50_2720166.png" referrerpolicy="no-referrer"></p><p>此外，LongEval 的评测结果显示，在窗口长度超过 100K 后 Baichuan2-192K 依然能够保持非常强劲的性能，而其他开源或者商用模型在窗口长度增长后效果都出现了近乎直线下降的情况。Claude2 也不例外，在窗口长度超过 80K 后整体效果下降非常严重。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e09d70b43d3d3782cda3982ae4d742587e1.png" referrerpolicy="no-referrer"></p><p>今年 9 月 25 日，百川智能已开放了 Baichuan2 的 API 接口，正式进军企业级市场，开启商业化进程。<strong>此次 Baichuan2-192K 将以 API 调用和私有化部署的方式提供给企业用户</strong>，目前百川智能已经启动 Baichuan2-192K 的 API 内测，开放给法律、媒体、金融等行业的核心合作伙伴。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 06:38:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264086</guid>
            <link>https://www.oschina.net/news/264086</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[华为申请注册「遥遥领先」商标]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>近日，华为技术有限公司申请注册「遥遥领先」商标，国际分类为运输工具、科学仪器，当前商标状态为等待实质审查。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-2632d0767a23ab6a185920e5193ba46340f.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-84f3e8ecf2f2e09c7cf0b5e8bafc4e67698.png" referrerpolicy="no-referrer"></p><p>最近因华为 Mate60 系列手机发售，「遥遥领先」成为网络热词。</p><p>「遥遥领先」一词最先是出现在华为手机 Mate40 的发布会上，余承东在介绍手机的处理器、屏幕、电池、充电、摄像头、音质等状况时，曾经说了 14 次「遥遥领先」。去年的 Mate50 发布，全球首发了衞星通信功能，余承东再次提及「遥遥领先」，并称其为捅破天的技术，又将「遥遥领先」的热度推高。</p><p>随后，华为的粉丝也经常在华为发布会上喊「遥遥领先」为华为加油。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-c2112b8f18f699b1b8299410458e1811c69.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 06:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264078</guid>
            <link>https://www.oschina.net/news/264078</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[📙《高并发的哲学原理》纸质版书稿完全开源，共 16 万多字]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><img alt="" height="600" src="https://oscimg.oschina.net/oscnet/up-1fb8b3621b91010af5c26e565583383ab83.png" width="1762" referrerpolicy="no-referrer"></p><p><strong>阅读地址：</strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpphc.lvwenhan.com" target="_blank">https://pphc.lvwenhan.com</a></p><p style="color:#24292f; text-align:start"><strong>pdf 下载链接在网站右上角。</strong></p><h3>写作目标</h3><p style="color:#2c3e50; text-align:start">本书的目标是在作者有限的认知范围内，讨论一下高并发问题背后隐藏的一个哲学原理——找出单点，进行拆分。</p><h3>内容梗概</h3><p style="color:#2c3e50; text-align:start">我们将从动静分离讲起，一步步深入 Apache、Nginx、epoll、虚拟机、k8s、异步非阻塞、协程、应用网关、L4/L7 负载均衡器、路由器 (网关)、交换机、LVS、软件定义网络 (SDN)、Keepalived、DPDK、ECMP、全冗余架构、用户态网卡、集中式存储、分布式存储、PCIe 5.0、全村的希望 CXL、InnoDB 三级索引、内存缓存、KV 数据库、列存储、内存数据库、Shared-Nothing、计算存储分离、Paxos、微服务架构、削峰、基于地理位置拆分、高可用等等等等。并最终基于地球和人类社会的基本属性，设计出可以服务地球全体人类的高并发架构。</p><p style="color:#2c3e50; text-align:start"><br> 全书共 167674 字。</p><h3>读者评价</h3><blockquote><p>会上一谈到架构和 I/O，我都想到你的文章。主讲解答清楚和没解答清楚的，都没你的文章清楚。</p><p>—— 秋收，于 RubyConf 2023</p></blockquote><hr><blockquote><p>像看小说一样把文章都看完了，全程无尿点，作者的脑袋是在哪里开过光，知识储备竟如此扎实</p><p>—— 观东山</p></blockquote><hr><blockquote><p>非常棒的技术分享！深入浅出，娓娓道来，让我想起了那本 csapp。</p><p>—— drhrchen</p></blockquote><hr><blockquote><p>写得真好，膜拜！作者愿意出书吗，一定买！</p><p>—— bean</p></blockquote><hr><blockquote><p>拜读了！应该算是架构顶级总结！！</p><p>—— 雨山前</p></blockquote><hr><blockquote><p>看完了，博主好厉害，学习到了各种骚技巧，和知识，膜拜</p><p>—— evanxian</p></blockquote><hr><blockquote><p>写的太好了，不仅充满了理工科的严谨较真，也充满了文科的浪漫</p><p>—— 一秒</p></blockquote><hr><blockquote><p>写得很好，视角也是我喜欢的，站在地球表面，述事宏大，思维自信。</p><p>—— 纳秒时光</p></blockquote><hr><blockquote><p>全部看完，博主太强了，很受启发</p><p>—— Bruce</p></blockquote><hr><blockquote><p>棒</p><p>—— JuniaWonter</p></blockquote><h2>作者信息</h2><h3>吕文翰</h3><ol><li>GitHub：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjohnlui" target="_blank">johnlui</a></li><li>职位：住范儿创始成员，CTO，监事</li></ol><h4>高并发系统处理经验</h4><ol><li>2017 年维护的单体 CMS 系统顶住了每日两百万 PV 的压力</li><li>2020 年优化一个单机 PHP 商城顶住了 QPS 1000+ 的压力</li><li>2021 年设计的分布式电商秒杀系统在实际业务中跑到了最高一分钟 GMV 500 万，QPS 10000+</li></ol><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3e128c2c71f79a9f5c551fa204024a7d6d1.jpg" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p>&nbsp;</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-c1a9c97d554a8c227a53037467944206a7c.jpg" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-ce3364f3899525cd6ffc33de9d05faf6e73.jpg" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-03c79aa58b98f1f7f757bc4647cc40a81b0.jpg" referrerpolicy="no-referrer"></p><p><img alt="" height="1818" src="https://oscimg.oschina.net/oscnet/up-bbca5aa6d8d33372882757c429ad5a13815.jpg" width="3320" referrerpolicy="no-referrer"></p><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 05:27:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264073</guid>
            <link>https://www.oschina.net/news/264073</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
