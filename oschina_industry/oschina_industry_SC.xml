<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Tue, 12 Dec 2023 09:34:49 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[FastUI —— 更快地构建更好的 UI]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 是一种构建由声明式 Python 代码来构建 Web 应用程序用户界面的新方法。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>这意味着：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><strong>如果你是一名 Python 开发人员</strong>，可以使用 React 构建响应式 Web 应用程序，而无需编写任何 JavaScript 代码，也无需接触<code>npm</code>。</li><li><strong>如果你是前端开发人员</strong>，可以专注于构建真正可重用的神奇组件，无需为每个视图复制粘贴组件。</li><li><strong>对于每个人来说&nbsp;</strong>—— 真正的关注点分离，后端定义了整个应用程序；而前端可以自由地仅实现用户界面</li></ul><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 的核心是一组匹配的&nbsp;<a href="https://docs.pydantic.dev/">Pydantic</a>&nbsp;模型和 TypeScript </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>interfaces<span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>，允许你定义用户界面。其在构建时由 TypeScript 和 Pyright/mypy 进行验证，并在运行时由 Pydantic 进行验证。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 由 4 部分组成：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><a href="https://pypi.python.org/pypi/fastui"><code>fastui</code>PyPI 包</a>— UI 组件的 Pydantic 模型和一些实用程序。虽然它与<a href="https://fastapi.tiangolo.com/">FastAPI</a>配合良好，但它不依赖于 FastAPI，并且其中大部分可以与任何 Python Web 框架一起使用。</li><li><a href="https://www.npmjs.com/package/@pydantic/fastui"><code>@pydantic/fastui</code>npm 包</a>— 一个 React TypeScript 包，让你在实现自己的组件时重用 FastUI 的机制和类型</li><li><a href="https://www.npmjs.com/package/@pydantic/fastui-bootstrap"><code>@pydantic/fastui-bootstrap</code>npm 包</a> — 使用&nbsp;<a href="https://getbootstrap.com/">Bootstrap</a>&nbsp;实现/定制所有 FastUI 组件</li><li><a href="https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt"><code>@pydantic/fastui-prebuilt</code>npm 包</a>（在&nbsp;<a href="https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt">jsdelivr.com CDN</a>&nbsp;上提供）提供了 FastUI React 应用程序的预构建版本，因此你无需安装任何 npm 包或自行构建任何内容即可使用它。Python 包提供了一个简单的 HTML 页面来服务此应用程序。</li></ul><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>以下是一个简单但完整的 FastAPI 应用程序，它使用 FastUI 来显示一些用户配置文件：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><pre><code>from datetime import date

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastui import FastUI, AnyComponent, prebuilt_html, components as c
from fastui.components.display import DisplayMode, DisplayLookup
from fastui.events import GoToEvent, BackEvent
from pydantic import BaseModel, Field

app = FastAPI()


class User(BaseModel):
    id: int
    name: str
    dob: date = Field(title='Date of Birth')


# define some users
users = [
    User(id=1, name='John', dob=date(1990, 1, 1)),
    User(id=2, name='Jack', dob=date(1991, 1, 1)),
    User(id=3, name='Jill', dob=date(1992, 1, 1)),
    User(id=4, name='Jane', dob=date(1993, 1, 1)),
]


@app.get("/api/", response_model=FastUI, response_model_exclude_none=True)
def users_table() -&gt; list[AnyComponent]:
    """
    Show a table of four users, `/api` is the endpoint the frontend will connect to
    when a user fixes `/` to fetch components to render.
    """
    return [
        c.Page(  # Page provides a basic container for components
            components=[
                c.Heading(text='Users', level=2),  # renders `&lt;h2&gt;Users&lt;/h2&gt;`
                c.Table[User](  # c.Table is a generic component parameterized with the model used for rows
                    data=users,
                    # define two columns for the table
                    columns=[
                        # the first is the users, name rendered as a link to their profile
                        DisplayLookup(field='name', on_click=GoToEvent(url='/user/{id}/')),
                        # the second is the date of birth, rendered as a date
                        DisplayLookup(field='dob', mode=DisplayMode.date),
                    ],
                ),
            ]
        ),
    ]


@app.get("/api/user/{user_id}/", response_model=FastUI, response_model_exclude_none=True)
def user_profile(user_id: int) -&gt; list[AnyComponent]:
    """
    User profile page, the frontend will fetch this when the user visits `/user/{id}/`.
    """
    try:
        user = next(u for u in users if u.id == user_id)
    except StopIteration:
        raise HTTPException(status_code=404, detail="User not found")
    return [
        c.Page(
            components=[
                c.Heading(text=user.name, level=2),
                c.Link(components=[c.Text(text='Back')], on_click=BackEvent()),
                c.Details(data=user),
            ]
        ),
    ]


@app.get('/{path:path}')
async def html_landing() -&gt; HTMLResponse:
    """Simple HTML page which serves the React app, comes last as it matches all paths."""
    return HTMLResponse(prebuilt_html(title='FastUI Demo'))</code></pre></div>
                                                                ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:29:45 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/fastui</guid>
            <link>https://www.oschina.net/p/fastui</link>
        </item>
        <item>
            <title>
                <![CDATA[🎁有奖问答 | 聊聊 NGINX 向云原生演进那点儿事]]>
            </title>
            <description>
                <![CDATA[<h1 class="header article-title"><a href="https://www.oschina.net/question/4700705_2331501">高手问答第 311 期 —— 聊聊 NGINX 向云原生演进那点儿事</a><div class="ui red label horizontal" data-tooltip="置顶">顶</div></h1><div class="extra ui horizontal list meta-wrap"><div class="item"><a href="https://my.oschina.net/u/4700705" class="__user"><span>小白兔爱吃大灰狼</span></a> 发布于，今天 11:35
                    </div><div class="item">阅读 42</div><div class="item collect-btn " data-id="2331501" data-user-id="4700705" data-obj-type="2" data-max="99" data-tag-required="" data-current-user-id="" data-recommend-tags=""><i class="star outline icon"></i> 收藏 <span data-collect-count="" data-id="2331501" data-obj-type="2">0</span></div><div class="item comment-count"><a href="https://www.oschina.net/question/4700705_2331501#comments" class="normal"><i class="comment outline icon"></i> 答案 <span data-article-reply-count="">0</span></a></div></div><div class="tags"><a class="ui horizontal label" href="https://www.oschina.net/question/topic/masteronline" target="_blank"><img src="https://static.oschina.net/uploads/logo/masteronline_9WTeU.png" referrerpolicy="no-referrer">高手问答</a></div><div class="content" id="articleContent"><p><span><span>据 Gartner 预测，到 2025 年，云原生架构将成为超过 95% 的新数字计划基础，高于 2021 年的不到 40%，云原生架构市场占有率不断提高。而如今，全球半数以上（55%） 的网站都基于 NGINX 运行，差不多相同比例 (53.7%) 的中国网站在 NGINX 开源版上运行。而 NGINX 存在难于动态配置、管理功能影响业务等问题，为了解决这些问题，OpenNJet 由此诞生。</span></span></p><p><span><span>OpenNJet 基于 NGINX1.19 基础 fork 并独立演进，具有高性能、稳定、易扩展的特点，通过数据面与控制面的隔离，能够在不重启进程的情况下基于动态配置能力进行配置的实时更新。最近还推出了 OpenNJet K8s Ingress Controller 1.0，基于 OpenNJet 的动态特性、高性能实现，弥补了 NGINX 在云原生场景中不足，而且提供了丰富的流量管理功能，如动态 location、host/path 路由、负载均衡、动态 upstream、金丝雀发布、SNI 等。</span></span></p><p><strong><span><span>OSCHINA 本期高手问答（12 月 13 日 - 12 月 19 日）我们请来了嘉宾<a href="https://my.oschina.net/u/6606114" rel="nofollow">单雷老师</a>和大家一起聊聊 NGINX 向云原生演进那点儿事。</span></span></strong></p><p><strong><span><span>可讨论的问题包括但不限于</span></span></strong><strong><span><span>：</span></span></strong></p><ul><li><span><span style="background-color:white"><span>OpenNJet 和 NGINX 是什么关系？</span></span></span></li><li><span><span style="background-color:white"><span>什么是云原生应用引擎？OpenNJet 的有哪些优势</span></span></span></li><li><span><span style="background-color:white"><span>我们如何解决数据面控制面隔离、国密、动态配置等问题？</span></span></span></li><li><span><span style="background-color:white"><span>读 NGINX/OpenNJet 源码的建议</span></span></span></li><li><span><span style="background-color:white"><span>如何上手开发一个开源项目？</span></span></span></li></ul><p><span><span style="background-color:white"><span>其他关于 NGINX、OpenNJet 的更多内容，也欢迎积极提问。</span></span></span></p><h2><span><span style="background-color:white"><span><strong>嘉宾介绍</strong></span></span></span></h2><p><img alt="" height="534" src="https://oscimg.oschina.net/oscnet/up-774dc1b75df829000896339c602574ff319.jpg" width="400" referrerpolicy="no-referrer"></p><p><span><span><strong><span><span style="color:#7030a0">通明智云产品总监，单雷</span></span></strong></span></span></p><p><span><span>20 年的 IT 行业经验，精通云原生以及高性能应用引擎技术。曾在亚信科技历任研发主管、首席架构师等职务，并主导多个云原生、高性能应用网关项目的设计开发工作，现任公司应用引擎产品总监。</span></span></p><hr><p><span><span style="background-color:white"><span><span>🎁</span> 为了鼓励踊跃提问，下一代云原生应用引擎 OpenNJet 开源社区会在问答结束后从提问者中抽取 5 名幸运会员，赠予精美棉马甲一件。</span></span></span></p><p><img alt="" height="436" src="https://oscimg.oschina.net/oscnet/up-6f9dfb1df3b4d3c9f22f9a02a21c1be62d5.jpg" width="400" referrerpolicy="no-referrer"></p><blockquote><p><span><span>OpenNJet&nbsp;应用引擎是基于 NGINX 的面向互联网和<strong>云原生</strong>应用提供的运行时组态服务程序，作为底层引擎，OpenNJet 实现了 NGINX 云原生功能增强、安全加固和代码重构，利用<strong>动态加载机制</strong>可以实现不同的产品形态，如 Web 服务器、流媒体服务器、负载均衡、代理 (Proxy)、应用中间件、API 网关、消息队列等产品形态等等。OpenNJet 在云原生架构中作为数据平面，除了提供南北向通信网关的功能以外，还提供了服务网格中东西向通信能力。在原有功能基础上增加了透明流量劫持、熔断、遥测与故障注入等新功能特性。</span></span></p><p><span><span>Gitee：<a href="https://gitee.com/njet-rd/njet" rel="nofollow"><span><span>https://gitee.com/njet-rd/njet</span></span></a></span></span></p><p><span><span>官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnjet.org.cn%2F" rel="nofollow" target="_blank">https://njet.org.cn/</a></span></span></p></blockquote><p><span style="background-color:#ffffff; color:#27ae60">OSChina 高手问答一贯的风格，不欢迎任何与主题无关的讨论和喷子。</span></p><p>下面欢迎大家就 「<span><span>NGINX 向云原生演进</span></span>」<span><span>&nbsp;</span>相关</span>问题向<span>&nbsp;<a href="https://my.oschina.net/u/6606114" rel="nofollow">单雷老师</a></span><a href="https://my.oschina.net/klblog" rel="nofollow"><strong><span style="color:#000000">&nbsp;</span></strong></a>提问，直接回帖提问既可。</p></div><div class="poll-wrap"></div><div class="additional-remarks"></div><div class="ui basic center aligned segment action"><div class="ui big buttons"><a class="ui basic button collect-btn hover" data-id="2331501" data-user-id="4700705" data-obj-type="2" data-max="99" data-tag-required="" data-current-user-id="" data-recommend-tags=""><i class="star outline icon"></i>收藏 (<span data-collect-count="" data-id="2331501" data-obj-type="2">0</span>)</a><div class="ui basic dropdown share button osc-share dropdown-share" data-tag="share-question"><i class="share icon"></i><span>分享</span><div class="menu"><a class="item" data-platform="weibo" data-value="weibo"><i class="weibo icon"></i>微博</a><a class="item" data-platform="qq" data-value="qq"><i class="qq icon"></i>QQ</a><a class="item" data-platform="wechat" data-value="wechat"><i class="weixin icon"></i>微信</a></div></div></div><div class="ui basic segment"><a class="ban" ban-report="" data-id="2331501" data-obj-type="2" data-url="https://www.oschina.net/question/4700705_2331501"><i class="flag red icon"></i>举报</a></div></div>
            ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:29:45 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/question/4700705_2331501</guid>
            <link>https://www.oschina.net/question/4700705_2331501</link>
        </item>
        <item>
            <title>
                <![CDATA[新技术 LINT 可强制 LLM 回答有毒问题]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">美国普渡大学（Purdue University）的研究人员发布了，一篇名为《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2312.04782" target="_blank"><span style="background-color:#ffffff">Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs</span></a>》的论文。描述了他们通过利用大模型厂商倾向于公开与提示响应相关的概率数据的特点，设计出了一种可以打破现有 LLM 规则，使其给出「有害」回答的新颖方法。</span></p><p><span style="color:#000000">研究人员使用了一种名为 LINT (LLM interrogation) 的技术，与绕过安全提示的一般越狱方式不同的是，这种技术更具强制性。它涉及到对概率值（logits）或软标签（soft labels）的理解 —— 从统计学上讲，其可以有效区分安全响应与有害响应。</span></p><blockquote><p><span style="color:#000000">「在我们的研究中，我们揭示了当不良行为者可以访问模型的输出 logits 时，LLM alignment 面临的的一种新威胁，这是开源 LLM 和许多商业 LLM API（如某些 GPT 模型）的共同特征。它并不依赖于制作特定的提示。相反，它利用了这样一个事实：即使 LLM 拒绝了有毒请求，有害响应也往往隐藏在输出 logits 的深处。通过在自动回归生成过程中在几个关键输出位置强行选择排名较低的输出&nbsp;tokens，我们可以迫使模型揭示这些隐藏的响应。我们将这一过程称为模型审讯（model interrogation）。」</span></p></blockquote><p><span style="color:#000000">具体<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theregister.com%2F2023%2F12%2F11%2Fchatbot_models_harmful_content%2F" target="_blank">表现为</a>，LINT 使用 LLM-based classifier 向 LLM 提出一个有害的问题：比如"说明如何将枪改装成全自动的"。然后，对响应中的前九个 tokens（如 "It's"、"It"、"We"和 "I"）进行排序，并用这些词组创建新句子，这样该 LLM 就会不断生成回复。</span></p><p><span style="color:#000000"><img alt="" height="263" src="https://oscimg.oschina.net/oscnet/up-a4cca4832d0d4ac8918b03eacf880f3455b.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">研究人员在创建 LINT 原型时，在一个包含 50 个有毒问题的数据集上询问了 7 个开源 LLM 和 3 个商业 LLM。他们称：「当模型只被审讯一次时，ASR（攻击成功率）达到 92%，当被询问五次时，ASR 可以达到达到 98%。」</span></p><p><span style="color:#000000">这种方法与越狱方法不同，但性能要<span style="background-color:#ffffff">远远优于目前最先进的两种越狱技术：GCG 和 GPTFuzzer。</span>相比之下越狱方法的 ASR 仅为 62%，且运行时间要长&nbsp;10 到 20 倍。「通过我们的方法揭露的有害内容更加相关、完整、清晰。此外，它可以补充越狱策略，从而进一步提高攻击性能。」</span></p><p><span style="color:#000000">更重要的是，这种技术甚至适用于根据特定任务（如代码生成）的基础模型定制的 LLM。研究人员还声称，这种技术可以用来损害隐私和安全，迫使模型公开电子邮件地址和猜测弱密码。</span></p><p><span style="color:#000000">因此，研究人员警告称，AI&nbsp;界在考虑是否开源 LLM 时应谨慎；并建议最好的解决方案是确保有毒内容被清除，而不是将其隐藏起来。</span></p><p><span style="color:#000000">更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2312.04782" target="_blank">查看完整论文</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:24:45 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270686/lint-llm-harmful-content</guid>
            <link>https://www.oschina.net/news/270686/lint-llm-harmful-content</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[铠侠向 Linux 基金会捐赠 Software-Enabled Flash SDK]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#121212">几年前从东芝分离出来的存储公司 Kioxia（</span>铠侠<span style="background-color:#ffffff; color:#121212">）向 Linux 基金会捐赠了一个软件开发工具包 (SDK)，用于建立 Software-Enabled Flash SDK。</span></p><p><span style="background-color:#ffffff; color:#121212">Linux 基金会发布<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linuxfoundation.org%2Fpress%2Fsoftware-enabled-flash-announces-software-development-kit-sdk" target="_blank">公告称</a>，「SEF SDK 的发布是存储技术领域的一个重要里程碑......SEF 项目对 KIOXIA 突破性地捐赠软件定义闪存原生 SDK 表示热烈欢迎，这将为开发人员提供前所未有的能力，使他们能够为闪存存储（flash storage）应用开发定制的独特软件。」</span></p><p><img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-67690b065c2207474d1a67124aa3ef403da.png" width="300" referrerpolicy="no-referrer">&nbsp; &nbsp;<img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-1056c78ed4258dcb84497a6e896204821c0.jpg" width="300" referrerpolicy="no-referrer"></p><p>该 SEF SDK 包括示例代码和文档，以充分利用 flash media control 的潜力；包括 WAF 减少、延迟控制、对 ZNS 和 FDP 或 Block 等多种协议的支持等。</p><p>SEF 项目旨在通过加强对驱动器的管理、增强工作负载隔离、加强延迟控制以及实现对闪存管理的更多&nbsp;host-control，在现代数据中心中开辟新的用途并最大限度地发挥基于闪存的存储潜力。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270608/software-enabled-flash-sdk</guid>
            <link>https://www.oschina.net/news/270608/software-enabled-flash-sdk</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[芯瞳正式加入 openKylin，为社区贡献高质量的国产 GPU 解决方案！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>近日，芯瞳半导体技术（山东）有限公司（以下简称「芯瞳」），签署 openKylin 社区 CLA（Contributor License Agreement 贡献者许可协议），正式加入 openKylin 开源社区。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="1079" src="https://oscimg.oschina.net/oscnet/up-4c9b13fca5452f4a217f1494d816e96a799.png" width="829" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>芯瞳（Sietium）成立于 2019 年，是一家自主设计研发 GPU 芯片及 GPU 解决方案的高科技公司，以行业先进的计算和图形渲染平台为依托，用高质量的产品和服务为云端、终端客户提供可持续发展的国产 GPU 解决方案；为数字时代的创新与发展提供算力支撑，构建自由算力的文明世界。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="410" src="https://oscimg.oschina.net/oscnet/up-6914c94ad47861f5f685cb96e9bc21450f1.png" width="940" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span><strong><span>加入 openKylin 社区后，芯瞳将参与维护社区 GPU SIG 和 Wayland SIG</span></strong><span>。<strong>凭借其自研的 GPU 显卡和深厚的行业经验，优化 openKylin 环境中显卡驱动的兼容性，确保与芯瞳显卡的完美适配</strong>。</span></span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>在 openKylin 平台上，芯瞳显卡将展现其在图形显示、渲染、视频编解码和大规模计算等方面的优势，以此提升 openKylin 的用户体验，并提供持续的 GPU 产品升级和技术支持，为用户提供安全可靠的使用体验。具体计划如下：</span></p><ul><li><p style="margin-left:0; margin-right:0"><span>积极参与社区合作，紧密关注社区的发展动态，与社区成员携手推动 openKylin 社区的生态及品牌建设，努力构建一个健康的生态环境，为开源生态的发展贡献力量。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>寻求与社区的技术合作，通过联合调试等方式，使 openKylin 的相关产品能更好地兼容并适应芯瞳的全新系列显卡，从而提高产品的稳定性和性能。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>在应用层面，芯瞳将持续优化软件算法，提高系统效率，充分发掘 openKylin 在芯瞳显卡平台上的性能潜力，从而提升整体性能，为用户提供卓越的产品体验。</span></p></li></ul><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>通过这一系列的举措，芯瞳将与 openKylin 社区并肩前行，共同推动 openKylin 社区生态良好发展，为用户带来更多的创新和惊喜。同时，芯瞳期待与社区成员进行深入的交流和分享，以推动技术的进步和产业的协同发展，共同为中国开源生态的繁荣作出贡献。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270607</guid>
            <link>https://www.oschina.net/news/270607</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Facebook 开源 StyleX —— 在 JavaScript 中写 CSS]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Meta（原 Facebook）<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstylexjs.com%2Fblog%2Fintroducing-stylex%2F" target="_blank">开源</a></u>了全新的 CSS-in-JS 库 StyleX。</p><p><img src="https://oscimg.oschina.net/oscnet/up-30f683ba9535a9f16ce5e615736da0460cd.png" referrerpolicy="no-referrer"></p><blockquote><p><em>GitHub 地址：<strong><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Fstylex" target="_blank">https://github.com/facebook/stylex</a></u></strong></em></p></blockquote><p>官方介绍道，StyleX 是一个富有表现力、具有确定性、可靠且可扩展的样式系统。它通过使用编译时 (compile-time) 工具融合了静态 CSS 的性能和可扩展性。</p><p>此外，StyleX 不仅仅是一个基于编译器的 CSS-in-JS 库，它经过精心设计，可以满足大型应用程序、可复用组件库和静态类型代码库的要求。Meta 旗下多款产品如 Facebook、WhatsApp、Instagram、Workplace、Threads 等都在使用 StyleX 作为其 CSS 样式解决方案。</p><p>StyleX 主要特性</p><ul><li><p><strong>快速</strong>：StyleX 在编译时和运行时都具备高效的性能。Babel 转换不会对构建过程产生显著影响。在运行时，StyleX 避免了使用 JavaScript 插入样式的开销，并仅在必要时高效地组合类名字符串。生成的 CSS 经过优化，确保即使是大型网站的样式也能被浏览器快速解析。</p></li><li><p><strong>可扩展</strong>：StyleX 旨在适应像 Meta 这样的超大型代码库。通过原子构建和文件级缓存，Babel 插件能够处理数万个组件在编译时的样式处理。由于 StyleX 设计为封装样式，它允许在隔离环境中开发新组件，并期望一旦在其他组件中使用时能够可预测地呈现。</p></li><li><p><strong>可预测性</strong>：StyleX 会自动管理 CSS 选择器的特异性，以确保生成的规则之间不会发生冲突。它为开发人员提供了一个可靠地应用样式的系统，并确保「最后应用的样式始终生效」。</p></li><li><p><strong>类型安全</strong>：使用 TypeScript 或 Flow 类型来约束组件接受的样式，每个样式属性和变量都具有完全的类型定义。这有助于提高代码的可读性和可维护性，同时减少潜在的错误和冲突。</p></li><li><p><strong>样式去重</strong>：StyleX 鼓励在同一文件中编写样式和组件。这种方法有助于使样式在长期内更具可读性和可维护性。StyleX 能够利用静态分析和构建时工具来跨组件去重样式，并删除未使用的样式。</p></li><li><p><strong>可测试性</strong>：StyleX 可以配置为输出调试类名，而不是功能性的原子类名。这可以用于生成快照，以便在对设计进行轻微更改时不会经常变化。通过这种方式，开发人员可以更轻松地测试和验证样式的正确性，从而提高开发效率和产品质量。</p></li></ul><p><strong>示例代码</strong></p><pre><code class="language-javascript">import stylex from '@stylexjs/stylex';

const styles = stylex.create({
  root: {
    padding: 10,
  },
  element: {
    backgroundColor: 'red',
  },
});

const styleProps = stylex.apply(styles.root, styles.element);</code></pre><p><strong>下面是一个按钮组件的示例代码</strong></p><pre><code class="language-javascript">import * as stylex from "@stylexjs/stylex";

const styles = stylex.create({
  base: {
    appearance: "none",
    borderWidth: 0,
    borderStyle: "none",
    backgroundColor: "blue",
    color: "white",
    borderRadius: 4,
    paddingBlock: 4,
    paddingInline: 8,
  },
});

export default function Button({
  onClick,
  children,
}: Readonly&lt;{
  onClick: () =&gt; void;
  children: React.ReactNode;
}&gt;) {
  return (
    &lt;button {...stylex.props(styles.base)} onClick={onClick}&gt;
      {children}
    &lt;/button&gt;
  );
}</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:39:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270597/facebook-stylex-css-in-js</guid>
            <link>https://www.oschina.net/news/270597/facebook-stylex-css-in-js</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Google Colab 现已支持直接使用 🤗 transformers 库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 编辑器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;"><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Google Colab，全称 Colaboratory，是 Google Research 团队开发的一款产品。在 Colab 中，任何人都可以通过浏览器编写和执行任意 Python 代码。它尤其适合机器学习、数据分析和教育目的。从技术上来说，Colab 是一种托管式 Jupyter 笔记本服务。用户无需设置，就可以直接使用，同时还能获得 GPU 等计算资源的免费使用权限。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005864" data-ratio="0.6592592592592592" src="https://oscimg.oschina.net/oscnet/6aca6440-d2d5-4972-8624-54894772e85a.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">通过与 Colab 团队的共同努力，Colab 托管的运行时镜像现已默认集成了 Hugging Face transformers 库，只需简单执行 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">import transformers</code> 即可轻松接入！对于使用 Colab 进行机器学习和深度学习研究的开发者来说，这是一个非常重要的更新。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你想使用最新版本的 transformers，Colab 团队也提供了一个简单的命令 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">!pip install transformers --upgrade</code>，以便于随时更新至最新版本。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">除了提升用户体验，这一更新还开启了一些有趣的新功能。例如，用户现在可以直接从 Pandas 读取 Hugging Face 数据集，这将大大简化数据处理和模型训练的工作流程。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005865" data-ratio="0.4203703703703704" src="https://oscimg.oschina.net/oscnet/8ecbb7d1-9659-48de-9e0f-64e60f62d9ef.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">本合作和更新还开启了一些有趣的新功能。例如，用户现在可以直接从 Pandas 读取 Hugging Face 数据集，这将大大简化数据处理和模型训练的工作流程。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">你可以通过 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">hf://datasets/</code> 的方式在 Pandas 中直接读取 Hugging Face Hub 上的数据集。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">感谢 Colab 团队的朋友们，也希望社区的成员们喜欢本次的合作和功能更新！</p></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - Hugging Face（gh_504339124f0f）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:07:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10316003</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10316003</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 开源调试工具 ixGDB]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-readme-for-ixgdb-release" class="anchor" href="https://gitee.com/deep-spark/ixgdb#readme-for-ixgdb-release"></a>README for ixGDB release</h1><h2><a id="user-content-introduction" class="anchor" href="https://gitee.com/deep-spark/ixgdb#introduction"></a>INTRODUCTION</h2><p>ixGDB is Iluvatar CUDA source-level debugger for Linux OS, based on NVIDIA <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fcuda-gdb">CUDA-GDB</a> 10.2.</p><p>ixGDB provides the following capabilities:</p><ul><li>Provides a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application.</li><li>Supports debugging C/C++ applications and all CUDA applications, which might use CUDA driver APIs or CUDA runtime APIs.</li><li>Supports setting breakpoints.</li></ul><h2><a id="user-content-build-instructions-example-only-adjust-as-needed" class="anchor" href="https://gitee.com/deep-spark/ixgdb#build-instructions-example-only-adjust-as-needed"></a>BUILD INSTRUCTIONS (example only, adjust as needed)</h2><p>First, make sure that libtermcap and other required dependent packages are
installed (try "sudo yum install ncurses-devel"). The "configure" command will
report an error if some packages are missing.</p><p>Please note that the libexpat development headers must be present if ixGDB is to be used for cross-platform debugging.</p><p>Issue the following commands to build ixGDB:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">./configure --program-prefix=cuda- \</span><span id="LC2" class="line">    --enable-cuda \</span><span id="LC3" class="line">    --enable-targets="x86_64-apple-darwin,x86_64-unknown-linux-gnu,\</span><span id="LC4" class="line">    arm-elf-linux-gnu,m68k-unknown-linux-gnu" \</span><span id="LC5" class="line">    CFLAGS='-I/usr/local/cuda/include' \</span><span id="LC6" class="line">    LDFLAGS='-lpthread'</span><span id="LC7" class="line">make</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-using-ixgdb" class="anchor" href="https://gitee.com/deep-spark/ixgdb#using-ixgdb"></a>USING ixGDB</h2><p>All standard GDB commands could be used for both CPU and GPU code debugging. In addition to that, ixGDB provides CUDA-specific command families like "info cuda ..." to query GPU states, "cuda .." to control debugger focus on GPU and "[get|set] cuda .." to alter/query CUDA debugger configuration. If you want to know more about how to use ixGDB, please go to Iluvatar CoreX support <a href="https://gitee.com/link?target=https%3A%2F%2Fsupport.iluvatar.com%2F%23%2FDocumentCentre%3Fid%3D1%26nameCenter%3D1%26productId%3D">official site</a> and use "ixgdb" as the keyword to find document "SDK Tools User Guide", which includes detailed usage of ixGDB.</p><h2><a id="user-content-communication" class="anchor" href="https://gitee.com/deep-spark/ixgdb#communication"></a>COMMUNICATION</h2><p><a href="https://gitee.com/deep-spark/ixgdb/issues">Gitee Issues</a>: bug reports, feature requests, install issues, usage issues, etc.</p><h2><a id="user-content-license" class="anchor" href="https://gitee.com/deep-spark/ixgdb#license"></a>LICENSE</h2><p>Licensee's use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">This product includes copyrighted third-party software licensed under the terms of the GNU General Public License v3 ("GPL v3"). All third-party software packages are copyright by their respective authors.</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses.</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Component    License</span><span id="LC2" class="line">ixGDB        GPL v3</span></pre><div class="markdown-code-block-copy-btn"></div></div></div>]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/deep-spark/ixgdb</guid>
            <link>https://gitee.com/deep-spark/ixgdb</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 机器学习硬件十年：性能变迁与趋势]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p><img src="https://oscimg.oschina.net/oscnet/b41ccc75-f4d2-460d-b101-02526ac0c450.jpg" width="578" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px"><span>本文分析了机器学习硬件性能的最新趋势，重点关注不同 GPU 和加速器的计算性能、内存、互连带宽、性价比和能效等指标。这篇分析旨在提供关于 ML 硬件能力及其瓶颈的全面视图。本文作者来自调研机构<span style="background-color:#efefef">E</span><span style="background-color:#efefef">poch，致力于研究 AI 发展轨迹与治理的关键问题和趋势。</span></span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p><span>（本文由 OneFlow 编译发布，转载请联系授权。原文：https://epochai.org/blog/trends-in-machine-learning-hardware#computational-price-performance</span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p style="text-align:left"><strong><span style="color:#3f3f3f">作者 |&nbsp;</span></strong><strong>Marius Hobbhahn、Lennart Heim、Gökçe Aydos</strong></p><p><strong><span style="color:#3f3f3f">OneFlow 编译</span></strong></p><p><strong><span style="color:#3f3f3f">翻译｜杨婷、宛子琳</span></strong></p><p>&nbsp;</p><p><strong><span>要点概览</span></strong></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/020ab97f-5645-4af9-bc97-767ea99b036a.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span>图 1</span></em></span></strong><span style="color:#888888"><em><span>：常见机器学习加速器在给定精度下的峰值计算性能。自 2016 年以来，已出现了新的数值格式。趋势线展示了带有八个或更多加速器的数值格式：FP32、FP16（FP = 浮点、张量-* = 张量核心处理、TF = Nvidia 张量浮点、INT = 整数）</span></em></span></p><p>&nbsp;</p><p><span>我们研究了 GPU 在不同数值表示、内存容量、带宽以及互连带宽方面的计算性能，使用的数据集包括 2010 年到 2023 年常用于机器学习实验的 47 个 ML 加速器（GPU 和其他 AI 芯片），以及 2006 年到 2021 年的 1948 个 GPU。主要发现如下：</span></p><p>&nbsp;</p><ol start="1"><li><p><span>与传统 32 位浮点数（FP32）相比，低精度数字格式如 16 位浮点数（FP16）和 8 位整数（INT8）等与专用张量核心单元相结合，可以为机器学习工作负载带来显著的性能提升。例如，尽管使用的数据量有限，但我们估计 tensor-FP16 比 FP32 的速度快约 10 倍。</span></p></li></ol><p>&nbsp;</p><p><span>2. 鉴于用于 SOTA ML 模型训练和推理的大型硬件集群的整体性能取决于计算性能以外的因素，所以我们研究了内存容量、内存带宽和互连，发现：</span></p><p>&nbsp;</p><ul><li><p><span>内存容量每 4 年翻一番，内存带宽每 4.1 年翻一番。它们的增长速度比计算性能慢（计算性能每 2.3 年翻一番）。这是一个常见发现，通常被称为内存墙（memory wall）。</span></p></li></ul><p>&nbsp;</p><ul><li><p><span>最新的 ML 硬件通常配备专有的芯片间互连协议（英伟达的 NVLink 或谷歌 TPU 的 ICI），与 PCI Express（PCIe）相比，这些协议在芯片之间提供了更高的通信带宽。例如，H100 上的 NVLink 支持的带宽是 PCIe 5.0 的 7 倍。</span></p><p>&nbsp;</p></li></ul><p><span>3. 分析中发现的关键硬件性能指标及其改进速度包括：ML 和通用 GPU 的计算性能（以 FLOP/s 计）都是每 2.3 年翻一番；ML GPU 的计算性价比（以每美元 FLOP 计）每 2.1 年翻一番，通用 GPU 每 2.5 年翻一番；ML GPU 的能效（以每瓦特 FLOP/s 计）每 3.0 年翻一番，通用 GPU 每 2.7 年翻一番。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/8666a251-442b-40ee-a6ad-e7b64707cd15.jpg" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span>表 1</span></em></span></strong><span style="color:#888888"><em><span>：关键性能趋势。所有估算仅针对机器学习硬件。方括号中的数字表示通过 1000 次 bootstrap 采样得出的[5; 95]百分位估算。OOM 代表数量级，N 表示数据集中的观测次数。请注意，性能数据是指稠密矩阵乘法性能。</span></em></span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">1</span></strong></span></strong></span></p><span id="OSC_h2_1"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span>引言</span></strong></span></h2><p>&nbsp;</p><p><span>过去十年中，机器学习的进步在很大程度上是通过扩大用于训练的计算资源（计算）规模实现的（Sevilla 等人，2022 年），硬件性能的提升在这一进展中发挥了一定作用。随着我们从少量芯片转向大规模超级计算机，对 ML R&amp;D（Cottier，2023）投资的增加导致硬件基础设施规模的相应提升。</span></p><p>&nbsp;</p><p><span>本文概述了在各种数字精度和专用组件（如张量核心）方面的计算性能趋势。此外，我们还分析了其他性能因素，如内存容量、内存带宽和互连带宽。总的来说，我们分析了 ML 硬件规格和组件的整体情况，这些规格和组件共同决定了硬件的实际性能，尤其是在大规模 ML 模型时代。</span></p><p>&nbsp;</p><p><span>在这个过程中，我们比较了各种度量标准下的峰值性能，这些指标来自硬件生产商的规格表。[2]通常，由于工作负载规格等各种因素和内存容量以及带宽等规格的限制，实际利用的计算性能只是指定峰值计算性能的一小部分。例如，根据 Leland 等人在 2016 年的研究，常见超级计算工作负载的实际利用率可能在 5% 到 20% 之间，而在机器学习训练中，这取决于模型的规模、并行化方式等因素（Sevilla 等人，2022），这个比例可能在 20% 到 70% 之间。尽管如此，峰值性能仍可作为比较不同硬件加速器和世代的有用上限和标准基础。</span></p><p>&nbsp;</p><p style="text-align:left"><strong><span>术语</span></strong></p><p>&nbsp;</p><ul><li><p><strong><span>数字表征</span></strong><span>：我们将数字表征分为三个维度：</span></p><p>&nbsp;</p></li><li><p><strong><span>位长/精度</span></strong><span>：从 4 位到 64 位不等，通常用于描述存储数字的位数。</span></p><p>&nbsp;</p></li><li><p><strong><span>数字格式</span></strong><span>：指特定的位（bit）布局，如整数或浮点数。数字格式通常包括 FP32 等位长度，但我们拆分了位布局和位长度[3]。</span></p></li></ul><p>&nbsp;</p><ul><li><p><strong><span>计算单元</span></strong><span>：显示是否使用了专用矩阵乘单元。在这篇文章中，我们只区分张量和非张量。</span></p><p>&nbsp;</p></li><li><p><strong><span>硬件加速器</span></strong><span>：指加速 ML 工作负载的芯片，如 GPU 或 TPU。我们在通用术语中可交替使用芯片和硬件加速器这两个术语，而在指代专门的加速器时则使用 GPU 和 TPU。</span></p></li></ul><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">2</span></strong></span></strong></span></p><span id="OSC_h2_2"></span><h2 style="text-align:center"><strong><span style="color:#1e2380"><span>数</span>据集</span></strong></h2><p>&nbsp;</p><p><span>我们从两个关键数据集中汇编了硬件规格。第一个数据集在 2019 年 Sun 等人的研究（<span style="color:#888888"><em>https://arxiv.org/abs/2202.05924</em></span>）基础上，包含了 2006 年至 2021 年期间发布的 1948 款 GPU，我们将其称为通用 GPU 数据集（主要基于一些不常用于机器学习训练的通用 GPU）。第二个数据集仅包含自 2010 年以来的 47 个 ML 硬件加速器，如 NVIDIA 的 GPU 和 Google 的 TPU，它们通常在重要的机器学习实验中使用（根据 2022 年 Sevilla 等人的定义）。</span></p><p style="text-align:left"><br><span>我们自己整理了后一个数据集，并将其称为 ML 硬件数据集，简称 ML 数据集（基于 ML GPU）。此数据集可以在我们的数据表中公开获取（</span><span><span style="color:#888888"><em>https://docs.google.com/spreadsheets/d/1NoUOfzmnepzuysr9FFVfF7dp-67OcnUzJO-LxqIPwD0/edit?usp=sharing</em></span></span><span>）。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">3</span></strong></span></strong></span></p><span id="OSC_h2_3"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span>主要性能指标趋势</span></strong></span></h2><p>&nbsp;</p><p><span>在本节中，我们将介绍不同数字表征、内存容量、计算性价比和能效的趋势。我们将简要解释每个指标与 ML 开发和部署的相关性，展示我们的发现，并简要讨论它们的含义。</span></p><p>&nbsp;</p><span id="OSC_h3_4"></span><h3><span><strong><span>数字表征</span></strong></span></h3><p>&nbsp;</p><p><span>用于计算的数值表征对计算性能有很大影响。具体说来，每个值的位数决定了计算密度（每秒每芯片面积的运算次数）。[4]近年来，硬件制造商已经为 ML 应用引入了专门的低精度数值格式。虽然 FP64 在高性能计算中很常见，[5]但在过去 15 年左右的时间里，FP32 的性能一直是大多数消费级应用关注的焦点。</span></p><p>&nbsp;</p><p style="text-align:left"><span>近年来，精度较低的数值格式变得更加普遍，因为低精度已经足够开发和部署 ML 模型（Dettmers 等人，2022；Suyog Gupta 等人，2015 年；Courbariaux 等人，2014 年）。根据 Rodriguez（<span style="color:#888888"><em>https://deeplearningsystems.ai/#ch06/#61-numerical-formats，2020</em></span>），到目前为止，FP32 仍然是机器学习训练和推断中采用最广泛的数值格式，行业越来越倾向于在某些训练和推理任务中过渡到更低精度的数值格式，如 FP16 和 Google 的 bfloat16（BF16），以及用于部分推理工作负载的整数格式 INT8。[6]其他知名新兴数值格式包括 16 位标准浮点格式 FP16，整数格式 INT4，以及 NVIDIA 开发的 19 位浮点格式 TF32。[7]</span></p><p>&nbsp;</p><p style="text-align:left"><strong><span>FP32 和 FP16 的计算性能</span></strong></p><p style="text-align:left">&nbsp;</p><p style="text-align:justify"><span>从历史上看，近 20 年来，FP32 精度的计算性能趋势一直相对稳定，呈现出 2.3 年翻倍一次的趋势，与摩尔定律的速度密切相关。在过去几年，特别是自 2016 年以来，我们已经看到了专门支持 FP16 精度的硬件的出现，这增加了绝对计算性能，同时减少了位长。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/48807fb3-a5fc-4cbc-8036-00251e0bcba5.jpg" referrerpolicy="no-referrer"></p><p><em><strong><span style="color:#888888">图 2</span></strong><span style="color:#888888">：过去二十年，FP32 和 FP16 精度下的通用和 ML GPU 峰值性能。上图显示，ML GPU 的中位性能高于所有通用 GPU，但增长率相似。下图显示，2014 年一些硬件加速器开始提供 FP16 性能细节。</span></em></p><p>&nbsp;</p><p style="text-align:justify"><span>在过去十年中，FP32 的通用硬件和 ML 硬件的计算性能显示出几乎相同的增长率，但在性能水平上有所不同。我们的 ML 硬件数据集中的加速器始终处于最佳可用硬件之列。我们认为，这在一定程度上是因为机器学习实践者选择了最强大的可用硬件，其次，这也是由于最近推出的专门针对机器学习市场的高端数据中心 GPU 的推出，例如英伟达的 V/A/H100 或谷歌的 TPU。</span></p><p>&nbsp;</p><p style="text-align:left"><span><strong><span>通过硬件支持更低精度的数值格式以提高计算性能</span></strong></span></p><p>&nbsp;</p><p><span>降低数值精度所带来的性能提升得益于现代机器学习芯片中多重架构的改进，而不仅仅是单纯降低位宽所能达到的。较小的数据类型使得每平方芯片面积可以进行更多的浮点运算，并减小了内存占用。</span></p><p>&nbsp;</p><p><span>然而，其他方面的进步也在很大程度上做出了贡献：引入了专门用于矩阵乘的新指令；[8]硬件数据压缩；消除了诸如 NVIDIA A100 中的矩阵乘硬件中多余的数据缓冲区，这有助于降低数据和指令内存需求，从而提高了单位芯片面积上的操作数。H100 更快的内存访问能力进一步优化了上述进展 (Choquette, 2023).。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/378ad9dd-46a8-4589-9bcd-6763ff5ed792.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 3</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：箱线图显示了不同精度数字格式下 ML 加速器性能相对于其 FP32 性能的比值，这展示了相对于 FP32 的性能改善。我们发现，相对于它们自身的 FP32 性能，采用新的数值表示方式 tensor-FP32/TF32、tensor-FP16 和 tensor-INT8 可以分别使平均计算性能提高约 5 倍、8 倍和 13 倍。并非所有 GPU 都专门支持低精度格式，我们从图中剔除了那些在较低精度格式上的计算性能未能超过较高精度格式的 GPU 型号，以便筛选出缺乏专门支持的 GPU。</span></em></span></p><p>&nbsp;</p><p><span>近年来，由于使用了较低的数字精度，GPU 在机器学习工作负载中的性能大幅提升。平均而言，与在同一 GPU 上使用 FP32 相比，使用 tensor-FP32（TF32）、tensor-FP16、tensor-INT8 和 tensor-INT4 等精度较低的数值格式分别可提供约 5 倍、8 倍、13 倍和 18 倍的计算性能。</span></p><p>&nbsp;</p><p><span>历史数据显示，FP32 性能峰值每 2.3 年翻一番，这些较低精度的加速效果相当于性能提升了 3 到 9 年。然而，最大的加速效果可能超过平均值。与 FP32 相比，NVIDIA 的 H100 在 TF32、FP16 和 INT8 下分别实现了约 7 倍、15 倍和 30 倍的加速效果。</span></p><p><br><span>因此，对于 H100 来说，与典型的 GPU 相比，较低的精度提供了比 FP32 更大的性能增益。正如我们所看到的，虽然使用较低精度能极大地提升计算性能，但出于模型准确性方面的权衡，通常还是会使用较高精度进行训练。[10]尽管 TF32、FP16 和 INT8 格式在 H100 上相较于 FP32 提供了加速效果，但需要注意的是，这不仅仅是因为较小的数值格式更高效，H100 很可能针对这些格式的操作进行了优化，从而促成了速度提升。</span></p><p>&nbsp;</p><span id="OSC_h3_5"></span><h3><strong><span>内存容量和带宽</span></strong></h3><p>&nbsp;</p><p><span>典型的处理器核心通过读取数据、处理数据，并将处理后的结果写回内存来执行计算。因此，内存充当了在处理周期之间存储数据的媒介。硬件倾向于使用内存层次结构：从在计算单元附近存储数百 KB 快速访问数据的寄存器文件，到能够容纳数十 GB 较慢访问数据的随机存取存储器（RAM）。[11] 数据定期从较大的慢速访问 RAM 通过中间缓存存储器传输到寄存器文件，必要时再写回。加速器数据表大多提供加速器卡上可用的最大 RAM[12]。我们称这些 RAM 位的数量为内存容量。数据以块的形式传输到最大 RAM 中，具体取决于所使用的内存技术，这需要一些处理周期。我们将能够每秒传输到最大 RAM 的最大位数（即峰值比特速率）称为内存带宽[13]。</span></p><p>&nbsp;</p><p><span>包含硬件加速器的系统通常包含一个主存储器，用于存储应用程序和数据。然后，这些数据被传输到加速器进行处理。为确保在训练或推理期间模型权重和训练数据在硬件加速器上随时可用，需要更大的内存容量。如果数据无法适应加速器的内存，逻辑（logic）将需要使用 CPU 内存，甚至更高级别的内存（例如硬盘），这将显著影响时延和带宽。实际上，为避免这种性能损失，模型数据分发到多个硬件加速器的内存中。</span></p><p>&nbsp;</p><p><span>硬件处理能力的进步需要更大的内存带宽。如果没有足够的数据输入，就无法达到峰值计算性能，内存带宽就会成为瓶颈[14]，这被称为带宽墙（Rogers 等人，2009）或通常所说的内存墙。</span></p><p>&nbsp;</p><p><span>如图 4 所示，相对于计算性能的改善，内存容量和带宽的增长速度较慢。具体而言，就通用 GPU 来说，内存容量每 3.04 年翻一番，而 ML 加速器则为 4 年，内存带宽分别为每 3.64 年和 4 年翻一番。相比之下，根据之前的分析，计算性能每 2.3 年翻一番。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/c2f89bdb-25a7-433f-8e48-e8922d6297e7.png" referrerpolicy="no-referrer"></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/50b955f7-0e03-4cc2-a915-d3916e601217.png" referrerpolicy="no-referrer"></p><p><span style="color:#888888"><em><span>图 4：通用硬件与 ML 硬件的内存容量和带宽的变化轨迹。我们发现所有这些趋势都比计算性能的趋势慢（计算性能每 2.34 年翻一番），这与通常所说的内存墙趋势一致。</span></em></span></p><p>&nbsp;</p><p><span>如人们所预期的那样，在内存容量和带宽方面，ML 硬件超过了中位的 GPU。然而，即使在这方面，这些指标的增长速度也一直落后于计算性能的增长速度（每 2.3 年翻一番）。这一趋势表明，对于大规模 ML 应用而言，内存正在成为一个日益关键的瓶颈。当前的架构改进，比如引入更少位的数字表征，可能会减轻这种内存限制。然而，如果不加快发展，这一内存瓶颈将在未来几年继续影响整体性能。[15]</span></p><p>&nbsp;</p><p><span>对于一些 ML 工作负载来说，单个加速器可能提供了足够的计算性能。然而，由于内存限制，通常需要将工作负载分布到多个加速器上。利用多个加速器可以增加总内存容量，从而完全将大型模型和数据集放入内存。这种策略确保了更大的内存容量，可以在多个硬件加速器上容纳模型的全部权重，从而减轻了从主机系统内存传输数据时所产生的时延。对于某些工作负载来说，增加内存带宽可能对满足时延和吞吐量要求至关重要。</span></p><p><br><span>值得注意的是，旨在减少内存占用的技术，比如重新计算激活值利用了计算资源来部分抵消这些限制（Rajbhandari 等, 2021）。然而，通过多个芯片并行化模型训练需要它们之间通过互连实现高效通信。</span></p><p>&nbsp;</p><span id="OSC_h3_6"></span><h3 style="text-align:left"><strong><span>互连带宽</span></strong></h3><p>&nbsp;</p><p><span>在 ML 的训练和部署中，由于不断增长的内存需求，除需要巨大的计算能力之外，还需要使用多个芯片来满足这些需求。例如，PaLM 的训练中使用了 6144 个芯片（Chowdhery 等人，2022 年），而对于 GPT-4 可能需要使用更多芯片。这一需求强调了有效互连这些芯片的需求，使它们能够在不借助 CPU 内存或磁盘的情况下有效地交换激活值和梯度。</span></p><p>&nbsp;</p><p><span>互连带宽是指通信通道能够传输的峰值比特率，通常以每秒传输的字节数为单位测算。当 ML 硬件之间频繁交换数据时，如果互连带宽跟不上处理速度，这个指标就成为了限制因素。</span></p><p>&nbsp;</p><p><span>互连协议定义了最大互联带宽。在我们的数据集中，ML 硬件涉及三种常见协议：a) PCI Express（PCIe）；b) Nvidia NVLink；c) Google Inter-Core Interconnect（ICI）[16] 。PCIe 是一种普遍采用的协议，用于在 CPU 和机器学习硬件之间进行本地互联。相比 PCIe 的基于集线器的网络架构，Nvidia 的专有 NVLink 通过实现设备之间的直接点对点连接，克服了 PCIe 的带宽限制。在无法使用点对点连接的情况下，PCIe 被用作备用方案。Google 的 ICI 用于连接他们的 TPU[17]。</span></p><p>&nbsp;</p><p><span>前面提到的互连协议主要设计用于近距离通信[18] 。当需要进行较长距离的通信时，会采用传统的计算机网络协议，比如以太网或者 InfiniBand。在所有传统网络协议中，数据都是通过 PCIe 路由到网络硬件[19] 。即使存在 NVLink 和 ICI，PCIe 仍然作为主机 CPU 和机器学习硬件之间的标准互连协议。在接下来的内容中，我们将始终指出对应于最快协议的互连速度。</span></p><p>&nbsp;</p><p><img src="https://oscimg.oschina.net/oscnet/9207e987-873b-48dc-92b4-9816e194b1c1.jpg" referrerpolicy="no-referrer"></p><p><span style="color:#888888"><em><span style="color:#888888">图 5: 不同硬件加速器中，每个芯片的聚合互连带宽。NVLink 和 ICI 等专有协议的互连带宽高于 PCIe。</span></em></span></p><p>&nbsp;</p><p><span>我们发现，自 2011 年以来，ML（机器学习）硬件的 PCIe 带宽仅从 32GB/s 增加到 2023 年的 128GB/s（见图 5）。[20]然而，英伟达（NVLink）和谷歌（ICI）的专用加速器互连协议可实现更高的互连带宽。此外，常用于大型计算集群的高端 ML 加速器（例如 TPU 和 V/A/H100）拥有迄今为止最高的互连速度。例如，搭载 18 个 NVLink 4.0 通道的英伟达 H100 实现了 900GB/s 的带宽，是单个 PCIe 5.0 16 通道链路的 7 倍。[21]</span></p><p>&nbsp;</p><p style="text-align:left"><span>一个计算集群可能配备了成千上万台不同程度耦合的硬件加速器。例如，英伟达的 DGX H100 服务器使用 NVSwitch 使每台 H100 互连，从而实现了最大互连带宽为 900GB/s 的紧密耦合加速器网络（参见<span style="color:#888888"><em>[Choquette, 2023]，https://doi.org/10.1109/MM.2023.3256796，"Scaling Up and Out"一章</em></span>）。许多 DGX H100 服务器又可以组成所谓的 SuperPOD，其中各个独立服务器中的加速器仍可使用 NVLink 传输数据，但耦合程度较低。每个 SuperPOD 使用以太网和 Infiniband 连接到另一个 SuperPOD。服务器之间的网络拓扑也会影响计算集群的整体性能。</span></p><p>&nbsp;</p><p><span>专用集群 ML 硬件的互连带宽远高于消费级硬件。这凸显了它在大规模 ML 实验中的重要性，因为这些实验需要在 ML 硬件节点之间进行高带宽的数据通信。因此，类似于内存容量和带宽，我们建议监测互连带宽，将其作为了解 ML 硬件趋势的一个相关附加指标。</span></p><p>&nbsp;</p><span id="OSC_h3_7"></span><h3 style="text-align:left"><strong><span>计算性价比</span></strong></h3><p>&nbsp;</p><p><span>性能——价格比（Price-performance ratio）通常比单纯的峰值计算性能更有用，它能反映 GPU 的整体技术改进情况，即每美元成本可获得的性能。我们采用两种方法来估算 ML 硬件的性价比：</span></p><p>&nbsp;</p><p><span>1. 在有数据的情况下，我们使用硬件的发布价格，根据通货膨胀进行调整，并假定两年的摊销时间，详见<span>（</span><span style="color:#888888"><em>Cotra (2020)，https://docs.google.com/document/d/1qjgBkoHO_kDuUYqy_Vws0fpf-dG5pTU4b8Uej6</em></span>）。</span></p><p>&nbsp;</p><p><span>2. 在仅提供租赁的 TPU 或其他硬件等硬件发布价格不可用或不明确的情况下，我们使用 Google Cloud 的云计算价格（截至 2023 年 7 月 3 日）。我们根据通货膨胀调整价格，以使价格与摊销价格相当，并假设云服务提供商的利润率为 40%[22]。如图 6 所示，在计算 FP32 精度的性价比时，需考虑估算 FP32 性价比时的一些重要注意事项。</span></p><p>&nbsp;</p><p><span>首先，集群硬件的定价通常会采用私下协商的方式，不公开发布，这使得难以准确定价。其次，尽管某些芯片在个体性价比上表现强劲，但由于互连带宽或可靠性不足，可能无法在工业集群部署中使用。再次，FP32 计算引入了对专用 ML 芯片的偏见，这些芯片使用较低精度数字格式和未在 FP32 指标中反映的张量核心。最后，由于缺乏有关功耗、冷却和更换率等数量的公开数据（<span style="color:#888888"><em>参见[Cottier, 2023]，https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems</em></span>），估算实际维护成本具有挑战性。尽管作为基准有用，但 FP32 性价比趋势必须考虑源自 ML 的特定架构因素和数据约束的限制。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/1ea660df-a2c3-4e78-8427-edd2d54f01b0.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 6</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：通用硬件和 ML 硬件的 FP32 性价比轨迹。我们发现，这些轨迹大致遵循与峰值计算性能相同的增长轨迹（2.3 年翻倍时间）。此外，我们发现 ML GPU 的绝对性价比低于其他硬件。FP32 性价比可能存在对 ML 硬件的偏见（详见正文）。</span></em></span></p><p>&nbsp;</p><p><span>我们看到 FP32 性价比的增长轨迹（2.5/2.1 年翻倍时间）大致与通用计算性能的增长轨迹（2.3 年翻倍时间）相似。</span></p><p>&nbsp;</p><p><span>此外，与其他 GPU 相比，我们发现 ML GPU 的性价比较低。我们推测至少有两个原因。</span></p><p>&nbsp;</p><p><span>首先，如上所述，由于它们忽略了在 ML 训练中常见的其他数值表示（如 FP16），上述注意事项系统地使 FP32 性价比对 ML 硬件产生了偏见。其次，正如前面的部分所述，大规模 ML 训练不仅依赖於单一性能指标，还依赖于互连带宽、内存容量和带宽等其他指标。然而，这些指标并未反映在 FP32 性价比中。例如，一款典型的消费级 GPU 在个体的性价比上可能更好，但对于 ML 训练来说却不太适用。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/bd56dc1f-dd39-47cf-b13e-04e13bff8816.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 7</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：不同数值表示 ML 硬件的计算性价比。其中的点表示 ML 硬件的发布日期和性能，颜色代表数值格式。虚线表示具有十个或更多加速器的数值格式（如 INT8、FP16 和 FP32）性能改进趋势。</span></em></span></p><p>&nbsp;</p><p><span>FP32 的性价比可能会误导对 ML 硬件成本效益的认识。例如，AMD Radeon RX 7900 XTX 消费级 GPU 在 FP32 性价比方面表现最佳。然而，NVIDIA RTX 4090 在使用 ML 训练中常见的低精度 INT4 格式时，提供了约 10 倍高的性价比。这得益于 RTX 4090 专为低精度计算而设计的张量核心，而 FP32 指标却忽略了这一点。</span></p><p><br><span>因此，仅凭 FP32 的性价比便会错误地认定 Radeon 优于 RTX 4090，而实际上 RTX 4090 在实际 ML 工作负载中更为经济实惠。这突显了仅依赖 FP32 性价比分析，不考虑 ML 特定架构和数值表示的整体评估的风险。</span></p><p>&nbsp;</p><p><span>性价比最好的 GPU 在很大程度上取决于所使用的数值表示。AMD Radeon RX 7900 XTX 消费级 GPU 在 FP32 计算上的性价比最高。然而，对于像 INT4 这样的低精度数字格式，NVIDIA RTX 4090 的每美元计算性能大约是 Radeon 的 10 倍。这说明按照性价比对 GPU 进行排名对精度非常敏感，而仅依靠 FP32 无法全面反映实际 ML 工作负载中的成本效益。</span></p><p>&nbsp;</p><span id="OSC_h3_8"></span><h3 style="text-align:left"><strong><span>能效</span></strong></h3><p>&nbsp;</p><p><span>运行硬件会消耗能源，而大多数组织的目标是尽可能充分地利用他们的硬件。因此，部署能效高的硬件是一种降低硬件加速器寿命周期成本的可能途径。此外，能效更高的硬件通常散热更少，有助于更好地实现可扩展性。</span></p><p>&nbsp;</p><p><span>为近似评估 ML 硬件的能效，我们使用每瓦特的 FLOP/s，其中能量组成部分是从热设计功耗（TDP）计算得出的。TDP 并不等同于平均能耗，因此不应该用于精确比较。然而，在 ML 训练和云计算中，我们认为它是一个相当不错的近似值，因为硬件是持续运行的（<span style="color:#888888"><em>参见附录中的 TDP 部分，https://epochai.org/blog/trends-in-machine-learning-hardware#thermal-design-power-tdp</em></span>）。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/ba66db12-918a-446c-9ee4-6cfa15257bdf.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 8</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：根据 TDP 数值计算的 FP32 精度能效轨迹。我们发现，机器学习 GPU 的平均能效比通用 GPU 高，且能效的增长速度略低于峰值计算性能（2.3 年翻倍时间）的增长速度。</span></em></span></p><p>&nbsp;</p><p style="text-align:left"><span>我们发现，机器学习 GPU 的平均能效比历史 GPU 更高。这是合理的，因为 ML GPU 通常在数据中心运行，能源消耗和碳足迹是重要的度量标准（<em><span style="color:#888888">参见 Jouppi 等，2023，https://arxiv.org/pdf/2304.01433.pdf，第 7.6 节</span></em>）。此外，我们发现能效的增长速率（分别为历史 GPU 和 ML GPU 的 2.70/3.0 年翻番时间）仅略低于峰值计算性能的增长速率（2.3 年翻番时间）。这一趋势表明能耗目前（尚）不是扩展的现实瓶颈，但有理由认为在未来可能会成为瓶颈（<span style="color:#888888"><em>参见 Hobbhahn &amp; Besiroglu, 2022b，https://epochai.org/blog/predicting-gpu-performance</em></span>）。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">4</span></strong></span></strong></span></p><span id="OSC_h2_9"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">结论</span></strong></span></h2><p>&nbsp;</p><p><span>最近的研究表明，对于开发和部署 ML 模型，低精度已经足够（<span style="color:#888888"><em>参见[Dettmers 等, 2022]；[Suyog Gupta 等, 2015]; [Courbariaux 等, 2014]</em></span>）。我们发现，ML 硬件遵循上述发现，并不断集成支持更低精度数值格式的硬件单元（如 FP16、TF32、BF16、INT8 和 INT4），以增加每秒的总操作次数。此外，张量核心等专用计算单元变得越来越普遍，并进一步提高了计算性能。</span></p><p>&nbsp;</p><p><span>结合这两个趋势，在我们的推测性占主导的估算中，从 FP32 到张量-FP16 的跃迁平均提供了约 8 倍的峰值性能增益。然而，旗舰级 ML 硬件加速器的这一比率可能更高，例如，NVIDIA H100 SXM 的 TF32 到 FP32 比率约为 7 倍，张量-FP16 到 FP32 比率约为 15 倍，张量-INT8 到 FP32 比率约为 30 倍。</span></p><p>&nbsp;</p><p><span>这一趋势表明了一种「硬件-软件协同设计」的模式，其中 ML 从业者尝试不同的数值表示，并已获得了一些小而有意义的性能提升，减少了内存占用。然后，硬件被调整以适应这些新的数值表示，从而获取进一步的增益。多次迭代这一循环可以促成性能的实质性改善。此外，硬件生产商也在积极寻求新的创新，这些创新随后将引领其进入 ML 实验室。</span></p><p>&nbsp;</p><p><span>此外，在大规模 ML 训练中，我们强调内存容量、内存带宽和互连带宽等因素的重要性。鉴于目前 ML 训练通常需要数千个芯片之间的有效交互，超越每个芯片峰值性能的因素变得至关重要。我们观察到，这些指标的增长速度比与计算相关的指标（例如峰值计算性能、性价比和能效）要慢。在大规模分布式 ML 训练场景中，内存和互连带宽成为利用峰值计算性能的瓶颈。</span></p><p>&nbsp;</p><p><span>专门的机器学习硬件和替代的数值表示是相对较新的趋势，这使得精确预测变得困难。正如我们已经明确指出，密切追踪数值格式、内存容量、内存带宽和互连带宽的发展对于更准确地评估未来机器学习能力至关重要。与其依赖静态假设，基于硬件和软件创新不断重新评估性能潜力才是关键。</span></p><p>&nbsp;</p><p><em><span>（<span style="color:#888888">我们要感谢 Dan Zhang、Gabriel Kulp、Yafah Edelman、 Ben Cottier、Tamay Besirogl 和 Jaime Sevilla 对本文提供的详尽反馈，还要感谢 Eduardo Roldán 将这篇文章搬运到网站上。</span>）</span></em></p><p>&nbsp;</p><span id="OSC_h2_10"></span><h2><strong><span>附录：次要性能指标的趋势</span></strong></h2><p>&nbsp;</p><p><span>我们补充了晶体管数量、热设计功耗（TDP）、时钟速度、芯片尺寸和张量核心数量等次要指标的趋势。尽管这些指标可能与理解 ML 硬件的某些趋势相关，但我们认为它们不如我们在文章主体中分析的指标重要或有影响力[23]。</span></p><p>&nbsp;</p><p><span>请注意，这些趋势中仍有大量缺失数据，因此可能存在偏见。例如，以下大部分数据不包括 TPU。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/876d3c1e-b2f8-4b16-adfe-1547abd73ec1.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><strong><span style="color:#3f3f3f">注释<span style="background-color:#ffffff; color:#a3a3a3">（请上下滑动）</span><span>&nbsp;</span></span></strong></p><span id="OSC_h3_11"></span><h3>&nbsp;</h3><p><span>1 NA 表示数据不可用，因为缺乏足够的数据来估计相关增长率。</span></p><p>&nbsp;</p><p><span>2 这些数字通常是基于硬件特性计算得出的。例如，计算性能通常被估算为处理核心数量、时钟速度和每个核心的每个时钟周期的浮点运算乘积。</span></p><p>&nbsp;</p><p><span>3 相同位数的比特可以表示不同的数值范围或浮点数精度。我们的硬件数据集不包括针对给定位数格式的每种可用数值格式的计算性能。例如，我们的 FP16 数据还包括 BF16，其在指数和尾数分配的比特数方面存在差异。我们不指望在相同位数的不同浮点数格式之间有太大的性能差异。最适合的数值表示（例如，从能源或运行时间效率的角度）取决于工作负载。[Rodriguez, 2020](https://deeplearningsystems.ai/#ch06/#61-numerical-formats) 第 6.1 节中还包含了一份 ML 应用的数值表示的综合列表。</span></p><p>&nbsp;</p><p style="text-align:left"><span>4 根据[Mao 等人 (2021)](https://doi.org/10.1109/TVLSI.2021.3128435) 中的表 VI，一个 FP64 乘法器单元的面积大约是 FP32 乘法器的五倍。类似的关系也存在于 FP32 和 FP16 乘法器之间。</span></p><p>&nbsp;</p><p><span>5 由于许多具有历史重要性的超级计算机工作负载对高精度的要求，例如计算流体力学、气象学、核蒙特卡洛模拟、蛋白质折叠等。</span></p><p>&nbsp;</p><p style="text-align:left"><span>6 [Rodriguez, 2020](https://deeplearningsystems.ai/#ch06/#61-numerical-formats), 第 6.1 节指出：最受欢迎和广泛采用的数值格式是用于训练和推理的 FP32。行业正在向用于训练和推理的 FP16 和 BF16 靠拢，并在某些工作负载的推理中采用 INT8。</span></p><p>&nbsp;</p><p><span>7 TF32 并非通用数值格式，它仅在 NVIDIA 张量核心中使用，通过在矩阵乘法之前减少 13 位精度位，加速使用 FP32 的模型处理，但保持与 FP32 相同的数值范围。TF32 与 FP32 的内存占用相同，因为 TF32 在张量核心中使用与 FP32 相同的寄存器（参见[Sun 等，2022](https://doi.org/10.1109/TPDS.2022.3217824)，第 8 节）。换句话说，TF32 被设计为 FP32 模型的即插即用替代品，但在矩阵乘法过程中可以接受更低的精度。</span></p><p>&nbsp;</p><p><span>8 请勿将其与张量核心乘法所需的新指令混淆。[Choquette 等人，2021](https://doi.org/10.1109/MM.2021.3061394)，SM Core 一节指出：在 A100 中，添加了一条新的异步组合加载-全局存储-共享存储指令，将数据直接传输到 SMEM，绕过寄存器文件，提高了效率。</span></p><p>&nbsp;</p><p><span>9 注意查看标题为‘SM Core’的部分。</span></p><p>&nbsp;</p><p><span>10 例如，目前 INT8 在训练当前系统中并未被广泛使用。INT8 的缺点在 Rodriguez，2020，第 6.1 节中有解释。</span></p><p>&nbsp;</p><p><span>11 由 ML 硬件数据表记录的内存容量通常指的是 RAM 容量，因为 GPU 在之前常被用于视频处理，所以也常被称为视频 RAM（VRAM）。</span></p><p>&nbsp;</p><p style="text-align:left"><span>12 例如，[AMD Instinct MI200 数据表](https://www.amd.com/system/files/documents/amd-instinct-mi200-datasheet.pdf) &nbsp;明确说明了 128 GB HBM2e。HBM 指的是高带宽内存，是一种 RAM 类型。[NVIDIA H100 Tensor Core GPU 数据表](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet) 表示 H100 SXM 的内存为 80GB，根据 [NVIDIA H100 Tensor Core GPU 架构](https://nvdam.widen.net/s/95bdhpsgrs#page=36)v1.04，第 36 页，这个数字对应于 HBM3 的内存容量。</span></p><p>&nbsp;</p><p><span>13 在应用中，实际带宽通常较低。一个原因是数据传输时延，这也影响了实际带宽，并取决于内存技术。到单独内存芯片的距离以及在大容量内存中的长路径，会导致数据在到达处理单元之前经历大量的周期。如果处理单元预先知道需要哪些数据，就可以以最大带宽进行数据传输。如果不知道，就需要对内存进行随机访问。通常，随机访问越多，实际带宽就越低。我们的数据集中不包含时延指标。</span></p><p>&nbsp;</p><p><span>14 图形处理和机器学习训练往往会遇到这个瓶颈，因此，现代机器学习硬件尝试通过两种技术来优化高内存带宽：(a) GDDR 内存或 (b) 高带宽内存（HBM）。GDDR 内存位于与处理芯片相同的板上，而 HBM 则实现在与处理芯片相同的封装中，从而实现更低的时延和更高的带宽（例如，在数据中心使用的最新机器学习加速器，如 NVIDIA A100 和 H100 采用了 HBM；而它们的游戏型 GPU 则没有采用 HBM，以节约成本）。将许多 DRAM 堆叠在一起，并在单个芯片封装中互连多个半导体芯片，与在印刷电路板上连接处理芯片和 DRAM 相比，需要昂贵的工具，因此 HBM 通常出现在性能最昂贵和性能最高的机器学习硬件加速器中，例如那些用于数据中心进行大规模机器学习训练和部署的加速器。</span></p><p>&nbsp;</p><p><span>15 可参阅 [Megatron-LM: 使用模型并行训练数十亿参数的语言模型](https://lilianweng.github.io/posts/2021-09-25-train-large/)，[如何在多个 GPU 上训练非常大的模型？](https://lilianweng.github.io/posts/2021-09-25-train-large/) 或者[训练大型神经网络的技术](https://openai.com/research/techniques-for-training-large-neural-networks) 。</span></p><p>&nbsp;</p><p><span>16 [Jouppi 等，《TPU v4：一种具有嵌入式硬件支持的光学可重构超级计算机用于机器学习》](https://arxiv.org/pdf/2304.01433.pdf) 的第 2 节中有详细内容。</span></p><p>&nbsp;</p><p><span>17 更多关于 ICI 的信息请参见[Jouppi 等人，2023](https://doi.org/10.48550/arXiv.2304.01433)，第 2 节。值得注意的是，TPUv4 使用光开关来满足长距离互连需求。</span></p><p>&nbsp;</p><p><span>18 例如，[PCIe 4.0 支持长达 30 厘米](https://www.elektronik-kompendium.de/sites/com/0904051.htm)。根据[Jouppi 等，2023](https://doi.org/10.48550/arXiv.2304.01433)，第 7.2 节，Google ICI 用于连接 1024 个 TPUv3，但最大长度并未提供。</span></p><p>&nbsp;</p><p><span>19 InfiniBand 和 Ethernet 支持的网络带宽低于 PCIe，因此它们定义了峰值带宽。</span></p><p>&nbsp;</p><p><span>20 按照 PCI-SIG 协会的标准；预计到 2025 年将增加到 256GB/s。需要注意的是，带宽变化的速度是由协会定义的，而该协会可能在采纳市场的即时需求方面较为缓慢。</span></p><p>&nbsp;</p><p style="text-align:left"><span>21 根据[NVIDIA H100 Tensor Core GPU Architecture, v1.04, p47 的说明](https://nvdam.widen.net/s/95bdhpsgrs#page=47)：在多 GPU IO 和共享内存访问中，总带宽达到 900GB/秒，新的 NVLink 提供的带宽是 PCIe Gen 5 的 7 倍。... H100 包括 18 条第四代 NVLink 连接，提供 900GB/秒的总带宽...</span></p><p>&nbsp;</p><p><span>22 Google Cloud 提供一年的 37% 的使用折扣。因此，我们估计 40% 是谷歌从正常云计算中获利的合理下限。有关云计算价格的更多考虑可以在 https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems 找到。</span></p><p>&nbsp;</p><p><span>23 相关性判断结合了作者直觉和我们在先前帖子中的推理。</span></p><p>&nbsp;</p><p><span>24 Chiplet 是一个将多个芯片集成到一个集成电路/封装中的例子。</span></p><p>&nbsp;</p><p><span>25 根据[Hennessy 等人，《计算机体系结构》，2017 年，第 24 页](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1) 的描述：TDP 既不是峰值功率（峰值功率通常要高 1.5 倍），也不是在特定计算过程中实际消耗的平均功率（平均功率可能更低）。</span></p><p>&nbsp;</p><p><span>26 支持这一观点的证据来自（Gigabyte 术语表，https://www.gigabyte.com/Glossary/tdp）：在一个稳定的、企业级的服务器房间或数据中心中，TDP 大致等同于计算设备的功耗，因为服务器通常处于最大容量或接近最大容量运行。</span></p><p>&nbsp;</p><span id="OSC_h2_12"></span><h2 style="margin-left:8px; margin-right:8px; text-align:left">&nbsp;</h2><p>&nbsp;</p><p><span style="background-color:#ffffff; color:#888888">其他人都在看</span></p><span id="OSC_h3_13"></span><h3 style="text-align:left">&nbsp;</h3><ul><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492895%26idx%3D1%26sn%3D572040d50dcc39bb7e93c1f75e121599%26chksm%3Dfe426b29c935e23f80ba9ec00f2bbbef26c4a6af6c0457bc41d1e10f2d9fb78b66b91fe5edb0%26scene%3D21%23wechat_redirect" target="_blank">GPU 架构与计算入门指南</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492849%26idx%3D1%26sn%3D51f53e04b4b97cd9dd38429784015c98%26chksm%3Dfe426ac7c935e3d1b5970441a68c53b6dae05792cd9a244ad8efb40ffc5ab4c60cdf3a7181b0%26scene%3D21%23wechat_redirect" target="_blank">LoRA 和 QLoRA 微调语言大模型</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247489016%26idx%3D1%26sn%3D3fc5d8ab05d2af9a7b95a1002ea128e9%26chksm%3Dfe419bcec93612d8220be748c1eea51e334fa489b72522ef45fe39141075a74d3669ec0f4110%26scene%3D21%23wechat_redirect" target="_blank">深度学习硬件的过去、现在和未来</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492976%26idx%3D1%26sn%3Dd919a508ce238048ae44e58b9cc06b71%26chksm%3Dfe426b46c935e2500178c2d2c8845fcd3e47fdeb5ec51f55b80cbca5f7b78382cdb3e6fe6a32%26scene%3D21%23wechat_redirect" target="_blank">可复现的语言大模型推理性能指标</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492990%26idx%3D1%26sn%3D50844c8911834baf44863a9e3754175f%26chksm%3Dfe426b48c935e25ede3f772624ba262011b1b48f8ee78ac6d3b1daa5aaf71e7583828740b5cd%26scene%3D21%23wechat_redirect" target="_blank">ChatGPT 规模化服务的经验与教训</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493030%26idx%3D1%26sn%3D58a43ed078977019c997a110526d7c02%26chksm%3Dfe426b90c935e28688b6e317a991bedaaa164471a275d64e60851a09b00f7f6b718e27d7b411%26scene%3D21%23wechat_redirect" target="_blank">语言大模型的分布式训练与高效微调指南</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492957%26idx%3D1%26sn%3D6ab02e219cb41ab8390cd6fc984125c6%26chksm%3Dfe426b6bc935e27d43802825c89eae6b2d346f7b62919270f8c77c0693913395eaf36af8d4a3%26scene%3D21%23wechat_redirect" target="_blank">开源语言大模型演进史：向 LLaMA2 看齐</a></p></li></ul><span id="OSC_h3_14"></span><h3 style="text-align:left">&nbsp;</h3><p><strong><span>试用 OneFlow: <a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgithub.com%2FOneflow-Inc%2Foneflow%2F" target="_blank">github.com/Oneflow-Inc/oneflow/</a></span></strong></p><p style="color:#3f3f3f; margin-left:8px; margin-right:8px; text-align:left"><img src="https://oscimg.oschina.net/oscnet/71c4a4ad-2729-4491-b991-856a2ff43999.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p></div><p style="color:#858585">本文分享自微信公众号 - OneFlow（OneFlowTechnology）。<br> 如有侵权，请联系 support@oschina.cn 删除。<br> 本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/oneflow/blog/10319840</guid>
            <link>https://my.oschina.net/oneflow/blog/10319840</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源 MoE 模型 Mixtral 8x7B 性能超过 GPT-3.5]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>大模型创业公司 Mistral AI 终于<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">介绍了</a></u>前两天「开源」的&nbsp;MoE 模型 <strong>Mixtral 8x7B</strong>。</p><blockquote><p><strong><em><u><a href="https://www.oschina.net/news/270317/mixtral-8x7b-32kseqlen">Mistral AI 用「磁链链接」开源了 87 GB 的 8x7B MoE 模型</a></u></em></strong></p></blockquote><p>官方称，Mixtral 8x7B 是开放权重的高质量<strong>稀疏混合专家模型 (SMoE)</strong>，采用 Apache 2.0 License 开源。在大多数基准测试中，Mixtral 的成绩都优于 Llama 2-70B，且推理速度提升了 6 倍。而且在大多数标准基准测试中超过 GPT-3.5。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7a689c4f538b591b9744038a052717945e6.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-84fefd9ee6c091c07c894031a1af2faf2e3.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-9f9aaad324856028fb1e796beb2d7685020.png" referrerpolicy="no-referrer"></p><p>因此，Mistral AI 称 Mixtral 是最强大的开放权重模型，也是成本/性能权衡方面的最佳模型。</p><p><strong>Mixtral 主要特性</strong></p><p>• 32k 上下文<br> • 支持英语、法语、意大利语、德语和西班牙语<br> • 性能超过 Llama 2 系列和 GPT-3.5<br> • 在代码生成方面具有强劲性能<br> • 在 MT-Bench 上获得 8.3 分</p><p>Mixtral 作为稀疏混合专家网络，是一个纯解码器模型，其中前馈块从 8 组不同的参数组中选择。在每一层，对于每个 token，路由网络选择两组「专家」来处理 token 并相加地结合它们的输出。</p><p>Mixtral 总共有 45B 个参数，但每个 token 只使用 12B 个参数。因此，它以与 12B 模型相同的速度和成本处理输入和生成输出。</p><p>更多细节查看：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">https://mistral.ai/news/mixtral-of-experts/</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 10:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270511/mixtral-of-experts</guid>
            <link>https://www.oschina.net/news/270511/mixtral-of-experts</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[荣耀申请魔方大模型商标]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#222222">天眼查信息显示，荣耀终端有限公司近日申请注册「荣耀魔方大模型」商标，国际分类为网站服务，当前商标状态为等待实质审查。</span></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-7baf34d7d00360b976559630121d67b0da4.png" width="700" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">此前，该公司曾申请两枚「MAGIC&nbsp;大模型」商标。荣耀 CEO 赵明曾发文称，荣耀即将推出自研端侧 AI 大模型和全新云服务。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 08:25:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270492</guid>
            <link>https://www.oschina.net/news/270492</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[HashiCorp 采用 BSL 后续，Linux 基金会孵化 Vault 开源替代品]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">今年 8 月，<span style="background-color:#ffffff">专注于云基础设施的软件供应商 HashiCorp&nbsp;</span>宣布<span style="background-color:#ffffff">修改其核心产品的开源协议。</span><strong style="color:#333333">所有 HashiCorp 产品的未来版本</strong><span style="background-color:#ffffff">将从 Mozilla Public License v2.0 (MPL 2.0) 变更为&nbsp;</span><strong style="color:#333333">Business Source License (BSL, also known as BUSL) v1.1</strong><span style="background-color:#ffffff">，其中包括&nbsp;Vault、Boundary、Consul、Nomad、Packer、Terraform、Vagrant 和 Waypoint 等。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">采用 BSL 1.1 的项目，其代码仍会公开 (source-available)，</span><strong style="color:#333333">但只允许在特定条件下进行复制、修改、重新分发、非商业使用和商业使用</strong><span style="background-color:#ffffff">&nbsp;—— 主要是添加了商业使用方面的限制。</span></span></p><p><span style="color:#000000">此后，社区在抗议无效后选择创建了 Terraform 的分支项目 OpenTofu（原名 OpenTF），并托管在了 Linux 基金会下。</span></p><p><span style="color:#000000"><span style="background-color:#ffffff">时至今日，有消息称 Linux 基金会正计划帮助孵化一个私密信息管理工具 Vault 的开源替代品。DevOps 自动化公司 Scalr 的联合创始人兼首席执行官 Sebastian Stadil 和 OpenTofu 的组织者之一<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theregister.com%2F2023%2F12%2F08%2Fhashicorp_openbao_fork%2F" target="_blank">透露</a>，Vault 开源替代品的项目名为 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwiki.lfedge.org%2Fdisplay%2FOH%2FOpenBao%2B%2528Hashicorp%2BVault%2BFork%2Beffort%2529%2BFAQ" target="_blank">OpenBao</a>，是竞争对手在 MPL 2.0 协议下创建的一个&nbsp;Vault 分支。</span></span></p><p><img height="287" src="https://oscimg.oschina.net/oscnet/up-1f318fa9f93212c7ed6a67c0b91e135c731.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000"><span style="background-color:#ffffff">OpenTofu 计划在本月晚些时候发布候选版本，OpenBao 也将开始接受新的贡献。Stadil 表示，「如果有两个相同的项目，一个是开源的，一个不是，我个人认为，道德上的选择是使用开源项目，并以某种方式提供帮助。」</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">不过鉴于 OpenTofu 和 OpenBao 都是新近开发的项目，项目的可行性和持久性受到了很多关注。针对这一担忧，Stadil 表示拒绝代表其他公司发言。事实上，他还被告知不要透露任何关于其他组织支持这些项目的消息。对于那些想要了解更多详情的人，他建议可以访问项目的 repos。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">当被问及 HashiCorp 重新授权其软件的理由时，Stadil 回答称，官方的说法是 Terraform 对互联网至关重要，而长期以来人们一直希望将其置于 Linux 基金会的监督之下。「如果 HashiCorp 将来愿意加入我们的 OpenTofu，我们会很乐见其成」。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">但</span><span style="background-color:#ffffff">他无法推测 HashiCorp 的内部决策过程。Stadil 指出，Hashicorp 一直在烧钱，随着利率的上升，这家软件公司选择采取措施创造更多收入也不足为奇。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">上周，HashiCorp 公布了 2024 财年第三财季的营收报告。营收 1.461 亿美元，同比增长 17%。按照美国通用会计准则（GAAP），净亏损为 3950 万美元，低于去年同期的 7200 万美元。</span></span></p><p><strong><span style="color:#000000"><span style="background-color:#ffffff">相关阅读：</span></span></strong></p><ul><li><a href="https://www.oschina.net/news/253275/hashicorp-adopts-business-source-license" target="_blank">HashiCorp 核心产品变更开源协议，未来将采用 BSL</a></li><li><p style="margin-left:0px; margin-right:0px; text-align:start"><a href="https://www.oschina.net/news/255700/opentf-fork-terraform" target="_blank">HashiCorp 采用 BSL 后，社区创建 Terraform 分支 OpenTF</a></p></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 07:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270477/hashicorp-vault-openbao-fork</guid>
            <link>https://www.oschina.net/news/270477/hashicorp-vault-openbao-fork</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[周热点 | Linus 收敛火爆脾气，谈内核社区「老龄化」问题；Firefox 或将被淘汰；谷歌发布最强 AI 模型 Gemini............]]>
            </title>
            <description>
                <![CDATA[回顾一周热门资讯。2023.12.04-2023.12.10]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 06:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094041&#38;idx=1&#38;sn=18ed1a99fdf7fbbbc52688a346795664&#38;chksm=880c4c8abf7bc59cbaf66865402e963af1309b4bb627cd89ae402f319f12ce5c56561126ea09&#38;token=1220110296&#38;lang=zh_CN#rd</guid>
            <link>https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094041&#38;idx=1&#38;sn=18ed1a99fdf7fbbbc52688a346795664&#38;chksm=880c4c8abf7bc59cbaf66865402e963af1309b4bb627cd89ae402f319f12ce5c56561126ea09&#38;token=1220110296&#38;lang=zh_CN#rd</link>
        </item>
        <item>
            <title>
                <![CDATA[GitHub.com 跑了 1200 多台 MySQL 主机，如何无缝升级到 8.0？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>GitHub 团队近日<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-12-07-upgrading-github-com-to-mysql-8-0%2F" target="_blank">分享</a></u>了他们将 GitHub.com 的底层数据库无缝升级到 MySQL 8.0 的经验。</p><p>据介绍，GitHub 使用 MySQL 来存储大量关系数据，因此在不影响网站服务级别目标 (SLO) 的情况下升级主机集群（<strong>1200 多台 MySQL 主机</strong>）绝非易事。其团队表示，为了升级到 MySQL 8.0，他们规划、测试和升级本身总共花费了一年多的时间，并且需要 GitHub 内部多个团队的协作。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-613c88c0257637ef029cdd9528c6f8a3217.png" referrerpolicy="no-referrer"></p><p><strong>GitHub 的 MySQL 基础设施概览：</strong></p><ul><li>由 1200 多台主机组成，包括数据中心中的<strong> Azure 虚拟机和裸机主机</strong></li><li>存储超过 300 TB 的数据，并在 50 多个数据库集群中每秒处理 550 万次查询</li><li>每个集群都配置为具有主副设置的高可用性</li><li>分区存储数据——利用水平和垂直分片来扩展 MySQL 集群，以及使用 MySQL 集群来存储特定产品领域的数据。此外还为大结构域 (large-domain) 提供了水平分片的 Vitess 集群，这些区域的增长超出了单主 MySQL 集群的规模</li><li>庞大的工具生态，包括 Percona Toolkit、gh-ost、orchestrator、freno 和用于操作主机集群的内部自动化工具</li></ul><p>由于需要操作两个版本的 MySQL，因此 GitHub 内部使用的工具和自动化设施需要能够兼容处理混合版本，并了解 5.7 和 8.0 之间<strong>新的、不同的或已弃用的语法</strong>。</p><p>为了满足可用性标准，GitHub 团队采取了逐步升级策略，满足在整个过程中进行 checkpoint 和回滚的需求。下面是他们制定的升级计划：</p><ul><li><strong>步骤 1：升级滚动副本 (rolling replica)</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-c9d574db1e2fec9bf7da0d7c92091b0fb19.png" referrerpolicy="no-referrer"><p>&nbsp;</p></li><li><strong>步骤 2：升级备份拓扑 (replication topology)</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-305231a10282f80062ca4f1d665c36305ee.png" referrerpolicy="no-referrer"><p>&nbsp;</p></li><li><strong>步骤 3：将 MySQL 8.0 主机提升为主集群</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-0e9f6defe7e920b0167c797000292c7e390.png" referrerpolicy="no-referrer"><p>&nbsp;</p></li><li><strong>步骤 4：升级面向内部的实例类型</strong></li><li><strong>步骤 5：清理，</strong>确认集群不需要回滚并成功升级到 MySQL 8.0 后，删除 5.7 服务器。验证工作会至少经历一个完整的 24 小时流量周期，以确保在高峰流量期间不会出现问题。</li></ul><p>至于为什么要升级到 MySQL 8.0，GitHub 团队表示主要是因为 MySQL 5.7 的生命周期即将结束。此外升级后可以获得最新安全补丁、错误修复和性能增强的 MySQL 版本。他们还希望测试 8.0 中的新功能并从中受益，包括即时 DDL、隐形索引和压缩的 bin 日志等。</p><p>详细的技术细节查看：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-12-07-upgrading-github-com-to-mysql-8-0%2F" target="_blank">https://github.blog/2023-12-07-upgrading-github-com-to-mysql-8-0/</a></u></em></p><hr><p>延伸阅读</p><ul><li><u><a href="https://www.oschina.net/news/188164/github-recent-service-disruptions">GitHub 解释近期频繁宕机原因：MySQL 不堪重负</a></u></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 05:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270460/upgrading-github-com-to-mysql-8-0</guid>
            <link>https://www.oschina.net/news/270460/upgrading-github-com-to-mysql-8-0</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[TIOBE 12 月：C# 有望成为年度编程语言]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#000000"><span style="background-color:#ffffff">TIOBE 公布了 2023&nbsp;年 12 月的</span></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F" target="_blank">编程语言排行榜</a><span style="background-color:#ffffff; color:#000000"><span style="background-color:#ffffff">。</span></span></p><p><img height="77" src="https://oscimg.oschina.net/oscnet/up-e944f70ee629593d3b3ba2ac7d008e89e4b.png" width="700" referrerpolicy="no-referrer"></p><p>2023 年度 TIOBE 编程语言名单即将出炉，其中最有望胜出的当属&nbsp;C#。事实上，早在 2022 年&nbsp;C# 就有望夺得该桂冠，但却在最后时刻被&nbsp;C++ 反超。而在今年，C# 的胜率又多出了几分；因为该语言在一年内的增长率为 +2.38%，与其最接近的竞争者 Fortran 和 F# 的增长率则仅分别上涨了 +0.64% 和 +0.48%。</p><p>此外，Top 20 中的大部分语言人气都出现了下降。<span style="background-color:#ffffff; color:#000000">TIOBE CEO&nbsp;Paul Jansen 评论称，</span>「答案就在所有小语言所在的长尾（long tail）部分。这些语言的受欢迎程度都在上升，而且越来越接近大语言」。例如：一年前，排名第 50 位的语言得分仅为 0.14%，但现在第 50 位语言的得分已经达到了 0.24%。</p><p><strong style="color:#333333">TIOBE 12 月 TOP 20 编程语言</strong></p><p><img height="414" src="https://oscimg.oschina.net/oscnet/up-b25283a71bbac81145079c4b2848ccc6e95.png" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#000000">相较上月，除了 Ruby<span>&nbsp;</span>(18→19)、R (19→20) 以及 Rust (20→18) 之间出现了小范围波动外，Top&nbsp;10-20 榜单没有其他任何排名变化，这也是近期以来榜单变动最小的一次。</span></p><p><strong style="color:#333333">TOP 10 编程语言 TIOBE 指数走势（2002-2024）</strong></p><p><img height="228" src="https://oscimg.oschina.net/oscnet/up-c048f61fdb18f5fa94fbc07b575f6acc8f9.png" width="700" referrerpolicy="no-referrer"></p><p><strong style="color:#333333">第 21-50 名编程语言排行</strong></p><p><img height="430" src="https://oscimg.oschina.net/oscnet/up-e1c00e5bc23507475a73c563cbdb213cdc9.png" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#000000">第 51-100 名如下，由于它们之间的数值差异较小，仅以文本形式列出（按字母排序）：</span></p><p>&nbsp;</p><blockquote><p>4th Dimension/4D, ABC, Algol, Apex, ATLAS, AutoLISP, Bash, Boo, Carbon, CIL, CL (OS/400), Clipper, Clojure, Curl, Eiffel, Elm, Erlang, GAMS, Groovy, Icon, Inform, Io, J#, LabVIEW, Ladder Logic, LiveCode, Maple, Modula-2, MOO, MQL5, NATURAL, Nim, OCaml, OpenEdge ABL, PostScript, Pure Data, Q, Racket, Ring, RPG, Smalltalk, Snap!, Solidity, SPARK, SPSS, Tcl, VHDL, Wolfram, X10, Zig</p></blockquote><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000">TIOBE 编程社区指数（The TIOBE Programming Community index）是一个衡量编程语言受欢迎程度的指标，该指数每月更新一次。评判的依据来自世界范围内的工程师、课程和第三方供应商，包括流行的搜索引擎，如 Google、必应、雅虎、维基百科、亚马逊、YouTube 和百度都被用于指数计算。值得注意的是，TIOBE 指数并不代表编程语言的好坏或编写代码的多少。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000">该指数可以用来检查你的编程技能是否还能跟上时代的步伐，或者在开始建立一个新的软件系统时，基于指数对采用何种编程语言做出决策。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2Fprogramminglanguages_definition%2F" target="_blank">TIOBE 指数</a><span style="color:#000000">的定义方式，以及详细榜单信息<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F" target="_blank">均可查看官网</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 03:47:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270438/tiobe-index-2023012</guid>
            <link>https://www.oschina.net/news/270438/tiobe-index-2023012</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[因 EXT4 数据损坏错误，Debian 12.3 推迟发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Debian 团队发布公告称，由于 Linux 内核 6.1.64-1 中的<strong> ext4 文件系统出现数据损坏问题</strong>，因此原计划昨天发布的 Debian 12.3 将会被推迟，同时进行修复。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-6b960c796ab8ff358469f03578c81866ec1.png" referrerpolicy="no-referrer"></p><p>来源：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.debian.org%2FNews%2F2023%2F2023120902" target="_blank">https://www.debian.org/News/2023/2023120902</a></u></p></blockquote><p>据介绍，此 bug 由从 Linux 6.5 回溯的一个有问题补丁导致，它引起了 EXT4 和 iomap 代码之间的干扰，可能导致旧内核上的数据损坏。</p><p>这个问题主要出现在最近的 Linux 6.1 LTS 点版本中，新的 Linux 6.1.66 版本已经回滚了有问题的提交。Debian 的 bug 报告称这个问题为「非严重的数据丢失」，因此应该是可以恢复的。</p><p>但由于 Debian 12.3 原本计划发布的内核版本受到了影响，因此被推迟发布。建议 Debian 12 用户在 Linux 6.1.66 内核镜像推出之前不要升级系统。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 03:21:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270435/debian-12-3-delayed-ext4-corrupt</guid>
            <link>https://www.oschina.net/news/270435/debian-12-3-delayed-ext4-corrupt</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[周鸿祎：有人找我做养猪大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在 2023 中国企业领袖年会上，360 创始人周鸿祎对于最近的 AI 大模型热潮发表了看法。</p><p>他表示，（感觉）大家对大模型充满了一种无限的向往或者不切实际的膜拜，之前还有人找他做养猪大模型。他认为，大模型的技术路线突破才短短几年，目前还存在着很多缺点。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-391bf18f540407673913ddee0ac73938969.png" referrerpolicy="no-referrer"></p><p>来源：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2Ftv%2Fshow%2F1034%3A4977511076134973%3Ffrom%3Dold_pc_videoshow" target="_blank">https://weibo.com/tv/show/1034:4977511076134973</a></u></p></blockquote><p>他希望大家对大模型有一个正确的认知，<strong>不要高估现在大模型的能力，不要低估大模型未来发展的潜力</strong>，虽然它现在已经可以跟实体产业相结合，但它还不能完全接管此类业务，应该扬长避短发挥它的长处，因为很多短板还有待解决。</p><p>目前国内各大企业、科研机构和高校等单位已公开的 AI 大模型至少已经达到了 188 个，而首批通过《生成式人工智能服务管理暂行办法》备案的大模型已于 8 月 31 日公布，第二批通过备案的 AI 大模型也已于 11 月开放服务。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3a3b3aaeff5f0043de536b6f5f44b963797.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 03:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270430</guid>
            <link>https://www.oschina.net/news/270430</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[理想汽车全自研多模态认知大模型 —— Mind GPT]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>理想汽车于 12 月 10 日晚正式发布 OTA 5.0 版本，并计划于 12 月 19 日开启全量用户推送。官方介绍称，在 OTA 5.0 中，理想同学最大的变化是引入了 Mind GPT 的能力。</p><p>Mind GPT 是理想全自研的多模态认知大模型，据称他们从 0 到 1 构建了 Mind GPT 原始基座模型，<strong>模型结构采用了自研的 TaskFormer 神经网络架构</strong>，基于用车、娱乐、出行等场景使用 SFT、RLHF 等技术进行了一系列的训练，让 Mind GPT 拥有了理解、生成、知识记忆及推理的三大能力。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7aa6319e367ca499b8541b306ec3892d181.png" referrerpolicy="no-referrer"></p><p>目前 Mind GPT 还处于内测版本阶段，那么 Mind GPT 在行业里到底是什么水平呢？</p><p>官方称在目前国内极具权威性的，中文大语言模型评测榜单 C-EVAL，覆盖了人文、社科、理工等多个方向共 52 个学科，Mind GPT 在 58 个参加测评的大模型中排行第一名；同时，还有涵盖从基础学科到高级专业包含 67 个主题领域的评测榜单 CMMLU，Mind GPT 也获得第一名，拿下了双冠军。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-d89ae63875873819f3b3676b6fea16d8c81.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-af86f00982833de97dd852d6a57f17288dd.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 02:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270426</guid>
            <link>https://www.oschina.net/news/270426</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[DDE V23 全新升级，引爆海外科技媒体关注！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>内容来源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fdde-overseas-concerns%2F" target="_blank">deepin 社区</a></p><hr><p>近日，DDE (Deepin Desktop Environment，深度桌面环境) 迎来版本升级，并被移植到多个主流 Linux 发行版中，引发多家海外知名科技媒体的关注和报道。</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="555" src="https://storage.deepin.org/thread/202312111015036481_1.png" width="1041" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:center"><em>TechBullion，英国知名科技媒体</em></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">报道称，诞生于中国 Linux 开源社区的桌面环境 DDE，自 2013 年推出以来，创造了中国 Linux 历史的多个「首次」：推出首个&nbsp;Linux 桌面应用商店，率先在 Linux 桌面引入智能助手，首家支持高分屏、软件包签名、人脸识别登录……与 GNOME、KDE、XFCE 等国际主流桌面环境一样，DDE 在全球广受欢迎，已经被移植到包括 Arch、openSUSE、Ubuntu、Fedora、Manjaro 等主流 Linux 发行版中。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><img alt="" height="508" src="https://storage.deepin.org/thread/202312111015122694_2.png" width="1037" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:center"><em>Gearrice，美国知名技术和创新媒体网站</em></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">DDE V23 预览版公布后，全新设计引发了广大开源社区网友们的关注。同时随着 DDE V23 各个项目的代码开源以及代码稳定程度的不断改善，不同发行版的包维护者们也开始了对全新 DDE 桌面环境的移植。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">现如今，在这些来自不同发行版的包维护者和社区贡献者们的一同努力下，已经有多个发行版完成 DDE V23 桌面环境的主要组件适配，包括 Arch Linux、UbuntuDDE、NixOS、openSUSE 等。还有很多发行版仍在积极的对 DDE V23 进行移植，其中知名的发行版包括 Debian、Gentoo、Fedora 等。</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="755" src="https://storage.deepin.org/thread/20231211101533641_3.png" width="1027" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">此外，针对 DDE V23 全新升级的报道，海外用户给出了积极的评价，认为「deepin（深度）操作系统已经很好地满足了用户所需要的一切，DDE is so pretty！」</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="605" src="https://storage.deepin.org/thread/20231211101547509_4.png" width="1032" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Flatest-news-deepin-v23%2F" target="_blank">同时，deepin（深度）社区研发负责人表示，DDE V23 即将在明年的 deepin V23 正式版里与大家见面。</a>deepin V23 是 deepin（深度）社区最新的发行版，将具备全新的系统结构、更多的系统架构的支持、桌面环境的持续优化、全自研 Wayland 合成器 Treeland、多会话全局窗口管理器、更强大的功能以及系统兼容性、稳健性的持续提升。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">deepin 积极拥抱新技术，努力推出更多创新的功能和服务。如果你对 DDE V23 的适配感兴趣，希望参与进来，为 DDE 的移植贡献一份力量，欢迎加入 deepin（深度）社区下的 [<strong>DDE-porting 兴趣小组</strong>]，一同展开 DDE 移植相关的话题讨论，让更多用户体验 DDE 桌面环境。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>DDE-porting 兴趣小组：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdeepin-community%2Fsig-dde-porting" target="_blank">https://github.com/deepin-community/sig-dde-porting</a></strong></p><hr><p style="text-align:right">内容来源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Findex%2Fzh" target="_blank">deepin 社区</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 02:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270420/dde-updated</guid>
            <link>https://www.oschina.net/news/270420/dde-updated</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[大湾区一体化算力服务平台正式发布，算力规模超 5000P]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>12 月 10 日，在第二届数字政府建设峰会暨数字湾区发展论坛上，深圳市前海管理局、国家（深圳·前海）新型互联网交换中心（下称「前海交换中心」）共同<strong>发布粤港澳大湾区一体化算力服务平台，并正式成立前海算力服务联盟</strong>。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-e379cb9cc7c4002e4f3fb84f03c47a13c02.png" referrerpolicy="no-referrer"></p></blockquote><p>据介绍，该平台由前海管理局提出设想和要求，在深圳市通管局、市工信局的支持下，由前海交换中心和紫金山实验室共同开发部署。</p><p>官方透露，该平台自 10 月 31 日试运行以来，汇聚的算力规模大幅增长近 4 倍，<strong>总规模已达 5180 PFLOPS</strong>，主流芯片覆盖率超 75%，并已为 10 余个企业、高校、科研机构的人工智能团队提供算力服务。</p><ul><li><p><strong>在算力调度方面</strong>，创新多维一体编排算法，实现算力高效调度和智能供给；</p></li><li><p><strong>在算力交易方面</strong>，平台不收取中介费用，促进供需双方合作与交易；</p></li><li><p><strong>在算力应用方面</strong>，高度集成各类算法工具，实现应用一键部署、资源秒级开通，进一步降低门槛、提升效率；</p></li><li><p><strong>在算力安全方面</strong>，构建算网一体化安全防护体系，持续强化算力安全保障。</p></li></ul><p><img height="360" src="https://static.oschina.net/uploads/space/2023/1211/102814_wKUa_2720166.png" width="640" referrerpolicy="no-referrer"></p><p>同时， <strong>大湾区首个算力服务行业组织 —— 前海算力服务联盟正式成立</strong>，首批成员单位包括前海科创集团、前海交换中心、紫金山实验室、华为、深圳商汤、万国数据、世纪互联、深圳数据交易所、深圳科创学院、香港中文大学未来智联网络研究院、粤港澳大湾区大数据研究院。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 02:30:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270418</guid>
            <link>https://www.oschina.net/news/270418</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
