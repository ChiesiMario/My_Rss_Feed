<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Tue, 12 Dec 2023 07:32:41 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[铠侠向 Linux 基金会捐赠 Software-Enabled Flash SDK]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#121212">几年前从东芝分离出来的存储公司 Kioxia（</span>铠侠<span style="background-color:#ffffff; color:#121212">）向 Linux 基金会捐赠了一个软件开发工具包 (SDK)，用于建立 Software-Enabled Flash SDK。</span></p><p><span style="background-color:#ffffff; color:#121212">Linux 基金会发布<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linuxfoundation.org%2Fpress%2Fsoftware-enabled-flash-announces-software-development-kit-sdk" target="_blank">公告称</a>，「SEF SDK 的发布是存储技术领域的一个重要里程碑......SEF 项目对 KIOXIA 突破性地捐赠软件定义闪存原生 SDK 表示热烈欢迎，这将为开发人员提供前所未有的能力，使他们能够为闪存存储（flash storage）应用开发定制的独特软件。」</span></p><p><img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-67690b065c2207474d1a67124aa3ef403da.png" width="300" referrerpolicy="no-referrer">&nbsp; &nbsp;<img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-1056c78ed4258dcb84497a6e896204821c0.jpg" width="300" referrerpolicy="no-referrer"></p><p>该 SEF SDK 包括示例代码和文档，以充分利用 flash media control 的潜力；包括 WAF 减少、延迟控制、对 ZNS 和 FDP 或 Block 等多种协议的支持等。</p><p>SEF 项目旨在通过加强对驱动器的管理、增强工作负载隔离、加强延迟控制以及实现对闪存管理的更多&nbsp;host-control，在现代数据中心中开辟新的用途并最大限度地发挥基于闪存的存储潜力。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270608/software-enabled-flash-sdk</guid>
            <link>https://www.oschina.net/news/270608/software-enabled-flash-sdk</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[芯瞳正式加入 openKylin，为社区贡献高质量的国产 GPU 解决方案！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>近日，芯瞳半导体技术（山东）有限公司（以下简称「芯瞳」），签署 openKylin 社区 CLA（Contributor License Agreement 贡献者许可协议），正式加入 openKylin 开源社区。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="1079" src="https://oscimg.oschina.net/oscnet/up-4c9b13fca5452f4a217f1494d816e96a799.png" width="829" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>芯瞳（Sietium）成立于 2019 年，是一家自主设计研发 GPU 芯片及 GPU 解决方案的高科技公司，以行业先进的计算和图形渲染平台为依托，用高质量的产品和服务为云端、终端客户提供可持续发展的国产 GPU 解决方案；为数字时代的创新与发展提供算力支撑，构建自由算力的文明世界。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="410" src="https://oscimg.oschina.net/oscnet/up-6914c94ad47861f5f685cb96e9bc21450f1.png" width="940" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span><strong><span>加入 openKylin 社区后，芯瞳将参与维护社区 GPU SIG 和 Wayland SIG</span></strong><span>。<strong>凭借其自研的 GPU 显卡和深厚的行业经验，优化 openKylin 环境中显卡驱动的兼容性，确保与芯瞳显卡的完美适配</strong>。</span></span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>在 openKylin 平台上，芯瞳显卡将展现其在图形显示、渲染、视频编解码和大规模计算等方面的优势，以此提升 openKylin 的用户体验，并提供持续的 GPU 产品升级和技术支持，为用户提供安全可靠的使用体验。具体计划如下：</span></p><ul><li><p style="margin-left:0; margin-right:0"><span>积极参与社区合作，紧密关注社区的发展动态，与社区成员携手推动 openKylin 社区的生态及品牌建设，努力构建一个健康的生态环境，为开源生态的发展贡献力量。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>寻求与社区的技术合作，通过联合调试等方式，使 openKylin 的相关产品能更好地兼容并适应芯瞳的全新系列显卡，从而提高产品的稳定性和性能。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>在应用层面，芯瞳将持续优化软件算法，提高系统效率，充分发掘 openKylin 在芯瞳显卡平台上的性能潜力，从而提升整体性能，为用户提供卓越的产品体验。</span></p></li></ul><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>通过这一系列的举措，芯瞳将与 openKylin 社区并肩前行，共同推动 openKylin 社区生态良好发展，为用户带来更多的创新和惊喜。同时，芯瞳期待与社区成员进行深入的交流和分享，以推动技术的进步和产业的协同发展，共同为中国开源生态的繁荣作出贡献。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270607</guid>
            <link>https://www.oschina.net/news/270607</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Facebook 开源 StyleX —— 在 JavaScript 中写 CSS]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Meta（原 Facebook）<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstylexjs.com%2Fblog%2Fintroducing-stylex%2F" target="_blank">开源</a></u>了全新的 CSS-in-JS 库 StyleX。</p><p><img src="https://oscimg.oschina.net/oscnet/up-30f683ba9535a9f16ce5e615736da0460cd.png" referrerpolicy="no-referrer"></p><blockquote><p><em>GitHub 地址：<strong><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Fstylex" target="_blank">https://github.com/facebook/stylex</a></u></strong></em></p></blockquote><p>官方介绍道，StyleX 是一个富有表现力、具有确定性、可靠且可扩展的样式系统。它通过使用编译时 (compile-time) 工具融合了静态 CSS 的性能和可扩展性。</p><p>此外，StyleX 不仅仅是一个基于编译器的 CSS-in-JS 库，它经过精心设计，可以满足大型应用程序、可复用组件库和静态类型代码库的要求。Meta 旗下多款产品如 Facebook、WhatsApp、Instagram、Workplace、Threads 等都在使用 StyleX 作为其 CSS 样式解决方案。</p><p>StyleX 主要特性</p><ul><li><p><strong>快速</strong>：StyleX 在编译时和运行时都具备高效的性能。Babel 转换不会对构建过程产生显著影响。在运行时，StyleX 避免了使用 JavaScript 插入样式的开销，并仅在必要时高效地组合类名字符串。生成的 CSS 经过优化，确保即使是大型网站的样式也能被浏览器快速解析。</p></li><li><p><strong>可扩展</strong>：StyleX 旨在适应像 Meta 这样的超大型代码库。通过原子构建和文件级缓存，Babel 插件能够处理数万个组件在编译时的样式处理。由于 StyleX 设计为封装样式，它允许在隔离环境中开发新组件，并期望一旦在其他组件中使用时能够可预测地呈现。</p></li><li><p><strong>可预测性</strong>：StyleX 会自动管理 CSS 选择器的特异性，以确保生成的规则之间不会发生冲突。它为开发人员提供了一个可靠地应用样式的系统，并确保「最后应用的样式始终生效」。</p></li><li><p><strong>类型安全</strong>：使用 TypeScript 或 Flow 类型来约束组件接受的样式，每个样式属性和变量都具有完全的类型定义。这有助于提高代码的可读性和可维护性，同时减少潜在的错误和冲突。</p></li><li><p><strong>样式去重</strong>：StyleX 鼓励在同一文件中编写样式和组件。这种方法有助于使样式在长期内更具可读性和可维护性。StyleX 能够利用静态分析和构建时工具来跨组件去重样式，并删除未使用的样式。</p></li><li><p><strong>可测试性</strong>：StyleX 可以配置为输出调试类名，而不是功能性的原子类名。这可以用于生成快照，以便在对设计进行轻微更改时不会经常变化。通过这种方式，开发人员可以更轻松地测试和验证样式的正确性，从而提高开发效率和产品质量。</p></li></ul><p><strong>示例代码</strong></p><pre><code class="language-javascript">import stylex from '@stylexjs/stylex';

const styles = stylex.create({
  root: {
    padding: 10,
  },
  element: {
    backgroundColor: 'red',
  },
});

const styleProps = stylex.apply(styles.root, styles.element);</code></pre><p><strong>下面是一个按钮组件的示例代码</strong></p><pre><code class="language-javascript">import * as stylex from "@stylexjs/stylex";

const styles = stylex.create({
  base: {
    appearance: "none",
    borderWidth: 0,
    borderStyle: "none",
    backgroundColor: "blue",
    color: "white",
    borderRadius: 4,
    paddingBlock: 4,
    paddingInline: 8,
  },
});

export default function Button({
  onClick,
  children,
}: Readonly&lt;{
  onClick: () =&gt; void;
  children: React.ReactNode;
}&gt;) {
  return (
    &lt;button {...stylex.props(styles.base)} onClick={onClick}&gt;
      {children}
    &lt;/button&gt;
  );
}</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:39:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270597/facebook-stylex-css-in-js</guid>
            <link>https://www.oschina.net/news/270597/facebook-stylex-css-in-js</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Google Colab 现已支持直接使用 🤗 transformers 库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 编辑器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;"><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Google Colab，全称 Colaboratory，是 Google Research 团队开发的一款产品。在 Colab 中，任何人都可以通过浏览器编写和执行任意 Python 代码。它尤其适合机器学习、数据分析和教育目的。从技术上来说，Colab 是一种托管式 Jupyter 笔记本服务。用户无需设置，就可以直接使用，同时还能获得 GPU 等计算资源的免费使用权限。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005864" data-ratio="0.6592592592592592" src="https://oscimg.oschina.net/oscnet/6aca6440-d2d5-4972-8624-54894772e85a.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">通过与 Colab 团队的共同努力，Colab 托管的运行时镜像现已默认集成了 Hugging Face transformers 库，只需简单执行 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">import transformers</code> 即可轻松接入！对于使用 Colab 进行机器学习和深度学习研究的开发者来说，这是一个非常重要的更新。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你想使用最新版本的 transformers，Colab 团队也提供了一个简单的命令 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">!pip install transformers --upgrade</code>，以便于随时更新至最新版本。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">除了提升用户体验，这一更新还开启了一些有趣的新功能。例如，用户现在可以直接从 Pandas 读取 Hugging Face 数据集，这将大大简化数据处理和模型训练的工作流程。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005865" data-ratio="0.4203703703703704" src="https://oscimg.oschina.net/oscnet/8ecbb7d1-9659-48de-9e0f-64e60f62d9ef.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">本合作和更新还开启了一些有趣的新功能。例如，用户现在可以直接从 Pandas 读取 Hugging Face 数据集，这将大大简化数据处理和模型训练的工作流程。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">你可以通过 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">hf://datasets/</code> 的方式在 Pandas 中直接读取 Hugging Face Hub 上的数据集。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">感谢 Colab 团队的朋友们，也希望社区的成员们喜欢本次的合作和功能更新！</p></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - Hugging Face（gh_504339124f0f）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:07:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10316003</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10316003</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 开源调试工具 ixGDB]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-readme-for-ixgdb-release" class="anchor" href="https://gitee.com/deep-spark/ixgdb#readme-for-ixgdb-release"></a>README for ixGDB release</h1><h2><a id="user-content-introduction" class="anchor" href="https://gitee.com/deep-spark/ixgdb#introduction"></a>INTRODUCTION</h2><p>ixGDB is Iluvatar CUDA source-level debugger for Linux OS, based on NVIDIA <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fcuda-gdb">CUDA-GDB</a> 10.2.</p><p>ixGDB provides the following capabilities:</p><ul><li>Provides a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application.</li><li>Supports debugging C/C++ applications and all CUDA applications, which might use CUDA driver APIs or CUDA runtime APIs.</li><li>Supports setting breakpoints.</li></ul><h2><a id="user-content-build-instructions-example-only-adjust-as-needed" class="anchor" href="https://gitee.com/deep-spark/ixgdb#build-instructions-example-only-adjust-as-needed"></a>BUILD INSTRUCTIONS (example only, adjust as needed)</h2><p>First, make sure that libtermcap and other required dependent packages are
installed (try "sudo yum install ncurses-devel"). The "configure" command will
report an error if some packages are missing.</p><p>Please note that the libexpat development headers must be present if ixGDB is to be used for cross-platform debugging.</p><p>Issue the following commands to build ixGDB:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">./configure --program-prefix=cuda- \</span><span id="LC2" class="line">    --enable-cuda \</span><span id="LC3" class="line">    --enable-targets="x86_64-apple-darwin,x86_64-unknown-linux-gnu,\</span><span id="LC4" class="line">    arm-elf-linux-gnu,m68k-unknown-linux-gnu" \</span><span id="LC5" class="line">    CFLAGS='-I/usr/local/cuda/include' \</span><span id="LC6" class="line">    LDFLAGS='-lpthread'</span><span id="LC7" class="line">make</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-using-ixgdb" class="anchor" href="https://gitee.com/deep-spark/ixgdb#using-ixgdb"></a>USING ixGDB</h2><p>All standard GDB commands could be used for both CPU and GPU code debugging. In addition to that, ixGDB provides CUDA-specific command families like "info cuda ..." to query GPU states, "cuda .." to control debugger focus on GPU and "[get|set] cuda .." to alter/query CUDA debugger configuration. If you want to know more about how to use ixGDB, please go to Iluvatar CoreX support <a href="https://gitee.com/link?target=https%3A%2F%2Fsupport.iluvatar.com%2F%23%2FDocumentCentre%3Fid%3D1%26nameCenter%3D1%26productId%3D">official site</a> and use "ixgdb" as the keyword to find document "SDK Tools User Guide", which includes detailed usage of ixGDB.</p><h2><a id="user-content-communication" class="anchor" href="https://gitee.com/deep-spark/ixgdb#communication"></a>COMMUNICATION</h2><p><a href="https://gitee.com/deep-spark/ixgdb/issues">Gitee Issues</a>: bug reports, feature requests, install issues, usage issues, etc.</p><h2><a id="user-content-license" class="anchor" href="https://gitee.com/deep-spark/ixgdb#license"></a>LICENSE</h2><p>Licensee's use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">This product includes copyrighted third-party software licensed under the terms of the GNU General Public License v3 ("GPL v3"). All third-party software packages are copyright by their respective authors.</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses.</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Component    License</span><span id="LC2" class="line">ixGDB        GPL v3</span></pre><div class="markdown-code-block-copy-btn"></div></div></div>]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/deep-spark/ixgdb</guid>
            <link>https://gitee.com/deep-spark/ixgdb</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 机器学习硬件十年：性能变迁与趋势]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p><img src="https://oscimg.oschina.net/oscnet/b41ccc75-f4d2-460d-b101-02526ac0c450.jpg" width="578" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px"><span>本文分析了机器学习硬件性能的最新趋势，重点关注不同 GPU 和加速器的计算性能、内存、互连带宽、性价比和能效等指标。这篇分析旨在提供关于 ML 硬件能力及其瓶颈的全面视图。本文作者来自调研机构<span style="background-color:#efefef">E</span><span style="background-color:#efefef">poch，致力于研究 AI 发展轨迹与治理的关键问题和趋势。</span></span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p><span>（本文由 OneFlow 编译发布，转载请联系授权。原文：https://epochai.org/blog/trends-in-machine-learning-hardware#computational-price-performance</span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p style="text-align:left"><strong><span style="color:#3f3f3f">作者 |&nbsp;</span></strong><strong>Marius Hobbhahn、Lennart Heim、Gökçe Aydos</strong></p><p><strong><span style="color:#3f3f3f">OneFlow 编译</span></strong></p><p><strong><span style="color:#3f3f3f">翻译｜杨婷、宛子琳</span></strong></p><p>&nbsp;</p><p><strong><span>要点概览</span></strong></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/020ab97f-5645-4af9-bc97-767ea99b036a.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span>图 1</span></em></span></strong><span style="color:#888888"><em><span>：常见机器学习加速器在给定精度下的峰值计算性能。自 2016 年以来，已出现了新的数值格式。趋势线展示了带有八个或更多加速器的数值格式：FP32、FP16（FP = 浮点、张量-* = 张量核心处理、TF = Nvidia 张量浮点、INT = 整数）</span></em></span></p><p>&nbsp;</p><p><span>我们研究了 GPU 在不同数值表示、内存容量、带宽以及互连带宽方面的计算性能，使用的数据集包括 2010 年到 2023 年常用于机器学习实验的 47 个 ML 加速器（GPU 和其他 AI 芯片），以及 2006 年到 2021 年的 1948 个 GPU。主要发现如下：</span></p><p>&nbsp;</p><ol start="1"><li><p><span>与传统 32 位浮点数（FP32）相比，低精度数字格式如 16 位浮点数（FP16）和 8 位整数（INT8）等与专用张量核心单元相结合，可以为机器学习工作负载带来显著的性能提升。例如，尽管使用的数据量有限，但我们估计 tensor-FP16 比 FP32 的速度快约 10 倍。</span></p></li></ol><p>&nbsp;</p><p><span>2. 鉴于用于 SOTA ML 模型训练和推理的大型硬件集群的整体性能取决于计算性能以外的因素，所以我们研究了内存容量、内存带宽和互连，发现：</span></p><p>&nbsp;</p><ul><li><p><span>内存容量每 4 年翻一番，内存带宽每 4.1 年翻一番。它们的增长速度比计算性能慢（计算性能每 2.3 年翻一番）。这是一个常见发现，通常被称为内存墙（memory wall）。</span></p></li></ul><p>&nbsp;</p><ul><li><p><span>最新的 ML 硬件通常配备专有的芯片间互连协议（英伟达的 NVLink 或谷歌 TPU 的 ICI），与 PCI Express（PCIe）相比，这些协议在芯片之间提供了更高的通信带宽。例如，H100 上的 NVLink 支持的带宽是 PCIe 5.0 的 7 倍。</span></p><p>&nbsp;</p></li></ul><p><span>3. 分析中发现的关键硬件性能指标及其改进速度包括：ML 和通用 GPU 的计算性能（以 FLOP/s 计）都是每 2.3 年翻一番；ML GPU 的计算性价比（以每美元 FLOP 计）每 2.1 年翻一番，通用 GPU 每 2.5 年翻一番；ML GPU 的能效（以每瓦特 FLOP/s 计）每 3.0 年翻一番，通用 GPU 每 2.7 年翻一番。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/8666a251-442b-40ee-a6ad-e7b64707cd15.jpg" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span>表 1</span></em></span></strong><span style="color:#888888"><em><span>：关键性能趋势。所有估算仅针对机器学习硬件。方括号中的数字表示通过 1000 次 bootstrap 采样得出的[5; 95]百分位估算。OOM 代表数量级，N 表示数据集中的观测次数。请注意，性能数据是指稠密矩阵乘法性能。</span></em></span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">1</span></strong></span></strong></span></p><span id="OSC_h2_1"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span>引言</span></strong></span></h2><p>&nbsp;</p><p><span>过去十年中，机器学习的进步在很大程度上是通过扩大用于训练的计算资源（计算）规模实现的（Sevilla 等人，2022 年），硬件性能的提升在这一进展中发挥了一定作用。随着我们从少量芯片转向大规模超级计算机，对 ML R&amp;D（Cottier，2023）投资的增加导致硬件基础设施规模的相应提升。</span></p><p>&nbsp;</p><p><span>本文概述了在各种数字精度和专用组件（如张量核心）方面的计算性能趋势。此外，我们还分析了其他性能因素，如内存容量、内存带宽和互连带宽。总的来说，我们分析了 ML 硬件规格和组件的整体情况，这些规格和组件共同决定了硬件的实际性能，尤其是在大规模 ML 模型时代。</span></p><p>&nbsp;</p><p><span>在这个过程中，我们比较了各种度量标准下的峰值性能，这些指标来自硬件生产商的规格表。[2]通常，由于工作负载规格等各种因素和内存容量以及带宽等规格的限制，实际利用的计算性能只是指定峰值计算性能的一小部分。例如，根据 Leland 等人在 2016 年的研究，常见超级计算工作负载的实际利用率可能在 5% 到 20% 之间，而在机器学习训练中，这取决于模型的规模、并行化方式等因素（Sevilla 等人，2022），这个比例可能在 20% 到 70% 之间。尽管如此，峰值性能仍可作为比较不同硬件加速器和世代的有用上限和标准基础。</span></p><p>&nbsp;</p><p style="text-align:left"><strong><span>术语</span></strong></p><p>&nbsp;</p><ul><li><p><strong><span>数字表征</span></strong><span>：我们将数字表征分为三个维度：</span></p><p>&nbsp;</p></li><li><p><strong><span>位长/精度</span></strong><span>：从 4 位到 64 位不等，通常用于描述存储数字的位数。</span></p><p>&nbsp;</p></li><li><p><strong><span>数字格式</span></strong><span>：指特定的位（bit）布局，如整数或浮点数。数字格式通常包括 FP32 等位长度，但我们拆分了位布局和位长度[3]。</span></p></li></ul><p>&nbsp;</p><ul><li><p><strong><span>计算单元</span></strong><span>：显示是否使用了专用矩阵乘单元。在这篇文章中，我们只区分张量和非张量。</span></p><p>&nbsp;</p></li><li><p><strong><span>硬件加速器</span></strong><span>：指加速 ML 工作负载的芯片，如 GPU 或 TPU。我们在通用术语中可交替使用芯片和硬件加速器这两个术语，而在指代专门的加速器时则使用 GPU 和 TPU。</span></p></li></ul><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">2</span></strong></span></strong></span></p><span id="OSC_h2_2"></span><h2 style="text-align:center"><strong><span style="color:#1e2380"><span>数</span>据集</span></strong></h2><p>&nbsp;</p><p><span>我们从两个关键数据集中汇编了硬件规格。第一个数据集在 2019 年 Sun 等人的研究（<span style="color:#888888"><em>https://arxiv.org/abs/2202.05924</em></span>）基础上，包含了 2006 年至 2021 年期间发布的 1948 款 GPU，我们将其称为通用 GPU 数据集（主要基于一些不常用于机器学习训练的通用 GPU）。第二个数据集仅包含自 2010 年以来的 47 个 ML 硬件加速器，如 NVIDIA 的 GPU 和 Google 的 TPU，它们通常在重要的机器学习实验中使用（根据 2022 年 Sevilla 等人的定义）。</span></p><p style="text-align:left"><br><span>我们自己整理了后一个数据集，并将其称为 ML 硬件数据集，简称 ML 数据集（基于 ML GPU）。此数据集可以在我们的数据表中公开获取（</span><span><span style="color:#888888"><em>https://docs.google.com/spreadsheets/d/1NoUOfzmnepzuysr9FFVfF7dp-67OcnUzJO-LxqIPwD0/edit?usp=sharing</em></span></span><span>）。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">3</span></strong></span></strong></span></p><span id="OSC_h2_3"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span>主要性能指标趋势</span></strong></span></h2><p>&nbsp;</p><p><span>在本节中，我们将介绍不同数字表征、内存容量、计算性价比和能效的趋势。我们将简要解释每个指标与 ML 开发和部署的相关性，展示我们的发现，并简要讨论它们的含义。</span></p><p>&nbsp;</p><span id="OSC_h3_4"></span><h3><span><strong><span>数字表征</span></strong></span></h3><p>&nbsp;</p><p><span>用于计算的数值表征对计算性能有很大影响。具体说来，每个值的位数决定了计算密度（每秒每芯片面积的运算次数）。[4]近年来，硬件制造商已经为 ML 应用引入了专门的低精度数值格式。虽然 FP64 在高性能计算中很常见，[5]但在过去 15 年左右的时间里，FP32 的性能一直是大多数消费级应用关注的焦点。</span></p><p>&nbsp;</p><p style="text-align:left"><span>近年来，精度较低的数值格式变得更加普遍，因为低精度已经足够开发和部署 ML 模型（Dettmers 等人，2022；Suyog Gupta 等人，2015 年；Courbariaux 等人，2014 年）。根据 Rodriguez（<span style="color:#888888"><em>https://deeplearningsystems.ai/#ch06/#61-numerical-formats，2020</em></span>），到目前为止，FP32 仍然是机器学习训练和推断中采用最广泛的数值格式，行业越来越倾向于在某些训练和推理任务中过渡到更低精度的数值格式，如 FP16 和 Google 的 bfloat16（BF16），以及用于部分推理工作负载的整数格式 INT8。[6]其他知名新兴数值格式包括 16 位标准浮点格式 FP16，整数格式 INT4，以及 NVIDIA 开发的 19 位浮点格式 TF32。[7]</span></p><p>&nbsp;</p><p style="text-align:left"><strong><span>FP32 和 FP16 的计算性能</span></strong></p><p style="text-align:left">&nbsp;</p><p style="text-align:justify"><span>从历史上看，近 20 年来，FP32 精度的计算性能趋势一直相对稳定，呈现出 2.3 年翻倍一次的趋势，与摩尔定律的速度密切相关。在过去几年，特别是自 2016 年以来，我们已经看到了专门支持 FP16 精度的硬件的出现，这增加了绝对计算性能，同时减少了位长。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/48807fb3-a5fc-4cbc-8036-00251e0bcba5.jpg" referrerpolicy="no-referrer"></p><p><em><strong><span style="color:#888888">图 2</span></strong><span style="color:#888888">：过去二十年，FP32 和 FP16 精度下的通用和 ML GPU 峰值性能。上图显示，ML GPU 的中位性能高于所有通用 GPU，但增长率相似。下图显示，2014 年一些硬件加速器开始提供 FP16 性能细节。</span></em></p><p>&nbsp;</p><p style="text-align:justify"><span>在过去十年中，FP32 的通用硬件和 ML 硬件的计算性能显示出几乎相同的增长率，但在性能水平上有所不同。我们的 ML 硬件数据集中的加速器始终处于最佳可用硬件之列。我们认为，这在一定程度上是因为机器学习实践者选择了最强大的可用硬件，其次，这也是由于最近推出的专门针对机器学习市场的高端数据中心 GPU 的推出，例如英伟达的 V/A/H100 或谷歌的 TPU。</span></p><p>&nbsp;</p><p style="text-align:left"><span><strong><span>通过硬件支持更低精度的数值格式以提高计算性能</span></strong></span></p><p>&nbsp;</p><p><span>降低数值精度所带来的性能提升得益于现代机器学习芯片中多重架构的改进，而不仅仅是单纯降低位宽所能达到的。较小的数据类型使得每平方芯片面积可以进行更多的浮点运算，并减小了内存占用。</span></p><p>&nbsp;</p><p><span>然而，其他方面的进步也在很大程度上做出了贡献：引入了专门用于矩阵乘的新指令；[8]硬件数据压缩；消除了诸如 NVIDIA A100 中的矩阵乘硬件中多余的数据缓冲区，这有助于降低数据和指令内存需求，从而提高了单位芯片面积上的操作数。H100 更快的内存访问能力进一步优化了上述进展 (Choquette, 2023).。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/378ad9dd-46a8-4589-9bcd-6763ff5ed792.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 3</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：箱线图显示了不同精度数字格式下 ML 加速器性能相对于其 FP32 性能的比值，这展示了相对于 FP32 的性能改善。我们发现，相对于它们自身的 FP32 性能，采用新的数值表示方式 tensor-FP32/TF32、tensor-FP16 和 tensor-INT8 可以分别使平均计算性能提高约 5 倍、8 倍和 13 倍。并非所有 GPU 都专门支持低精度格式，我们从图中剔除了那些在较低精度格式上的计算性能未能超过较高精度格式的 GPU 型号，以便筛选出缺乏专门支持的 GPU。</span></em></span></p><p>&nbsp;</p><p><span>近年来，由于使用了较低的数字精度，GPU 在机器学习工作负载中的性能大幅提升。平均而言，与在同一 GPU 上使用 FP32 相比，使用 tensor-FP32（TF32）、tensor-FP16、tensor-INT8 和 tensor-INT4 等精度较低的数值格式分别可提供约 5 倍、8 倍、13 倍和 18 倍的计算性能。</span></p><p>&nbsp;</p><p><span>历史数据显示，FP32 性能峰值每 2.3 年翻一番，这些较低精度的加速效果相当于性能提升了 3 到 9 年。然而，最大的加速效果可能超过平均值。与 FP32 相比，NVIDIA 的 H100 在 TF32、FP16 和 INT8 下分别实现了约 7 倍、15 倍和 30 倍的加速效果。</span></p><p><br><span>因此，对于 H100 来说，与典型的 GPU 相比，较低的精度提供了比 FP32 更大的性能增益。正如我们所看到的，虽然使用较低精度能极大地提升计算性能，但出于模型准确性方面的权衡，通常还是会使用较高精度进行训练。[10]尽管 TF32、FP16 和 INT8 格式在 H100 上相较于 FP32 提供了加速效果，但需要注意的是，这不仅仅是因为较小的数值格式更高效，H100 很可能针对这些格式的操作进行了优化，从而促成了速度提升。</span></p><p>&nbsp;</p><span id="OSC_h3_5"></span><h3><strong><span>内存容量和带宽</span></strong></h3><p>&nbsp;</p><p><span>典型的处理器核心通过读取数据、处理数据，并将处理后的结果写回内存来执行计算。因此，内存充当了在处理周期之间存储数据的媒介。硬件倾向于使用内存层次结构：从在计算单元附近存储数百 KB 快速访问数据的寄存器文件，到能够容纳数十 GB 较慢访问数据的随机存取存储器（RAM）。[11] 数据定期从较大的慢速访问 RAM 通过中间缓存存储器传输到寄存器文件，必要时再写回。加速器数据表大多提供加速器卡上可用的最大 RAM[12]。我们称这些 RAM 位的数量为内存容量。数据以块的形式传输到最大 RAM 中，具体取决于所使用的内存技术，这需要一些处理周期。我们将能够每秒传输到最大 RAM 的最大位数（即峰值比特速率）称为内存带宽[13]。</span></p><p>&nbsp;</p><p><span>包含硬件加速器的系统通常包含一个主存储器，用于存储应用程序和数据。然后，这些数据被传输到加速器进行处理。为确保在训练或推理期间模型权重和训练数据在硬件加速器上随时可用，需要更大的内存容量。如果数据无法适应加速器的内存，逻辑（logic）将需要使用 CPU 内存，甚至更高级别的内存（例如硬盘），这将显著影响时延和带宽。实际上，为避免这种性能损失，模型数据分发到多个硬件加速器的内存中。</span></p><p>&nbsp;</p><p><span>硬件处理能力的进步需要更大的内存带宽。如果没有足够的数据输入，就无法达到峰值计算性能，内存带宽就会成为瓶颈[14]，这被称为带宽墙（Rogers 等人，2009）或通常所说的内存墙。</span></p><p>&nbsp;</p><p><span>如图 4 所示，相对于计算性能的改善，内存容量和带宽的增长速度较慢。具体而言，就通用 GPU 来说，内存容量每 3.04 年翻一番，而 ML 加速器则为 4 年，内存带宽分别为每 3.64 年和 4 年翻一番。相比之下，根据之前的分析，计算性能每 2.3 年翻一番。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/c2f89bdb-25a7-433f-8e48-e8922d6297e7.png" referrerpolicy="no-referrer"></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/50b955f7-0e03-4cc2-a915-d3916e601217.png" referrerpolicy="no-referrer"></p><p><span style="color:#888888"><em><span>图 4：通用硬件与 ML 硬件的内存容量和带宽的变化轨迹。我们发现所有这些趋势都比计算性能的趋势慢（计算性能每 2.34 年翻一番），这与通常所说的内存墙趋势一致。</span></em></span></p><p>&nbsp;</p><p><span>如人们所预期的那样，在内存容量和带宽方面，ML 硬件超过了中位的 GPU。然而，即使在这方面，这些指标的增长速度也一直落后于计算性能的增长速度（每 2.3 年翻一番）。这一趋势表明，对于大规模 ML 应用而言，内存正在成为一个日益关键的瓶颈。当前的架构改进，比如引入更少位的数字表征，可能会减轻这种内存限制。然而，如果不加快发展，这一内存瓶颈将在未来几年继续影响整体性能。[15]</span></p><p>&nbsp;</p><p><span>对于一些 ML 工作负载来说，单个加速器可能提供了足够的计算性能。然而，由于内存限制，通常需要将工作负载分布到多个加速器上。利用多个加速器可以增加总内存容量，从而完全将大型模型和数据集放入内存。这种策略确保了更大的内存容量，可以在多个硬件加速器上容纳模型的全部权重，从而减轻了从主机系统内存传输数据时所产生的时延。对于某些工作负载来说，增加内存带宽可能对满足时延和吞吐量要求至关重要。</span></p><p><br><span>值得注意的是，旨在减少内存占用的技术，比如重新计算激活值利用了计算资源来部分抵消这些限制（Rajbhandari 等, 2021）。然而，通过多个芯片并行化模型训练需要它们之间通过互连实现高效通信。</span></p><p>&nbsp;</p><span id="OSC_h3_6"></span><h3 style="text-align:left"><strong><span>互连带宽</span></strong></h3><p>&nbsp;</p><p><span>在 ML 的训练和部署中，由于不断增长的内存需求，除需要巨大的计算能力之外，还需要使用多个芯片来满足这些需求。例如，PaLM 的训练中使用了 6144 个芯片（Chowdhery 等人，2022 年），而对于 GPT-4 可能需要使用更多芯片。这一需求强调了有效互连这些芯片的需求，使它们能够在不借助 CPU 内存或磁盘的情况下有效地交换激活值和梯度。</span></p><p>&nbsp;</p><p><span>互连带宽是指通信通道能够传输的峰值比特率，通常以每秒传输的字节数为单位测算。当 ML 硬件之间频繁交换数据时，如果互连带宽跟不上处理速度，这个指标就成为了限制因素。</span></p><p>&nbsp;</p><p><span>互连协议定义了最大互联带宽。在我们的数据集中，ML 硬件涉及三种常见协议：a) PCI Express（PCIe）；b) Nvidia NVLink；c) Google Inter-Core Interconnect（ICI）[16] 。PCIe 是一种普遍采用的协议，用于在 CPU 和机器学习硬件之间进行本地互联。相比 PCIe 的基于集线器的网络架构，Nvidia 的专有 NVLink 通过实现设备之间的直接点对点连接，克服了 PCIe 的带宽限制。在无法使用点对点连接的情况下，PCIe 被用作备用方案。Google 的 ICI 用于连接他们的 TPU[17]。</span></p><p>&nbsp;</p><p><span>前面提到的互连协议主要设计用于近距离通信[18] 。当需要进行较长距离的通信时，会采用传统的计算机网络协议，比如以太网或者 InfiniBand。在所有传统网络协议中，数据都是通过 PCIe 路由到网络硬件[19] 。即使存在 NVLink 和 ICI，PCIe 仍然作为主机 CPU 和机器学习硬件之间的标准互连协议。在接下来的内容中，我们将始终指出对应于最快协议的互连速度。</span></p><p>&nbsp;</p><p><img src="https://oscimg.oschina.net/oscnet/9207e987-873b-48dc-92b4-9816e194b1c1.jpg" referrerpolicy="no-referrer"></p><p><span style="color:#888888"><em><span style="color:#888888">图 5: 不同硬件加速器中，每个芯片的聚合互连带宽。NVLink 和 ICI 等专有协议的互连带宽高于 PCIe。</span></em></span></p><p>&nbsp;</p><p><span>我们发现，自 2011 年以来，ML（机器学习）硬件的 PCIe 带宽仅从 32GB/s 增加到 2023 年的 128GB/s（见图 5）。[20]然而，英伟达（NVLink）和谷歌（ICI）的专用加速器互连协议可实现更高的互连带宽。此外，常用于大型计算集群的高端 ML 加速器（例如 TPU 和 V/A/H100）拥有迄今为止最高的互连速度。例如，搭载 18 个 NVLink 4.0 通道的英伟达 H100 实现了 900GB/s 的带宽，是单个 PCIe 5.0 16 通道链路的 7 倍。[21]</span></p><p>&nbsp;</p><p style="text-align:left"><span>一个计算集群可能配备了成千上万台不同程度耦合的硬件加速器。例如，英伟达的 DGX H100 服务器使用 NVSwitch 使每台 H100 互连，从而实现了最大互连带宽为 900GB/s 的紧密耦合加速器网络（参见<span style="color:#888888"><em>[Choquette, 2023]，https://doi.org/10.1109/MM.2023.3256796，"Scaling Up and Out"一章</em></span>）。许多 DGX H100 服务器又可以组成所谓的 SuperPOD，其中各个独立服务器中的加速器仍可使用 NVLink 传输数据，但耦合程度较低。每个 SuperPOD 使用以太网和 Infiniband 连接到另一个 SuperPOD。服务器之间的网络拓扑也会影响计算集群的整体性能。</span></p><p>&nbsp;</p><p><span>专用集群 ML 硬件的互连带宽远高于消费级硬件。这凸显了它在大规模 ML 实验中的重要性，因为这些实验需要在 ML 硬件节点之间进行高带宽的数据通信。因此，类似于内存容量和带宽，我们建议监测互连带宽，将其作为了解 ML 硬件趋势的一个相关附加指标。</span></p><p>&nbsp;</p><span id="OSC_h3_7"></span><h3 style="text-align:left"><strong><span>计算性价比</span></strong></h3><p>&nbsp;</p><p><span>性能——价格比（Price-performance ratio）通常比单纯的峰值计算性能更有用，它能反映 GPU 的整体技术改进情况，即每美元成本可获得的性能。我们采用两种方法来估算 ML 硬件的性价比：</span></p><p>&nbsp;</p><p><span>1. 在有数据的情况下，我们使用硬件的发布价格，根据通货膨胀进行调整，并假定两年的摊销时间，详见<span>（</span><span style="color:#888888"><em>Cotra (2020)，https://docs.google.com/document/d/1qjgBkoHO_kDuUYqy_Vws0fpf-dG5pTU4b8Uej6</em></span>）。</span></p><p>&nbsp;</p><p><span>2. 在仅提供租赁的 TPU 或其他硬件等硬件发布价格不可用或不明确的情况下，我们使用 Google Cloud 的云计算价格（截至 2023 年 7 月 3 日）。我们根据通货膨胀调整价格，以使价格与摊销价格相当，并假设云服务提供商的利润率为 40%[22]。如图 6 所示，在计算 FP32 精度的性价比时，需考虑估算 FP32 性价比时的一些重要注意事项。</span></p><p>&nbsp;</p><p><span>首先，集群硬件的定价通常会采用私下协商的方式，不公开发布，这使得难以准确定价。其次，尽管某些芯片在个体性价比上表现强劲，但由于互连带宽或可靠性不足，可能无法在工业集群部署中使用。再次，FP32 计算引入了对专用 ML 芯片的偏见，这些芯片使用较低精度数字格式和未在 FP32 指标中反映的张量核心。最后，由于缺乏有关功耗、冷却和更换率等数量的公开数据（<span style="color:#888888"><em>参见[Cottier, 2023]，https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems</em></span>），估算实际维护成本具有挑战性。尽管作为基准有用，但 FP32 性价比趋势必须考虑源自 ML 的特定架构因素和数据约束的限制。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/1ea660df-a2c3-4e78-8427-edd2d54f01b0.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 6</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：通用硬件和 ML 硬件的 FP32 性价比轨迹。我们发现，这些轨迹大致遵循与峰值计算性能相同的增长轨迹（2.3 年翻倍时间）。此外，我们发现 ML GPU 的绝对性价比低于其他硬件。FP32 性价比可能存在对 ML 硬件的偏见（详见正文）。</span></em></span></p><p>&nbsp;</p><p><span>我们看到 FP32 性价比的增长轨迹（2.5/2.1 年翻倍时间）大致与通用计算性能的增长轨迹（2.3 年翻倍时间）相似。</span></p><p>&nbsp;</p><p><span>此外，与其他 GPU 相比，我们发现 ML GPU 的性价比较低。我们推测至少有两个原因。</span></p><p>&nbsp;</p><p><span>首先，如上所述，由于它们忽略了在 ML 训练中常见的其他数值表示（如 FP16），上述注意事项系统地使 FP32 性价比对 ML 硬件产生了偏见。其次，正如前面的部分所述，大规模 ML 训练不仅依赖於单一性能指标，还依赖于互连带宽、内存容量和带宽等其他指标。然而，这些指标并未反映在 FP32 性价比中。例如，一款典型的消费级 GPU 在个体的性价比上可能更好，但对于 ML 训练来说却不太适用。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/bd56dc1f-dd39-47cf-b13e-04e13bff8816.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 7</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：不同数值表示 ML 硬件的计算性价比。其中的点表示 ML 硬件的发布日期和性能，颜色代表数值格式。虚线表示具有十个或更多加速器的数值格式（如 INT8、FP16 和 FP32）性能改进趋势。</span></em></span></p><p>&nbsp;</p><p><span>FP32 的性价比可能会误导对 ML 硬件成本效益的认识。例如，AMD Radeon RX 7900 XTX 消费级 GPU 在 FP32 性价比方面表现最佳。然而，NVIDIA RTX 4090 在使用 ML 训练中常见的低精度 INT4 格式时，提供了约 10 倍高的性价比。这得益于 RTX 4090 专为低精度计算而设计的张量核心，而 FP32 指标却忽略了这一点。</span></p><p><br><span>因此，仅凭 FP32 的性价比便会错误地认定 Radeon 优于 RTX 4090，而实际上 RTX 4090 在实际 ML 工作负载中更为经济实惠。这突显了仅依赖 FP32 性价比分析，不考虑 ML 特定架构和数值表示的整体评估的风险。</span></p><p>&nbsp;</p><p><span>性价比最好的 GPU 在很大程度上取决于所使用的数值表示。AMD Radeon RX 7900 XTX 消费级 GPU 在 FP32 计算上的性价比最高。然而，对于像 INT4 这样的低精度数字格式，NVIDIA RTX 4090 的每美元计算性能大约是 Radeon 的 10 倍。这说明按照性价比对 GPU 进行排名对精度非常敏感，而仅依靠 FP32 无法全面反映实际 ML 工作负载中的成本效益。</span></p><p>&nbsp;</p><span id="OSC_h3_8"></span><h3 style="text-align:left"><strong><span>能效</span></strong></h3><p>&nbsp;</p><p><span>运行硬件会消耗能源，而大多数组织的目标是尽可能充分地利用他们的硬件。因此，部署能效高的硬件是一种降低硬件加速器寿命周期成本的可能途径。此外，能效更高的硬件通常散热更少，有助于更好地实现可扩展性。</span></p><p>&nbsp;</p><p><span>为近似评估 ML 硬件的能效，我们使用每瓦特的 FLOP/s，其中能量组成部分是从热设计功耗（TDP）计算得出的。TDP 并不等同于平均能耗，因此不应该用于精确比较。然而，在 ML 训练和云计算中，我们认为它是一个相当不错的近似值，因为硬件是持续运行的（<span style="color:#888888"><em>参见附录中的 TDP 部分，https://epochai.org/blog/trends-in-machine-learning-hardware#thermal-design-power-tdp</em></span>）。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/ba66db12-918a-446c-9ee4-6cfa15257bdf.png" referrerpolicy="no-referrer"></p><p><strong><span style="color:#888888"><em><span style="color:#888888">图 8</span></em></span></strong><span style="color:#888888"><em><span style="color:#888888">：根据 TDP 数值计算的 FP32 精度能效轨迹。我们发现，机器学习 GPU 的平均能效比通用 GPU 高，且能效的增长速度略低于峰值计算性能（2.3 年翻倍时间）的增长速度。</span></em></span></p><p>&nbsp;</p><p style="text-align:left"><span>我们发现，机器学习 GPU 的平均能效比历史 GPU 更高。这是合理的，因为 ML GPU 通常在数据中心运行，能源消耗和碳足迹是重要的度量标准（<em><span style="color:#888888">参见 Jouppi 等，2023，https://arxiv.org/pdf/2304.01433.pdf，第 7.6 节</span></em>）。此外，我们发现能效的增长速率（分别为历史 GPU 和 ML GPU 的 2.70/3.0 年翻番时间）仅略低于峰值计算性能的增长速率（2.3 年翻番时间）。这一趋势表明能耗目前（尚）不是扩展的现实瓶颈，但有理由认为在未来可能会成为瓶颈（<span style="color:#888888"><em>参见 Hobbhahn &amp; Besiroglu, 2022b，https://epochai.org/blog/predicting-gpu-performance</em></span>）。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00"><strong><span style="color:#f6ab00">4</span></strong></span></strong></span></p><span id="OSC_h2_9"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">结论</span></strong></span></h2><p>&nbsp;</p><p><span>最近的研究表明，对于开发和部署 ML 模型，低精度已经足够（<span style="color:#888888"><em>参见[Dettmers 等, 2022]；[Suyog Gupta 等, 2015]; [Courbariaux 等, 2014]</em></span>）。我们发现，ML 硬件遵循上述发现，并不断集成支持更低精度数值格式的硬件单元（如 FP16、TF32、BF16、INT8 和 INT4），以增加每秒的总操作次数。此外，张量核心等专用计算单元变得越来越普遍，并进一步提高了计算性能。</span></p><p>&nbsp;</p><p><span>结合这两个趋势，在我们的推测性占主导的估算中，从 FP32 到张量-FP16 的跃迁平均提供了约 8 倍的峰值性能增益。然而，旗舰级 ML 硬件加速器的这一比率可能更高，例如，NVIDIA H100 SXM 的 TF32 到 FP32 比率约为 7 倍，张量-FP16 到 FP32 比率约为 15 倍，张量-INT8 到 FP32 比率约为 30 倍。</span></p><p>&nbsp;</p><p><span>这一趋势表明了一种「硬件-软件协同设计」的模式，其中 ML 从业者尝试不同的数值表示，并已获得了一些小而有意义的性能提升，减少了内存占用。然后，硬件被调整以适应这些新的数值表示，从而获取进一步的增益。多次迭代这一循环可以促成性能的实质性改善。此外，硬件生产商也在积极寻求新的创新，这些创新随后将引领其进入 ML 实验室。</span></p><p>&nbsp;</p><p><span>此外，在大规模 ML 训练中，我们强调内存容量、内存带宽和互连带宽等因素的重要性。鉴于目前 ML 训练通常需要数千个芯片之间的有效交互，超越每个芯片峰值性能的因素变得至关重要。我们观察到，这些指标的增长速度比与计算相关的指标（例如峰值计算性能、性价比和能效）要慢。在大规模分布式 ML 训练场景中，内存和互连带宽成为利用峰值计算性能的瓶颈。</span></p><p>&nbsp;</p><p><span>专门的机器学习硬件和替代的数值表示是相对较新的趋势，这使得精确预测变得困难。正如我们已经明确指出，密切追踪数值格式、内存容量、内存带宽和互连带宽的发展对于更准确地评估未来机器学习能力至关重要。与其依赖静态假设，基于硬件和软件创新不断重新评估性能潜力才是关键。</span></p><p>&nbsp;</p><p><em><span>（<span style="color:#888888">我们要感谢 Dan Zhang、Gabriel Kulp、Yafah Edelman、 Ben Cottier、Tamay Besirogl 和 Jaime Sevilla 对本文提供的详尽反馈，还要感谢 Eduardo Roldán 将这篇文章搬运到网站上。</span>）</span></em></p><p>&nbsp;</p><span id="OSC_h2_10"></span><h2><strong><span>附录：次要性能指标的趋势</span></strong></h2><p>&nbsp;</p><p><span>我们补充了晶体管数量、热设计功耗（TDP）、时钟速度、芯片尺寸和张量核心数量等次要指标的趋势。尽管这些指标可能与理解 ML 硬件的某些趋势相关，但我们认为它们不如我们在文章主体中分析的指标重要或有影响力[23]。</span></p><p>&nbsp;</p><p><span>请注意，这些趋势中仍有大量缺失数据，因此可能存在偏见。例如，以下大部分数据不包括 TPU。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/876d3c1e-b2f8-4b16-adfe-1547abd73ec1.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><strong><span style="color:#3f3f3f">注释<span style="background-color:#ffffff; color:#a3a3a3">（请上下滑动）</span><span>&nbsp;</span></span></strong></p><span id="OSC_h3_11"></span><h3>&nbsp;</h3><p><span>1 NA 表示数据不可用，因为缺乏足够的数据来估计相关增长率。</span></p><p>&nbsp;</p><p><span>2 这些数字通常是基于硬件特性计算得出的。例如，计算性能通常被估算为处理核心数量、时钟速度和每个核心的每个时钟周期的浮点运算乘积。</span></p><p>&nbsp;</p><p><span>3 相同位数的比特可以表示不同的数值范围或浮点数精度。我们的硬件数据集不包括针对给定位数格式的每种可用数值格式的计算性能。例如，我们的 FP16 数据还包括 BF16，其在指数和尾数分配的比特数方面存在差异。我们不指望在相同位数的不同浮点数格式之间有太大的性能差异。最适合的数值表示（例如，从能源或运行时间效率的角度）取决于工作负载。[Rodriguez, 2020](https://deeplearningsystems.ai/#ch06/#61-numerical-formats) 第 6.1 节中还包含了一份 ML 应用的数值表示的综合列表。</span></p><p>&nbsp;</p><p style="text-align:left"><span>4 根据[Mao 等人 (2021)](https://doi.org/10.1109/TVLSI.2021.3128435) 中的表 VI，一个 FP64 乘法器单元的面积大约是 FP32 乘法器的五倍。类似的关系也存在于 FP32 和 FP16 乘法器之间。</span></p><p>&nbsp;</p><p><span>5 由于许多具有历史重要性的超级计算机工作负载对高精度的要求，例如计算流体力学、气象学、核蒙特卡洛模拟、蛋白质折叠等。</span></p><p>&nbsp;</p><p style="text-align:left"><span>6 [Rodriguez, 2020](https://deeplearningsystems.ai/#ch06/#61-numerical-formats), 第 6.1 节指出：最受欢迎和广泛采用的数值格式是用于训练和推理的 FP32。行业正在向用于训练和推理的 FP16 和 BF16 靠拢，并在某些工作负载的推理中采用 INT8。</span></p><p>&nbsp;</p><p><span>7 TF32 并非通用数值格式，它仅在 NVIDIA 张量核心中使用，通过在矩阵乘法之前减少 13 位精度位，加速使用 FP32 的模型处理，但保持与 FP32 相同的数值范围。TF32 与 FP32 的内存占用相同，因为 TF32 在张量核心中使用与 FP32 相同的寄存器（参见[Sun 等，2022](https://doi.org/10.1109/TPDS.2022.3217824)，第 8 节）。换句话说，TF32 被设计为 FP32 模型的即插即用替代品，但在矩阵乘法过程中可以接受更低的精度。</span></p><p>&nbsp;</p><p><span>8 请勿将其与张量核心乘法所需的新指令混淆。[Choquette 等人，2021](https://doi.org/10.1109/MM.2021.3061394)，SM Core 一节指出：在 A100 中，添加了一条新的异步组合加载-全局存储-共享存储指令，将数据直接传输到 SMEM，绕过寄存器文件，提高了效率。</span></p><p>&nbsp;</p><p><span>9 注意查看标题为‘SM Core’的部分。</span></p><p>&nbsp;</p><p><span>10 例如，目前 INT8 在训练当前系统中并未被广泛使用。INT8 的缺点在 Rodriguez，2020，第 6.1 节中有解释。</span></p><p>&nbsp;</p><p><span>11 由 ML 硬件数据表记录的内存容量通常指的是 RAM 容量，因为 GPU 在之前常被用于视频处理，所以也常被称为视频 RAM（VRAM）。</span></p><p>&nbsp;</p><p style="text-align:left"><span>12 例如，[AMD Instinct MI200 数据表](https://www.amd.com/system/files/documents/amd-instinct-mi200-datasheet.pdf) &nbsp;明确说明了 128 GB HBM2e。HBM 指的是高带宽内存，是一种 RAM 类型。[NVIDIA H100 Tensor Core GPU 数据表](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet) 表示 H100 SXM 的内存为 80GB，根据 [NVIDIA H100 Tensor Core GPU 架构](https://nvdam.widen.net/s/95bdhpsgrs#page=36)v1.04，第 36 页，这个数字对应于 HBM3 的内存容量。</span></p><p>&nbsp;</p><p><span>13 在应用中，实际带宽通常较低。一个原因是数据传输时延，这也影响了实际带宽，并取决于内存技术。到单独内存芯片的距离以及在大容量内存中的长路径，会导致数据在到达处理单元之前经历大量的周期。如果处理单元预先知道需要哪些数据，就可以以最大带宽进行数据传输。如果不知道，就需要对内存进行随机访问。通常，随机访问越多，实际带宽就越低。我们的数据集中不包含时延指标。</span></p><p>&nbsp;</p><p><span>14 图形处理和机器学习训练往往会遇到这个瓶颈，因此，现代机器学习硬件尝试通过两种技术来优化高内存带宽：(a) GDDR 内存或 (b) 高带宽内存（HBM）。GDDR 内存位于与处理芯片相同的板上，而 HBM 则实现在与处理芯片相同的封装中，从而实现更低的时延和更高的带宽（例如，在数据中心使用的最新机器学习加速器，如 NVIDIA A100 和 H100 采用了 HBM；而它们的游戏型 GPU 则没有采用 HBM，以节约成本）。将许多 DRAM 堆叠在一起，并在单个芯片封装中互连多个半导体芯片，与在印刷电路板上连接处理芯片和 DRAM 相比，需要昂贵的工具，因此 HBM 通常出现在性能最昂贵和性能最高的机器学习硬件加速器中，例如那些用于数据中心进行大规模机器学习训练和部署的加速器。</span></p><p>&nbsp;</p><p><span>15 可参阅 [Megatron-LM: 使用模型并行训练数十亿参数的语言模型](https://lilianweng.github.io/posts/2021-09-25-train-large/)，[如何在多个 GPU 上训练非常大的模型？](https://lilianweng.github.io/posts/2021-09-25-train-large/) 或者[训练大型神经网络的技术](https://openai.com/research/techniques-for-training-large-neural-networks) 。</span></p><p>&nbsp;</p><p><span>16 [Jouppi 等，《TPU v4：一种具有嵌入式硬件支持的光学可重构超级计算机用于机器学习》](https://arxiv.org/pdf/2304.01433.pdf) 的第 2 节中有详细内容。</span></p><p>&nbsp;</p><p><span>17 更多关于 ICI 的信息请参见[Jouppi 等人，2023](https://doi.org/10.48550/arXiv.2304.01433)，第 2 节。值得注意的是，TPUv4 使用光开关来满足长距离互连需求。</span></p><p>&nbsp;</p><p><span>18 例如，[PCIe 4.0 支持长达 30 厘米](https://www.elektronik-kompendium.de/sites/com/0904051.htm)。根据[Jouppi 等，2023](https://doi.org/10.48550/arXiv.2304.01433)，第 7.2 节，Google ICI 用于连接 1024 个 TPUv3，但最大长度并未提供。</span></p><p>&nbsp;</p><p><span>19 InfiniBand 和 Ethernet 支持的网络带宽低于 PCIe，因此它们定义了峰值带宽。</span></p><p>&nbsp;</p><p><span>20 按照 PCI-SIG 协会的标准；预计到 2025 年将增加到 256GB/s。需要注意的是，带宽变化的速度是由协会定义的，而该协会可能在采纳市场的即时需求方面较为缓慢。</span></p><p>&nbsp;</p><p style="text-align:left"><span>21 根据[NVIDIA H100 Tensor Core GPU Architecture, v1.04, p47 的说明](https://nvdam.widen.net/s/95bdhpsgrs#page=47)：在多 GPU IO 和共享内存访问中，总带宽达到 900GB/秒，新的 NVLink 提供的带宽是 PCIe Gen 5 的 7 倍。... H100 包括 18 条第四代 NVLink 连接，提供 900GB/秒的总带宽...</span></p><p>&nbsp;</p><p><span>22 Google Cloud 提供一年的 37% 的使用折扣。因此，我们估计 40% 是谷歌从正常云计算中获利的合理下限。有关云计算价格的更多考虑可以在 https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems 找到。</span></p><p>&nbsp;</p><p><span>23 相关性判断结合了作者直觉和我们在先前帖子中的推理。</span></p><p>&nbsp;</p><p><span>24 Chiplet 是一个将多个芯片集成到一个集成电路/封装中的例子。</span></p><p>&nbsp;</p><p><span>25 根据[Hennessy 等人，《计算机体系结构》，2017 年，第 24 页](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1) 的描述：TDP 既不是峰值功率（峰值功率通常要高 1.5 倍），也不是在特定计算过程中实际消耗的平均功率（平均功率可能更低）。</span></p><p>&nbsp;</p><p><span>26 支持这一观点的证据来自（Gigabyte 术语表，https://www.gigabyte.com/Glossary/tdp）：在一个稳定的、企业级的服务器房间或数据中心中，TDP 大致等同于计算设备的功耗，因为服务器通常处于最大容量或接近最大容量运行。</span></p><p>&nbsp;</p><span id="OSC_h2_12"></span><h2 style="margin-left:8px; margin-right:8px; text-align:left">&nbsp;</h2><p>&nbsp;</p><p><span style="background-color:#ffffff; color:#888888">其他人都在看</span></p><span id="OSC_h3_13"></span><h3 style="text-align:left">&nbsp;</h3><ul><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492895%26idx%3D1%26sn%3D572040d50dcc39bb7e93c1f75e121599%26chksm%3Dfe426b29c935e23f80ba9ec00f2bbbef26c4a6af6c0457bc41d1e10f2d9fb78b66b91fe5edb0%26scene%3D21%23wechat_redirect" target="_blank">GPU 架构与计算入门指南</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492849%26idx%3D1%26sn%3D51f53e04b4b97cd9dd38429784015c98%26chksm%3Dfe426ac7c935e3d1b5970441a68c53b6dae05792cd9a244ad8efb40ffc5ab4c60cdf3a7181b0%26scene%3D21%23wechat_redirect" target="_blank">LoRA 和 QLoRA 微调语言大模型</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247489016%26idx%3D1%26sn%3D3fc5d8ab05d2af9a7b95a1002ea128e9%26chksm%3Dfe419bcec93612d8220be748c1eea51e334fa489b72522ef45fe39141075a74d3669ec0f4110%26scene%3D21%23wechat_redirect" target="_blank">深度学习硬件的过去、现在和未来</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492976%26idx%3D1%26sn%3Dd919a508ce238048ae44e58b9cc06b71%26chksm%3Dfe426b46c935e2500178c2d2c8845fcd3e47fdeb5ec51f55b80cbca5f7b78382cdb3e6fe6a32%26scene%3D21%23wechat_redirect" target="_blank">可复现的语言大模型推理性能指标</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492990%26idx%3D1%26sn%3D50844c8911834baf44863a9e3754175f%26chksm%3Dfe426b48c935e25ede3f772624ba262011b1b48f8ee78ac6d3b1daa5aaf71e7583828740b5cd%26scene%3D21%23wechat_redirect" target="_blank">ChatGPT 规模化服务的经验与教训</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493030%26idx%3D1%26sn%3D58a43ed078977019c997a110526d7c02%26chksm%3Dfe426b90c935e28688b6e317a991bedaaa164471a275d64e60851a09b00f7f6b718e27d7b411%26scene%3D21%23wechat_redirect" target="_blank">语言大模型的分布式训练与高效微调指南</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492957%26idx%3D1%26sn%3D6ab02e219cb41ab8390cd6fc984125c6%26chksm%3Dfe426b6bc935e27d43802825c89eae6b2d346f7b62919270f8c77c0693913395eaf36af8d4a3%26scene%3D21%23wechat_redirect" target="_blank">开源语言大模型演进史：向 LLaMA2 看齐</a></p></li></ul><span id="OSC_h3_14"></span><h3 style="text-align:left">&nbsp;</h3><p><strong><span>试用 OneFlow: <a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgithub.com%2FOneflow-Inc%2Foneflow%2F" target="_blank">github.com/Oneflow-Inc/oneflow/</a></span></strong></p><p style="color:#3f3f3f; margin-left:8px; margin-right:8px; text-align:left"><img src="https://oscimg.oschina.net/oscnet/71c4a4ad-2729-4491-b991-856a2ff43999.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p></div><p style="color:#858585">本文分享自微信公众号 - OneFlow（OneFlowTechnology）。<br> 如有侵权，请联系 support@oschina.cn 删除。<br> 本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/oneflow/blog/10319840</guid>
            <link>https://my.oschina.net/oneflow/blog/10319840</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源 MoE 模型 Mixtral 8x7B 性能超过 GPT-3.5]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>大模型创业公司 Mistral AI 终于<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">介绍了</a></u>前两天「开源」的&nbsp;MoE 模型 <strong>Mixtral 8x7B</strong>。</p><blockquote><p><strong><em><u><a href="https://www.oschina.net/news/270317/mixtral-8x7b-32kseqlen">Mistral AI 用「磁链链接」开源了 87 GB 的 8x7B MoE 模型</a></u></em></strong></p></blockquote><p>官方称，Mixtral 8x7B 是开放权重的高质量<strong>稀疏混合专家模型 (SMoE)</strong>，采用 Apache 2.0 License 开源。在大多数基准测试中，Mixtral 的成绩都优于 Llama 2-70B，且推理速度提升了 6 倍。而且在大多数标准基准测试中超过 GPT-3.5。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7a689c4f538b591b9744038a052717945e6.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-84fefd9ee6c091c07c894031a1af2faf2e3.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-9f9aaad324856028fb1e796beb2d7685020.png" referrerpolicy="no-referrer"></p><p>因此，Mistral AI 称 Mixtral 是最强大的开放权重模型，也是成本/性能权衡方面的最佳模型。</p><p><strong>Mixtral 主要特性</strong></p><p>• 32k 上下文<br> • 支持英语、法语、意大利语、德语和西班牙语<br> • 性能超过 Llama 2 系列和 GPT-3.5<br> • 在代码生成方面具有强劲性能<br> • 在 MT-Bench 上获得 8.3 分</p><p>Mixtral 作为稀疏混合专家网络，是一个纯解码器模型，其中前馈块从 8 组不同的参数组中选择。在每一层，对于每个 token，路由网络选择两组「专家」来处理 token 并相加地结合它们的输出。</p><p>Mixtral 总共有 45B 个参数，但每个 token 只使用 12B 个参数。因此，它以与 12B 模型相同的速度和成本处理输入和生成输出。</p><p>更多细节查看：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">https://mistral.ai/news/mixtral-of-experts/</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 10:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270511/mixtral-of-experts</guid>
            <link>https://www.oschina.net/news/270511/mixtral-of-experts</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[荣耀申请魔方大模型商标]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#222222">天眼查信息显示，荣耀终端有限公司近日申请注册「荣耀魔方大模型」商标，国际分类为网站服务，当前商标状态为等待实质审查。</span></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-7baf34d7d00360b976559630121d67b0da4.png" width="700" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">此前，该公司曾申请两枚「MAGIC&nbsp;大模型」商标。荣耀 CEO 赵明曾发文称，荣耀即将推出自研端侧 AI 大模型和全新云服务。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 08:25:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270492</guid>
            <link>https://www.oschina.net/news/270492</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[HashiCorp 采用 BSL 后续，Linux 基金会孵化 Vault 开源替代品]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">今年 8 月，<span style="background-color:#ffffff">专注于云基础设施的软件供应商 HashiCorp&nbsp;</span>宣布<span style="background-color:#ffffff">修改其核心产品的开源协议。</span><strong style="color:#333333">所有 HashiCorp 产品的未来版本</strong><span style="background-color:#ffffff">将从 Mozilla Public License v2.0 (MPL 2.0) 变更为&nbsp;</span><strong style="color:#333333">Business Source License (BSL, also known as BUSL) v1.1</strong><span style="background-color:#ffffff">，其中包括&nbsp;Vault、Boundary、Consul、Nomad、Packer、Terraform、Vagrant 和 Waypoint 等。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">采用 BSL 1.1 的项目，其代码仍会公开 (source-available)，</span><strong style="color:#333333">但只允许在特定条件下进行复制、修改、重新分发、非商业使用和商业使用</strong><span style="background-color:#ffffff">&nbsp;—— 主要是添加了商业使用方面的限制。</span></span></p><p><span style="color:#000000">此后，社区在抗议无效后选择创建了 Terraform 的分支项目 OpenTofu（原名 OpenTF），并托管在了 Linux 基金会下。</span></p><p><span style="color:#000000"><span style="background-color:#ffffff">时至今日，有消息称 Linux 基金会正计划帮助孵化一个私密信息管理工具 Vault 的开源替代品。DevOps 自动化公司 Scalr 的联合创始人兼首席执行官 Sebastian Stadil 和 OpenTofu 的组织者之一<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theregister.com%2F2023%2F12%2F08%2Fhashicorp_openbao_fork%2F" target="_blank">透露</a>，Vault 开源替代品的项目名为 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwiki.lfedge.org%2Fdisplay%2FOH%2FOpenBao%2B%2528Hashicorp%2BVault%2BFork%2Beffort%2529%2BFAQ" target="_blank">OpenBao</a>，是竞争对手在 MPL 2.0 协议下创建的一个&nbsp;Vault 分支。</span></span></p><p><img height="287" src="https://oscimg.oschina.net/oscnet/up-1f318fa9f93212c7ed6a67c0b91e135c731.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000"><span style="background-color:#ffffff">OpenTofu 计划在本月晚些时候发布候选版本，OpenBao 也将开始接受新的贡献。Stadil 表示，「如果有两个相同的项目，一个是开源的，一个不是，我个人认为，道德上的选择是使用开源项目，并以某种方式提供帮助。」</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">不过鉴于 OpenTofu 和 OpenBao 都是新近开发的项目，项目的可行性和持久性受到了很多关注。针对这一担忧，Stadil 表示拒绝代表其他公司发言。事实上，他还被告知不要透露任何关于其他组织支持这些项目的消息。对于那些想要了解更多详情的人，他建议可以访问项目的 repos。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">当被问及 HashiCorp 重新授权其软件的理由时，Stadil 回答称，官方的说法是 Terraform 对互联网至关重要，而长期以来人们一直希望将其置于 Linux 基金会的监督之下。「如果 HashiCorp 将来愿意加入我们的 OpenTofu，我们会很乐见其成」。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">但</span><span style="background-color:#ffffff">他无法推测 HashiCorp 的内部决策过程。Stadil 指出，Hashicorp 一直在烧钱，随着利率的上升，这家软件公司选择采取措施创造更多收入也不足为奇。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">上周，HashiCorp 公布了 2024 财年第三财季的营收报告。营收 1.461 亿美元，同比增长 17%。按照美国通用会计准则（GAAP），净亏损为 3950 万美元，低于去年同期的 7200 万美元。</span></span></p><p><strong><span style="color:#000000"><span style="background-color:#ffffff">相关阅读：</span></span></strong></p><ul><li><a href="https://www.oschina.net/news/253275/hashicorp-adopts-business-source-license" target="_blank">HashiCorp 核心产品变更开源协议，未来将采用 BSL</a></li><li><p style="margin-left:0px; margin-right:0px; text-align:start"><a href="https://www.oschina.net/news/255700/opentf-fork-terraform" target="_blank">HashiCorp 采用 BSL 后，社区创建 Terraform 分支 OpenTF</a></p></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 07:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270477/hashicorp-vault-openbao-fork</guid>
            <link>https://www.oschina.net/news/270477/hashicorp-vault-openbao-fork</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[周热点 | Linus 收敛火爆脾气，谈内核社区「老龄化」问题；Firefox 或将被淘汰；谷歌发布最强 AI 模型 Gemini............]]>
            </title>
            <description>
                <![CDATA[回顾一周热门资讯。2023.12.04-2023.12.10]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 06:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094041&#38;idx=1&#38;sn=18ed1a99fdf7fbbbc52688a346795664&#38;chksm=880c4c8abf7bc59cbaf66865402e963af1309b4bb627cd89ae402f319f12ce5c56561126ea09&#38;token=1220110296&#38;lang=zh_CN#rd</guid>
            <link>https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094041&#38;idx=1&#38;sn=18ed1a99fdf7fbbbc52688a346795664&#38;chksm=880c4c8abf7bc59cbaf66865402e963af1309b4bb627cd89ae402f319f12ce5c56561126ea09&#38;token=1220110296&#38;lang=zh_CN#rd</link>
        </item>
        <item>
            <title>
                <![CDATA[GitHub.com 跑了 1200 多台 MySQL 主机，如何无缝升级到 8.0？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>GitHub 团队近日<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-12-07-upgrading-github-com-to-mysql-8-0%2F" target="_blank">分享</a></u>了他们将 GitHub.com 的底层数据库无缝升级到 MySQL 8.0 的经验。</p><p>据介绍，GitHub 使用 MySQL 来存储大量关系数据，因此在不影响网站服务级别目标 (SLO) 的情况下升级主机集群（<strong>1200 多台 MySQL 主机</strong>）绝非易事。其团队表示，为了升级到 MySQL 8.0，他们规划、测试和升级本身总共花费了一年多的时间，并且需要 GitHub 内部多个团队的协作。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-613c88c0257637ef029cdd9528c6f8a3217.png" referrerpolicy="no-referrer"></p><p><strong>GitHub 的 MySQL 基础设施概览：</strong></p><ul><li>由 1200 多台主机组成，包括数据中心中的<strong> Azure 虚拟机和裸机主机</strong></li><li>存储超过 300 TB 的数据，并在 50 多个数据库集群中每秒处理 550 万次查询</li><li>每个集群都配置为具有主副设置的高可用性</li><li>分区存储数据——利用水平和垂直分片来扩展 MySQL 集群，以及使用 MySQL 集群来存储特定产品领域的数据。此外还为大结构域 (large-domain) 提供了水平分片的 Vitess 集群，这些区域的增长超出了单主 MySQL 集群的规模</li><li>庞大的工具生态，包括 Percona Toolkit、gh-ost、orchestrator、freno 和用于操作主机集群的内部自动化工具</li></ul><p>由于需要操作两个版本的 MySQL，因此 GitHub 内部使用的工具和自动化设施需要能够兼容处理混合版本，并了解 5.7 和 8.0 之间<strong>新的、不同的或已弃用的语法</strong>。</p><p>为了满足可用性标准，GitHub 团队采取了逐步升级策略，满足在整个过程中进行 checkpoint 和回滚的需求。下面是他们制定的升级计划：</p><ul><li><strong>步骤 1：升级滚动副本 (rolling replica)</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-c9d574db1e2fec9bf7da0d7c92091b0fb19.png" referrerpolicy="no-referrer"><p>&nbsp;</p></li><li><strong>步骤 2：升级备份拓扑 (replication topology)</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-305231a10282f80062ca4f1d665c36305ee.png" referrerpolicy="no-referrer"><p>&nbsp;</p></li><li><strong>步骤 3：将 MySQL 8.0 主机提升为主集群</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-0e9f6defe7e920b0167c797000292c7e390.png" referrerpolicy="no-referrer"><p>&nbsp;</p></li><li><strong>步骤 4：升级面向内部的实例类型</strong></li><li><strong>步骤 5：清理，</strong>确认集群不需要回滚并成功升级到 MySQL 8.0 后，删除 5.7 服务器。验证工作会至少经历一个完整的 24 小时流量周期，以确保在高峰流量期间不会出现问题。</li></ul><p>至于为什么要升级到 MySQL 8.0，GitHub 团队表示主要是因为 MySQL 5.7 的生命周期即将结束。此外升级后可以获得最新安全补丁、错误修复和性能增强的 MySQL 版本。他们还希望测试 8.0 中的新功能并从中受益，包括即时 DDL、隐形索引和压缩的 bin 日志等。</p><p>详细的技术细节查看：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-12-07-upgrading-github-com-to-mysql-8-0%2F" target="_blank">https://github.blog/2023-12-07-upgrading-github-com-to-mysql-8-0/</a></u></em></p><hr><p>延伸阅读</p><ul><li><u><a href="https://www.oschina.net/news/188164/github-recent-service-disruptions">GitHub 解释近期频繁宕机原因：MySQL 不堪重负</a></u></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 05:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270460/upgrading-github-com-to-mysql-8-0</guid>
            <link>https://www.oschina.net/news/270460/upgrading-github-com-to-mysql-8-0</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[因 EXT4 数据损坏错误，Debian 12.3 推迟发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Debian 团队发布公告称，由于 Linux 内核 6.1.64-1 中的<strong> ext4 文件系统出现数据损坏问题</strong>，因此原计划昨天发布的 Debian 12.3 将会被推迟，同时进行修复。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-6b960c796ab8ff358469f03578c81866ec1.png" referrerpolicy="no-referrer"></p><p>来源：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.debian.org%2FNews%2F2023%2F2023120902" target="_blank">https://www.debian.org/News/2023/2023120902</a></u></p></blockquote><p>据介绍，此 bug 由从 Linux 6.5 回溯的一个有问题补丁导致，它引起了 EXT4 和 iomap 代码之间的干扰，可能导致旧内核上的数据损坏。</p><p>这个问题主要出现在最近的 Linux 6.1 LTS 点版本中，新的 Linux 6.1.66 版本已经回滚了有问题的提交。Debian 的 bug 报告称这个问题为「非严重的数据丢失」，因此应该是可以恢复的。</p><p>但由于 Debian 12.3 原本计划发布的内核版本受到了影响，因此被推迟发布。建议 Debian 12 用户在 Linux 6.1.66 内核镜像推出之前不要升级系统。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 03:21:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270435/debian-12-3-delayed-ext4-corrupt</guid>
            <link>https://www.oschina.net/news/270435/debian-12-3-delayed-ext4-corrupt</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[周鸿祎：有人找我做养猪大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在 2023 中国企业领袖年会上，360 创始人周鸿祎对于最近的 AI 大模型热潮发表了看法。</p><p>他表示，（感觉）大家对大模型充满了一种无限的向往或者不切实际的膜拜，之前还有人找他做养猪大模型。他认为，大模型的技术路线突破才短短几年，目前还存在着很多缺点。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-391bf18f540407673913ddee0ac73938969.png" referrerpolicy="no-referrer"></p><p>来源：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2Ftv%2Fshow%2F1034%3A4977511076134973%3Ffrom%3Dold_pc_videoshow" target="_blank">https://weibo.com/tv/show/1034:4977511076134973</a></u></p></blockquote><p>他希望大家对大模型有一个正确的认知，<strong>不要高估现在大模型的能力，不要低估大模型未来发展的潜力</strong>，虽然它现在已经可以跟实体产业相结合，但它还不能完全接管此类业务，应该扬长避短发挥它的长处，因为很多短板还有待解决。</p><p>目前国内各大企业、科研机构和高校等单位已公开的 AI 大模型至少已经达到了 188 个，而首批通过《生成式人工智能服务管理暂行办法》备案的大模型已于 8 月 31 日公布，第二批通过备案的 AI 大模型也已于 11 月开放服务。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3a3b3aaeff5f0043de536b6f5f44b963797.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 03:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270430</guid>
            <link>https://www.oschina.net/news/270430</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[理想汽车全自研多模态认知大模型 —— Mind GPT]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>理想汽车于 12 月 10 日晚正式发布 OTA 5.0 版本，并计划于 12 月 19 日开启全量用户推送。官方介绍称，在 OTA 5.0 中，理想同学最大的变化是引入了 Mind GPT 的能力。</p><p>Mind GPT 是理想全自研的多模态认知大模型，据称他们从 0 到 1 构建了 Mind GPT 原始基座模型，<strong>模型结构采用了自研的 TaskFormer 神经网络架构</strong>，基于用车、娱乐、出行等场景使用 SFT、RLHF 等技术进行了一系列的训练，让 Mind GPT 拥有了理解、生成、知识记忆及推理的三大能力。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7aa6319e367ca499b8541b306ec3892d181.png" referrerpolicy="no-referrer"></p><p>目前 Mind GPT 还处于内测版本阶段，那么 Mind GPT 在行业里到底是什么水平呢？</p><p>官方称在目前国内极具权威性的，中文大语言模型评测榜单 C-EVAL，覆盖了人文、社科、理工等多个方向共 52 个学科，Mind GPT 在 58 个参加测评的大模型中排行第一名；同时，还有涵盖从基础学科到高级专业包含 67 个主题领域的评测榜单 CMMLU，Mind GPT 也获得第一名，拿下了双冠军。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-d89ae63875873819f3b3676b6fea16d8c81.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-af86f00982833de97dd852d6a57f17288dd.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 02:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270426</guid>
            <link>https://www.oschina.net/news/270426</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[DDE V23 全新升级，引爆海外科技媒体关注！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>内容来源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fdde-overseas-concerns%2F" target="_blank">deepin 社区</a></p><hr><p>近日，DDE (Deepin Desktop Environment，深度桌面环境) 迎来版本升级，并被移植到多个主流 Linux 发行版中，引发多家海外知名科技媒体的关注和报道。</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="555" src="https://storage.deepin.org/thread/202312111015036481_1.png" width="1041" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:center"><em>TechBullion，英国知名科技媒体</em></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">报道称，诞生于中国 Linux 开源社区的桌面环境 DDE，自 2013 年推出以来，创造了中国 Linux 历史的多个「首次」：推出首个&nbsp;Linux 桌面应用商店，率先在 Linux 桌面引入智能助手，首家支持高分屏、软件包签名、人脸识别登录……与 GNOME、KDE、XFCE 等国际主流桌面环境一样，DDE 在全球广受欢迎，已经被移植到包括 Arch、openSUSE、Ubuntu、Fedora、Manjaro 等主流 Linux 发行版中。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><img alt="" height="508" src="https://storage.deepin.org/thread/202312111015122694_2.png" width="1037" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:center"><em>Gearrice，美国知名技术和创新媒体网站</em></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">DDE V23 预览版公布后，全新设计引发了广大开源社区网友们的关注。同时随着 DDE V23 各个项目的代码开源以及代码稳定程度的不断改善，不同发行版的包维护者们也开始了对全新 DDE 桌面环境的移植。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">现如今，在这些来自不同发行版的包维护者和社区贡献者们的一同努力下，已经有多个发行版完成 DDE V23 桌面环境的主要组件适配，包括 Arch Linux、UbuntuDDE、NixOS、openSUSE 等。还有很多发行版仍在积极的对 DDE V23 进行移植，其中知名的发行版包括 Debian、Gentoo、Fedora 等。</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="755" src="https://storage.deepin.org/thread/20231211101533641_3.png" width="1027" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">此外，针对 DDE V23 全新升级的报道，海外用户给出了积极的评价，认为「deepin（深度）操作系统已经很好地满足了用户所需要的一切，DDE is so pretty！」</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="605" src="https://storage.deepin.org/thread/20231211101547509_4.png" width="1032" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Flatest-news-deepin-v23%2F" target="_blank">同时，deepin（深度）社区研发负责人表示，DDE V23 即将在明年的 deepin V23 正式版里与大家见面。</a>deepin V23 是 deepin（深度）社区最新的发行版，将具备全新的系统结构、更多的系统架构的支持、桌面环境的持续优化、全自研 Wayland 合成器 Treeland、多会话全局窗口管理器、更强大的功能以及系统兼容性、稳健性的持续提升。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">deepin 积极拥抱新技术，努力推出更多创新的功能和服务。如果你对 DDE V23 的适配感兴趣，希望参与进来，为 DDE 的移植贡献一份力量，欢迎加入 deepin（深度）社区下的 [<strong>DDE-porting 兴趣小组</strong>]，一同展开 DDE 移植相关的话题讨论，让更多用户体验 DDE 桌面环境。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>DDE-porting 兴趣小组：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdeepin-community%2Fsig-dde-porting" target="_blank">https://github.com/deepin-community/sig-dde-porting</a></strong></p><hr><p style="text-align:right">内容来源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Findex%2Fzh" target="_blank">deepin 社区</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 02:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270420/dde-updated</guid>
            <link>https://www.oschina.net/news/270420/dde-updated</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[大湾区一体化算力服务平台正式发布，算力规模超 5000P]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>12 月 10 日，在第二届数字政府建设峰会暨数字湾区发展论坛上，深圳市前海管理局、国家（深圳·前海）新型互联网交换中心（下称「前海交换中心」）共同<strong>发布粤港澳大湾区一体化算力服务平台，并正式成立前海算力服务联盟</strong>。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-e379cb9cc7c4002e4f3fb84f03c47a13c02.png" referrerpolicy="no-referrer"></p></blockquote><p>据介绍，该平台由前海管理局提出设想和要求，在深圳市通管局、市工信局的支持下，由前海交换中心和紫金山实验室共同开发部署。</p><p>官方透露，该平台自 10 月 31 日试运行以来，汇聚的算力规模大幅增长近 4 倍，<strong>总规模已达 5180 PFLOPS</strong>，主流芯片覆盖率超 75%，并已为 10 余个企业、高校、科研机构的人工智能团队提供算力服务。</p><ul><li><p><strong>在算力调度方面</strong>，创新多维一体编排算法，实现算力高效调度和智能供给；</p></li><li><p><strong>在算力交易方面</strong>，平台不收取中介费用，促进供需双方合作与交易；</p></li><li><p><strong>在算力应用方面</strong>，高度集成各类算法工具，实现应用一键部署、资源秒级开通，进一步降低门槛、提升效率；</p></li><li><p><strong>在算力安全方面</strong>，构建算网一体化安全防护体系，持续强化算力安全保障。</p></li></ul><p><img height="360" src="https://static.oschina.net/uploads/space/2023/1211/102814_wKUa_2720166.png" width="640" referrerpolicy="no-referrer"></p><p>同时， <strong>大湾区首个算力服务行业组织 —— 前海算力服务联盟正式成立</strong>，首批成员单位包括前海科创集团、前海交换中心、紫金山实验室、华为、深圳商汤、万国数据、世纪互联、深圳数据交易所、深圳科创学院、香港中文大学未来智联网络研究院、粤港澳大湾区大数据研究院。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 02:30:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270418</guid>
            <link>https://www.oschina.net/news/270418</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[郭炜：开源大侠是怎样炼成的]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>2023 年 6 月 1 日，首个由国人主导的开源数据集成工具 Apache SeaTunnel 正式宣布从 Apache 软件基金会孵化器毕业成为顶级项目。历经 18 个月的孵化，这个项目终于瓜熟蒂落，社区贡献者 200+，代码 24.5 万行，上千家企业使用，风光无限。令人难以想象，郭炜刚接手它时，仓库被封、贡献者四散的落魄样子。</p><p>&nbsp;</p><span id="OSC_h1_1"></span><h1>大侠出手， SeaTunnel 浴火重生</h1><p>SeaTunnel 原名 Waterdrop，于 2017 年由乐视创建，并于同年在 Github 上开源，是一个大数据集成处理平台。当时国内各种数据引擎风起云涌，却少有项目解决数据源之间的无缝集成和高速同步问题，因此 Waterdrop 在其中显得颇为亮眼。可惜这种亮眼却为它招来了横祸——开源项目 Waterdrop 的商标被抢注了，而且对方的法务还发送律师函给开源的发起者和 Github 。</p><p>由于开源项目的名称不属于【商标】，而国内的商标又是【申请在先】原则，谁先申请谁先得到，因此 Waterdrop</p><p>一下子百口莫辩，落了下风， Github 在收到律师函后，把 Github 上 Waterdrop 的整个仓库都封了，所有的代码、PR、Issue 也全都无法访问，而且 Waterdrop 的创始团队还面临诉讼纠纷。没办法，团队只能在圈内四处求助，机缘巧合之下，遇到了郭炜。</p><p>郭炜，人称「郭大侠」，平时就爱在开源圈内热心给大家帮忙。创始团队找到郭炜后，郭炜看到这样的事情，又是这样有前途的项目，不忍心袖手旁观，他便把项目接了过来，一边找律师解决法律纠纷，一边利用自己的资源辗转联系微软的 Github 管理人员解释，帮助项目解封。</p><p>2021 年，争议终于告一段落，Waterdrop 改名 SeaTunnel，得以继续运转。可大侠并不放心，毕竟团队才 3 个人，维护社区已是够呛，哪还有能力顾及法律合规的事情？万一这剧情重来一遍，可就不是闹着玩的了。郭炜开始当 SeaTunnel 的 Mentor，手把手带起了开源项目，并致力于把项目孵化到 Apache 基金会里面。一方面，基金会是专业的，有专人管理法务，比现在的草台班子好多了。另一方面，SeaTunnel 也可以接替退役的 Apache Sqoop，解决数据源之间数据打通的问题。</p><p>最终，在多位导师的帮助下，2021 年 12 月 9 日，SeaTunnel 正式通过 Apache 软件基金会的投票决议，顺利进入 Apache 孵化器，成为基金会中第一个诞生自中国的数据集成平台项目，目标是「连接万源，同步如飞」。</p><p>如今，SeaTunnel 从 Apache 软件基金会孵化器毕业成为顶级项目，也在全球拥有很多企业用户和开发者，早已告别最初的窘迫。郭大侠也轻轻招手，奔赴了下一个难题。</p><p style="text-align:center"><img height="664" src="https://oscimg.oschina.net/oscnet/up-8c5e3b11d68a2569fa44bc1e66d5f395f95.png" width="500" referrerpolicy="no-referrer"></p><p>&nbsp;</p><span id="OSC_h1_2"></span><h1>大侠当年也是开源「小趴菜」</h1><p>SeaTunnel 团队最初之所以求助郭炜，是因为他成功运营过多个开源社区，在圈内早已小有名气。时间回拨到 2010 年，郭炜就开始接触开源了。那时候他在 Hadoop 社区里当「潜水党」，以一个小白的身份，旁观各路大神在里面交流技术问题，给出」炫酷「的解决方案。「在开源社区里，你能看到很多全新的项目，全新的技术，能不断学到新东西，保持走在技术圈的前排，这是别的渠道无法替代的。书上的东西太陈旧了，网上的东西又特别杂，只有在开源社区，才能纯粹地了解新技术，了解开源圈在关注啥。」</p><p>当然，郭炜这样的「 e 人」不会一直坐边角。很快他就融入了社区，经常参加线下 meetup，也 contribute 过不少文档。但在开源社区里，郭炜这个名字就是一个「 nobody 」。到联想工作之后，郭炜继续坚持开源，也把开源带到了联想。在联想 COC 核心技术架构委员会，郭炜作为全球大数据平台负责人，一直在当开源布道者，推动开源技术的应用，许多同事都是因为他的宣传才「入坑」的。</p><p>但那时候在企业内部做开源布道，也是困难重重。首先开源当时并没有现下这么火，很多人对开源知之甚少，唯一的印象就是「免费」。其次，习惯了商业软件的企业，更倾向于沿用原来的选择，毕竟商业软件虽然收费，可是有人售后，有人负责。而开源软件，虽然免费，却有风险，遇到问题，谁来解决呢？尤其是对于全球化的大公司而言，开源在当地还可能存在法律风险，哪怕这是个「省钱」的决定，想拍板也不容易。</p><p><strong>郭炜坦言，在大企业内部做开源推广，就是要承担很多的责任。</strong>说白了就是，这个锅一开始你得背一背，才能让一些关键的业务用户用起来。等他们用起来觉得不错了，你才能压住质疑，谈下一步的推广。当时为了推广 Hadoop、Spark 且要符合各国的法务规范，郭炜要跟全球的同事开会，会议从早上六点排到了夜里两三点，一遍一遍地跟大家科普这个项目是什么、怎么用、出问题怎么办、合不合规、为什么要用它......经过跟业务部门「过五关斩六将」的 battle，最后一个美国的部门率先接受了 Spark，之后因为口碑不错，才慢慢推广到了其他国家、其他部门。</p><p>「我们开源社区里面的每一个用户都是很珍贵、很不容易的，尤其是那些为刚出来的新开源项目做企业内部推广的小伙伴，每一个都是勇士。他们在企业内部推广一项新技术，不仅需要做很多工作，更是拿自己头上的乌纱帽在为社区布道、保驾护航。所以，我们关注开源社区，我们不能只看到 contributor、committer、PMC，更要看到我们社区里的普通用户、他们的艰辛和不易。」郭炜说，<strong>「其实每一个使用开源的人，都是这个社区的 contributor，他们做了很多的 contribution，只不过没有体现在代码上面而已。」</strong></p><p>&nbsp;</p><span id="OSC_h1_3"></span><h1>从开源 User 变 Owner，大侠不好当</h1><p>2016 年，郭炜加入易观，担任 CTO（首席技术官）。当时公司在做一款用户行为分析的产品，主要依靠 Presto 进行二次修改来适配场景。有一天，郭炜正在网上闲逛，突然发现有个新项目，跟自家产品的场景有点像。于是就测试了一下，结果发现比自家产品快 10 倍！郭炜一下子就被震惊了。</p><p><strong>这个项目就是 ClickHouse，俄罗斯的 Yandex 于 2016 年开源的</strong><strong>列式存储数据库</strong><strong>（</strong><strong>DBMS</strong><strong>），主要用于在线分析处理查询（</strong><strong>OLAP</strong><strong>），能够使用 </strong><strong>SQL</strong><strong> 查询实时生成分析数据报告。</strong></p><p>郭炜自问在数据技术圈已属「先锋达人」，各种研究都是随时关注的，可即便这样也没听说过这个项目，想来其他人知道它的概率就更低了。这样的好东西，怎么能忍住不分享呢？于是，郭炜联系了 ClickHouse 的全球社区负责人 Ivan，提出帮忙运营中国的社区。ClickHouse 同意了。</p><p>但是，万事开头难，从 0 到 1 新建一个开源社区，就更难。没人知道你是谁，没人愿意用你。郭炜访谈了早期快手、新浪用户，并组建了社区群。但是这第一个群，花了一年半的时间才凑满。线下社区的人就更少了，第一次 ClickHouse meetup，才来了 11 个人。</p><p>由于这是属于个人爱好的行为，ClickHouse 的各种运营活动都得自己做。日常的建群、验证、答疑、指导等等，都是下班和周末抽空完成的，每天晚上 11 点，就是郭炜的 ClickHouse 支持时间。最开始的时候，还要到每个群里手把手教大家 ClickHouse 怎么用、怎么装、怎么配？周末还要找到一些关键用户，跟他们聊天、吃饭，把他们组织起来，邀请他们来参加线下的组局等等。</p><p>「<strong>做开源不是到各种大会上去讲一讲就完了，</strong><strong>开源</strong><strong>布</strong><strong>道师</strong><strong>高光背后其实是无数的日常琐碎。</strong>想要运营好一个社区是很繁琐的，比方说群里有人发广告，你得把他踢出去；有人在里边吵架了，你要怎么维护？有人向社区扔臭鸡蛋了，你怎么判断是不是开源项目的问题？如果项目有问题，我们怎么样虚心接受？这些都是在社区维护里面要去做的事。一点一滴长年累月的积累，才能真的把社区这件事做好。」郭炜说，「你看前 Apache 的董事会主席 Craig，这样的顶级大佬，都 70 多岁了，还在基金会里做 secretary 给大家建 Apache 的账号，你就知道社区运营有多琐碎了。在哪里都一样的。」</p><p>所幸，在这条路上，郭炜不是一个人在战斗。随着 ClickHouse 用户的增加，社区队伍也愈发壮大了。微信群达到 10 个的时候，郭炜开始招募志愿者，帮忙处理群事务。线下的 meetup，一开始一二十人，在公司找个会议室就能办。后来发展到线下两三百人，线上一千多人，普通场地都装不下了，郭炜就到处找朋友借场地，再自掏腰包飞过去组织。有一次在上海的 Meetup，报名的有 300 多人，但是找不到 Meetup 的地方，当时的趣头条大数据负责人金海就找公司帮忙提供了一个酒店，有布台，有大屏，有 4 个 session，跟开源大会一样。还有当年在阅文集团的刘文成，是 ClickHouse 的小 C，帮忙回答各种问题。在这些贡献者的帮助下，ClickHouse 中国社区终于办上了正规的 meetup。</p><p style="text-align:center"><img height="898" src="https://oscimg.oschina.net/oscnet/up-3c53346abc745644c05c6c876f2b686ccc7.png" width="1860" referrerpolicy="no-referrer"></p><p>三年后的 2019 年，ClickHouse 爆火，截至目前，ClickHouse 仍是 OLAP 方面用户最多的社区。在整个社区里，中国用户也是最多的。头条、阿里等企业用户也相继加入。在这一年的 meetup，社区邀请了俄罗斯 Yandex 公司 ClickHouse 开源社区创始人 Alexey Milovidov，他说：<strong>「中国的 ClickHouse 用户量能取得这样爆发性的增长（一个季度内用户增长了四倍），离不开 William（郭炜）在中国的推广。」</strong><strong></strong></p><p>&nbsp;</p><span id="OSC_h1_4"></span><h1>功成不在我，失败犹更多</h1><p>能得到 ClickHouse 创始人的认可，郭炜很开心。不过他还是觉得，ClickHouse 能达到现在的程度，与其说是因为他这个推动者，不如说是因为这个产品本身的优秀和中国开源小伙伴们的支持。「在数据和大数据领域里，中国对开源的接受程度和开源的使用速度在全球都是最快的，比美国还要快。这得益于中国互联网的发展速度，和大量互联网公司的使用。也许开源商业的天花板没有美国那么高，但是中国卷起来的速度更快。<strong>中国往往能快速接受一个新技术，然后快速卷，快速迭代，加上中国有广大的开发者和用户基础，做起开源来有得天独厚的优势。</strong>」</p><p>现在回头看，这四五年里，郭炜自己和小伙伴们，都受益良多。当初跟他一起在社区里改代码的小伙伴们，现在薪资都翻了四五倍了。其中一个志愿者小 C 刘文成，被腾讯选中，从一个小厂跳槽到了微信里面做 ClickHouse 的维护。「人人为我，我为人人。你在社区里面做的贡献，大家都是看得见的。你的技术水平被大家认可了，那你获得的机会自然也会比别人多。<strong>我觉得这就是开源社区的魅力吧，在这里大家都是平等的，是金子很快就会发光。</strong>这也算是对社区贡献者的一种回报吧。只不过这种回报不是金钱上的，而是别人对你的认可和你的影响力上的。」郭炜说。</p><p>当然，也不是所有的开源项目都能像 ClickHouse 那么幸运。大侠也会遇到挫折，运营的开源项目中失败的更多，有好多开源项目亲自运营了两三年，star 数才十几个。自己做开源项目，哪有那么容易成功呢？「犯错没关系，犯的错误多了你积累的经验也会多。你看我现在做产品能成功，背后反而是那些失败的经验在发挥作用。做其他事也一样。」郭炜两手一摊，「因为每个人的成功，都有当时特殊的时代背景和需求，所以成功的经验，反而不重要，失败的经验更重要，它才能指导你怎么避免犯错。所以每一个成功的背后，可能都有 99 个失败，只不过大家最后只能看到那 1 个成功的而已。」</p><p>经过无数失败的郭炜，也锻炼出了自己看项目的眼光。「<strong>我觉得做开源社区，最关键的是要看准这个产品的定位：它到底解决什么问题，用什么样的技术框架？如果真的看好这个社区的发展的话，就到里面去跟社区一起成长好了。」</strong>郭炜说，「产品有 bug 没关系，每个社区都不是完美的，当初 ClickHouse 也有各种各样的问题，但只要你把大的架构定好之后，剩下的细节就在这个基础上去迭代、去完善就好了。ClickHouse 当时解决的其实就是宽表和日志查询问题，就这一件事。然后它把当时最新的技术——向量计算，直接放到引擎里，速度就是比我原来的 Presto 快十倍。它就解决这个问题，且解决得最好，所以在社区也能发展得很好。」</p><p><strong>看准了产品思路、底层逻辑和创始团队之后，剩下的事情就是坚持了。</strong>「 ClickHouse 2016 年刚刚开源的时候，我就把它引进中国了，那时候还默默无闻，直到 2019 年才爆火。前面这几年，完全就是靠熬过去的。你要相信你的眼光，持续坚持，不能半途而废。有时候一个开源社区最后能不能成功，就看你坚持的时间够不够长了。」郭炜说，「等到社区真的成长起来，影响力足够大的时候，里面的每一个小伙伴都会受益。」</p><p>&nbsp;</p><span id="OSC_h1_5"></span><h1>多重身份，在开源与商业间做平衡</h1><p>2022 年 4 月，Ted Liu（刘天栋）突然来通知郭炜：我们提名你做 Apache Software Foundation（ASF）Member，你写个材料吧！就这样，郭炜成了 Apache 基金会 Member。「收到这个荣誉的时候，特别开心，觉得这是大家对我的肯定，同时觉得自己身上的责任更重了，也更有动力去考察和维护好 Apache 的每一个项目。」</p><p>而在 2023 年，郭炜身上又多了一个身份，白鲸开源的 CEO。很少有人同时当基金会 Member 和商业公司的领导人，郭炜会不会觉得冲突呢？做决策的时候，是先考虑开源还是商业化？如果开源和商业化功能打架，大侠不就很难办？</p><p>不过，郭炜对此很淡定，他认为，开源和商业化并不冲突，甚至是相辅相成的。<strong>一个开源项目如果想长治久安可持续</strong><strong>发展</strong><strong>，那商业化大概是不可避免的。</strong>如果没有商业公司去承接对核心开发者和贡献者的支持，去满足深度用户的需求，久而久之，纯靠爱发电的核心贡献者可能也会难以为继。</p><p>「像白鲸开源这样做（Apache SeaTunnel 和 DolphinScheduler）商业化的公司，不是开源的对立面，而是开源的促进者。」郭炜说，「商业能够更好地保住开源的调性和核心贡献者的饭碗，让他们能够持续地在开源上发力。同样地，有些深度的用户，当开源项目无法完全满足他的需求，或者需要有人帮他在企业内部做推广的时候，有一个商业实体来帮他一起做这件事，那这个布道师也会轻松一点，而不必像我当初那样独自一个人舌战群儒，过五关斩六将。」</p><p>可是，开源项目之所以商业化困难，恰恰是因为公开了代码。商业和开源究竟如何取舍？哪些应该开源，哪些不开源？遇到冲突的时候，又该如何抉择？</p><p>郭炜笑笑，露了一手聪明的「切糕大法」：「首先从产品定位来讲，你得把你的开源主力用户群和你的非开源主力用户群分开——如果技术水平很强，而且自己还有时间有预算去折腾，那就用开源的好了。如果时间不够，人力预算又不足，那使用商业版更省心。所以，这两者的使用人群是不同的，你的开源软件和商业软件定位也不一样。明白了这个，你纠结的点也就没那么多了。」</p><p>按照惯例，最新的功能都会被放到开源版里面，相对稳定的、有行业属性的功能则通常放到商业版里，两边不时互通有无。郭炜要做的，就是把握好两边放功能的时间和节奏就行了。「至于具体哪些功能放到商业版、哪些功能放到开源版，这就是刀法怎么切的问题了：切得少了，你这个商业版没有价值；切多了，又会影响社区。那怎么来把握，就是一门艺术而不是技术了，这只可意会不可言传哪（笑~）」</p><p>总的来说，郭炜对中国的开源商业环境非常看好。毕竟中国对开源的接受程度很高。虽然从开源社区到商业公司和商业产品这一条路大家还在摸索，但至少，郭炜接触到的新一代决策者，已经跟过去不一样了：他们明白开源会让公司的技术和国际接轨、和全球最新的科技接轨。无论是传统公司还是互联网企业，都在逐步尝试使用开源原生的商业软件。</p><p>「中国开源商业的氛围和整体的步伐，正在觉醒。」甚至中国开源走向全球，郭炜也觉得大有希望：「毕竟中国有这么好的土壤，特别在大数据领域里，有这么多的数据、终端、场景、性能......卷出来的项目，它一定是全球排名前列的，最终跟海外商业场景相结合，一定能卖得很好。」</p><p>&nbsp;</p><span id="OSC_h1_6"></span><h1>开源老将，在酝酿下一个社区</h1><p>在开源圈里，郭大侠也有自己的偶像：「Craig 给我做了一个榜样，他都 70 多了还在坚持为开源做贡献，我觉得我活到 70 岁时候也能继续做开源，他就是我的榜样。哈哈。」</p><p>活到老学到老，这也许不止是郭炜一个人的开源理想，但至少，郭炜坚持到了现在。</p><p>如今，作为开源老将，郭炜又在关注下一个热点了——大模型，特别是开源的大模型。「我认为将来的每一款软件，都会被大模型和相关的 AI 技术再改造一遍、重做一遍。下一步如果再去孵化项目，可能就是跟大模型相关的了。」郭炜说，「如果只是训练大模型，那么国内外只有寥寥几家公司能玩得起。但是大模型生态上下游的公司如果要做好，还是有很多机会的。那么，哪些东西能够促进大模型的应用、降低大模型的使用门槛、让大模型真正跑起来，尤其是大模型跟数据之间的关联，将会是我关注的重点。」</p><p>郭大侠收拾行囊，又奔赴了下一场挑战。</p><p>不知道接下来，他又会遇到怎样的故事呢？</p><div style="text-align:center"><img height="750" src="https://oscimg.oschina.net/oscnet/up-5203b04a5dc96550855c4bae1487b99f11a.png" width="500" referrerpolicy="no-referrer"></div><div>
  &nbsp; 
</div><div>
  &nbsp; 
</div><div><blockquote><div><span style="color:#16a085"><strong><span style="background-color:#f6f6f6">【溯源】</span><span style="background-color:#f6f6f6">在每一场对话中，追溯关于开源的故事，认识那些极客、自由，并坚持着的开源人。</span></strong></span></div></blockquote><p style="color:#494949; margin-left:0; margin-right:0; text-align:left"><span><span>OSCHINA 推出的开源人物专访栏目【溯源】。</span></span></p><p style="color:#494949; margin-left:0; margin-right:0; text-align:left"><span><span>溯源，意指向源头追溯，为开源求解。问渠哪得清如许，为有源头活水来。每一个开源参与者，都是掀起开源浪潮最鲜活的源泉。所有开源故事，共同构建着我们今天看到的开源世界。</span></span></p><p style="color:#494949; margin-left:0; margin-right:0; text-align:left"><span><span>开源刚出现的数十年里，为开源奔走的黑客团体都在遭受来自社会主流的冷漠和排斥。即便现在的软件行业已经大喊出 「拥抱开源」 的口号，问题也依然存在。</span></span></p><p style="color:#494949; margin-left:0; margin-right:0; text-align:left"><span><span>我们不知道开源贡献者、开源布道师，以及所有参与开源的人还会面临多少阻碍，但给予我们信心的是，更多的人在投身开源事业。</span></span></p><p style="color:#494949; margin-left:0; margin-right:0; text-align:left"><span><span>所以 OSCHINA 希望面向开发者社区，寻找每一个积极参与开源、对开源有想法的人，了解他们以及他们的开源故事，窥探故事中的开源事业发展规律。</span></span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">【溯源】系列文章：</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">01&nbsp;<a href="https://my.oschina.net/u/4105562/blog/4721676"><span style="background-color:#ffffff; color:#494949">适兕</span><span>&nbsp;</span>：成为开源布道师</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">02&nbsp;<a href="https://my.oschina.net/u/4489239/blog/4875125">衞剑钒：开源圈的 「世外高手」</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">03&nbsp;<a href="https://my.oschina.net/u/4489239/blog/4945872" target="_blank">「工具人」 赵生宇：清北本硕，为开源从阿里辞职去同济读博</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">04&nbsp;<a href="https://my.oschina.net/u/4489239/blog/5047833" target="_blank">吴晟：开源对我来说，社交是最重要的</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">05&nbsp;<a href="https://my.oschina.net/u/4489239/blog/6215354" target="_blank">悟空刘歧：技术瑕疵不除不快，开源社区代码说话</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">06&nbsp;<a href="https://my.oschina.net/u/3859945/blog/5504643" target="_blank">姜宁，带程序员前往开源 「乌托邦」</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">【溯源】专栏正在征集开源人物故事，如果你认为自己或是身边的人对开源做出过独特贡献，欢迎留言评论，让我们听听 TA 的故事。</p></div></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 02:21:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6852546/blog/10320168</guid>
            <link>https://my.oschina.net/u/6852546/blog/10320168</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Laravel Pulse —— 实时应用程序性能监控工具和仪表板]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#1f2328">Laravel Pulse 是一款适用于 Laravel 应用程序的实时应用程序性能监控工具和仪表板。</span>可让你一目了然地了解应用程序的性能和使用情况。跟踪缓慢的作业和端点等瓶颈，找到最活跃的用户等等。</p><p><img alt="" height="349" src="https://static.oschina.net/uploads/space/2023/1205/142731_blmT_4252687.png" width="500" referrerpolicy="no-referrer"></p><p>特性：</p><ul><li><strong>应用程序使用情况。</strong>找出在整个 Laravel 应用程序中发出最多请求、使用最慢端点和派遣最多任务的用户。</li><li><strong>服务器统计。</strong>监控服务器的 CPU、内存和磁盘使用情况。运行多个服务器？没问题。Pulse 可以在一个地方监控所有服务器。</li><li><strong>队列监控。</strong>在优化 queue workers 的过程中消除猜测。查看实时和历史统计数据，了解有多少作业待处理、有多少作业失败、有多少作业已成功处理。</li><li><p><strong>性能。</strong>查看应用程序性能瓶颈的高级概览。查看影响用户的最慢端点、查询、作业和发出请求。</p></li><li><strong>异常趋势。</strong>概述应用程序中发生的异常。将异常情况与应用程序的完整健康状况概览并排显示，有助于您发现整个堆栈中的异常情况。</li><li><strong>Bring Your Own。</strong>为自己定制卡片，或为 Laravel 社区创建可共享的卡片。你甚至可以自定义 Pulse 面板的布局。</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 01:58:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/laravel-pulse</guid>
            <link>https://www.oschina.net/p/laravel-pulse</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 开源基础密码库，铜锁/Tongsuo]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-概述" class="anchor" href="https://gitee.com/babassl/Tongsuo#%E6%A6%82%E8%BF%B0"></a>概述</h1><p>铜锁/Tongsuo 是一个提供现代密码学算法和安全通信协议的开源基础密码库，为存储、网络、密钥管理、隐私计算等诸多业务场景提供底层的密码学基础能力，实现数据在传输、使用、存储等过程中的私密性、完整性和可认证性，为数据生命周期中的隐私和安全提供保护能力。</p><p>铜锁获得了国家密码管理局商用密码检测中心颁发的商用密码产品认证证书，助力用户在国密改造、密评、等保等过程中，更加严谨地满足我国商用密码技术合规的要求。可在<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fmisc%2Fst247r05s8b5dtct">此处</a>下载资质原始文件。</p><img src="https://github.com/Tongsuo-Project/Tongsuo/blob/master/validation-android.png" width="50%" height="50%" referrerpolicy="no-referrer"><h1><a id="user-content-特性" class="anchor" href="https://gitee.com/babassl/Tongsuo#%E7%89%B9%E6%80%A7"></a>特性</h1><p>铜锁提供如下主要的功能特性：</p><ul><li>技术合规能力
<ul><li>符合 GM/T 0028《密码模块安全技术要求》的"软件密码模块安全一级"资质</li><li>符合 GM/T 0005-2021《随机性检测规范》</li></ul></li><li>零知识证明（ZKP）
<ul><li>Bulletproofs range</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fts%2Fbulletproofs">Bulletproofs R1CS</a></li></ul></li><li>密码学算法
<ul><li>中国商用密码算法：SM2、SM3、SM4、<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fts%2Fcopzp3">祖冲之</a>等</li><li>国际主流算法：ECDSA、RSA、AES、SHA 等</li><li>同态加密算法：<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fmisc%2Fec-elgamal">EC-ElGamal</a>、<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fmisc%2Frdibad">Paillier</a>等</li><li>后量子密码学*：Kyber、Dilithium 等</li></ul></li><li>安全通信协议
<ul><li>支持 GB/T 38636-2020 TLCP 标准，即<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fts%2Fhedgqf">双证书国密</a>通信协议</li><li>支持<a href="https://gitee.com/link?target=https%3A%2F%2Fdatatracker.ietf.org%2Fdoc%2Fhtml%2Frfc8998">RFC 8998</a>，即 TLS 1.3 +<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fts%2Fgrur3x">国密单证书</a></li><li>支持<a href="https://gitee.com/link?target=https%3A%2F%2Fdatatracker.ietf.org%2Fdoc%2Fhtml%2Frfc9000">QUIC</a> API</li><li>支持<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fts%2Fleubbg">Delegated Credentials</a>功能，基于<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.ietf.org%2Farchive%2Fid%2Fdraft-ietf-tls-subcerts-10.txt">draft-ietf-tls-subcerts-10</a></li><li>支持<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Ftsdoc%2Fts%2Fdf5pyi">TLS 证书压缩</a></li><li>支持紧凑 TLS 协议*</li></ul></li></ul><p>注：*号表示正在支持中</p><h1><a id="user-content-典型应用" class="anchor" href="https://gitee.com/babassl/Tongsuo#%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8"></a>典型应用</h1><p>开源应用（Opensource Application）</p><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fangie.software%2Fen%2F">Angie</a>, Angie 是一个可以替换掉 NGINX 的新型 Web 服务器，我们建议使用铜锁的用户优先选择 Angie (We highly recommend you to replace NGINX with Angie to enable Tongsuo's functionality)</li><li>Apache APISIX</li><li>Tengine</li></ul><p>商业应用 (Commercial Application)</p><ul><li>支付宝 App</li><li>OceanBase 数据库</li><li>阿里云</li><li>天威诚信</li></ul><h1><a id="user-content-编译和安装" class="anchor" href="https://gitee.com/babassl/Tongsuo#%E7%BC%96%E8%AF%91%E5%92%8C%E5%AE%89%E8%A3%85"></a>编译和安装</h1><p>一般来说，典型的编译和安装过程如下：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">./config --prefix=/path/to/install/dir</span><span id="LC2" class="line">make</span><span id="LC3" class="line">make install</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果是 Windows，则需要：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">perl Configure enable-ntls</span><span id="LC2" class="line">nmake</span><span id="LC3" class="line">nmake install</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>以上将会安装铜锁的头文件、library 文件和铜锁二进制程序。如果需要在独立的 build 目录中编译铜锁以保证源代码仓库的整洁，则可以：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">cd tongsuo-build</span><span id="LC2" class="line">/path/to/Tongsuo/source/config --prefix=/path/to/dest</span><span id="LC3" class="line">make</span><span id="LC4" class="line">make install</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>目前铜锁支持的操作系统有：各种 Linux 发行版、macOS、Android、iOS 和 Windows。在这些操作系统上，还需要事先准备好对应的环境：</p><ul><li>make</li><li>Perl 5，以及 Text::Template 模块</li><li>C 编译器</li><li>C 库</li></ul><p>铜锁对第三方库的依赖很少，但是目前依然对 Perl 依赖较大。</p><p>如果希望执行自动化测试用例，则需：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">make test</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>在安装的时候，可以选择只安装 library 文件：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">make install_runtime_libs</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果还需要安装头文件以便于基于铜锁开发应用程序，则可以：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">make install_dev</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>也可以只安装铜锁二进制程序和其依赖的铜锁 library 文件：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">make install_programs</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>铜锁的 Configure 脚本提供了大量的用于开关各种特性的选项。一般来讲，使用<code>enable-xxx</code>做为对某个特性的开启，而使用<code>no-xxx</code>来关闭某个特性。例如，<code>enable-ntls</code>即开启 TLCP，而<code>no-rsa</code>则是不编译 RSA 算法。</p><h1><a id="user-content-文档" class="anchor" href="https://gitee.com/babassl/Tongsuo#%E6%96%87%E6%A1%A3"></a>文档</h1><p>铜锁的相关文档组织在 <a href="https://gitee.com/link?target=https%3A%2F%2Fyuque.com%2Ftsdoc">铜锁文档网站</a> 上。</p><h1><a id="user-content-交流群" class="anchor" href="https://gitee.com/babassl/Tongsuo#%E4%BA%A4%E6%B5%81%E7%BE%A4"></a>交流群</h1><p>铜锁使用钉钉群进行用户答疑和交流，欢迎扫码入群（也可直接搜索群号：44810299）：
<img src="https://github.com/Tongsuo-Project/Tongsuo/blob/master/tongsuo-dingtalk.jpg" width="50%" height="50%" referrerpolicy="no-referrer"></p><h1><a id="user-content-报告安全缺陷" class="anchor" href="https://gitee.com/babassl/Tongsuo#%E6%8A%A5%E5%91%8A%E5%AE%89%E5%85%A8%E7%BC%BA%E9%99%B7"></a>报告安全缺陷</h1><p>铜锁目前使用蚂蚁集团的威胁搜集系统，请访问如下地址进行安全缺陷的报告：</p><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fsecurity.alipay.com%2F">https://security.alipay.com/</a></li></ul><p>注意：对于非安全相关的 Bug，请使用 GitHub 的 Issues 进行提交。</p>]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 01:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/babassl/Tongsuo</guid>
            <link>https://gitee.com/babassl/Tongsuo</link>
        </item>
        <item>
            <title>
                <![CDATA[TIOBE 12 月：C# 有望成为年度编程语言]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#000000"><span style="background-color:#ffffff">TIOBE 公布了 2023&nbsp;年 12 月的</span></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F" target="_blank">编程语言排行榜</a><span style="background-color:#ffffff; color:#000000"><span style="background-color:#ffffff">。</span></span></p><p><img height="77" src="https://oscimg.oschina.net/oscnet/up-e944f70ee629593d3b3ba2ac7d008e89e4b.png" width="700" referrerpolicy="no-referrer"></p><p>2023 年度 TIOBE 编程语言名单即将出炉，其中最有望胜出的当属&nbsp;C#。事实上，早在 2022 年&nbsp;C# 就有望夺得该桂冠，但却在最后时刻被&nbsp;C++ 反超。而在今年，C# 的胜率又多出了几分；因为该语言在一年内的增长率为 +2.38%，与其最接近的竞争者 Fortran 和 F# 的增长率则仅分别上涨了 +0.64% 和 +0.48%。</p><p>此外，Top 20 中的大部分语言人气都出现了下降。<span style="background-color:#ffffff; color:#000000">TIOBE CEO&nbsp;Paul Jansen 评论称，</span>「答案就在所有小语言所在的长尾（long tail）部分。这些语言的受欢迎程度都在上升，而且越来越接近大语言」。例如：一年前，排名第 50 位的语言得分仅为 0.14%，但现在第 50 位语言的得分已经达到了 0.24%。</p><p><strong style="color:#333333">TIOBE 12 月 TOP 20 编程语言</strong></p><p><img height="414" src="https://oscimg.oschina.net/oscnet/up-b25283a71bbac81145079c4b2848ccc6e95.png" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#000000">相较上月，除了 Ruby<span>&nbsp;</span>(18→19)、R (19→20) 以及 Rust (20→18) 之间出现了小范围波动外，Top&nbsp;10-20 榜单没有其他任何排名变化，这也是近期以来榜单变动最小的一次。</span></p><p><strong style="color:#333333">TOP 10 编程语言 TIOBE 指数走势（2002-2024）</strong></p><p><img height="228" src="https://oscimg.oschina.net/oscnet/up-c048f61fdb18f5fa94fbc07b575f6acc8f9.png" width="700" referrerpolicy="no-referrer"></p><p><strong style="color:#333333">第 21-50 名编程语言排行</strong></p><p><img height="430" src="https://oscimg.oschina.net/oscnet/up-e1c00e5bc23507475a73c563cbdb213cdc9.png" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#000000">第 51-100 名如下，由于它们之间的数值差异较小，仅以文本形式列出（按字母排序）：</span></p><p>&nbsp;</p><blockquote><p>4th Dimension/4D, ABC, Algol, Apex, ATLAS, AutoLISP, Bash, Boo, Carbon, CIL, CL (OS/400), Clipper, Clojure, Curl, Eiffel, Elm, Erlang, GAMS, Groovy, Icon, Inform, Io, J#, LabVIEW, Ladder Logic, LiveCode, Maple, Modula-2, MOO, MQL5, NATURAL, Nim, OCaml, OpenEdge ABL, PostScript, Pure Data, Q, Racket, Ring, RPG, Smalltalk, Snap!, Solidity, SPARK, SPSS, Tcl, VHDL, Wolfram, X10, Zig</p></blockquote><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000">TIOBE 编程社区指数（The TIOBE Programming Community index）是一个衡量编程语言受欢迎程度的指标，该指数每月更新一次。评判的依据来自世界范围内的工程师、课程和第三方供应商，包括流行的搜索引擎，如 Google、必应、雅虎、维基百科、亚马逊、YouTube 和百度都被用于指数计算。值得注意的是，TIOBE 指数并不代表编程语言的好坏或编写代码的多少。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000">该指数可以用来检查你的编程技能是否还能跟上时代的步伐，或者在开始建立一个新的软件系统时，基于指数对采用何种编程语言做出决策。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2Fprogramminglanguages_definition%2F" target="_blank">TIOBE 指数</a><span style="color:#000000">的定义方式，以及详细榜单信息<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.tiobe.com%2Ftiobe-index%2F" target="_blank">均可查看官网</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 10 Dec 2023 03:47:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270438/tiobe-index-2023012</guid>
            <link>https://www.oschina.net/news/270438/tiobe-index-2023012</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
    </channel>
</rss>
