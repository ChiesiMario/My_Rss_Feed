<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Mon, 01 Jan 2024 05:44:45 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[回顾 30 年前的 IDE：只有 TUI、背景颜色亮瞎眼……]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>当谈到 30 年前的集成开发环境 (IDE)，纯文本模式大行其道。在 20 世纪 80 年代末/90 年代初，DOS 操作系统上的 IDE 都是<strong>基于文本用户界面 (TUI)</strong>。这些 IDE 虽然没有图形界面，但却提供了一些令人印象深刻的功能，让程序员们能够进行编码、编译和调试他们的程序。</p><p>比如 MS-DOS 自带的编辑器 EDIT.COM。这是一个全屏 TUI 编辑器，提供了菜单栏、对话框、状态栏等功能。虽然不太适合编码，但它展示了当时的技术水平。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-5028193deeef003d71722a0e0206c7cba5a.png" referrerpolicy="no-referrer"></p><p>另一个经典的 IDE 是 Borland Turbo 系列，比如 Turbo C++。这些 IDE 提供了语法高亮、编译器集成、调试器、项目管理等功能，甚至还有完整的参考手册。这些功能在当时来说非常先进，让程序员们能够在没有互联网的情况下完成整个开发过程。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-88e0ddd9805e24b5d3fab4b5e6479936743.png" referrerpolicy="no-referrer"></p><p><em>（语法高亮）</em></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0b6a07d4eff9299bbe1994abb4e8e822709.png" referrerpolicy="no-referrer"></p><p><em>（集成编译器和诊断）</em></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0d05a776e3a60a221c7cbec17db45f81932.png" referrerpolicy="no-referrer"></p><p><em>（集成项目和构建系统管理）</em></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-93863267f7c7ff62eff8100b50b2e2bd32e.png" referrerpolicy="no-referrer"></p><p><em>（包含断点、堆栈跟踪等功能的调试器）</em></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-bf6c8d95bd4f5f049c98cbaf4a30987ad5d.png" referrerpolicy="no-referrer"></p><p><em>（完整的使用手册）</em></p><p>有人表示，Bordland 在当时出品的 IDE 确实十分惊艳，但在使用 Bordland Turbo C++ 一段时间后，眼睛都要被亮瞎了……</p><p>与此同时，Linux 上的 IDE 并不如 DOS 上的 IDE 那么成熟。虽然也有一些文本模式的程序，比如 Vim 和 Emacs，但它们并不像 Borland Turbo 系列那样提供完整的集成开发环境。这导致了在当时许多程序员还是更倾向于使用 DOS 上的 IDE 进行开发。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-affdf2cf4e49f6240d455b0ad65df6324d2.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 01 Jan 2024 03:58:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273632/the-ides-we-had-30-years-ago</guid>
            <link>https://www.oschina.net/news/273632/the-ides-we-had-30-years-ago</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[盘点 2023 十大宕机事故「冥场面」]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>名场面？冥场面！</p><p>速来围观 2023 十大宕机事故「冥场面」——</p><hr><h1><a href="https://www.oschina.net/news/231236" target="_blank">哔哩哔哩（B 站）崩了两次</a></h1><p>2023 年 3 月 5 日晚 20:20 左右，许多网友表示在使用 B 站时，手机和电脑端都无法访问视频详情页，且手机端无法查看收藏夹与历史记录。还有网友表示，首页能够正常加载，但全部是繁体字。</p><p><img src="https://oscimg.oschina.net/oscnet/up-affa896050135c6a15c206de322a4e28acc.png" referrerpolicy="no-referrer"></p><p><u><a href="https://www.oschina.net/news/252405">8 月 4 日晚间</a></u>，距离上次事故 5 个月后，又有许多网友反馈 B 站图片（视频封面）无法加载、视频无法打开、视频一直在缓冲。</p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/236219" target="_blank">腾讯「3.29」一级事故</a></h1><p>2023 年 3 月 29 日凌晨，腾讯旗下的微信和 QQ 等业务曾出现崩溃状况，包括微信语音对话、朋友圈、微信支付，以及 QQ 文件传输、QQ 空间和 QQ 邮箱在内的多个功能无法使用。</p><p>直到 29 日早间，腾讯微信团队才回应表示，经工程师抢修，系统正在逐步恢复。</p><p><img src="https://oscimg.oschina.net/oscnet/up-127213d84c63650497a340914182c4a89f9.png" referrerpolicy="no-referrer"></p><p>本次事故由广州电信机房冷却系统故障导致，腾讯将它定义为公司一级事故，并对大量相关领导做出了处罚。</p><p>4 月 12 日，工业和信息化部信息通信管理局<u><a href="https://www.oschina.net/news/236943">听取腾讯公司关于 「3・29」 微信业务异常情况汇报</a></u>，要求腾讯公司进一步健全安全生产管理制度、落实网络运行保障措施，坚决避免发生重大安全生产事故，切实提升公众业务安全稳定运行水平。</p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/244039" target="_blank">唯品会 329 事故处罚结果：基础平台部负责人被免职</a></h1><p>今年 3 月 29 日，「唯品会崩了」 登上热搜，由于崩溃时间太长，影响了很多消费者无法正常下单。唯品会官方对此回应称，因系统短时故障，主站 「加购」 等功能或出现异常。</p><p>6 月 5 日，唯品会发布 「关于 329 机房宕机故障处理的公告」。公告称，3 月 29 日（00:14-12:01）南沙 IDC 冷冻系统故障，导致机房设备温度快速升高宕机，造成线上商城停止服务。此次事故影响时间持续 12 个小时，导致唯品会业绩损失超亿元，影响客户达 800 万，唯品会将此次故障判定为 P0 级故障。据了解，P0 属于最高级别事故，比如崩溃、页面无法访问、主流程不通、主功能未实现，或在影响面上影响很大（即使 Bug 本身不严重）。</p><p>公告指出，唯品会决定对此次事件严肃处理，对应部门的直接管理者承担此次事故责任，基础平台部负责人予以免职作相应处理。</p><p><img src="https://oscimg.oschina.net/oscnet/up-168b9a6041c9b94335f6c23063ef7f9ce95.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/244118/microsoft-azure-outage-brazil" target="_blank">微软 Azure 故障，17 个生产级数据库被删</a></h1><p>5 月 24 日，微软 Azure DevOps 在巴西南部地区的一处 scale-unit 发生故障，导致宕机约 10.5 个小时。后续微软首席软件工程经理 Eric Mattingly 出面针对此次故障事件道歉，并<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstatus.dev.azure.com%2F_event%2F392143683%2Fpost-mortem">透露了</a>导致中断的原因：即，一个简单的拼写错误致使 17 个生产级数据库被删除。</p><p><img alt="up-d28b235003ee1390973397efd32e59d2ee1.png" src="https://oscimg.oschina.net/oscnet/up-d28b235003ee1390973397efd32e59d2ee1.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/244330" target="_blank">中国电信出现大规模无服务问题</a></h1><p>2023 年 6 月 8 日下午，中国电信的网络和通信服务出现无信号等失灵现象，绝大部分反馈的用户都在广东区域，疑似广东省内故障。</p><p>此后中国电信客服回应表示，电信基站全省（广东电信）故障，暂时不能拨打电话，请耐心等待，现在紧急加急处理中，不便之处，敬请谅解。</p><p><img src="https://oscimg.oschina.net/oscnet/up-3ccdb0730d3b6ee9d3faf43d5d2a31c7d6e.png" referrerpolicy="no-referrer"></p><p>历时 4 个小时左右，广东省内电信网络全面恢复。</p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/263266" target="_blank">语雀 10.23 重大服务故障，持续 7 小时</a></h1><p>2023 年 10 月 23 日语雀出现重大服务故障，且持续 7 个多小时才完全恢复。语雀团队后续公布了故障原因及处理过程：</p><p>10 月 23 日下午，服务语雀的数据存储运维团队在进行升级操作时，由于新的运维升级工具 bug，导致华东地区生产环境存储服务器被误下线。受其影响，语雀数据服务发生严重故障，造成大面积的服务中断。</p><p><img src="https://oscimg.oschina.net/oscnet/up-d0d73c9d391b9afb17b5d8a1fdfe8babfa4.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/266694" target="_blank">阿里云 11.12 重大服务故障，全线产品受影响</a></h1><p>2023 年 11 月 12 日下午，阿里云出现严重故障，全线产品受影响。</p><p>后续官方确认故障原因与某个底层服务组件有关。在历时约 5 个小时后，阿里云宣布受影响云产品均已恢复，因故障影响部分云产品的数据（如监控、账单等）可能存在延迟推送情况，不影响业务运行。</p><p><img src="https://oscimg.oschina.net/oscnet/up-12e038da50562b9fb7806fac53534a272ac.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/268480" target="_blank">滴滴 11.27 系统服务故障，技术团队连夜修复</a></h1><p>2023 年 11 月 27 日晚间，滴滴因系统故障导致 App 服务异常，不显示定位且无法打车。11 月 27 日晚，滴滴出行进行了回复：非常抱歉，由于系统故障，今天晚间滴滴 App 服务出现异常，经技术同学紧急修复，目前正陆续恢复中。</p><p>2023 年 11 月 28 日早间，滴滴出行消息称，网约车等服务已恢复，骑车等在陆续修复中。11 月 28 日，在滴滴发出公告的同时，记者在上海、深圳等地使用滴滴呼叫网约车，发现网约车功能并未恢复使用，网络加载异常，仍无法打车。11 月 28 日，滴滴向记者回应称，网约车服务已恢复，司机乘客权益陆续恢复补发。</p><p><strong>11 月 29 日，滴滴再次发文致歉，称初步确定事故起因是底层系统软件发生故障</strong>。</p><p><img src="https://oscimg.oschina.net/oscnet/up-de730d329966eb2d4a658c5008f11be82a7.png" referrerpolicy="no-referrer"></p><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1129/113103_VOdZ_2720166.png" referrerpolicy="no-referrer"></p></blockquote><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/231624/a-single-engineer-brought-down-twitter" target="_blank">推特严重宕机，马斯克暴怒</a></h1><p>2023 年 2 月，马斯克因其关于超级碗的推文曝光度不如美国总统拜登，而深夜紧急召集约 80 人解决算法问题。</p><p>3 月份，因一名工程师修改配置导致推特出现严重的宕机故障，马斯克扬言要将代码全部进行重构。</p><p><img src="https://static.oschina.net/uploads/space/2023/0308/083022_yiJO_2720166.png" referrerpolicy="no-referrer"></p><p>7 月份，用户反馈平台再次出现问题，无法发布新推文，收到 「超出限制」 的错误提示。马斯克则回应称，Twitter 正在努力应对 「极端程度的数据抓取」 和 「系统操纵」，这些新的限制是遏制这些紧迫问题的重要措施。</p><p>&nbsp;</p><h1><a href="https://www.oschina.net/news/265693" target="_blank">ChatGPT 服务中断近 2 小时，CEO 奥特曼道歉：流量远超预期</a></h1><p>北京时间 11 月 8 日晚 22 点左右，OpenAI 旗下 ChatGPT 以及相关 API 出现中断故障，导致面向用户和开发者的服务近 2 小时无法正常使用。</p><p><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstatus.openai.com%2F">随后 OpenAl 更新事故报告称</a></u>，已确定了一个导致 API 和 ChatGPT 错误率高的问题，正在努力修复。</p><p><img src="https://oscimg.oschina.net/oscnet/up-9dfae5273c9fd8f249f7e5bbc7592211806.png" referrerpolicy="no-referrer"></p><p>与此同时，OpenAI CEO 山姆・奥特曼<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2Fsama%2Fstatus%2F1722315204242149788">公开致歉称</a></u>，本周发布的新功能遇到远超预期的使用量。公司原计划在周一为所有订阅者启用 GPTs 服务，但目前还无法实现。由于负载的原因，短期内可能会出现服务不稳定的情况，对此情况向用户道歉。</p><p>&nbsp;</p><p><em>延伸阅读：<u><a href="https://www.oschina.net/news/270052">网信办发布《网络安全事件报告管理办法（征求意见稿）》</a></u></em></p><hr><p>更多年度重磅事件回顾，查看<strong><u><a href="https://talk.gitee.com/report/china-open-source-2023-annual-report.pdf?fr=shida_news1231" target="_blank">《2023 中国开源开发者报告》</a></u></strong>。</p><p><img height="4950" src="https://oscimg.oschina.net/oscnet/up-742bb3d98bf476a2aa6120928bae7b2ee33.png" width="3497" referrerpolicy="no-referrer"><img height="4950" src="https://oscimg.oschina.net/oscnet/up-e2e5b4d5020eec787e184044fd4f42d8c7b.png" width="3497" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 31 Dec 2023 04:02:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273501</guid>
            <link>https://www.oschina.net/news/273501</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[恶意软件滥用 Google OAuth 端点「恢复」cookie、劫持帐户]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">多个信息窃取恶意软件系列正在滥用名为「MultiLogin」的未记录的 Google OAuth 端点来恢复过期的身份验证 cookie 并登录用户帐户（即使账户密码已被重置）。</span></p><p style="color:#404040; margin-left:0; margin-right:0; text-align:justify"><span style="color:#000000">科技网站&nbsp;<span style="background-color:#ffffff"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Fmalware-abuses-google-oauth-endpoint-to-revive-cookies-hijack-accounts%2F" target="_blank">BleepingComputer</a> 指出，他们在今年 11 月底曾报道了两名黑客：Lumma 和 Rhadamanthys，两人声称可以恢复在攻击中窃取的过期谷歌验证 cookie。</span></span><span style="color:#000000"><span style="background-color:#ffffff">即使合法所有者已经注销、重置密码或会话过期，这些 cookie 仍可让网络犯罪分子在未经授权的情况下访问谷歌账户。但在这一个多月来，BleepingComputer&nbsp;曾多次联系谷歌，询问相关说法的真实性以及他们计划如何缓解这一问题，却从未收到过回复。</span></span></p><p><span style="color:#000000">CloudSEK 研究人员日前发布的一份报告则进一步揭示了这种零日漏洞利用的工作原理，并描绘了有关其利用规模的可怕景象。10 月 20 日，一个名为 PRISMA 的威胁行为者首次披露了这一漏洞称，他们发现了一种恢复过期谷歌验证 cookie 的方法。</span></p><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>对漏洞进行逆向工程后，CloudSEK 发现它使用了一个名为「MultiLogin」的未记录的 Google OAuth 端点，该端点旨在通过接受帐户 ID 和 auth-login&nbsp;tokens 向量来同步不同 Google 服务之间的帐户。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>「此请求用于在多个 Google 网站（例如 YouTube）的 Google 身份验证 cookie 中设置浏览器中的 Chrome 帐户。」&nbsp; &nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>「这个请求是 Gaia Auth API 的一部分，只要 cookie 中的帐户与浏览器中的帐户不一致就会触发。」</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>CloudSEK 表示，滥用该终端的信息窃取恶意软件会提取登录到谷歌账户的 Chrome 配置文件的 tokens 和账户 ID。这些被盗信息包含两个关键数据：service (GAIA ID) 和 encrypted_token。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>加密令牌使用存储在 Chrome 浏览器"Local State"文件中的加密密钥进行解密。同样的加密密钥也用于解密浏览器中保存的密码。利用窃取的 token：GAIA 与多重登录端点配对，威胁行为者可以重新生成过期的 Google Service cookies，并保持对受损账户的持久访问。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p><img height="263" src="https://oscimg.oschina.net/oscnet/up-0d39112d34ae544e0bc2bc03fa265fea829.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#404040">CloudSek 研究员 Pavan Karthick 表示，他们对该漏洞进行了逆向工程，并能够使用它来重新生成过期的 Google 身份验证 cookie，如下所示：</span></p><p><img height="253" src="https://oscimg.oschina.net/oscnet/up-b05c5e78446b2fe30d3c88ba80361617dc5.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#404040">Karthick 解释称，如果用户重置其 Google 密码，身份验证 cookie 只能重新生成一次。否则，它可以多次重新生成，从而提供对帐户的持久访问。</span></p><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>Lumma stealer 于 11 月 14 日首次利用了该漏洞，</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>Radamanthys 是第一个在 11 月 17 日效仿的人；</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>此后还有 12 月 1 日的 Stealc、12 月 11 日的 Medusa、12 月 12 日的 RisePro 和 12 月 26 日的 Whitesnake。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>因此，目前至少有 6 个信息窃取者声称能够使用此 API 端点重新生成 Google cookie。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>此后，</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="background-color:#ffffff; color:#070707">Lumma 还发布了该漏洞的更新版本：转而使用 SOCKS 代理来逃避 Google 的滥用检测措施，并在恶意软件和 MultiLogin 端点之间实现加密通信；以抵消谷歌的缓解措。</span></p><p style="margin-left:0; margin-right:0; text-align:start"><span style="color:#000000"><span style="background-color:#ffffff">BleepingComputer 认为，这实际上也表明，谷歌方面是知道这一漏洞的存在的。但该公司</span>尚未确认 MultiLogin 端点被滥用的这一事件，因此目前该漏洞利用的状态及其缓解措施仍不清楚。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 31 Dec 2023 03:58:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273500/malware-abuses-google-oauth-endpoint-cookies</guid>
            <link>https://www.oschina.net/news/273500/malware-abuses-google-oauth-endpoint-cookies</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[1-11 月我国规上互联网企业完成业务收入 15668 亿元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="background-color:#ffffff; color:#000000"><span style="background-color:#ffffff">工信部最新</span></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.miit.gov.cn%2Fgxsj%2Ftjfx%2Fhlw%2Fart%2F2023%2Fart_e133fb8083b84cc5993dfe7ae5eb32d0.html" target="_blank">发布</a><span style="background-color:#ffffff; color:#000000"><span style="background-color:#ffffff">的&nbsp;2023 年 1-11 月份互联网和相关服务业运行情况指出，</span></span><span style="color:#070707">1-11 月份，互联网业务收入增速持续提升，利润总额增势放缓，研发经费持续下滑。</span></p><h4 style="margin-left:0px; margin-right:0px; text-align:justify"><strong><span>一、总体情况</span></strong></h4><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>互联网业务收入</span></strong><strong><span>增速持续提升</span></strong><strong><span>。</span></strong></span><span>1－11</span><span>月份</span><span><span>，我国规模以上互联网和相关服务企业</span><span><span>1</span></span><span>（以下简称互联网企业）完成互联网业务收入</span></span><span>15668 亿元，同比增长 6.1%</span><span>。</span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><img height="249" src="https://oscimg.oschina.net/oscnet/up-207c3b83268b47a98ecb744cb5cdb7b82fb.png" width="500" referrerpolicy="no-referrer"></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>利润</span></strong><strong><span>总额</span></strong><strong><span>增势放缓</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span><span>，我国规模以上互联网企业营业成本同比增长</span></span><span>9.7%，增速较</span><span>1</span><span>－10 月份回落 0.1</span><span><span>个百分点。实现利润总额</span></span><span>1189 亿元，同比增长 2.5</span><span>%。</span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><img height="252" src="https://oscimg.oschina.net/oscnet/up-593de36ec44bc8dd102524f5b7a772b98b3.png" width="500" referrerpolicy="no-referrer"></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>研发经费</span></strong><strong><span>持续下滑</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span><span>，我国规模以上互联网企业共投入研发经费</span></span><span>822.7 亿元，同比下降 4.5</span><span>%。</span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><img height="249" src="https://oscimg.oschina.net/oscnet/up-6fa68b7451c0f67c4ed961f6c2559e4f35a.png" width="500" referrerpolicy="no-referrer"></p><h4 style="margin-left:0px; margin-right:0px; text-align:justify"><strong><span>二、分领域情况</span></strong></h4><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>（一）信息服务领域企业</span></strong><strong><span>收入实现正增长</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span><span>，以信息服务为主的企业（包括新闻资讯、搜索、社交、游戏、音乐视频等）互联网业务收入同比</span></span><span>增长 0.4</span><span>%。</span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>（二）生活服务领域企业收入</span></strong><strong><span>保持快速增长</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span><span>，以提供生活服务为主的平台企业（包括本地生活、租车约车、旅游出行、金融服务、汽车、房屋住宅等）互联网业务收入同比增长</span></span><span>22.1%。</span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>（三）网络销售领域企业收入</span></strong><strong><span>增速有所回落</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span><span>，主要提供网络销售服务的企业（包括大宗商品、农副产品、综合电商、医疗用品、快递等）互联网业务收入同比增长</span></span><span>23.7%。</span></p><h4 style="margin-left:0px; margin-right:0px; text-align:justify"><strong><span>三、分地区情况</span></strong></h4><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>中</span></strong><strong><span>部地区互联网业务收入增</span></strong><strong><span>速持续提升</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span><span>，东部地区完成互联网业务收入</span></span><span>14016 亿元，同比增长 6.5</span><span>%，增速与 1</span><span>－10 月份持平</span><span><span>，高于全国增速</span></span><span>0.4 个百分点，占全国互联网业务收入的比重为 89.5%。中部地区完成互联网业务收入</span><span><span>701</span></span><span>亿元，同比增长 10.5%，</span><span>增速</span><span>较 1－10 月份提升 4</span><span><span>个百分点，</span></span><span>高于全国增速 4.4 个百分点。西部地区完成互联网业务收入 915.9 亿元，同比下降 1.6</span><span>%，降幅较 1</span><span>－10 月份扩大 0.2</span><span><span>个百分点。东北地区完成互联网业务收入</span></span><span>35.3 亿元，同比下降 25.9</span><span>%，降幅较 1</span><span>－10 月份收窄 3.2</span><span><span>个百分点。</span></span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><img height="246" src="https://oscimg.oschina.net/oscnet/up-f13b705f77044296551c3a3d3f3473ce1a2.png" width="500" referrerpolicy="no-referrer"></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>长三角</span></strong><strong><span>地区互联网业务收入</span></strong><strong><span>增速领先</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span><span>，</span></span><span>京津冀地区完成互联网业务收入 6000 亿元，同比增长 5.6%，增速较</span><span>1</span><span>－10 月份提升 0.6 个百分点，占全国互联网业务收入的比重为 38.3%。</span><span>长三角</span><span>地区完成互联网业务收入 5242 亿元，同比增长 12.8%，增速较</span><span>1</span><span>－10 月份回落 0.4</span><span><span>个百分点，占全国互联网业务收入的比重为</span></span><span>37.5%。</span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><img height="307" src="https://oscimg.oschina.net/oscnet/up-ba7cb957eefa1894ad7636e89bb0f561fa4.png" width="500" referrerpolicy="no-referrer"></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><span><strong><span>半数</span></strong><strong><span>地区互联网业务</span></strong><strong><span>增速实现正增长</span></strong><strong><span>。</span></strong><span>1</span></span><span>－11 月份</span><span>，互联网业务累计收入居前</span><span>5 名的</span><span>北京</span><span>（增长 3.2</span><span>%）、上海</span><span>（增长 17.4</span><span>%）、浙江</span><span>（增长 4.8%）、</span><span>广东</span><span>（下降 6.6%）和天津（增长</span><span><span>19.1</span></span><span>%）共完成业务收入</span><span>13075</span><span>亿元，同比增长 6.6</span><span>%，占全国比重达</span><span>83.</span><span><span>5</span></span><span>%。全国互联网业务增速实现正增长的省（区、市）有 16 个。</span></p><p style="color:#070707; margin-left:0; margin-right:0; text-align:justify"><img height="242" src="https://oscimg.oschina.net/oscnet/up-196ef056da869e93a9d75c6e816c15b64ef.png" width="500" referrerpolicy="no-referrer"></p><p style="color:#070707; margin-left:2px; margin-right:0; text-align:justify"><span><strong><span>附注：</span></strong>1.规模以上互联网和相关服务企业口径由上年互联网和相</span><span style="color:#070707">关服务收入 500 万元以上调整为 2000 万元及以上，文中所有同比增速均按可比口径计算。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 31 Dec 2023 03:00:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273491</guid>
            <link>https://www.oschina.net/news/273491</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Shiori —— Go 编写的书签管理器]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Shiori 是一个用 Go 语言编写的简单书签管理器。旨在作为 Pocket&nbsp;的简单克隆。你可以将其用作命令行应用程序或 Web 应用程序。该应用程序作为单个二进制文件分发，这意味着它可以轻松安装和使用。</p><p><img height="244" src="https://static.oschina.net/uploads/space/2023/0816/150851_4oPv_4252687.png" width="500" referrerpolicy="no-referrer"></p><p><strong>特性：</strong></p><ul><li>基本书签管理，即添加、编辑、删除和搜索。</li><li>从 Netscape 书签文件导入和导出书签。</li><li>从 Pocket 导入书签。</li><li>简单干净的命令行界面。</li><li>简单而漂亮的网络界面，适合那些不想使用命令行应用程序的人。</li><li>由于其单一二进制格式，因此可移植。</li><li>支持 sqlite3、PostgreSQL 和 MySQL 作为其数据库。</li><li>如果可能，默认情况下<code>shiori</code>将解析可读内容并创建网页的离线存档。</li><li>[BETA]对 Firefox 和 Chrome 的<a href="https://github.com/go-shiori/shiori-web-ext">网络扩展支持。</a></li></ul><p><img height="406" src="https://static.oschina.net/uploads/space/2023/0816/151010_pUOz_4252687.png" width="500" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Sun, 31 Dec 2023 02:45:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/shiori</guid>
            <link>https://www.oschina.net/p/shiori</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 领先的开源数据库自治运维平台 openGauss-DBMind]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-dbmind" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#dbmind"></a>DBMind</h1><p><a href="https://gitee.com/opengauss/openGauss-DBMind#dbmind-%E4%B8%AD%E6%96%87">中文</a> | <a href="https://gitee.com/opengauss/openGauss-DBMind#dbmind-engish">English</a></p><p>Maintainer: <a href="mailto:ai@opengauss.org">openGauss AI-SIG</a></p><h1><a id="user-content-dbmind-中文" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#dbmind-%E4%B8%AD%E6%96%87"></a>DBMind-中文</h1><p>DBMind 作为 openGauss 数据库的一部分，为 openGauss 数据库提供了自动驾驶能力，是一款领先的开源数据库自治运维平台。通过 DBMind, 您可以很容易地发现数据库的问题，同时可以实现秒级的数据库问题根因分析。</p><p>DBMind 的特点：</p><ul><li>DBMind 采用了先进的插件化的架构形式，支持海量插件扩展；</li><li>支持多种运行模式，具备命令行交互式运行、服务式运行；</li><li>面向云原生进行设计，支持 Prometheus，并提供多种丰富的 exporter 插件；</li><li>提供丰富的对接模式，可以很容易地与现有管理系统进行对接，支持 RESTful API、Python SDK、命令行、Prometheus 协议等模式；</li><li>支持端到端全流程的数据库自治运维能力，包括慢 SQL 根因分析、workload 索引推荐、多指标关联挖掘、故障自修复、异常检测与根因分析等功能；</li></ul><p>DBMind 支持的主要能力：</p><ul><li>索引推荐</li><li>异常检测与分析</li><li>多指标关联分析</li><li>慢 SQL 根因分析</li><li>时序预测</li><li>参数调优与推荐</li><li>SQL 改写与优化</li><li>故障自动修复</li></ul><p><img src="https://gitee.com/opengauss/openGauss-DBMind/raw/master/docs/dbmind.png" alt="DBMind 架构图" referrerpolicy="no-referrer"></p><h2><a id="user-content-开始使用 dbmind" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8dbmind"></a>开始使用 DBMind</h2><h3><a id="user-content-下载并安装 dbmind" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85dbmind"></a>下载并安装 DBMind</h3><p>DBMind 基于 Python 语言实现，在使用 DBMind 时，需要运行环境具备 Python 虚拟机，同时安装好所需的第三方依赖。</p><h4><a id="user-content-方式一直接下载代码部署" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E6%96%B9%E5%BC%8F%E4%B8%80%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E4%BB%A3%E7%A0%81%E9%83%A8%E7%BD%B2"></a>方式一：直接下载代码部署</h4><p>DBMind 主要使用 Python 语言进行编写，因此，可以在下载获取 DBMind 的源代码后，使用操作系统上安装的 Python 虚拟机直接运行，不过该过程中的第三方依赖需要用户手动安装。</p><p>用户可以通过 <code>git clone</code> 命令从 Gitee 或者 Github 上下载代码，例如：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone --depth 1 https://gitee.com/opengauss/openGauss-DBMind.git</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>也可以通过 Gitee 或者 Github 提供的 zip 包下载路径进行下载，而后解压缩该 zip 包即可。</p><p>下载 DBMind 后，会产生一个名为 <code>openGauss-DBMind</code> 的目录， 将该目录的路径添加到环境变量<code>PATH</code>中，即可调用该目录中的可执行文件。例如可以执行下述命令完成：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">chmod +x openGauss-DBMind/gs_dbmind</span><span id="LC2" class="line"></span><span id="LC3" class="line">echo PATH=`pwd`/openGauss-DBMind:'$PATH' &gt;&gt; ~/.bashrc</span><span id="LC4" class="line">echo 'export PATH' &gt;&gt; ~/.bashrc</span><span id="LC5" class="line"></span><span id="LC6" class="line">source ~/.bashrc</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-方式二使用安装包进行部署" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E6%96%B9%E5%BC%8F%E4%BA%8C%E4%BD%BF%E7%94%A8%E5%AE%89%E8%A3%85%E5%8C%85%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2"></a>方式二：使用安装包进行部署</h4><p>DBMind 会定期在 openGauss-DBMind 项目的 release 页面发布 DBMind 的安装包，可以通过下载该 DBMind 安装包进行安装部署。该安装包会自动将 DBMind 解压到指定目录，并配置好环境变量。</p><p>安装包和校验码的下载地址为：</p><table><thead><tr><th>Name</th><th>Download</th><th>Remarks</th></tr></thead><tbody><tr><td>DBMind X86</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fopengauss.obs.cn-south-1.myhuaweicloud.com%2Flatest%2Fdbmind%2Fx86%2Fdbmind-installer-x86_64-python3.11.sh.tar.gz">dbmind-installer-x86_64-python3.11.sh.tar.gz</a></td><td>X86 架构下 DBMind 安装包</td></tr><tr><td>DBMind X86 SHA256</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fopengauss.obs.cn-south-1.myhuaweicloud.com%2Flatest%2Fdbmind%2Fx86%2Fdbmind-installer-x86_64-python3.11.sh.tar.gz.sha256">dbmind-installer-x86_64-python3.11.sh.tar.gz.sha256</a></td><td>DBMind X86 安装包 SHA256 校验文件</td></tr><tr><td>DBMind ARM</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fopengauss.obs.cn-south-1.myhuaweicloud.com%2Flatest%2Fdbmind%2Farm%2Fdbmind-installer-aarch64-python3.11.sh.tar.gz">dbmind-installer-aarch64-python3.11.sh.tar.gz</a></td><td>ARM 架构下 DBMind 安装包</td></tr><tr><td>DBMind ARM SHA256</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fopengauss.obs.cn-south-1.myhuaweicloud.com%2Flatest%2Fdbmind%2Farm%2Fdbmind-installer-aarch64-python3.11.sh.tar.gz.sha256">dbmind-installer-aarch64-python3.11.sh.tar.gz.sha256</a></td><td>DBMind ARM 安装包 SHA256 校验文件</td></tr></tbody></table><p>安装包使用：</p><p>  解压：tar zxvf dbmind-installer-x86_64-python3.11.sh.tar.gz</p><p>  DBMind 安装: sh dbmind-installer-x86_64-python3.11.sh</p><h4><a id="user-content-关于 python 运行环境" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E5%85%B3%E4%BA%8Epython%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"></a>关于 Python 运行环境</h4><p>需要至少为 Python3.7 的版本。虽然在 DBMind 的实现中对 Python3.7 以下的环境尽可能地进行了兼容，但是这些低版本的 Python 环境疏于测试，可能会引发意料之外的异常。同时，在 DBMind 启动时，也会尝试校验 Python 版本，如果 Python 版本不符合要求，则默认不会继续执行后续的动作。</p><p><em>DBMind 的 Python 版本由根目录下的 constant 文件中的变量做约束</em></p><p>如果您的环境需要安装多个版本的 Python 运行时，并且它们可能会引起冲突，那么我们建议您将 DBMind 所需的 Python 运行环境安装到 DBMind 根目录下的 <code>python</code> 目录中，DBMind 会优先选择使用在其根目录下 <code>python</code> 目录中的环境。即 <code>gs_dbmind</code> 命令会首先在<code>python/bin</code> 目录下寻找 <code>python3</code> 命令执行后续的 Python 功能。</p><h4><a id="user-content-关于第三方依赖" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E5%85%B3%E4%BA%8E%E7%AC%AC%E4%B8%89%E6%96%B9%E4%BE%9D%E8%B5%96"></a>关于第三方依赖</h4><p>DBMind 所使用的第三方依赖通过 DBMind 根目录下的 <code>requirements-xxx.txt</code> 文件指定。对于 x86 架构（amd64）以及 ARM 架构（aarch64），使用了不同的文件名进行标识。这是因为 ARM 平台对于某些第三方依赖并不友好，必须指定特定的版本才可以安装。</p><p>可以使用 pip 工具对第三方依赖进行安装。与前文所述的情况类似，如果您当前的操作系统不得不安装多个 Python 运行环境，那么，DBMind 也支持对第三方依赖进行优先选择。即可以将第三方依赖库存储到 DBMind 根目录下的 <code>3rd</code> 目录中。 在通过 <code>gs_dbmind</code> 命令使用 DBMind 功能时，会优先选择该目录下的 <code>3rd</code> 目录中的第三方依赖库进行加载。</p><p>以 x86 环境为例，可以使用下述<code>pip</code>命令安装 DBMind 的第三方依赖库：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python3 -m pip install -r requirements-x86.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果希望指定下载的第三方依赖库地址，则可以通过 <code>--target</code> 或 <code>-t</code> 选项进行指定，例如</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python3 -m pip install -r requirements-x86.txt -t 3rd</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-使用 dbmind" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E4%BD%BF%E7%94%A8dbmind"></a>使用 DBMind</h3><h4><a id="user-content-部署 prometheus" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E9%83%A8%E7%BD%B2prometheus"></a>部署 Prometheus</h4><p>可以通过 <a href="https://gitee.com/link?target=https%3A%2F%2Fprometheus.io%2F">Prometheus</a> 官方网站获取下载方式，下载并部署 Prometheus，以便汇集对 openGauss 实例的监控结果。</p><h4><a id="user-content-部署 node-exporter" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E9%83%A8%E7%BD%B2node-exporter"></a>部署 Node Exporter</h4><p>下载并启动 <a href="https://gitee.com/link?target=https%3A%2F%2Fprometheus.io%2Fdownload%2F%23node_exporter">Prometheus node exporter</a>.</p><p>Node exporter 可以用于监控 Linux 系统，因此每个 Linux 环境（或容器内）只需要部署一个实例即可。</p><h3><a id="user-content-启动-dbmind-组件" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E5%90%AF%E5%8A%A8-dbmind-%E7%BB%84%E4%BB%B6"></a>启动 DBMind 组件</h3><p>如果希望将 DBMind 作为后台服务运行，则下面的 DBMind 组件是必须安装的，否则获取不到数据库的监控信息。为了获得更高的安全机制，DBMind 提供的 exporter 默认是使用 Https 协议的，如果您觉得您的场景中不需要使用 Https 协议，则可以通过 <code>--disable-https</code> 选项禁用。</p><h4><a id="user-content-opengauss-exporter" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#opengauss-exporter"></a>openGauss Exporter</h4><p>openGauss exporter 从 openGauss 数据库中读取系统表（或系统视图）的数据，并通过 Prometheus 存储起来。由于 openGauss exporter 需要读取监控数据库的系统表信息，因此至少应该具备 <strong>monadmin</strong> 权限。例如，可以通过下述 SQL 语句为名为 <code>dbmind_monitor</code> 用户赋予权限：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">ALTER USER dbmind_monitor monadmin;</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>使用 <code>gs_dbmind component opengauss_exporter ...</code> 命令即可启动该 openGauss exporter 组件。例如，可以通过下述命令监控某个数据库，通过 <code>--url</code> 参数指定被监控的数据库实例地址：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component opengauss_exporter --url postgresql://username:password@host:port/database --web.listen-address 0.0.0.0 --web.listen-port 9187 --log.level warn --disable-https ...</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><code>--url</code> 表示的是数据库的 DSN 地址，其格式可以<a href="https://gitee.com/opengauss/openGauss-DBMind#dsn%E7%9A%84%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E">参考此处</a>。</p><p>可以通过下述命令检查 openGauss exporter 是否已经启动：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">curl -vv http://localhost:9187/metrics</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-reprocessing-exporter" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#reprocessing-exporter"></a>Reprocessing Exporter</h4><p>reprocessing exporter 是一个用于二次加工处理数据的 exporter. 由于 node exporter、openGauss exporter 保存到 Prometheus 中的数据是即时的监控信息，而只通过这些信息是无法反应某些指标的瞬时增量信息的，例如 TPS、iops 信息等。因此，reprocessing exporter 可以用来计算增量信息或者聚合结果等。</p><p>由于 reprocessing 是从 Prometheus 中获取指标数据，进行二次加工处理后再返回给 Prometheus. 因此，它与 Prometheus 是一一对应的，即如果只有一个 Prometheus 服务，则只需要一个 reprocessing exporter 即可。例如，可以通过下述命令启动 reprocessing exporter:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component reprocessing_exporter 127.0.0.1 9090 --web.listen-address 0.0.0.0 --web.listen-port 9189</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果您的 Prometheus 使用了<code>basic authorization</code>方式进行登录校验，则需要额外指定 <code>--prometheus-auth-user</code> 以及 <code>--prometheus-auth-password</code> 选项的值。</p><h3><a id="user-content-配置以及启动" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%90%AF%E5%8A%A8"></a>配置以及启动</h3><p>DBMind 后台服务是常驻内存的。因此，您需要首先配置一个配置文件目录，在该目录中保存多个 DBMind 的配置文件。可以通过 <code>gs_dbmind service</code> 命令来进行配置文件目录的生成以及服务的启动。该命令的使用说明为：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">$ gs_dbmind service --help</span><span id="LC2" class="line">usage:  service [-h] -c DIRECTORY [--only-run {...}] [--interactive | --initialize] {setup,start,stop}</span><span id="LC3" class="line"></span><span id="LC4" class="line">positional arguments:</span><span id="LC5" class="line">  {setup,start,stop}    perform an action for service</span><span id="LC6" class="line"></span><span id="LC7" class="line">optional arguments:</span><span id="LC8" class="line">  -h, --help            show this help message and exit</span><span id="LC9" class="line">  -c DIRECTORY, --conf DIRECTORY</span><span id="LC10" class="line">                        set the directory of configuration files</span><span id="LC11" class="line">  --only-run {slow_query_diagnosis,forecast}</span><span id="LC12" class="line">                        explicitly set a certain task running in the backend</span><span id="LC13" class="line">  --interactive         configure and initialize with interactive mode</span><span id="LC14" class="line">  --initialize          initialize and check configurations after configuring.</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>下面，分别介绍配置文件目录生成，以及服务的启停操作。</p><h4><a id="user-content-配置 dbmind" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E9%85%8D%E7%BD%AEdbmind"></a>配置 DBMind</h4><p>DBMind 提供两种方式进行配置文件的生成。一种是交互式的，通过 <code>--interactive</code> 选项指定；另一种则需要用户自己手动来修改，这也是默认方式。</p><p><strong>交互式配置方式</strong></p><p>下面是一些使用示例，这里我们用 <code>CONF_DIRECTORY</code> 标识我们的配置文件目录：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service setup -c CONF_DIRECTORY --interactive</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>通过上述命令，用户可以在交互式界面中，根据提示信息输入需要监控的 openGauss 实例信息和参数。</p><p><strong>手动配置方式</strong></p><p>下面的命令演示了如何通过手动方式进行 DBMind 配置：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service setup -c CONF_DIRECTORY</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>在执行完上述命令后，会生成一个名为 <code>CONF_DIRECTORY</code> 的目录，这个目录里面包含有很多的配置文件。不过，用户需要配置 <code>CONF_DIRECTORY/dbmind.conf</code> 文件即可。当用户配置完该文件后，则需要执行一下下述命令，DBMind 会根据用户刚刚配置的信息初始化 DBMind 系统：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service setup -c CONF_DIRECTORY --initialize</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-启动与停止 dbmind 服务" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%81%9C%E6%AD%A2dbmind%E6%9C%8D%E5%8A%A1"></a>启动与停止 DBMind 服务</h4><p>当用户配置完 DBMind 数据库后，则可以直接通过下述命令启动 DBMind 后台服务：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service start -c CONF_DIRECTORY</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>通过下述命令关闭 DBMind 服务：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service stop -c CONF_DIRECTORY</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-dbmind 的组件" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#dbmind%E7%9A%84%E7%BB%84%E4%BB%B6"></a>DBMind 的组件</h3><p>如前文所述，DBMind 基于一种插件化设计，这个组件（component）即为 DBMind 提供的插件（plugin）。通过插件式设计，DBMind 可以任意进行功能扩展。如果想要使用某个组件的功能，则需要执行<code>component</code>子命令。例如某个名为<code>xtuner</code>的组件可以进行数据的参数调优，那么可以执行下述命令来使用<code>xtuner</code>的功能。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component xtuner --help</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-使用 docker 运行 dbmind" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E4%BD%BF%E7%94%A8docker%E8%BF%90%E8%A1%8Cdbmind"></a>使用 Docker 运行 DBMind</h3><p>DBMind 支持 Docker, 同时也会在 Docker Hub 上定期发布 openGauss-DBMind 的 docker 镜像，镜像的地址是：</p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fhub.docker.com%2Fr%2Fdbmind%2Fopengauss_dbmind">https://hub.docker.com/r/dbmind/opengauss_dbmind</a></p><p>可以通过下述命令拉取该镜像：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">docker pull dbmind/opengauss_dbmind</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-创建 docker 镜像" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E5%88%9B%E5%BB%BAdocker%E9%95%9C%E5%83%8F"></a>创建 Docker 镜像</h4><p>在某些情况下，您可能希望手动创建 DBMind 的 docker 镜像，例如想要创建基于最新代码的镜像时。那么，可以通过 DBMind 代码根目录下的 Dockerfile 文件创建。例如在 DBMind 的根目录中执行下述命令，即可创建名为 <code>opengauss_dbmind</code> 的镜像：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">docker build -t opengauss_dbmind .</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-docker-镜像的使用" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#docker-%E9%95%9C%E5%83%8F%E7%9A%84%E4%BD%BF%E7%94%A8"></a>Docker 镜像的使用</h4><p>DBMind 的 docker 镜像的默认执行文件是 <code>docker_run.py</code>，该启动脚本可以在容器中启动 DBMind 所需的大多数依赖服务，包括 Prometheus, openGauss exporter, reprocessing exporter. 但是，却无法在该镜像容器内运行 node exporter 来监控远端服务器上的信息。</p><p>用户可以通过下述环境变量，将需要监控的 openGauss 服务信息传递给 DBMind 的 docker 镜像：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">OPENGAUSS_DSNS: 需要监控的 openGauss 数据库实例的 DSN 信息，多个 DSN 信息用逗号 (,) 隔开</span><span id="LC2" class="line">NODE_EXPORTERS: openGauss 数据库实例所在机器的 node exporter 地址，多个地址用逗号 (,) 隔开</span><span id="LC3" class="line">METADATABASE: 可选，将 DBMind 的离线计算结果存储起来的位置，用 DSN 形式标识数据库的连接信息；若为空，则默认使用 SQLite 进行存储</span><span id="LC4" class="line">SCRAPE_INTERVAL: 可选，指标信息的采集间隔，单位是秒；默认为 15 秒</span><span id="LC5" class="line">MASTER_USER: 可选，具有管理员权限的数据库用户名，可以用来执行某些数据库变更动作或者查询当前数据库的即时状态信息；若为空，则采用 OPENGAUSS_DSNS 中提供的用户</span><span id="LC6" class="line">MASTER_USER_PWD: 可选，上述 MASTER_USER 对应的用户密码</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>注：DSN 的配置格式可以参考<a href="https://gitee.com/opengauss/openGauss-DBMind#dsn%E7%9A%84%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E">常见问题</a>中的说明。</p><p>使用<code>docker run</code>的<code>-v</code>参数可以将路径进行映射，docker 容器内的日志统一写到 <code>/log</code> 目录中，持久化的数据统一存放在 <code>/data</code> 目录中。使用 <code>-p</code> 参数可以将容器内的端口号进行映射，Prometheus 的容器内端口是 9090, DBMind 的 web 服务则使用 8080 端口。下面是个启动 docker 服务的例子：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">docker run -it \</span><span id="LC2" class="line">    -e OPENGAUSS_DSNS="dbname=postgres user=dbmind_monitor password=DBMind@123 port=6789 host=192.168.1.100, dbname=postgres user=dbmind_monitor password=DBMind@123 port=6789 host=192.168.1.101, dbname=postgres user=dbmind_monitor password=DBMind@123 port=6789 host=192.168.1.102" \</span><span id="LC3" class="line">    -e NODE_EXPORTERS="http://192.168.1.100:9100,http://192.168.1.101:9100,http://192.168.1.102:9100" \</span><span id="LC4" class="line">    -e METADATABASE='postgresql://dbmind_metadb:DBMind%40123@192.168.1.100:6789/dbmind_metadb' \</span><span id="LC5" class="line">    -e MASTER_USER='dbmind_sys' \</span><span id="LC6" class="line">    -e MASTER_USER_PWD='DBMind@123' \</span><span id="LC7" class="line">    -e SCRAPE_INTERVAL=30 \</span><span id="LC8" class="line">    -p 38080:8080 -p 39090:9090 \</span><span id="LC9" class="line">    -v `pwd`/data:/data -v `pwd`/log:/log \</span><span id="LC10" class="line">    dbmind/opengauss_dbmind </span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>上面的例子是一主二备节点的部署形态，他们的 IP 地址分别是<code>192.168.1.100</code>、<code>192.168.1.101</code>以及<code>192.168.1.102</code>，数据库的端口号都是 6789. 上面我们使用了三个用户，为了方便演示，它们的密码都设置为<code>DBMind@123</code>。其中<code>dbmind_monitor</code>负责从 openGauss 数据库中抓取指标监控，需要具备 <code>monitor admin</code>权限；<code>dbmind_sys</code> 至少需要具备 <code>monitor admin</code>权限，以便可以获取数据库的即时状态，如果具备<code>sysadmin</code>权限，则可以完成一些数据库变更动作，如慢 SQL 查杀；<code>dbmind_metadb</code> 只是负责数据保存，具备指定数据库的使用权限即可；同时，这里也进行了端口和目录的映射。</p><p>如果希望使用命令行的形式运行 DBMind，则可以直接在该 docker 镜像内调用 <code>gs_dbmind</code> 命令即可，Python 运行时和第三方依赖等都已经打包在 docker 镜像中了，无需再次安装。例如，希望使用 DBMind 的参数调优组件提供的功能，则可以执行下述命令：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">docker run -it dbmind/opengauss_dbmind \</span><span id="LC2" class="line">   gs_dbmind component xtuner recommend \</span><span id="LC3" class="line">   --database tpcds \</span><span id="LC4" class="line">   --db-host 192.168.1.100 \</span><span id="LC5" class="line">   --host-user omm \</span><span id="LC6" class="line">   --db-user tpcds \</span><span id="LC7" class="line">   --db-port 16000</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>注：在使用<code>docker run</code> 命令运行 <code>gs_dbmind</code> 时，需要指定 <code>-it</code> 参数，以便创建一个 tty.</p><h2><a id="user-content-常见问题" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"></a>常见问题</h2><h3><a id="user-content-dsn 的格式说明" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#dsn%E7%9A%84%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E"></a>DSN 的格式说明</h3><p>DSN 是 Database Source Name 的缩写，这里支持两种格式，一种是 K-V 格式，如<code>dbname=postgres user=username password=password_value port=6789 host=127.0.0.1</code>；另一种是 URL 形式，例如<code>postgresql://username:password_value@127.0.0.1:6789/postgres</code>；对于采用 URL 格式的 DSN，由于<code>@</code>等特殊字符用来分割 URL 串中各个部分的内容，故需要 URL 编码（URL encode）。例如某个用户<code>dbmind</code>的密码为<code>DBMind@123</code>，则 URL 形式的 DSN 可以是<code>postgresql://dbmind:DBMind%40123@127.0.0.1:6789</code>，即将<code>@</code>字符编码为<code>%40</code>. 类似地，需要编码的字符还包括其他可能引起歧义的字符，如<code>/</code>, <code>\</code>, <code>?</code>, <code>&amp;</code>.</p><h2><a id="user-content-相关资料" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99"></a>相关资料</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fdocs.opengauss.org%2Fzh%2Fdocs%2Flatest%2Fdocs%2FDeveloperguide%2FAI4DB-%25E6%2595%25B0%25E6%258D%25AE%25E5%25BA%2593%25E8%2587%25AA%25E6%25B2%25BB%25E8%25BF%2590%25E7%25BB%25B4.html">openGauss 在线手册</a></li><li><a href="https://gitee.com/opengauss/openGauss-DBMind/wikis">DBMind wiki</a></li><li><a href="mailto:ai@opengauss.org">openGauss AI-SIG</a></li></ul><hr><h1><a id="user-content-dbmind-engish" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#dbmind-engish"></a>DBMind-Engish</h1><p>DBMind is a part of openGauss, which empowers openGauss to carry the autonomous operations and maintenance capabilities. DBMind is leading and open-source. Through DBMind, users can easily discover database problems and the root causes of the problems in seconds.</p><p><img src="https://gitee.com/opengauss/openGauss-DBMind/raw/master/docs/dbmind.png" alt="DBMind overview" referrerpolicy="no-referrer"></p><h2><a id="user-content-getting-started" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#getting-started"></a>Getting Started</h2><h3><a id="user-content-prerequisites" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#prerequisites"></a>Prerequisites</h3><p>In order to run DBMind, the following components should be configured and running.</p><h4><a id="user-content-python-runtime" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#python-runtime"></a>Python Runtime</h4><p>At least Python 3.7.</p><h4><a id="user-content-third-party-dependencies" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#third-party-dependencies"></a>Third-party Dependencies</h4><p>Use <code>pip3 install</code> to install the python dependencies.
Type the <code>pip3 install</code> command with dependencies according to the environment you are running:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pip install -r requirements-aarch64.txt | requirements-x86.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-prometheus-up-and-running" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#prometheus-up-and-running"></a>Prometheus up and Running</h4><p>Download and run the <a href="https://gitee.com/link?target=https%3A%2F%2Fprometheus.io%2F">Prometheus</a> time-series database.</p><h4><a id="user-content-node-exporter" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#node-exporter"></a>Node Exporter</h4><p>Download and run the <a href="https://gitee.com/link?target=https%3A%2F%2Fprometheus.io%2Fdownload%2F%23node_exporter">Prometheus node exporter</a>. Node-exporter is to monitor the Linux system. Hence, one Linux environment only needs to deploy one node-exporter.</p><h3><a id="user-content-dbmind-components" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#dbmind-components"></a>DBMind Components</h3><p>The following DBMind components are required:</p><p><strong>Note: If you want to get higher security, you should use the HTTPS scheme.</strong></p><h4><a id="user-content-opengauss-exporter-1" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#opengauss-exporter-1"></a>openGauss Exporter</h4><p>The openGauss-exporter reads data from the database and places it on the Prometheus time-series database.
OpenGauss-exporter is to monitor only one database instance. So if your deployment environment has not only one instance, you should start multiple openGauss-exporters to correspond to monitor multiple database instances.
It needs database access with a user having the role of at least <strong>monadmin</strong> (monitoring administrator) granted to run it. For example, you can grant monadmin privilege to role dbmind as below:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">ALTER USER dbmind monadmin;</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Use the following command with the parameters below:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component opengauss_exporter ...</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>You can get detailed explanations of this component through passing <code>--help</code>:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component opengauss_exporter --help</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>For example, the following command starts it:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component opengauss_exporter --url postgresql://username:password@host:port/database --web.listen-address 0.0.0.0 --web.listen-port 9187 --log.level warn --disable-https ...</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>To test that the exporter is up, type the following command on its host (or use change the localhost to the server address):</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">curl -vv http://localhost:9187/metrics</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-reprocessing-exporter-1" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#reprocessing-exporter-1"></a>Reprocessing Exporter</h4><p>Reprocessing-exporter is a re-processing module for metrics stored in the Prometheus server. It helps Prometheus to reprocess the metric data then dump the new data into Prometheus. Therefore, only one needs to be started in a deployment environment.
To run it use the command below:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component reprocessing_exporter ...</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Users can see usage by using <code>--help</code> too.</p><p>See this example for running the exporter in a single machine development environment:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component reprocessing_exporter 127.0.0.1 9090 --web.listen-address 0.0.0.0 --web.listen-port 9189</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Use the following command to check that the service is up:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">curl http://127.0.0.1:9189/metrics</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-configure-start-and-stop-the-dbmind-service" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#configure-start-and-stop-the-dbmind-service"></a>Configure, Start and Stop the DBMind Service</h3><p>DBMind service is a memory-resident backend service. Therefore, users should configure it first then start or stop the service by using the configuration.</p><p>Service usages:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">$ gs_dbmind service --help</span><span id="LC2" class="line">usage:  service [-h] -c DIRECTORY [--only-run {slow_query_diagnosis,forecast,anomaly_detection,alarm_log_diagnosis,index_recommendation,knob_recommendation}] [--dry-run] [-f]</span><span id="LC3" class="line">                [--interactive | --initialize]</span><span id="LC4" class="line">                {setup,start,stop,restart}</span><span id="LC5" class="line"></span><span id="LC6" class="line">positional arguments:</span><span id="LC7" class="line">  {setup,start,stop,restart}</span><span id="LC8" class="line">                        perform an action for service</span><span id="LC9" class="line"></span><span id="LC10" class="line">optional arguments:</span><span id="LC11" class="line">  -h, --help            show this help message and exit</span><span id="LC12" class="line">  -c DIRECTORY, --conf DIRECTORY</span><span id="LC13" class="line">                        set the directory of configuration files</span><span id="LC14" class="line">  --only-run {slow_query_diagnosis,forecast,anomaly_detection,alarm_log_diagnosis,index_recommendation,knob_recommendation}</span><span id="LC15" class="line">                        explicitly set a certain task running in the backend</span><span id="LC16" class="line">  --dry-run             run the backend task(s) once. the task to run can be specified by the --only-run argument</span><span id="LC17" class="line">  -f, --force           force to stop the process and cancel all in-progress tasks</span><span id="LC18" class="line">  --interactive         configure and initialize with interactive mode</span><span id="LC19" class="line">  --initialize          initialize and check configurations after configuring.</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-configure" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#configure"></a>Configure</h4><p>DBMind offers two methods to configure. The one is an interactive mode by using <code>--interactive</code> argument, the other is a modification by hands.</p><p>See this example for configuring in the interactive mode:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service setup -c CONF_DIRECTORY --interactive</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Then users can type parameters into the shell terminal.</p><p>See the following example for configuring by hands:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service setup -c CONF_DIRECTORY</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>After executing the above command, the directory <code>CONF_DIRECTORY</code> will generate too many configuration files. Therefore, users should modify these parameters in the <code>CONF_DIRECTORY/dbmind.conf</code>. While users finish configuring, this command needs to be run to initialize DBMind according to the <code>CONF_DIRECTORY/dbmind.conf</code>.</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service setup -c CONF_DIRECTORY --initialize</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-start-or-stop-the-dbmind-service" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#start-or-stop-the-dbmind-service"></a>Start or Stop the DBMind Service</h4><p>After configuring, specify your CONF_DIRECTORY, users can start or stop the service directly.</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind service start/stop -c CONF_DIRECTORY</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-component" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#component"></a>Component</h3><p>If users want to use a specific component offline. They can use the sub-command <code>component</code>:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component xxx ...</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><code>xxx</code> is the name of a component. Users can also get the component list by using the <code>--help</code> argument.</p><p>For example, use the following component to tune the knobs of a database:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">gs_dbmind component xtuner --help</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h1><a id="user-content-license" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#license"></a>LICENSE</h1><p>Mulan PSL v2</p><h1><a id="user-content-reference" class="anchor" href="https://gitee.com/opengauss/openGauss-DBMind#reference"></a>Reference</h1><ol><li><a href="https://gitee.com/link?target=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPercent-encoding">https://en.wikipedia.org/wiki/Percent-encoding</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fdba.stackexchange.com%2Fquestions%2F243219%2Fin-postgresql-url-i-cant-use-a-password-containing-special-characters">https://dba.stackexchange.com/questions/243219/in-postgresql-url-i-cant-use-a-password-containing-special-characters</a></li></ol>]]>
            </description>
            <pubDate>Sun, 31 Dec 2023 02:37:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/opengauss/openGauss-DBMind</guid>
            <link>https://gitee.com/opengauss/openGauss-DBMind</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 反向 Debug 了解一下？揭秘 Java DEBUG 的基本原理]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Debug 的时候，都遇到过手速太快，直接跳过了自己想调试的方法、代码的时候吧……</p><p>一旦跳过，可能就得重新执行一遍，准备数据、重新启动可能几分钟就过去了。</p><p><img alt="Untitled.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-08oleiHOrAlh8kTie.png" referrerpolicy="no-referrer"></p><p>好在 IDE 们都很强大，还给你后悔的机会，可以直接删除某个 Stack Frame，直接返回到之前的状态，确切的说是返回到之前的某个 Stack Frame，从而实现让程序「逆向运行」。</p><p><img alt="Untitled 1.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-08tZjMjyHSbNdpAu8.png" referrerpolicy="no-referrer"></p><p>这个 Reset Frame 的能力，可不只是返回上一步，上 N 步也是可以的；选中你期望的那个帧，直接 Reset Frame/Drop Frame，可以直接回到调用栈上的某个栈帧，时间反转！</p><p>可惜这玩意也不是那么万能，毕竟是通过 stack pop 这种操作实现，实际上只是给调用栈栈顶的 N 个 frame pop 出来而已，还谈不上是真正的「反向 DEBUG」。</p><p>相比之下， GDB 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sourceware.org%2Fgdb%2Fnews%2Freversible.html" rel="nofollow" target="_blank">Reverse Debugging</a>就比较强大，真正的 「反向」 DEBUG，逆向运行，实现回放。</p><p>所以吧在运行过程中，已经修改的数据，比如引用传递的方法参数、变量，一旦修改肯定回退不了，不然真的成时光机了。</p><p>这些乱七八糟的调试功能，都是基于 Java 内置的 Debug 体系来实现的。</p><span id="OSC_h1_1"></span><h1>JAVA DEBUG 体系</h1><p>Java 提供了一个完整的 Debug 体系<strong>JPDA</strong>(Java Platform Debugger Architecture)，这个 JPDA 架构体系由 3 部分组成：</p><ol><li><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.oracle.com%2Fjavase%2F8%2Fdocs%2Ftechnotes%2Fguides%2Fjvmti%2Findex.html" rel="nofollow" target="_blank">JVM TI</a>- Java VM Tool Interface</p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.oracle.com%2Fjavase%2F8%2Fdocs%2Ftechnotes%2Fguides%2Fjpda%2Fjdwp-spec.html" rel="nofollow" target="_blank">JDWP</a>- Java Debug Wire Protocol</p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.oracle.com%2Fjavase%2F8%2Fdocs%2Fjdk%2Fapi%2Fjpda%2Fjdi%2Findex.html" rel="nofollow" target="_blank">JDI</a>- Java Debug Interface</p></li></ol><p>如果结合 IDE 来看，那么一个完整的 Debug 功能看起来就是这个样子：</p><p><img alt="Untitled 2.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-08129VuqLlh19HeCExh.png" referrerpolicy="no-referrer"></p><p>解释一下这个体系：</p><p>JVM TI 是一个 JVM 提供的一个调试接口，提供了一系列控制 JVM 行为的功能，比如分析、调试、监控、线程分析等等。也就是说，这个接口定义了一系列调试分析功能，而 JVM 实现了这个接口，从而提供调试能力。</p><p>不过吧，这个接口毕竟是 C++的，调用起来确实不方便，所以 Java 还提供了 JDI 这么个 Java 接口。</p><p>JDI 接口使用 JDWP 这个私有的应用层协议，通过 TCP 和目标 VM 的 JVMTI 接口进行交互。</p><p>也可以把简单这个 JDWP 协议理解为 JSF/Dubbo 协议；相当于 IDE 里通过 JDI 这个 SDK，使用 JDWP 协议调用远程 JVMTI 的 RPC 接口，来传输调试时的各种断点、查看操作。</p><p>可能有人会问，搞什么套壳！要什么 JDWP，我直接 JVMTI 调试不是更香，链路越短性能越高！</p><p>当然可以，比如 Arthas 里的部分功能，就直接使用了 JVMTI 接口，要什么 JDI！直接 JVMTI 干就完了。</p><p>开个玩笑，Arthas 毕竟不是 Debug 工具，人家根本就不用 JDI 接口。而且 JVMTI 的能力也不只是断点，它的功能非常多：</p><p><img alt="Untitled 3.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-0819od43PdyVrEOjtTP.png" referrerpolicy="no-referrer"></p><p>左边的功能类，提供了各种乱七八糟的功能，比如我们常用的添加一个断点：</p><pre><code>jvmtiError
SetBreakpoint(jvmtiEnv* env,
            jmethodID method,
            jlocation location)

</code></pre><p>右边的事件类，可以简单的理解为回调；还是拿断点举例，如果我用上面的 SetBreakpoint 添加了一个断点，那么当执行到该位置时，就会触发这个事件:</p><pre><code>void JNICALL
Breakpoint(jvmtiEnv *jvmti_env,
            JNIEnv* jni_env,
            jthread thread,
            jmethodID method,
            jlocation location)

</code></pre><p>JVMTI 的功能非常之多，而 JDI 只是实现了部分 JVMTI 的方法，所以某些专业的 Profiler 工具，可能会直接使用 JVMTI，从而实现更丰富的诊断分析功能。</p><span id="OSC_h1_2"></span><h1>远程调试与本地调试</h1><p>不知道大家有没有留意过本地 Debug 启动时的日志：</p><p><img alt="Untitled 4.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-08UAIUfwqu8mWHWTH.png" referrerpolicy="no-referrer"></p><p>第一行是隐藏了后半段的启动命令，展开后是这个样子：</p><pre><code>/path/to/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:53631,suspend=y,server=n -javaagent:/path/to/jetbrains/debugger-agent.jar ...

</code></pre><p>第二行是一个 Connected 日志，意思是使用 socket 连接到远程 VM 的 53631 端口</p><p>上一段说到，IDE 通过 JDI 接口，使用 JDWP 协议和目标 VM 的 JVMTI 交互。这里的 53631 端口，就是目标 JVM 暴露出的 JVM TI 的 server 端口。</p><p>而第一行里，IDEA 自动给我们加上了<code>-agentlib:jdwp=transport=dt_socket,address=127.0.0.1:53631</code>这么一段，这个参数的意思就是，让 jvm 以 53631 暴露 jdwp 协议</p><p>小知识，这个 agentlib 可不只是为 jvmti 提供的。它还可以让 JVM 加载其他的 native lib 包，直接「外挂」到你的 jvm 上，下面是「外挂」的参数格式：</p><p><img alt="Untitled 5.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-08Vy0912rKBUuOBMye.png" referrerpolicy="no-referrer"></p><p>所以吧，上面的描述其实不太严谨，更专业的说法是：</p><p>让 JVM 加载 JDWP 这个 agent 库，参数为<code>transport=dt_socket,address=127.0.0.1:53631</code>，这个 jdwp agent 库以 53631 端口提供了 jdwp 协议的 server。只不过这个 jdwp 是 jvm 内部的库，不需要额外的 so/dylib/dll 文件。</p><p>如有需要，你完全可以弄个 「datupiao」 的 agentlib，「外挂」到这个 jvm 上，然后在这个 lib 里调用 JVMTI 接口，然后暴露个端口提供服务和远程交互，实现自己的 jdwp！</p><p>可能某些老板们注意到了，本地调试还要 127.0.0.1 走 tcp 交互一遍，那远程调试呢？</p><p>基于上面的解释，本地调试和远程调试真的没啥区别！或者说，在目前 IDEA/Eclipse 的实现下，不存在本地调试，都是远程！只不过一个是 127.0.0.1，一个是远程的 IP 而已。</p><p>在本地调试时，IDEA 会自动给我们的 JVM 增加<code>agent</code>参数，随机指定一个端口，然后通过 JDI 接口连接，代码大概长这样（JDI 的 SDK 在 JDK_HOME/lib/tools.jar ）：</p><pre><code>Map&lt;String, Connector.Argument&gt; env = connector.defaultArguments();
env.get("hostname").setValue(hostname);
env.get("port").setValue(port);

VirtualMachine vm = connector.attach(env);

</code></pre><p>瞅瞅， VirtualMachine 里的就这点方法，能力上比 JVMTI 还是差远了</p><pre><code>List&lt;ReferenceType&gt; classesByName(String className);

List&lt;ReferenceType&gt; allClasses();

void redefineClasses(Map&lt;? extends ReferenceType, byte[]&gt; classToBytes);

List&lt;ThreadReference&gt; allThreads();

void suspend();

void resume();

List&lt;ThreadGroupReference&gt; topLevelThreadGroups();

EventQueue eventQueue();

EventRequestManager eventRequestManager();

VoidValue mirrorOfVoid();

Process process();

</code></pre><p>再回来看看 IDEA 中独立的远程调试，配置好之后，红框里的信息会提示你 ，远程的 JVM 需增加这一段启动参数，而且支持多个版本 JDK 的格式，CV 大法就能直接用。</p><p><img alt="Untitled 6.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-09aZdTrWaGxcoBVtI.png" referrerpolicy="no-referrer"></p><span id="OSC_h1_3"></span><h1>-agentlib 和 -javaagent</h1><p>有些细心的同学可能发现了，IDEA 默认的启动脚本里，同时配置了 -agentlib 和 -javaagent。</p><pre><code>-javaagent:/path/to/jetbrains/debugger-agent.jar 

</code></pre><p>这个 debugger-agent 吧，其实也没干啥事，只是对 JDK 内置的一些线程做了些增强，辅助 IDEA 的 debug 功能，支持一些异步的调试。</p><p><img alt="Untitled 7.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-11-19-12-09uMEzNf19FBA12z11N.png" referrerpolicy="no-referrer"></p><p>agentlib、javaagent 这俩兄弟，定位其实很像，都是加载自定义的代码。</p><p>不过区别在于，agentlib 是加载 native lib，需要 c/cpp 去写，相当于外挂自己的代码在 jvm 上，可以为所欲为，比如在 agentlib 里调用上面说的 JVMTI 。</p><p>而 javaagent 是用 java 写的，可以直接用上层的 Instrumentation API，做一些类的增强转换之类，这也是大多数 APM Agent、Profiler Agent 实现的基本原理。</p><span id="OSC_h1_4"></span><h1>Arthas 的玩法</h1><p>Arthas 的核心入口，其实还是 javaagent，支持静态加载和动态加载两种玩法。</p><p>静态没啥好说的，启动脚本里增加一个<code>-javaagent:/tmp/test/arthas-agent.jar</code>，然后为所欲为。</p><p>动态的叫 attach，使用 Java 提供的<code>VirtualMachine</code>就可以实现运行时添加 -javaagent，效果一样：</p><pre><code>VirtualMachine virtualMachine = VirtualMachine.attach(virtualMachineDescriptor);
virtualMachine.loadAgent(agentPath, agentArgs);

</code></pre><p>这个 Agent 在 JVM 里启动了一个 TCP server，用于收发 Arthas Client 的各种 trace、watch 、Dashboard 等指令，然后通过 Instrumentation 增强 Class 插入代码、或者直接调用某些 Java API，实现各种功能。</p><p>注意到了吗？Arthas 可以直接下载一个 jar 包，java -jar 就能连上。</p><p>其实吧，它这个直接启动的 jar 包，是一个 boot 包，启动之后把乱七八糟的 jar 都下载下来。接着动态 attach 的方式，连接到本机指定进程号的 JVM，然后再为所欲为。</p><p>在 3.5 版本之后，Arthas 还新增了一个<strong><strong>vmtool</strong></strong>命令，这个命令可以直接获取内存中的指定对象实例。</p><pre><code>$ vmtool --action getInstances --className java.lang.String --limit 10
@String[][
    @String[com/taobao/arthas/core/shell/session/Session],
    @String[com.taobao.arthas.core.shell.session.Session],
    @String[com/taobao/arthas/core/shell/session/Session],
    @String[com/taobao/arthas/core/shell/session/Session],
    @String[com/taobao/arthas/core/shell/session/Session.class],
    @String[com/taobao/arthas/core/shell/session/Session.class],
    @String[com/taobao/arthas/core/shell/session/Session.class],
    @String[com/],
    @String[java/util/concurrent/ConcurrentHashMap$ValueIterator],
    @String[java/util/concurrent/locks/LockSupport],
]

</code></pre><p>直接获取内存对象，这玩意只靠 Instrumentation API 可做不到。Arthas 搞了个骚操作，直接 JNI 调用自定义 lib，用过 cpp 直接调用了 JVMTI 的 API，融合了 Instrumentation 和 JVMTI 的能力，这下是真的为所欲为了！</p><pre><code>#include &lt;stdio.h&gt;
#include &lt;jni.h&gt;
#include &lt;jni_md.h&gt;
#include &lt;jvmti.h&gt;
#include "arthas_VmTool.h" // under target/native/javah/

static jvmtiEnv *jvmti;

...

extern "C"
JNIEXPORT jobjectArray JNICALL
Java_arthas_VmTool_getInstances0(JNIEnv *env, jclass thisClass, jclass klass, jint limit) {
    jlong tag = getTag();
    limitCounter.init(limit);
    jvmtiError error = jvmti-&gt;IterateOverInstancesOfClass(klass, JVMTI_HEAP_OBJECT_EITHER,
                                               HeapObjectCallback, &amp;tag);
    if (error) {
        printf("ERROR: JVMTI IterateOverInstancesOfClass failed!%u\n", error);
        return NULL;
    }

    jint count = 0;
    jobject *instances;
    error = jvmti-&gt;GetObjectsWithTags(1, &amp;tag, &amp;count, &amp;instances, NULL);
    if (error) {
        printf("ERROR: JVMTI GetObjectsWithTags failed!%u\n", error);
        return NULL;
    }

    jobjectArray array = env-&gt;NewObjectArray(count, klass, NULL);
    //添加元素到数组
    for (int i = 0; i &lt; count; i++) {
        env-&gt;SetObjectArrayElement(array, i, instances[i]);
    }
    jvmti-&gt;Deallocate(reinterpret_cast&lt;unsigned char *&gt;(instances));
    return array;
}

</code></pre><span id="OSC_h1_5"></span><h1>总结</h1><ol><li><p>Debug 基于 JDPA 体系</p><ol><li><p>IDE 直接接入 JDPA 体系中的 JDI 接口完成</p></li><li><p>JDI 通过 JDWP 协议，调用远程 VM 的 JVMTI 接口</p></li><li><p>JDWP 是通过 agentlib 加载的，agentlib 算是一个 native 的静态「外挂」接口</p></li></ol></li><li><p>javaagent 是 JAVA 层面的「外挂」接口，用过 Instrumentation API（Java）实现各种功能，主要用于 APM、Profiler 工具</p></li><li><p>如果你想，在 javaagent 里调用功能更丰富的 JVMTI 也不是不行。</p></li></ol><blockquote><p>作者：京东保险，蒋信</p><p>来源：京东云开发者社区，转载请注明来源</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Sun, 31 Dec 2023 02:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/10388524</guid>
            <link>https://my.oschina.net/u/4090830/blog/10388524</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[致敬过去，迎接未来：DataCap 感恩有您的 2023，翘首期盼 2024 的精进与共创]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>亲爱的 <code>DataCap</code> 软件用户，开发者：</p><p>&nbsp;&nbsp;&nbsp;&nbsp;时光荏苒，<code>2023</code> 年即将成为过去，我们深感荣幸与感慨地站在这个时刻，对您们表达我们最深切的感谢。在过去的一年中，我们一直感受到了您们对我们软件的不离不弃和坚定支持，这是我们最宝贵的动力。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;<code>2023</code> 年是我们 <code>DataCap</code> 软件发展的关键一年。我们荣幸地宣布，预定支持的大部分功能在这一年里得以实现。然而，我们也要坦诚地承认，虽然这些功能在某些方面还存在不足和改进的空间，但正是在这个过程中，您们对我们软件的宽容与理解成为我们前行路上最强大的支持。无论是在社交媒体上的反馈，还是通过邮件和客服的沟通以及 <code>GitHub</code> 和 <code>Gitee</code> 反馈，您们的每一次建议和反馈都是我们前进的明灯，是我们改进的方向。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;在这个感恩的季节，我们要向每一位 <code>DataCap</code> 用户致以最衷心的感谢。是你们的热情使用和持续支持，让我们能够不断发展、完善软件。每一个新的用户，每一个老用户，都是我们成长历程中不可或缺的一部分。感谢您们的信任，让我们得以在软件开发的道路上越走越远。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;随着 <code>2023</code> 年即将谢幕，我们更加兴奋地展望着 <code>2024</code> 年。我们承诺，为了回馈您们的信任，我们将不遗余力地投入到软件的不断完善中。我们将不懈努力，以确保软件在稳定性、安全性和用户友好性方面取得更大的进步。我们也将持续关注用户反馈，不断优化用户体验，确保软件在您们手中能够发挥最大的价值。同时也希望您在使用软件中遇到的任何问题以及技术与我们沟通，我们将会以最大能力去解决并修复它。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;在此，我们要特别感谢那些默默为软件发展贡献的开发者们 (<strong>排名不分先后，开发者的列表为 github 中的 id</strong>，如果您有兴趣可以关注他 (她) 们)。<code>mlboy</code>、<code>why198852</code>、<code>javalover123</code>、<code>pan3793</code>、<code>GtoCm</code>、<code>Smilewh888</code>、<code>chenwenming-zj</code>、<code>Stacey1018</code>、<code>hometownglory</code>、<code>shuangzishuai</code> 等等，感谢你们的辛勤付出和无私奉献。正是有了你们的技术支持，软件才能不断创新、不断进步。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;感谢 <code>GitHub</code>，<code>Gitee</code> 对软件的托管和支持，在此向他们所有的工作人员致敬。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;感谢 <code>OpenTiny</code> 对我们的支持。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;最后，让我们向新老用户们表达最真挚的感激之情。是你们的陪伴，让我们在软件的道路上走得更加坚定。在新的一年里，我们期许能够为您们带来更多的惊喜和便捷。无论您是一位新用户，还是一位老用户，都请相信，您的支持是我们前行的最大动力。</p><p><code>2024</code> 年，愿我们继续携手，共同创造更加美好、更加智能的未来。</p><p>再次感谢您们的支持！</p><p>DataCap 软件团队 (Devlive 开源组织)</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 11:01:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273459/datacap-news</guid>
            <link>https://www.oschina.net/news/273459/datacap-news</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Farewell to Pika, Embracing the Arrival of PikiwiDB in 2024]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>今年 (2023 年)&nbsp;3&nbsp;月份于某接手项目时，OpenAtom&nbsp;基金会&nbsp;Pika&nbsp;项目（ <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenAtomFoundation%2Fpika" target="_blank">https://github.com/OpenAtomFoundation/pika</a> ）对接人告诉我，OpenAtom&nbsp;基金会旗下的多个项目都面临了一个共同问题：项目名称被第三方注册为商标。出于合规要求，余三月份重点工作之一就是给项目重新申请一个全新的名称。</p><p>在与&nbsp;Pika&nbsp;老团队的成员进行商讨后，结合了&nbsp;「兔子哥」&nbsp;和&nbsp;「YYJ」&nbsp;的建议，我决定将其命名为&nbsp;"Pi-kiwi-DB"：</p><ol><li>"Pi"&nbsp;念派 2.&nbsp;"Pik"&nbsp;恰好保留了&nbsp;"Pika"&nbsp;的前三个字母 3.&nbsp;"kiwi"&nbsp;音同&nbsp;"KV"，寓意几维鸟</li></ol><p><img src="https://oscimg.oschina.net/oscnet/up-d73a02aed0d881c1ab72b620a395aac8a55.png" alt="" referrerpolicy="no-referrer"></p><p>Kiwi&nbsp;鸟孵化的鸟蛋占据身体容量的一半，象征着计算机大部分数据存储在磁盘上，代表着&nbsp;「极大容量」；Kiwi&nbsp;鸟羽翼退化，身体小巧，双腿强壮占体重&nbsp;1/3，跑速快如人类，象征着&nbsp;「极致性能」。所以，这一命名的选择充分考虑了项目的发展方向和原有名称的延续。</p><p>在&nbsp;2023&nbsp;年&nbsp;7&nbsp;月底，PikiwiDB（前身为&nbsp;Pika）发布了自&nbsp;2021&nbsp;年加入&nbsp;OpenAtom&nbsp;基金会以来的首个生产可用版本&nbsp;v3.5.0。该版本通过采用&nbsp;C++17&nbsp;对整个代码进行了重构，显著提升了项目的代码质量。全新的全量同步机制取代了备受诟病的&nbsp;Rsync&nbsp;方案，该方案在过去的&nbsp;8&nbsp;年里一直在使用。此外，升级了&nbsp;RocksDB&nbsp;版本、引入新的集群方案、增强了可观测性、跨平台支持&nbsp;Mac&nbsp;等方面都取得了显著的改进。</p><p>在接下来的半年中，PikiwiDB&nbsp;陆续发布了&nbsp;v3.5.1&nbsp;和&nbsp;v3.5.2&nbsp;两个版本，并计划在不久的将来发布&nbsp;v3.5.3。这些版本的更新实现了数据的冷热分离、命令的快慢分离、Redis&nbsp;事务、云原生&nbsp;K8s&nbsp;Operator&nbsp;以及&nbsp;Go&nbsp;测试集的集成。这一系列的改进将读性能提升到了微秒级别，单机读取&nbsp;QPS&nbsp;翻倍，可达&nbsp;60&nbsp;万&nbsp;/s。在稳定性和性能方面都取得了显著的提升。社区活跃度方面，贡献者数量增加了近&nbsp;3&nbsp;倍，达到&nbsp;121&nbsp;人（包括&nbsp;PikiwiDB&nbsp;和&nbsp;Pika），同时&nbsp;PR&nbsp;和&nbsp;Issue&nbsp;的总量也翻番，许多老用户纷纷回归。</p><p>到&nbsp;12&nbsp;月，OpenAtom&nbsp;基金会告知：Pika&nbsp;新名称&nbsp;PikiwiDB&nbsp;已经在政府相关部门获得批准，商标也已审批下来。这标志着整个过程的顺利完成。</p><p>回顾&nbsp;2023&nbsp;年&nbsp;12&nbsp;月份，社会第三方机构对&nbsp;PikiwiDB&nbsp;(原&nbsp;Pika)&nbsp;的评价：</p><p>这一过程展示了&nbsp;PikiwiDB&nbsp;对项目的持续改进，不仅在技术上取得了显著的进步，而且在品牌命名和合规性方面也取得了圆满成功。</p><p><img src="https://oscimg.oschina.net/oscnet/up-dddcab84de08ed2d058294f472911e37f90.png" alt="" referrerpolicy="no-referrer"></p><ul><li>12&nbsp;月&nbsp;08&nbsp;日，Pika&nbsp;社区和&nbsp;dubbogo&nbsp;社区双双荣获&nbsp;Oschina&nbsp;「2023 年度优秀开源技术团队」</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-3d425b37dbb7dcd23e4cdf3d273695db0ad.png" alt="" referrerpolicy="no-referrer"></p><blockquote></blockquote><ul><li>12&nbsp;月&nbsp;13&nbsp;日，Pika[已更名&nbsp;PikiwiDB]&nbsp;被第三方独立机构&nbsp;艾瑞咨询研究院&nbsp;列为&nbsp;2023&nbsp;年&nbsp;&nbsp;「中国基础软件开源产业主要参与者」【DUBBO&nbsp;亦列其中】</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-846de83809b0b9fbab55bee81a04e84f452.png" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-89332cd585fca59db07e2cedcb162bbf7b3.png" alt="" referrerpolicy="no-referrer"></p><ul><li>12&nbsp;月&nbsp;29&nbsp;日，PikiwiDB(Pika)&nbsp;第一次以&nbsp;PikiwiDB&nbsp;的身份亮相&nbsp;Oschina&nbsp;2023&nbsp;年《中国开源开发者报告》</li></ul><p>展望&nbsp;2024&nbsp;年，PikiwiDB&nbsp;将重点发力于&nbsp;<a href="">云原生方向</a>，继续在&nbsp;「极大容量、极高性能、极致弹性」&nbsp;方向上进行探索。诚邀&nbsp;PikiwiDB（原&nbsp;Pika）社区的用户积极参与共建，共同推动&nbsp;PikiwiDB（原&nbsp;Pika）在云计算时代的发展。</p><p><img src="https://oscimg.oschina.net/oscnet/up-708fbac71201d8d036a6f6efa9fb2e1ed79.png" alt="" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 04:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/dubbogo/blog/10475258</guid>
            <link>https://my.oschina.net/dubbogo/blog/10475258</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Vue 2 生命周期即将结束]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在新的一年即将到来之际，尤雨溪于日前<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.vuejs.org%2Fposts%2Fvue-2-eol" target="_blank">发文</a>提醒 Vue 社区称，Vue 2 将于 2023 年 12 月 31 日达到生命周期结束 (EOL)；并呼吁还在使用 Vue 2 的开发团队考虑迁移至最新的&nbsp;Vue 3 版本。</p><p>Vue 2.0 于 2016 年发布，距今已有 7 年多的时间。尤雨溪表示，2.0 版本是 Vue 成为主流框架历程中的一个重要里程碑。「然而，并行地主动维护两个主要版本对我们来说是不可持续的。随着 Vue 3 及其生态系统的成熟，团队是时候继续前进并将精力集中在最新的主要版本上。」</p><p><img height="303" src="https://oscimg.oschina.net/oscnet/up-624c109953c0efc8aa6cc403c47e6b21d3b.png" width="700" referrerpolicy="no-referrer"></p><p>随着&nbsp;Vue 2.0 版本 EOL 日期的临近，他建议&nbsp;Vue 社区应该为 Vue 2 的弃用做好准备。12 月 31 日，Vue 团队将在 npm 上将以下软件包标记为已弃用：</p><ul><li>Vue 2 核心的所有主要和次要版本</li><li>专门支持 Vue 2 的 vue-router 版本（3.x 及更低版本）</li><li>专门支持 Vue 2 的 vuex&nbsp;版本（3.x 及更低版本）</li></ul><p>2023 年 12 月 31 日之后，Vue 2 将不再接收新功能、更新或修复，但仍可在所有现有分发渠道（CDN、包管理器、GitHub 等）上使用。<span style="color:#374151">换句话说，用户的应用程序可以继续工作，但会从包管理器中收到弃用警告，提醒其 Vue 2 不再是受支持的版本。</span></p><p><span style="color:#374151">Vue 3 自 2022 年 2 月 7 日以来就一直是 Vue 的默认版本。尤雨溪表示，迁移后的用户将可以享受：</span></p><p>&nbsp;</p><ul style="margin-left:0; margin-right:0"><li>更小的包尺寸和更快的渲染带来更好的性能。</li><li>增强的 TypeScript 支持，更轻松地进行大规模应用程序开发。</li><li>更高效的基于代理的反应系统。</li><li>新的内置组件，如 Fragment、Teleport 和 Suspense。</li><li>改进了构建工具支持和 Vue Devtools 体验，等等。</li></ul><p>对于暂时无法迁移或者步向前一的用户，他也提供了一些其他建议：<span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="color:var(--tw-prose-headings)"><span><span><span><span><span><span><span><span><span><span><span><span><span><span>更新到 Vue 2 的最终版本、或购买 Vue 2 的扩展支持，以及和用户分享相关的</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>&nbsp;Vue 2 EOL 计划</span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="color:var(--tw-prose-headings)"><span><span><span><span><span><span><span><span><span><span><span><span><span><span>。于 12 月 24 日发布的&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="background-color:#ffffff; color:#333333">2.7.16 是 Vue 2 的最终版本，包括了对 2.7 功能的一些最终修复，并改进了与 Vue 3 的类型对齐。</span></p><p><span style="background-color:#ffffff; color:#333333">「Vue 2 的结束仅标志着一个新的开始——2024 年对 Vue 来说将是激动人心的一年！」</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 03:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273400/vue-2-eol</guid>
            <link>https://www.oschina.net/news/273400/vue-2-eol</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[GitHub Copilot Chat 普遍可用]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>今年早些时候，GitHub 推出了 Copilot Chat；一个类似于 ChatGPT 的以编程为中心的聊天机器人，适用于订阅 Copilot for Business 的组织。前不久，Copilot Chat 的测试版也面向 Copilot 个人用户推出，每月收费 10 美元。</p><p>时至今日，GitHub 发文<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.blog%2F2023-12-29-github-copilot-chat-now-generally-available-for-organizations-and-individuals%2F" target="_blank">宣布</a>，GitHub Copilot Chat 现已普遍适用于 Visual Studio Code 和 Visual Studio，经过验证的教师、学生和流行开源项目的维护人员也可以免费使用。</p><p><img alt="" height="266" src="https://oscimg.oschina.net/oscnet/up-a3b881ee5bc1674182cfdc6412c6fab4398.webp" width="500" referrerpolicy="no-referrer"></p><p>「所有 GitHub Copilot 个人用户现在都可使用 GitHub Copilot Chat 功能。企业和组织管理员可通过为其用户启用 Copilot Chat 设置，授予开发团队访问 Copilot Chat 的权限。如果你已经在测试版中使用了 Copilot Chat，或者已经为你的开发团队提供了访问权限，则无需进行其他操作。」</p><p>GitHub Copilot Chat 由 GPT-4 提供支持，并专门针对开发场景进行了微调。开发人员可以用自然语言提示 Copilot Chat，以获得实时指导，例如要求 Copilot Chat 解释概念、检测漏洞或编写单元测试。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 03:05:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273397/github-copilot-chat-now-generally-available</guid>
            <link>https://www.oschina.net/news/273397/github-copilot-chat-now-generally-available</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Infinigen —— 无限高质量 3D 数据生成器]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Infinigen 是无限高质量 3D 数据生成器，使用程序生成的无限逼真世界。这些数据 100% 通过程序化生成，不需要外部资产，也不依赖 AI，并且是免费开源的，生成质量非常高，据称可以达到以假乱真的地步，甚至是花瓣上的皱纹都可定制。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-89f0a56c9b3a6cc8ff8cedb7501aecff0e0.png" referrerpolicy="no-referrer"></p><p>Infinigen 由普林斯顿视觉和学习实验室开发：</p><ul><li>基于 Blender 编写</li><li>每个小细节都是随机的和可定制的，甚至是花瓣上的皱纹</li><li>自然界中多样的物体和场景：植物、动物、地形；火、云、雨和雪</li><li>Groundtruth 自动标注：光流、3D 场景流、深度、表面法线、全景分割、遮挡边界</li></ul><p>其主要特性和功能包括：</p><p>1. 程序化：Infinigen 是一个程序生成器，它完全使用随机的数学规则来创建所有的形状和材料，从宏观结构到微观细节。Infinigen 可以创建无限的变化。用户可以通过覆盖随机化的默认参数来完全控制资产的生成。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-f9d8159ed3837b185ace5578b1e2d6b0973.png" referrerpolicy="no-referrer"></p><p>2. 多样化：Infinigen 为自然世界中的多样化对象和场景提供生成器，包括植物、动物、地形，以及火、云、雨、雪等自然现象。当前对自然的关注是由于观察到哺乳动物的视觉在自然世界中进化。然而，预计 Infinigen 将随着时间的推移扩展到覆盖建筑环境和人造物体。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e4874348efa3dd1ae71f5d170027ed35b43.png" referrerpolicy="no-referrer"></p><p>3. 真实的几何形状：Infinigen 针对计算机视觉研究进行了优化，特别是 3D 视觉。Infinigen 不使用 bump/normal-maps、全透明度或其他伪造几何细节的技术。Infinigen 的所有细微的几何细节都是真实的，确保了精确的 3D 地面真实性。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-2579dc0164572ce5aa1add312a321e16803.png" referrerpolicy="no-referrer"></p><p>4. 自动注释：Infinigen 可以自动生成各种计算机视觉任务的高质量注释，包括光流、3D 场景流、深度、表面法线、全景分割、遮挡边界。因为用户可以完全访问渲染过程，所以注释很容易定制。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-cbb0ac43739fd281ebc6ccdd23cead6aff3.png" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 02:40:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/infinigen</guid>
            <link>https://www.oschina.net/p/infinigen</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 嵌入式软件平台框架 VSF]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-vsf----versaloon-software-framework" class="anchor" href="https://gitee.com/vsfteam/vsf#vsf----versaloon-software-framework"></a>VSF -- Versaloon Software Framework</h1><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Fblob%2Fmaster%2FLICENSE"><img src="https://img.shields.io/github/license/vsfteam/vsf.svg" alt="GitHub" referrerpolicy="no-referrer"></a></p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Factions%2Fworkflows%2Fwindows-build.yml"><img src="https://github.com/vsfteam/vsf/actions/workflows/windows-build.yml/badge.svg" alt="windows-build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Factions%2Fworkflows%2Fcmake-native-build.yml"><img src="https://github.com/vsfteam/vsf/actions/workflows/cmake-native-build.yml/badge.svg" alt="cmake-native-build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsfteam%2Fvsf%2Factions%2Fworkflows%2Fcmake-arm-cross-build.yml"><img src="https://github.com/vsfteam/vsf/actions/workflows/cmake-arm-cross-build.yml/badge.svg" alt="cmake-arm-cross-build" referrerpolicy="no-referrer"></a></p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsf-linux%2Fvsf.linux%2Factions%2Fworkflows%2Fwindows-build.yml"><img src="https://github.com/vsf-linux/vsf.linux/actions/workflows/windows-build.yml/badge.svg?branch=vsf-sync" alt="vsf.linux windows build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsf-linux%2Fvsf.linux%2Factions%2Fworkflows%2Fcmake-arm-cross-build.yml"><img src="https://github.com/vsf-linux/vsf.linux/actions/workflows/cmake-arm-cross-build.yml/badge.svg?branch=vsf-sync" alt="cmake-arm-cross-build" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fvsf-linux%2Fvsf.linux%2Factions%2Fworkflows%2Fcmake-native-build.yml"><img src="https://github.com/vsf-linux/vsf.linux/actions/workflows/cmake-native-build.yml/badge.svg?branch=vsf-sync" alt="cmake-native-build" referrerpolicy="no-referrer"></a></p><p><a href="https://gitee.com/vsfteam/vsf/blob/master/README.md">English</a> |</p><p>VSF 全称是 Versaloon Software Framework，是一个基于 Apache2.0 协议的开源嵌入式软件平台框架。包含了从底层硬件的 hal 驱动、抢占式多任务内核、各种服务和组件。全部代码使用 C 语言，以及面向对象的方式实现。</p><h2><a id="user-content-整体框架" class="anchor" href="https://gitee.com/vsfteam/vsf#%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6"></a>整体框架</h2><h2><a id="user-content-目录" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%9B%AE%E5%BD%95"></a>目录</h2><table><thead><tr><th>目录名</th><th>描述</th></tr></thead><tbody><tr><td>document</td><td>文档</td></tr><tr><td>doxygen</td><td>doxygen 配置</td></tr><tr><td>example</td><td>示例代码</td></tr><tr><td>hardware</td><td>VSF 开发板硬件资料</td></tr><tr><td>patch</td><td>一些补丁（第三方库补丁等等）</td></tr><tr><td>script</td><td>一些工具脚本</td></tr><tr><td> cmake</td><td>cmake 工具脚本</td></tr><tr><td>source</td><td>VSF 源代码</td></tr><tr><td> component</td><td>组件（文件系统、协议栈、UI、外部芯片驱动）</td></tr><tr><td> hal</td><td>硬件抽象层（芯片 arch 支持、芯片驱动）</td></tr><tr><td> kernel</td><td>内核</td></tr><tr><td> osa_service</td><td>依赖内核的软件服务组件</td></tr><tr><td> service</td><td>软件服务组件</td></tr><tr><td> shell</td><td>「皮肤」</td></tr><tr><td> utilities</td><td>基础软件工具（一些预处理功能、编译器支持、列表等等）</td></tr></tbody></table><h2><a id="user-content-内核" class="anchor" href="https://gitee.com/vsfteam/vsf#%E5%86%85%E6%A0%B8"></a>内核</h2><p>基于事件驱动的抢占式多任务内核，支持 51、8bit MCU、32/64 bit arm、riscv、x86 等等各种构架的芯片。</p><ul><li>事件驱动，有事件运行，没事件休眠</li><li>抢占模式下，任务切换由硬件实现，任务优先级就是硬件 swi（software interrupt）的优先级</li><li>不同优先级抢占，同一优先级协作</li><li>可以运行在其他系统或者 RTOS 中，也可以运行在一个或者几个 SWI 中断中（和其他 RTOS 并存）。</li><li>多种任务形式
<ul><li>事件处理任务 -- 最小资源占用，最简配置下占用 20 字节 ram，常用配置下占用 40 字节 ram</li><li>pt 任务 -- 接近独立堆栈任务开发方式的共享堆栈任务</li><li>独立堆栈任务 -- 依赖 libc 中的 setjmp 库</li><li>fsm 状态机任务</li><li>「皮肤」中的其他任务封装形式，比如 pthread</li></ul></li><li>信号量、互斥量、触发器、队列等等常用 IPC 工具</li></ul><h2><a id="user-content-组件" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%BB%84%E4%BB%B6"></a>组件</h2><ul><li>合理的框架设计，软件高度可以复用</li><li>尽可能提供申明式的开发方式</li><li>标准化接口，第三方软件一次性移植，全平台适配</li><li>软件组件/框架
<ul><li>distbus -- 分布式总线框架</li><li>fifo</li><li>heap</li><li>json</li><li>pool -- 内存池</li><li>stream -- 流接口</li><li>trace</li></ul></li><li>组件
<ul><li>fs -- 文件系统，支持 VFS（可使用第三方的文件系统）</li><li>input -- 输入系统</li><li>mal -- 块设备</li><li>scsi -- SCSI 设备</li><li>tcpip -- TCPIP 协议栈以及 netdrv 网络设备（可使用第三方的 TCPIP 协议栈）</li><li>ui -- UI 以及显示设备（可使用第三方的 GUI）</li><li>usb -- USB 主从机协议栈</li><li>bt -- 蓝牙协议栈（使用第三方的 btstack）</li></ul></li></ul><h2><a id="user-content-硬件抽象层" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%A1%AC%E4%BB%B6%E6%8A%BD%E8%B1%A1%E5%B1%82"></a>硬件抽象层</h2><ul><li>标准 hal 接口，统一 API -- 比如：vsf_spi_init 可以用于所有 VSF 中支持的 SPI，包括芯片自带 SPI、GPIO 模拟的 SPI、通过 USB 外扩的 SPI，通过分布式总线访问的远端 SPI</li><li>简化开发的 IP 核驱动 -- 移植仅需要实现时钟、复位、中断等等 IP 核心之外的功能</li><li>各种接口封装模板</li><li>接口
<ul><li>PM</li><li>GPIO</li><li>SPI</li><li>I2C</li><li>PWM</li><li>ADC</li><li>SWI</li><li>USART</li><li>FLASH</li><li>USB</li><li>ethernet</li></ul></li></ul><h2><a id="user-content-皮肤" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%9A%AE%E8%82%A4"></a>「皮肤」</h2><p>「皮肤」可以把 VSF「伪装」成其他系统，使得可以直接使用基于其他系统的应用代码。</p><ul><li>SDL -- 可以直接使用一些基于 SDL 的应用层代码</li><li>linux -- 可以直接使用一些基于 linux 的应用层代码
<ul><li>posix</li><li>devfs</li><li>socket</li><li>console</li><li>一些 lib 库的实现
<ul><li>libusb</li><li>libgen</li></ul></li></ul></li></ul><h2><a id="user-content-第三方" class="anchor" href="https://gitee.com/vsfteam/vsf#%E7%AC%AC%E4%B8%89%E6%96%B9"></a>第三方</h2><table><thead><tr><th>名字</th><th>路径</th><th>许可</th><th>链接</th></tr></thead><tbody><tr><td>btstack</td><td>source/component/3rd-party/btstack/raw</td><td>Other</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbluekitchen%2Fbtstack">https://github.com/bluekitchen/btstack</a></td></tr><tr><td>coremark</td><td>source/component/3rd-party/coremark/raw</td><td>Apache</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Feembc%2Fcoremark">https://github.com/eembc/coremark</a></td></tr><tr><td>freetype</td><td>source/component/3rd-party/freetype/raw</td><td>FreeType</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Ffreetype.org%2F">https://freetype.org/</a></td></tr><tr><td>zlib</td><td>source/component/3rd-party/zlib/raw</td><td>zlib</td><td><a href="https://gitee.com/link?target=http%3A%2F%2Fzlib.net%2F">http://zlib.net/</a></td></tr><tr><td>nuklear</td><td>source/component/3rd-party/nuklear/raw</td><td>MTI</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FImmediate-Mode-UI%2FNuklear">https://github.com/Immediate-Mode-UI/Nuklear</a></td></tr><tr><td>nnom</td><td>source/component/3rd-party/nnom/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fmajianjia%2Fnnom">https://github.com/majianjia/nnom</a></td></tr><tr><td>lua</td><td>source/component/3rd-party/lua/raw</td><td>MIT</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.lua.org%2F">https://www.lua.org/</a></td></tr><tr><td>lwip</td><td>source/component/3rd-party/lwip/raw</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fsavannah.nongnu.org%2Fprojects%2Flwip%2F">https://savannah.nongnu.org/projects/lwip/</a></td></tr><tr><td>libpng</td><td>source/component/3rd-party/libpng/raw</td><td>PNG2</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flibpng.sf.net">https://libpng.sf.net</a></td></tr><tr><td>libjpeg-turbo</td><td>source/component/3rd-party/libjpeg-turbo/raw</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flibjpeg-turbo.org%2F">https://libjpeg-turbo.org/</a></td></tr><tr><td>SDL_ttf</td><td>source/shell/media/sdl2/3rd-party/SDL_ttf</td><td>zlib</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fhg.libsdl.org%2FSDL_ttf%2F">https://hg.libsdl.org/SDL_ttf/</a></td></tr><tr><td>SDL_image</td><td>source/shell/media/sdl2/3rd-party/SDL_image</td><td>zlib</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fhg.libsdl.org%2FSDL_image%2F">https://hg.libsdl.org/SDL_image/</a></td></tr><tr><td>lvgl</td><td>source/component/3rd-party/lvgl/raw/lvgl</td><td>MIT</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flvgl.io%2F">https://lvgl.io/</a></td></tr><tr><td>lv_lib_freetype</td><td>source/component/3rd-party/lvgl/extension/lv_lib_freetype/raw</td><td>MIT</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Flvgl.io%2F">https://lvgl.io/</a></td></tr><tr><td>CMSIS</td><td>source/utilities/compiler/arm/3rd-party/CMSIS</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FARM-software%2FCMSIS_5">https://github.com/ARM-software/CMSIS_5</a></td></tr><tr><td>evm</td><td>source/component/3rd-party/evm/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fscriptiot%2Fevm">https://github.com/scriptiot/evm</a></td></tr><tr><td>LingLongGUI</td><td>source/component/3rd-party/LingLongGUI/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/gzbkey/LingLongGUI">https://gitee.com/gzbkey/LingLongGUI</a></td></tr><tr><td>PLOOC</td><td>source/utilities/3rd-party/PLOOC/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FGorgonMeducer%2FPLOOC">https://github.com/GorgonMeducer/PLOOC</a></td></tr><tr><td>mbedtls</td><td>source/component/3rd-party/mbedtls/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Ftls.mbed.org%2F">https://tls.mbed.org/</a></td></tr><tr><td>GuiLite</td><td>source/component/3rd-party/GuiLite/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fidea4good%2FGuiLite">https://github.com/idea4good/GuiLite</a></td></tr><tr><td>Segger_RTT</td><td>source/component/3rd-party/segger/raw/RTT</td><td>segger</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwiki.segger.com%2FRTT">https://wiki.segger.com/RTT</a></td></tr><tr><td>Segger_SystemView</td><td>source/component/3rd-party/segger/raw/SystemView</td><td>segger</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwiki.segger.com%2FSystemView">https://wiki.segger.com/SystemView</a></td></tr><tr><td>nuconsole</td><td>source/component/3rd-party/nuconsole/raw</td><td>nuvoton</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.nuvoton.com.cn%2F">https://www.nuvoton.com.cn/</a></td></tr><tr><td>AIC8800M_SDK</td><td>source/hal/driver/AIC/AIC8800/vendor</td><td>aic</td><td><a href="https://gitee.com/link?target=http%3A%2F%2Fwww.aicsemi.com%2F">http://www.aicsemi.com/</a></td></tr><tr><td>awtk</td><td></td><td>LGPL 2.1</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.zlg.cn%2Findex%2Fpub%2Fawtk.html">https://www.zlg.cn/index/pub/awtk.html</a></td></tr><tr><td>littlefs</td><td>source/component/3rd-party/littlefs/raw</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Flittlefs-project%2Flittlefs">https://github.com/littlefs-project/littlefs</a></td></tr><tr><td>getopt_long</td><td>source/shell/sys/linux/lib/3rd-party/getopt</td><td>OpenBSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fopenbsd%2Fsrc">https://github.com/openbsd/src</a></td></tr><tr><td>regex</td><td>source/shell/sys/linux/lib/3rd-party/regex</td><td>OpenBSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fopenbsd%2Fsrc">https://github.com/openbsd/src</a></td></tr><tr><td>fnmatch</td><td>source/shell/sys/linux/lib/3rd-party/fnmatch</td><td>BSD</td><td><a href="https://gitee.com/link?target=http%3A%2F%2Fwww.jbox.dk%2Fsanos%2Fsource%2Flib%2Ffnmatch.c.html">http://www.jbox.dk/sanos/source/lib/fnmatch.c.html</a></td></tr><tr><td>glob</td><td>source/shell/sys/linux/lib/3rd-party/glob</td><td>BSD</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcloudius-systems%2Fmusl">https://github.com/cloudius-systems/musl</a></td></tr><tr><td>setjmp</td><td>source/hal/arch/x86/win</td><td>BSD</td><td></td></tr><tr><td>libtuv</td><td>source/shell/sys/linux/lib/3rd-party/libtuv/raw</td><td>Apache 2.0</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FSamsung%2Flibtuv">https://github.com/Samsung/libtuv</a></td></tr></tbody></table><h2><a id="user-content-文档" class="anchor" href="https://gitee.com/vsfteam/vsf#%E6%96%87%E6%A1%A3"></a><a href="https://gitee.com/vsfteam/vsf/blob/master/document/README_zh.md">文档</a></h2>]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 02:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/vsfteam/vsf</guid>
            <link>https://gitee.com/vsfteam/vsf</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 混合专家模型 (MoE) 详解]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 编辑器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;" data-mpa-powered-by="yiban.io"><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">随着 Mixtral 8x7B (announcement, model card) 的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs) 的 Transformer 模型在开源人工智能社区引起了广泛关注。在本篇博文中，我们将深入探讨 MoEs 的核心组件、训练方法，以及在推理过程中需要考量的各种因素。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们开始吧！</p><span id="OSC_h2_1"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">简短总结</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoEs):</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      与稠密模型相比， 
     <strong style="color: black;">预训练速度更快</strong></section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      与具有相同参数数量的模型相比，具有更快的 
     <strong style="color: black;">推理速度</strong></section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      需要 
     <strong style="color: black;">大量显存</strong>，因为所有专家系统都需要加载到内存中 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      在 
     <strong style="color: black;">微调方面存在诸多挑战</strong>，但，近期的研究，表明，对混合专家模型进行 
     <strong style="color: black;">指令调优具有很大的潜力</strong>。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们开始吧！</p><span id="OSC_h2_2"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">什么是混合专家模型？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">那么，究竟什么是一个混合专家模型 (MoE) 呢？作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">稀疏 MoE 层</strong>: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干「专家」(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">门控网络或路由</strong>: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，「More」这个令牌可能被发送到第二个专家，而「Parameters」这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。 
    </section></li></ul><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006177" data-ratio="0.7703703703703704" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/8f46f9ec-4ee7-42d6-8b1d-68962ccb7e8b.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Switch Transformers paper 论文中的 MoE layer 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">总结来说，在混合专家模型 (MoE) 中，我们将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">尽管混合专家模型 (MoE) 提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">训练挑战</strong>: 虽然 MoE 能够实现更高效的计算预训练，但它们在微调阶段往往面临泛化能力不足的问题，长期以来易于引发过拟合现象。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">推理挑战</strong>: MoE 模型虽然可能拥有大量参数，但在推理过程中只使用其中的一部分，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到内存中，因此对内存的需求非常高。以 Mixtral 8x7B 这样的 MoE 为例，需要足够的 VRAM 来容纳一个 47B 参数的稠密模型。之所以是 47B 而不是 8 x 7B = 56B，是因为在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。此外，假设每个令牌只使用两个专家，那么推理速度 (以 FLOPs 计算) 类似于使用 12B 模型 (而不是 14B 模型)，因为虽然它进行了 2x7B 的矩阵乘法计算，但某些层是共享的。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">了解了 MoE 的基本概念后，让我们进一步探索推动这类模型发展的研究。</p><span id="OSC_h2_3"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">混合专家模型简史</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoE) 的理念起源于 1991 年的论文 Adaptive Mixture of Local Experts。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为「专家」) 处理训练样本的不同子集，专注于输入空间的特定区域。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络发挥作用的地方，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在 2010 至 2015 年间，两个独立的研究领域为混合专家模型 (MoE) 的后续发展做出了显著贡献:</p><ol data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">组件专家</strong>: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，Eigen、Ranzato 和 Ilya 的研究，探索了将 MoE 作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">条件计算</strong>: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活或停用网络组件的方法。 
    </section></li></ol><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这些研究的融合促进了在自然语言处理 (NLP) 领域对混合专家模型的探索。特别是在 2017 年，Shazeer 等人 (团队包括 Geoffrey Hinton 和 Jeff Dean，后者有时被戏称为 「谷歌的 Chuck Norris」) 将这一概念应用于 137B 的 LSTM (当时被广泛应用于 NLP 的架构，由 Schmidhuber 提出)。通过引入稀疏性，这项工作在保持极高规模的同时实现了快速的推理速度。这项工作主要集中在翻译领域，但面临着如高通信成本和训练不稳定性等多种挑战。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006175" data-ratio="0.48240635641316687" data-type="png" data-w="881" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/48db56ce-0292-4d84-a174-c5c7a03d5e8b.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Outrageously Large Neural Network 论文中的 MoE layer 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">混合专家模型 (MoE) 的引入使得训练具有数千亿甚至万亿参数的模型成为可能，如开源的 1.6 万亿参数的 Switch Transformers 等。这种技术不仅在自然语言处理 (NLP) 领域得到了广泛应用，也开始在计算机视觉领域进行探索。然而，本篇博客文章将主要聚焦于自然语言处理领域的应用和探讨。</p><span id="OSC_h2_4"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">什么是稀疏性?</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">稀疏性的概念采用了条件计算的思想。在传统的稠密模型中，所有的参数都会对所有输入数据进行处理。相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们深入分析 Shazeer 对混合专家模型 (MoE) 在翻译应用中的贡献。条件计算的概念 (即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这种稀疏性设置确实带来了一些挑战。例如，在混合专家模型 (MoE) 中，尽管较大的批量大小通常有利于提高性能，但当数据通过激活的专家时，实际的批量大小可能会减少。比如，假设我们的输入批量包含 10 个令牌， <strong style="color: black;">可能会有五个令牌被路由到同一个专家，而剩下的五个令牌分别被路由到不同的专家。这导致了批量大小的不均匀分配和资源利用效率不高的问题</strong>。在接下来的部分中，将会讨论让 MoE 高效运行的其他挑战以及相应的解决方案。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">那我们应该如何解决这个问题呢？一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E):</p><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="y = \sum_{i=1}^{n} G(x)_i E_i(x)
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><svg
                xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewBox="0 -1562.5 8246.1 2808.5" aria-hidden="true" style="-webkit-overflow-scrolling: touch;vertical-align: -2.819ex;width: 18.656ex;height: 6.354ex;max-width: 300% !important;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(1823.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(509.9, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mi" transform="translate(3434.2, 0)"><path data-c="47" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(4220.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4609.2, 0)"><path data-c="78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="msub" transform="translate(5181.2, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(389, -150) scale(0.707)"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msub" transform="translate(5864.2, 0)"><g data-mml-node="mi"><path data-c="45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(738, -150) scale(0.707)"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(6896.1, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7285.1, 0)"><path data-c="78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(7857.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在这种设置下，虽然所有专家都会对所有输入进行运算，但通过门控网络的输出进行加权乘法操作。但是，如果 G (门控网络的输出) 为 0 会发生什么呢？如果是这种情况，就没有必要计算相应的专家操作，因此我们可以节省计算资源。那么一个典型的门控函数是什么呢？一个典型的门控函数通常是一个带有 softmax 函数的简单的网络。这个网络将学习将输入发送给哪个专家。</p><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="G_\sigma(x) = \text{Softmax}(x \cdot W_g)
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -0.667ex;width: 24.749ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/addcdb77-1331-4c14-845f-06df5414abd2.svg" data-type="svg+xml" data-imgfileid="100006169"></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Shazeer 等人的工作还探索了其他的门控机制，其中包括带噪声的 TopK 门控 (Noisy Top-K Gating)。这种门控方法引入了一些可调整的噪声，然后保留前 k 个值。具体来说:</p><ol data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      添加一些噪声 
    </section></li></ol><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="H(x)_i = (x \cdot W_{\text{g}})_i + \text{StandardNormal()} \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i)
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -0.669ex;width: 60.565ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/26f83d98-1595-404e-9d4a-58c616d102c1.svg" data-type="svg+xml" data-imgfileid="100006173"></section></span><ol start="2" data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      选择保留前 K 个值 
    </section></li></ol><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="\text{KeepTopK}(v, k)_i = \begin{cases}
v_i &amp; \text{if } v_i \text{ is in the top } k \text{ elements of } v, \\
-\infty &amp; \text{otherwise.}
\end{cases}
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -2.148ex;width: 58.794ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/4505b4a9-5c66-45c5-9f72-1b2377d74dca.svg" data-type="svg+xml" data-imgfileid="100006171"></section></span><ol start="3" data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      应用 Softmax 函数 
    </section></li></ol><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k))
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -0.566ex;width: 37.6ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/70f4a245-7a7f-483c-b524-8795deb3ae04.svg" data-type="svg+xml" data-imgfileid="100006170"></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这种稀疏性引入了一些有趣的特性。通过使用较低的 k 值 (例如 1 或 2)，我们可以比激活多个专家时更快地进行训练和推理。为什么不仅选择最顶尖的专家呢？最初的假设是，需要将输入路由到不止一个专家，以便门控学会如何进行有效的路由选择，因此至少需要选择两个专家。Switch Transformers 就这点进行了更多的研究。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">我们为什么要添加噪声呢？这是为了专家间的负载均衡！</p><span id="OSC_h2_5"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">混合专家模型中令牌的负载均衡</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">正如之前讨论的，如果所有的令牌都被发送到只有少数几个受欢迎的专家，那么训练效率将会降低。在通常的混合专家模型 (MoE) 训练中，门控网络往往倾向于主要激活相同的几个专家。这种情况可能会自我加强，因为受欢迎的专家训练得更快，因此它们更容易被选择。为了缓解这个问题，引入了一个 <strong style="color: black;">辅助损失</strong>，旨在鼓励给予所有专家相同的重要性。这个损失确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。接下来的部分还将探讨专家容量的概念，它引入了一个关于专家可以处理多少令牌的阈值。在 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">transformers</code> 库中，可以通过 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">aux_loss</code> 参数来控制辅助损失。</p><span id="OSC_h2_6"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">MoEs and Transformers</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Transformer 类模型明确表明，增加参数数量可以提高性能，因此谷歌使用 GShard 尝试将 Transformer 模型的参数量扩展到超过 6000 亿并不令人惊讶。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">GShard 将在编码器和解码器中的每个前馈网络 (FFN) 层中的替换为使用 Top-2 门控的混合专家模型 (MoE) 层。下图展示了编码器部分的结构。这种架构对于大规模计算非常有效: 当扩展到多个设备时，MoE 层在不同设备间共享，而其他所有层则在每个设备上覆制。我们将在 「让 MoE 起飞」部分对这一点进行更详细的讨论。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006178" data-ratio="0.6046296296296296" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/8246ce4e-437b-4539-bf15-17cea720b24b.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     GShard 论文中的 MoE Transformer Encoder 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">为了保持负载平衡和训练效率，GShard 的作者除了引入了上一节中讨论的类似辅助损失外，还引入了一些关键变化:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">随机路由</strong>: 在 Top-2 设置中，我们始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">专家容量</strong>: 我们可以设定一个阈值，定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。专家容量是 MoE 中最重要的概念之一。为什么需要专家容量呢？因为所有张量的形状在编译时是静态确定的，我们无法提前知道多少令牌会分配给每个专家，因此需要一个固定的容量因子。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">GShard 的工作对适用于 MoE 的并行计算模式也做出了重要贡献，但这些内容的讨论超出了这篇博客的范围。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);"><strong style="color: black;">注意</strong>: 在推理过程中，只有部分专家被激活。同时，有些计算过程是共享的，例如自注意力 (self-attention) 机制，它适用于所有令牌。这就解释了为什么我们可以使用相当于 12B 稠密模型的计算资源来运行一个包含 8 个专家的 47B 模型。如果我们采用 Top-2 门控，模型会使用高达 14B 的参数。但是，由于自注意力操作 (专家间共享) 的存在，实际上模型运行时使用的参数数量是 12B。</p><span id="OSC_h2_7"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">Switch Transformers</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">尽管混合专家模型 (MoE) 显示出了很大的潜力，但它们在训练和微调过程中存在稳定性问题。Switch Transformers 是一项非常激动人心的工作，它深入研究了这些话题。作者甚至在 Hugging Face 上发布了一个 1.6 万亿参数的 MoE，拥有 2048 个专家，你可以使用 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">transformers</code> 库来运行它。Switch Transformers 实现了与 T5-XXL 相比 4 倍的预训练速度提升。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006176" data-ratio="0.5101851851851852" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/56bee9cf-5842-4bfd-b5a5-cdc79c60b93d.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Switch Transformer 论文中的 Switch Transformer Layer 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">就像在 GShard 中一样，作者用混合专家模型 (MoE) 层替换了前馈网络 (FFN) 层。Switch Transformers 提出了一个 Switch Transformer 层，它接收两个输入 (两个不同的令牌) 并拥有四个专家。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">与最初使用至少两个专家的想法相反，Switch Transformers 采用了简化的单专家策略。这种方法的效果包括:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      减少门控网络 (路由) 计算负担 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      每个专家的批量大小至少可以减半 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      降低通信成本 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      保持模型质量 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformers 也对 <strong style="color: black;">专家容量</strong> 这个概念进行了研究。</p><span style="cursor:pointer;" data-tool="mdnice 编辑器"><section role="presentation" data-formula="\text{Expert Capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) \times \text{capacity factor}
" data-formula-type="block-equation" style="text-align: center;overflow: auto;"><embed style="vertical-align: -2.148ex;width: 58.45ex;height: auto;max-width: 300% !important;" src="https://oscimg.oschina.net/oscnet/52d4608e-4c61-4d62-b055-dc8f790f459a.svg" data-type="svg+xml" data-imgfileid="100006172"></section></span><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">上述建议的容量是将批次中的令牌数量均匀分配到各个专家。如果我们使用大于 1 的容量因子，我们为令牌分配不完全平衡时提供了一个缓冲。增加容量因子会导致更高的设备间通信成本，因此这是一个需要考虑的权衡。特别值得注意的是，Switch Transformers 在低容量因子 (例如 1 至 1.25) 下表现出色。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformer 的作者还重新审视并简化了前面章节中提到的负载均衡损失。在训练期间，对于每个 Switch 层的辅助损失被添加到总模型损失中。这种损失鼓励均匀路由，并可以使用超参数进行加权。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">作者还尝试了混合精度的方法，例如用 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">bfloat16</code> 精度训练专家，同时对其余计算使用全精度进行。较低的精度可以减少处理器间的通信成本、计算成本以及存储张量的内存。然而，在最初的实验中，当专家和门控网络都使用 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">bfloat16</code> 精度训练时，出现了不稳定的训练现象。这种不稳定性特别是由路由计算引起的，因为路由涉及指数函数等操作，这些操作对精度要求较高。因此，为了保持计算的稳定性和精确性，保持更高的精度是重要的。为了减轻不稳定性，路由过程也使用了全精度。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006174" data-ratio="0.2253731343283582" data-type="png" data-w="670" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/66532187-c893-46e4-8d55-3a04955edef9.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     使用混合精度不会降低模型质量并可实现更快的训练 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">这个 Jupyter Notebook 展示了如何对 Switch Transformers 进行微调以进行摘要生成的详细指南。然而，在开始微调 Switch Transformers 之前，强烈建议您先阅读关于微调混合专家模型部分的内容。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformers 采用了编码器 - 解码器的架构，实现了与 T5 类似的混合专家模型 (MoE) 版本。GLaM 这篇工作探索了如何使用仅为原来 1/3 的计算资源 (因为 MoE 模型在训练时需要的计算量较少，从而能够显著降低碳足迹) 来训练与 GPT-3 质量相匹配的模型来提高这些模型的规模。作者专注于仅解码器 (decoder-only) 的模型以及少样本和单样本评估，而不是微调。他们使用了 Top-2 路由和更大的容量因子。此外，他们探讨了将容量因子作为一个动态度量，根据训练和评估期间所使用的计算量进行调整。</p><span id="OSC_h2_8"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">用 Router z-loss 稳定模型训练</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">之前讨论的平衡损失可能会导致稳定性问题。我们可以使用许多方法来稳定稀疏模型的训练，但这可能会牺牲模型质量。例如，引入 dropout 可以提高稳定性，但会导致模型质量下降。另一方面，增加更多的乘法分量可以提高质量，但会降低模型稳定性。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">ST-MoE 引入的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">Router z-loss</code> 在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">logits</code> 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要。为了深入了解这一机制，建议参考原始论文以获得更全面的细节。</p><span id="OSC_h2_9"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">专家如何学习？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">ST-MoE 的研究者们发现，编码器中不同的专家倾向于专注于特定类型的令牌或浅层概念。例如，某些专家可能专门处理标点符号，而其他专家则专注于专有名词等。与此相反，解码器中的专家通常具有较低的专业化程度。此外，研究者们还对这一模型进行了多语言训练。尽管人们可能会预期每个专家处理一种特定语言，但实际上并非如此。由于令牌路由和负载均衡的机制，没有任何专家被特定配置以专门处理某一特定语言。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006181" data-ratio="0.9258760107816711" data-type="png" data-w="742" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/1137474f-1950-4033-8f38-785166343282.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     ST-MoE 论文中显示了哪些令牌组被发送给了哪个专家的表格 
   </figcaption></figure><span id="OSC_h2_10"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">专家的数量对预训练有何影响？</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">增加更多专家可以提升处理样本的效率和加速模型的运算速度，但这些优势随着专家数量的增加而递减 (尤其是当专家数量达到 256 或 512 之后更为明显)。同时，这也意味着在推理过程中，需要更多的显存来加载整个模型。值得注意的是，Switch Transformers 的研究表明，其在大规模模型中的特性在小规模模型下也同样适用，即便是每层仅包含 2、4 或 8 个专家。</p><span id="OSC_h2_11"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">微调混合专家模型</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);"><code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">4.36.0</code> 版本的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">transformers</code> 库支持 Mixtral 模型。你可以用以下命令进行安装: <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">pip install "transformers==4.36.0 --upgrade</code></p></blockquote><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">稠密模型和稀疏模型在过拟合的动态表现上存在显著差异。稀疏模型更易于出现过拟合现象，因此在处理这些模型时，尝试更强的内部正则化措施是有益的，比如使用更高比例的 dropout。例如，我们可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在微调过程中是否使用辅助损失是一个需要决策的问题。ST-MoE 的作者尝试关闭辅助损失，发现即使高达 11% 的令牌被丢弃，模型的质量也没有显著受到影响。令牌丢弃可能是一种正则化形式，有助于防止过拟合。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Switch Transformers 的作者观察到，在相同的预训练困惑度下，稀疏模型在下游任务中的表现不如对应的稠密模型，特别是在重理解任务 (如 SuperGLUE) 上。另一方面，对于知识密集型任务 (如 TriviaQA)，稀疏模型的表现异常出色。作者还观察到，在微调过程中，较少的专家的数量有助于改善性能。另一个关于泛化问题确认的发现是，模型在小型任务上表现较差，但在大型任务上表现良好。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006182" data-ratio="0.38010471204188484" data-type="png" data-w="955" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/47346e7f-b742-4923-a63d-7ccecd2a6427.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     在小任务 (左图) 中，我们可以看到明显的过拟合，因为稀疏模型在验证集中的表现要差得多。在较大的任务 (右图) 中，MoE 则表现良好。该图来自 ST-MoE 论文 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">一种可行的微调策略是尝试冻结所有非专家层的权重。实践中，这会导致性能大幅下降，但这符合我们的预期，因为混合专家模型 (MoE) 层占据了网络的主要部分。我们可以尝试相反的方法: 仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006180" data-ratio="0.77" data-type="png" data-w="400" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/401ba984-a5f5-4772-95db-99c12f026ef7.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     通过仅冻结 MoE 层，我们可以在保持质量的同时加快训练速度。该图来自 ST-MoE 论文 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在微调稀疏混合专家模型 (MoE) 时需要考虑的最后一个问题是，它们有特别的微调超参数设置——例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006183" data-ratio="0.37570303712035996" data-type="png" data-w="889" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/0f29d064-6852-4055-99ce-fab522c4d6bd.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     降低学习率和调大批量可以提升稀疏模型微调质量。该图来自 ST-MoE 论文 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">此时，您可能会对人们微调 MoE 中遇到的这些挑战而感到沮丧，但最近的一篇论文 《MoEs Meets Instruction Tuning》 (2023 年 7 月) 带来了令人兴奋的发现。这篇论文进行了以下实验:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      单任务微调 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      多任务指令微调 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      多任务指令微调后接单任务微调 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">当研究者们对 MoE 和对应性能相当的 T5 模型进行微调时，他们发现 T5 的对应模型表现更为出色。然而，当研究者们对 Flan T5 (一种 T5 的指令优化版本) 的 MoE 版本进行微调时，MoE 的性能显著提升。更值得注意的是，Flan-MoE 相比原始 MoE 的性能提升幅度超过了 Flan T5 相对于原始 T5 的提升，这意味着 MoE 模型可能从指令式微调中获益更多，甚至超过了稠密模型。此外，MoE 在多任务学习中表现更佳。与之前关闭 <strong style="color: black;">辅助损失</strong> 函数的做法相反，实际上这种损失函数可以帮助防止过拟合。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006179" data-ratio="0.4456140350877193" data-type="png" data-w="855" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/dc2a1d41-2a2d-4e63-b2c3-015fb22f56f3.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     与稠密模型相比，稀疏模型从指令微调中受益更多。该图来自 MoEs Meets instructions Tuning 论文 
   </figcaption></figure><span id="OSC_h2_12"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">稀疏 VS 稠密，如何选择?</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">稀疏混合专家模型 (MoE) 适用于拥有多台机器且要求高吞吐量的场景。在固定的预训练计算资源下，稀疏模型往往能够实现更优的效果。相反，在显存较少且吞吐量要求不高的场景，稠密模型则是更合适的选择。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);"><strong style="color: black;">注意</strong>: 直接比较稀疏模型和稠密模型的参数数量是不恰当的，因为这两类模型基于的概念和参数量的计算方法完全不同。</p><span id="OSC_h2_13"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">让 MoE 起飞</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">最初的混合专家模型 (MoE) 设计采用了分支结构，这导致了计算效率低下。这种低效主要是因为 GPU 并不是为处理这种结构而设计的，而且由于设备间需要传递数据，网络带宽常常成为性能瓶颈。在接下来的讨论中，我们会讨论一些现有的研究成果，旨在使这些模型在预训练和推理阶段更加高效和实用。我们来看看如何优化 MoE 模型，让 MoE 起飞。</p><span id="OSC_h3_14"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">并行计算</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">让我们简要回顾一下并行计算的几种形式:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">数据并行</strong>: 相同的权重在所有节点上覆制，数据在节点之间分割。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">模型并行</strong>: 模型在节点之间分割，相同的数据在所有节点上覆制。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">模型和数据并行</strong>: 我们可以在节点之间同时分割模型和数据。注意，不同的节点处理不同批次的数据。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);"><strong style="color: black;">专家并行</strong>: 专家被放置在不同的节点上。如果与数据并行结合，每个节点拥有不同的专家，数据在所有节点之间分割。 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在专家并行中，专家被放置在不同的节点上，每个节点处理不同批次的训练样本。对于非 MoE 层，专家并行的行为与数据并行相同。对于 MoE 层，序列中的令牌被发送到拥有所需专家的节点。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006184" data-ratio="0.5910194174757282" data-type="png" data-w="824" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/d166997f-8e65-483f-91e6-e5e4c1ccdf96.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     Switch Transformers 论文中展示如何使用不同的并行技术在节点上分割数据和模型的插图 
   </figcaption></figure><span id="OSC_h3_15"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">容量因子和通信开销</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">提高容量因子 (Capacity Factor, CF) 可以增强模型的性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。一个合理的初始设置是采用 Top-2 路由、1.25 的容量因子，同时每个节点配置一个专家。在评估性能时，应根据需要调整容量因子，以在设备间的通信成本和计算成本之间找到一个平衡点。</p><span id="OSC_h3_16"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">部署技术</span><span style="display: none;"></span></h3><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);">您可以在 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);padding: 3px;margin: 3px;">Inference Endpoints</code> 部署 mistralai/Mixtral-8x7B-Instruct-v0.1。</p></blockquote><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">部署混合专家模型 (MoE) 的一个关键挑战是其庞大的参数规模。对于本地使用情况，我们可能希望使用更小的模型。为了使模型更适合部署，下面是几种有用的技术:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      预先蒸馏实验: Switch Transformers 的研究者们进行了预先蒸馏的实验。他们通过将 MoE 模型蒸馏回其对应的稠密模型，成功保留了 30-40% 的由稀疏性带来的性能提升。预先蒸馏不仅加快了预训练速度，还使得在推理中使用更小型的模型成为可能。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      任务级别路由: 最新的方法中，路由器被修改为将整个句子或任务直接路由到一个专家。这样做可以提取出一个用于服务的子网络，有助于简化模型的结构。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。这样可以在不显著牺牲性能的情况下降低模型的复杂度。 
    </section></li></ul><span id="OSC_h3_17"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">高效训练</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">FasterMoE (2022 年 3 月) 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Megablocks (2022 年 11 月) 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。其核心优势在于，它不会丢弃任何令牌，并能高效地适应现代硬件架构 (支持块稀疏矩阵乘)，从而达到显著的加速效果。Megablocks 的创新之处在于，它不像传统 MoE 那样使用批量矩阵乘法 (这通常假设所有专家形状相同且处理相同数量的令牌)，而是将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的令牌分配。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100006185" data-ratio="0.2851851851851852" data-type="png" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/d09a85e7-f59f-421c-8e9f-d75df34954c8.png" referrerpolicy="no-referrer"> &nbsp; 
   <figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     针对不同规模的专家和令牌数量的块稀疏矩阵乘法。该图来自 MegaBlocks 论文 
   </figcaption></figure><span id="OSC_h2_18"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">开源混合专家模型</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">目前，下面这些开源项目可以用于训练混合专家模型 (MoE):</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Megablocks: https://github.com/stanford-futuredata/megablocks 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Fairseq: https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      OpenMoE: https://github.com/XueFuzhao/OpenMoE 
    </section></li></ul><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">对于开源的混合专家模型 (MoE)，你可以关注下面这些:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Switch Transformers (Google): 基于 T5 的 MoE 集合，专家数量从 8 名到 2048 名。最大的模型有 1.6 万亿个参数。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      NLLB MoE (Meta): NLLB 翻译模型的一个 MoE 变体。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      OpenMoE: 社区对基于 Llama 的模型的 MoE 尝试。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Mixtral 8x7B (Mistral): 一个性能超越了 Llama 2 70B 的高质量混合专家模型，并且具有更快的推理速度。此外，还发布了一个经过指令微调的模型。有关更多信息，可以在 Mistral 的，公告博客文章，中了解。 
    </section></li></ul><span id="OSC_h2_19"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">一些有趣的研究方向</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">首先是尝试将稀疏混合专家模型 (SMoE) <strong style="color: black;">蒸馏</strong> 回到具有更少实际参数但相似等价参数量的稠密模型。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">MoE 的 <strong style="color: black;">量化</strong> 也是一个有趣的研究领域。例如，QMoE (2023 年 10 月) 通过将 MoE 量化到每个参数不到 1 位，将 1.6 万亿参数的 Switch Transformer 所需的存储从 3.2TB 压缩到仅 160GB。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">简而言之，一些值得探索的有趣领域包括:</p><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      将 Mixtral 蒸馏成一个稠密模型。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      探索合并专家模型的技术及其对推理时间的影响。 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      尝试对 Mixtral 进行极端量化的实验。 
    </section></li></ul><span id="OSC_h2_20"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">相关资源</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><ul data-tool="mdnice 编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;" class="list-paddingleft-1"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Adaptive Mixture of Local Experts (1991) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Learning Factored Representations in a Deep Mixture of Experts (2013) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Jun 2020) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      GLaM: Efficient Scaling of Language Models with Mixture-of-Experts (Dec 2021) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      ST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models(April 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (Nov 2022) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (May 2023) 
    </section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(58, 58, 58);">
      Mixtral-8x7B-v0.1, Mixtral-8x7B-Instruct-v0.1. 
    </section></li></ul><span id="OSC_h2_21"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">Citation</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">@misc {sanseviero2023moe,<br>    author       = { Omar Sanseviero and<br>                     Lewis Tunstall and<br>                     Philipp Schmid and<br>                     Sourab Mangrulkar and<br>                     Younes Belkada and<br>                     Pedro Cuenca<br>                   },<br>    title        = { Mixture of Experts Explained },<br>    year         = 2023,<br>    url          = { https://huggingface.co/blog/moe },<br>    publisher    = { Hugging Face Blog }<br>}<br></code></pre><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">Sanseviero,&nbsp;et&nbsp;al.,&nbsp;<span style="color: #d14;line-height: 26px;">"Mixture&nbsp;of&nbsp;Experts&nbsp;Explained"</span>,&nbsp;Hugging&nbsp;Face&nbsp;Blog,&nbsp;2023.<br></code></pre><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;border-left-color: rgb(255, 177, 27);background: rgb(255, 245, 227);"><p style="font-size: 16px;line-height: 26px;color: rgb(89, 89, 89);">🤗 宝子们可以戳 <strong style="color: black;">阅读原文</strong> 查看文中所有的外部链接哟！</p></blockquote><hr data-tool="mdnice 编辑器" style="height: 1px;border-right: none;border-bottom: none;border-left: none;border-top-style: solid;border-top-color: rgb(249, 191, 69);margin-top: 20px;margin-bottom: 20px;"><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;color: rgb(91, 91, 91);background: rgba(158, 158, 158, 0.1);padding-top: 1px;padding-bottom: 1px;padding-left: 5px;margin-top: 0px;margin-bottom: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">英文原文:&nbsp;<span style="color: rgb(136, 136, 136);letter-spacing: 0px;">https://hf.co/blog/moe</span></p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">原文作者: Omar Sanseviero, Lewis Tunstall, Philipp Schmid, Sourab Mangrulkar, Younes Belkada, Pedro Cuenca</p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">译者: xinyu66 (Xinyu Yang)</p></blockquote></blockquote></blockquote></blockquote></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - Hugging Face（gh_504339124f0f）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 30 Dec 2023 02:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10444582</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10444582</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[恭喜 LinkWeChat 荣获 2023 开源创新榜「优秀开源项目」]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span>&nbsp; &nbsp; &nbsp; 近日，由<strong>中国科协科学技术传播中心、中国计算机学会、中国通信学会、中国科学院软件研究所</strong>共同主办，CSDN 承办的 2023 开源创新榜专家评审会在国家科技传播中心成功举办。评委会主任、中国计算机学会开源发展委员会主任王怀民院士，评委会副主任、中国科协科学技术传播中心副主任陈锐，评委会副主任、中国通信学会副理事长兼秘书长张延川，评委会副主任、中国科学院软件研究所所长赵琛与来自全国学会、大学、科研院所、企业、开源基金会、行业联盟等二十多位开源专家共同参与了本届榜单评审工作，会议由陈锐主持。</span></p><p><span>&nbsp; &nbsp; &nbsp; &nbsp; 2023 年开源创新榜相较往年有以下几个变化。<strong>一是进一步提升权威性，</strong>主办单位新加入中国计算机学会、中国通信学会、中国科学院软件研究所，四家主办单位优势互补，共同推动榜单策划、征集申报、专家评审等工作重点。<strong>二是进一步提升公信力，</strong>由王怀民院士担任评委会主任，指导组建了结构更加科学、领域更加全面的评审专家库，从中提名形成最终评审专家。<strong>三是进一步提升专业度，</strong>围绕项目、社区、人物三大类别，四家主办单位打磨了更加客观、严谨、贴合实际的评审标准和更加开放、公平、科学的评审办法，在征集过程中公开标准细节，接受社会的意见反馈，形成良性循环。</span></p><p><span>评委会最终评选出优秀开源项目 20 个，LinkWeChat<span style="color:#f2622e"><strong>成功入选。</strong></span></span></p><p><span><span style="color:#f2622e"><strong><img alt="" height="472" src="https://oscimg.oschina.net/oscnet/up-c75591fff8654955776eeed8c0294671c38.png" width="284" referrerpolicy="no-referrer"></strong></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 15:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273361</guid>
            <link>https://www.oschina.net/news/273361</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[告别 2023，迎接 2024]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>顺便带一下开放签开源电子签章上线两周的小结，本周关键字「惊喜」。官网访问量稳定在 200 左右/天，github/gitee:start 总计 130。5 个企业版意向客户，以上便是开放签上线两周的运营数据。</span></span></span></span></span></span></span></span>&nbsp;</span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>用「坚持」「艰辛」来总结 2023 年，这是我们真实的写照。团队这几年确实很辛苦，经历了创业的艰辛，失去了几载青春、大把的票子，用很低的收入维持生活，这是我们的现状。但是这些依然打不倒，也压不跨我们，对我们来说也不是什么」苦「。因为我们早已看淡了这些，我们相信我们这些年的积累、经验总有一天有用武之地。因为我们有梦想（说梦想可能会有人笑话吧），我们可以把开放签做好。其实毫不夸张的说，直至开放签上线后，才体会到十年磨一剑的感受。不瞒大家，从开始做电子签直至开放签这个项目上线，真的已经十年了，期间被家人和朋友调侃十年才玩明白这点事儿。调侃归调侃，真正想做好一个产品，我们觉得十年只是一个开始，没有这十来年足够的客户、技术、服务等方方面面积累，以及我们对开放签的坚持，估计开放签上线后也是个没血肉、立不住的产品吧。对开放签来说，十年只是一个开始，我们深知没有任何一件事情可以随便、容易的完成的，接下来的几年、几十年，我们将持续的、顽强的深耕我们的产品、服务，让电子签章更简单不是说说而已。</span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>用「惊喜」「憧憬」来展望 2024 年，惊喜发生在开放签上线后，而且是持续的。惊喜的是没想到有特别多的朋友在关注、关心我们，给我们提了不少战略级别的建议，真的感谢你们。在此特别感谢@jack，感谢你给予我们的指导和建议。你给我们带来了更多的正能量，我们更加坚定了能做好开放签的信心。只要用心做好产品，我们相信 24 年会有更多的惊喜。24 年用「憧憬」一词来展望新年，代表了我们对新的一年美好的期冀。我们期待有更多用户可以低门槛的应用电子签章，电子签章可以更加简单的得到普及。新的一年，我们更加期待开放签可以更多的参与到公益和对社会有益的事业中，让开源、开放的意义更大。</span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>写在最后，我们还很年轻，还有很多不足，肯定在发展的道路上会犯很多错误，但是我们是开放的，有激情的，我们愿意接受各种批评和质疑，最后还是用一句我们自己坚信的话来总结我们「我们相信开源开放会为产品与用户之间带来更多信任「，这就是开放签的价值观，是我们坚定走下去的信念。 </span></span></span></span></span></span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span><span style="color:#171a1d"><span><span><span><span>祝各位在新的一年顺遂、如意。新年好！</span></span></span></span></span></span></span></span></span></span></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 08:20:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273299</guid>
            <link>https://www.oschina.net/news/273299</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2023 大语言模型技术报告.pdf]]>
            </title>
            <description>
                <![CDATA[看之前先买瓶水，货实在太干了[Facepalm]]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 06:43:00 GMT</pubDate>
            <guid isPermaLink="false">https://talk.gitee.com/report/china-open-source-2023-llm-report.pdf?fr=news1229</guid>
            <link>https://talk.gitee.com/report/china-open-source-2023-llm-report.pdf?fr=news1229</link>
        </item>
        <item>
            <title>
                <![CDATA[Ubuntu 23.04 将于 2024 年 1 月 25 日结束支持]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">对 <a href="https://www.oschina.net/news/237763/ubuntu-23-04-released">Ubuntu 23.04「Lunar Lobster」</a>的官方支持将于 2024 年 1 月 25 日结束，还剩不到一个月的时间。</span></p><p><span style="color:#000000">Ubuntu 23.04 版本于 2023 年 4 月正式发布，作为短期支持版本获得 9 个月的支持。还在使用该版本的用户<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2023%2F12%2Fubuntu-2304-support-ends-january-2024" target="_blank">建议</a>可以考虑升级到 10 月份发布的 <a href="https://www.oschina.net/news/261571/ubuntu-23-10-ga">Ubuntu 23.10"Mantic Minotaur"</a>，以确保可继续收到来自 Canonical 的安全补丁、关键错误修复以及精选软件的重要更新。</span></p><p><span style="color:#000000"><img alt="" height="263" src="https://oscimg.oschina.net/oscnet/up-4bdb313301be4efb73863c20508102b5a86.jpg" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">Ubuntu 23.10 附带了最新的 GNOME 45 版本（其中包含大量改进）、使用 Linux 6.5 内核、更新了图形驱动程序，并首次发布了 2 个全新的应用程序，这些应用程序目前为该版本<span style="background-color:#ffffff">独有</span>： App Center 和 Firmware。</span></p><p><span style="color:#000000">同样作为短期支持版本，Ubuntu 23.10 计划将于 2024 年 7 月中旬达到 EOL。不过预计明年 4 月下旬，<span style="background-color:#ffffff"><a href="https://www.oschina.net/news/263525/ubuntu-24-04-release-date-april-25-2024">Ubuntu 24.04 LTS</a> 版本就会正式发布，LTS 版本将获得 5 年的安全更新、错误修复和精选应用程序更新；LTS 版本预期每 2 年发布一次。</span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 06:13:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273275/ubuntu-2304-support-ends-january-2024</guid>
            <link>https://www.oschina.net/news/273275/ubuntu-2304-support-ends-january-2024</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2023 mybatis-mp - 亮点一：可自定义默认值]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>1：默认值设置&nbsp;</p><div><pre><span style="color:#9e880d">@Table
</span><span style="color:#9e880d">@Data
</span><span style="color:#0033b3">public class </span><span style="color:#000000">DefaultValueTest </span>{

    <span style="color:#9e880d">@TableId
</span><span style="color:#9e880d"></span><span style="color:#0033b3">private </span><span style="color:#000000">Integer </span><span style="color:#871094">id</span>;

    <span style="color:#9e880d">@TableField</span>(defaultValue = <span style="color:#067d17">"{BLANK}"</span>)
    <span style="color:#0033b3">private </span><span style="color:#000000">String </span><span style="color:#871094">value1</span>;

    <span style="color:#9e880d">@TableField</span>(defaultValue = <span style="color:#067d17">"1"</span>)
    <span style="color:#0033b3">private </span><span style="color:#000000">Integer </span><span style="color:#871094">value2</span>;

    <span style="color:#9e880d">@TableField</span>(defaultValue = <span style="color:#067d17">"{NOW}"</span>)
    <span style="color:#0033b3">private </span><span style="color:#000000">LocalDateTime </span><span style="color:#871094">createTime</span>;
}
</pre></div><p>2：如何自定义默认值：</p><pre><code class="language-java">MybatisMpConfig.setDefaultValue("{NOW}", (type) -&gt; {
    if (type == LocalDateTime.class) {
        return LocalDateTime.now();
    } else if (type == LocalDate.class) {
        return LocalDate.now();
    } else if (type == Date.class) {
        return new Date();
    } else if (type == Long.class) {
        return System.currentTimeMillis();
    } else if (type == Integer.class) {
        return (int) (System.currentTimeMillis() / 1000);
    }
    throw new RuntimeException("Inconsistent types");
});</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 06:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273274</guid>
            <link>https://www.oschina.net/news/273274</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CNCF 报告：Kubernetes 推动云支出增长]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CNCF 发布的一份调查报告<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2023%2F12%2F20%2Fcncf-cloud-native-finops-cloud-financial-management-microsurvey%2F" target="_blank">指出</a>，Kubernetes 的到来导致了云支出急剧增加；有<span style="background-color:#fdfdfd; color:#000000">近一半 (49%) 的受访者表示 Kubernetes 推动了云支出增长。</span>其中，17% 的人表示成本大幅增加，32% 的人表示成本仅略有增加。</p><p>另一方面，13% 的受访者在实施 Kubernetes 后成功显着减少了云支出，11% 的受访者成功略微减少了支出。28% 的受访者表示采用 Kubernetes 后没有任何变化。</p><p><img height="273" src="https://oscimg.oschina.net/oscnet/up-153215e0c5e6743aa6ba642aee27efef80b.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">约 28% 的受访者表示，Kubernetes 占用了他们一半的预算，10% 的受访者表示，这一数字高达 75%，还有极少数 5% 的受访者表示，Kubernetes 占用了他们的全部预算。</span></p><p><img height="279" src="https://oscimg.oschina.net/oscnet/up-0189729f7a76027da4a44209791e910f940.png" width="500" referrerpolicy="no-referrer"></p><p>26% 的人每月在云计算上的支出高达 50000 美元；还有 22% 的人表示他们的支出是前者的 20 倍，每月高达 100 万美元以上。此外，21% 的人每月云计算支出不到 1 万美元。</p><p>在受访者中，Kubernetes 基础设施的规模存在很大差异。近一半的受访者 (49%) 只<span style="background-color:#fdfdfd; color:#000000">拥有最多 50 个节点</span>。15% 拥有 51-100 个节点，17% 拥有 101-250 个节点，18% 拥有超过 251 个节点。&nbsp;</p><p>许多人力和技术因素被认为是云环境中支出以及不必要和意外成本增加的原因。过度配置以 70% 的比例遥遥领先，个人或团队层面缺乏责任意识位居第二，为 45%。使用资源后未能停用资源以及存在技术债务（定义为尚未重新架构以利用云原生环境的可扩展性的工作负载）并列第三，各占 43%。</p><p><img alt="" height="417" src="https://oscimg.oschina.net/oscnet/up-4a03450fb93c2128ee80b363c64d6f5c9f6.png" width="500" referrerpolicy="no-referrer"></p><p>只有 19% 的受访者表示他们能够准确监控 Kubernetes 成本。40% 的人只是进行了估计，38% 的人表示他们根本没有进行任何监控。</p><p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2023%2F12%2F20%2Fcncf-cloud-native-finops-cloud-financial-management-microsurvey%2F" target="_blank">查看完整报告</a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 29 Dec 2023 03:38:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/273259/cncf-report-kubernetes-cloud-spen</guid>
            <link>https://www.oschina.net/news/273259/cncf-report-kubernetes-cloud-spen</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
    </channel>
</rss>
