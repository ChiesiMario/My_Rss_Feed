<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Thu, 29 Feb 2024 02:40:57 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[替代 Nginx，Cloudflare 开源 Pingora Rust 框架]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">早在 2022 年，Cloudflare 就<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fzh-cn%2Fhow-we-built-pingora-the-proxy-that-connects-cloudflare-to-the-internet-zh-cn%2F" target="_blank">曾宣布</a>将放弃 Nginx，转而采用 Pingora —— 一个他们使用 Rust 在内部构建的新 HTTP 代理。时至今日，Cloudflare <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fpingora-open-source" target="_blank">宣布</a>在 Apache 2.0 许可下开源了 Pingora 框架。</span></p><p><span style="color:#000000">Pingora 是一个 Rust 异步多线程框架，可以帮助构建 HTTP 代理服务。截至目前，Pingora 已在 Cloudflare 的全球网络中处理了近千万亿的互联网请求。</span></p><p><span style="color:#000000">「我们正在开源 Pingora，以帮助在我们自己的基础设施之外构建一个更好、更安全的互联网。我们希望为我们的客户、用户和其他人提供工具、想法和灵感，以使用内存安全框架构建自己的互联网基础设施。」</span></p><p><img height="212" src="https://oscimg.oschina.net/oscnet/up-cc64bac2552b3cc892837fa2c59fb251036.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#000000">根据介绍，Pingora 提供库和 API 来在 HTTP/1 和 HTTP/2、TLS 或 TCP/UDP 之上构建服务。作为代理，它支持 HTTP/1 和 HTTP/2 端到端、gRPC 和 websocket 代理，HTTP/3 支持也在规划当中。它还具有可定制的负载平衡和故障转移策略。为了合规性和安全性，它支持常用的 OpenSSL 和 BoringSSL 库。</span></p><p><span style="color:#000000">除了提供这些功能之外，Pingora 还提供 filters 和 callbacks，以允许用户完全自定义服务应如何处理、转换和转发请求。在运行方面，Pingora 提供零停机优雅重启，可在不丢弃任何传入请求的情况下进行自我升级。Syslog、Prometheus、Sentry、OpenTelemetry 和其他必备的可观测工具也可以轻松与 Pingora 集成。</span></p><p><span style="color:#000000">值得注意的是，Pingora 尚处于 1.0 之前的阶段，不具备 API 稳定性，且 Cloudflare 目前没有计划支持非 Unix 操作系统。</span></p><p><span style="color:#000000">更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.cloudflare.com%2Fpingora-open-source" target="_blank">查看官方博客</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 29 Feb 2024 02:27:53 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280738/cloudflare-pingora-open-source</guid>
            <link>https://www.oschina.net/news/280738/cloudflare-pingora-open-source</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[李彦宏：大模型为云业务带来约 6.6 亿元增量收入]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">在<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Ftl74wEZbthzW-si0gcyeVg" target="_blank">百度</a> 2023 年第四季度及全年财报电话会上，百度创始人、董事长兼首席执行官李彦宏透露，百度智能云四季度总营收 84 亿元，其中大模型为云业务带来约 6.6 亿元增量收入。预计到 2024 年，这一增量收入有望达到数十亿元人民币，主要来源将包括广告业务和人工智能云服务的销售。</span></p><p><span style="color:#000000">百度于近期上线了千帆 AppBuilder 和千帆 ModelBuilder 两款 MaaS 产品。李彦宏表示，迄今为止，千帆 ModelBuilder 累计精调了 10000 个模型。而文心大模型的日调用量已超过 5000 万次，季度环比增长 190%。12 月，约有 2.6 万家企业调用文心大模型，季度环比增长 150%。</span></p><p><img height="331" src="https://oscimg.oschina.net/oscnet/up-900ee6f0e60fdcc24240d16d68062635f76.png" width="300" referrerpolicy="no-referrer"></p><p><span style="color:#000000">大模型高昂的使用成本，是影响其更大规模落地的难点之一。李彦宏透露，文心大模型的推理成本已降低至去年 3 月版本的 1%。李彦宏还回应了 AI 芯片供应对百度的影响。「短期来看，这对我们的模型开发、产品改造或商业化几乎没有影响。百度已经把文心大模型升级到 4.0。模型推理需要的是性能较低的芯片，我们的 AI 芯片储备以及市场上可用的芯片，足以支持为客户提供 AI 原生应用，让百度在未来一两年内继续提升收益。」李彦宏称。</span></p><p><span style="color:#000000">李彦宏也表示，从长期来看，百度可能无法获得最先进 GPU 芯片，「不过，通过最高效的、自主研发的软件堆栈，可以让用户体验不受影响。在应用层、模型层和框架层，端到端、自主研发的 AI 架构都有创新的空间。」</span></p><p><span style="color:#000000">文生视频大模型 Sora 的爆火引发了业内的极大关注。对此，李彦宏回应称，文本到音频和视频的集成，是未来大模型发展的重要方向，百度已经在这个领域进行了投资。李彦宏认为，在视觉大模型领域，一大显著的应用和潜在市场是自动驾驶，能够把道路上捕获的图像和视频转化为具体任务，从而产生更智能、更适应性强、更安全的自动驾驶技术。</span></p><p><span style="color:#000000">清华大学战略新兴产业研究中心副主任胡麒牧认为，百度 2023 年的营收同比增速 39%，净利润同比增速 44%，其中 AI 业务带来的业绩增长是主要动力。</span></p><p><span style="color:#000000">在大模型与产业的融合方面，胡麒牧指出，AIGC 仍处于初级阶段。「像 AI 与制造业的深度融合，现有的互联网公司还没找到特别好的突破口。在行政办公、供应链领域已经有在应用 AI 技术，场景也在持续增加，但核心的加工制造环节还是进展不够。」</span></p><p><span style="color:#000000">胡麒牧认为，百度需要与制造企业合作，实现双向奔赴。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 29 Feb 2024 02:11:53 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280730</guid>
            <link>https://www.oschina.net/news/280730</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Pkl —— 用于生成配置的编程语言]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Pkl 是专用于创建配置文件的脚本编程语言。该项目旨在应对 JSON、YAML 和属性列表等静态配置格式的不足，提供一种介于静态语言和通用语言之间、「两全其美」的方案。</p><p>Pkl 的三个设计目标是语法安全、可扩展和 IDE 集成，使用声明式语法、易读易写，但也支持类、函数、条件和循环等常见的编程语言功能。</p><p>Pkl 可用于生成任何格式的静态配置文件，也可以作为库嵌入在 Java、Kotlin、Swift、Go 等语言的代码中运行。</p><p><img src="https://oscimg.oschina.net/oscnet/up-d89a660e2346e9f1536d4df60f0ba725793.png" referrerpolicy="no-referrer"></p><p><strong>示例代码</strong></p><p>bird.pkl</p><pre><code class="language-pkl">name = "Swallow"

job {
  title = "Sr. Nest Maker"
  company = "Nests R Us"
  yearsOfExperience = 2
}
</code></pre><p>↓</p><p>bird.json</p><pre><code class="language-json">{
  "name": "Swallow",
  "job": {
    "title": "Sr. Nest Maker",
    "company": "Nests R Us",
    "yearsOfExperience": 2
  }
}
</code></pre></div>
                                                                ]]>
            </description>
            <pubDate>Thu, 29 Feb 2024 02:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/pkl</guid>
            <link>https://www.oschina.net/p/pkl</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 多模型对话 ChatMaster]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-chat-master-web" class="anchor" href="https://gitee.com/panday94/chat-master-web#chat-master-web"></a>Chat Master Web</h1><p><a href="https://gitee.com/panday94/chat-master-web#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC"><img src="https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-%E5%A4%A7%E5%B8%88%E5%AD%A6Java-blue" alt="公众号" referrerpolicy="no-referrer"></a></p><blockquote><p>声明：此项目只发布于码云，基于 MIT 协议，免费且作为开源学习使用。并且不会有任何形式的卖号、付费服务、讨论群、讨论组等行为。谨防受骗。</p></blockquote><blockquote><p>项目框架基于<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FChanzhaoyu%2Fchatgpt-web">chatgpt-web</a>项目改造，页面 UI 借鉴 ChatGLM 项目。后端项目使用 java 服务搭建，如需使用移步<a href="https://gitee.com/panday94/chat-master">ChatMASTER</a>，支持 ChatGPT(3.5、4.0) 模型，同时也支持国内文心一言、通义千问、讯飞星火、智谱清言 (ChatGLM) 等主流模型，支持文心一言 (Stable-Diffusion-XL 作图) 功能，支持模型及助手后台自定义配置，如需使用移步<a href="https://gitee.com/panday94/chat-master-admin">Chat-Master-Admin</a></p></blockquote><p><img src="https://gitee.com/panday94/chat-master-web/raw/master/docs/login.jpg" alt="cover" referrerpolicy="no-referrer"><img src="https://gitee.com/panday94/chat-master-web/raw/master/docs/chat.gif" alt="cover2" referrerpolicy="no-referrer"></p><ul><li><a href="https://gitee.com/panday94/chat-master-web#chat-master-web">Chat Master Web</a><ul><li><a href="https://gitee.com/panday94/chat-master-web#%E4%BB%8B%E7%BB%8D">介绍</a></li><li><a href="https://gitee.com/panday94/chat-master-web#%E5%BE%85%E5%AE%9E%E7%8E%B0%E8%B7%AF%E7%BA%BF">待实现路线</a></li><li><a href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%BD%AE%E8%A6%81%E6%B1%82">前置要求</a><ul><li><a href="https://gitee.com/panday94/chat-master-web#node">Node</a></li><li><a href="https://gitee.com/panday94/chat-master-web#pnpm">PNPM</a></li></ul></li><li><a href="https://gitee.com/panday94/chat-master-web#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96">安装依赖</a><ul><li><a href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%AB%AF">前端</a></li></ul></li><li><a href="https://gitee.com/panday94/chat-master-web#%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E8%BF%90%E8%A1%8C">测试环境运行</a><ul><li><a href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%AB%AF%E7%BD%91%E9%A1%B5">前端网页</a></li></ul></li><li><a href="https://gitee.com/panday94/chat-master-web#%E6%89%93%E5%8C%85">打包</a><ul><li><a href="https://gitee.com/panday94/chat-master-web#%E9%98%B2%E6%AD%A2%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96">防止爬虫抓取</a></li><li><a href="https://gitee.com/panday94/chat-master-web#%E6%89%8B%E5%8A%A8%E6%89%93%E5%8C%85">手动打包</a><ul><li><a href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%AB%AF%E7%BD%91%E9%A1%B5-1">前端网页</a></li></ul></li></ul></li><li><a href="https://gitee.com/panday94/chat-master-web#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98">常见问题</a></li><li><a href="https://gitee.com/panday94/chat-master-web#%E5%8F%82%E4%B8%8E%E8%B4%A1%E7%8C%AE">参与贡献</a></li><li><a href="https://gitee.com/panday94/chat-master-web#%E8%B5%9E%E5%8A%A9">赞助</a></li><li><a href="https://gitee.com/panday94/chat-master-web#license">License</a></li></ul></li></ul><h2><a id="user-content-介绍" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E4%BB%8B%E7%BB%8D"></a>介绍</h2><p>项目基于 ChatGpt、文心一言、通义千问、讯飞星火、智谱清言等主流模型开发</p><table><thead><tr><th>名称</th><th>免费？</th><th>是否国内</th><th>地址</th></tr></thead><tbody><tr><td>ChatGpt</td><td>否</td><td>否</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fchat.openai.com%2F">https://chat.openai.com/</a></td></tr><tr><td>文心一言</td><td>否</td><td>是</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fyiyan.baidu.com%2F">https://yiyan.baidu.com/</a></td></tr><tr><td>通义千问</td><td>否</td><td>是</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Ftongyi.aliyun.com%2F">https://tongyi.aliyun.com/</a></td></tr><tr><td>讯飞星火</td><td>否</td><td>是</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fxinghuo.xfyun.cn%2F">https://xinghuo.xfyun.cn/</a></td></tr><tr><td>智谱清言</td><td>否</td><td>是</td><td><a href="https://gitee.com/link?target=https%3A%2F%2Fchatglm.cn%2F">https://chatglm.cn/</a></td></tr></tbody></table><p>提示：</p><ol><li>ChatGpt 通过<code>Cloudflare</code>访问 openai 接口</li><li>ChatGPT 及国内模型密钥由后台系统配置</li><li>后期可接入使用自己 token 或者 key 使用</li><li>如需了解更多可访问<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.yuque.com%2Fthe6%2Fct0azl%2Ftgi5wfbn162qugxk">这里</a></li></ol><h2><a id="user-content-已实现路线" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%B7%B2%E5%AE%9E%E7%8E%B0%E8%B7%AF%E7%BA%BF"></a>已实现路线</h2><p>[✓] 多模型</p><p>[✓] 多会话储存和上下文逻辑</p><p>[✓] 对代码等消息类型的格式化美化处理</p><p>[✓] 个人信息修改及分享</p><p>[✓] 界面多语言</p><p>[✓] 界面主题</p><p>[✗] More...</p><h2><a id="user-content-前置要求" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%BD%AE%E8%A6%81%E6%B1%82"></a>前置要求</h2><h3><a id="user-content-node" class="anchor" href="https://gitee.com/panday94/chat-master-web#node"></a>Node</h3><p><code>node</code> 需要 <code>^16 || ^18 || ^19</code> 版本（<code>node &gt;= 14</code> 需要安装 <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fdevelopit%2Funfetch%23usage-as-a-polyfill">fetch polyfill</a>），使用 <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fnvm-sh%2Fnvm">nvm</a> 可管理本地多个 <code>node</code> 版本</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">node <span class="nt">-v</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-pnpm" class="anchor" href="https://gitee.com/panday94/chat-master-web#pnpm"></a>PNPM</h3><p>如果你没有安装过 <code>pnpm</code></p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">npm <span class="nb">install </span>pnpm <span class="nt">-g</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-安装依赖" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"></a>安装依赖</h2><h3><a id="user-content-前端" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%AB%AF"></a>前端</h3><p>根目录下运行以下命令</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pnpm bootstrap</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-测试环境运行" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E8%BF%90%E8%A1%8C"></a>测试环境运行</h2><h3><a id="user-content-前端网页" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%AB%AF%E7%BD%91%E9%A1%B5"></a>前端网页</h3><p>根目录下运行以下命令</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pnpm dev</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="user-content-防止爬虫抓取" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E9%98%B2%E6%AD%A2%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96"></a>防止爬虫抓取</h4><p><strong>nginx</strong></p><p>将下面配置填入 nginx 配置文件中，可以参考 <code>docker-compose/nginx/nginx.conf</code> 文件中添加反爬虫的方法</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">    # 防止爬虫抓取</span><span id="LC2" class="line">    if ($http_user_agent ~* "360Spider|JikeSpider|Spider|spider|bot|Bot|2345Explorer|curl|wget|webZIP|qihoobot|Baiduspider|Googlebot|Googlebot-Mobile|Googlebot-Image|Mediapartners-Google|Adsbot-Google|Feedfetcher-Google|Yahoo! Slurp|Yahoo! Slurp China|YoudaoBot|Sosospider|Sogou spider|Sogou web spider|MSNBot|ia_archiver|Tomato Bot|NSPlayer|bingbot")</span><span id="LC3" class="line">    {</span><span id="LC4" class="line">      return 403;</span><span id="LC5" class="line">    }</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-手动打包" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E6%89%8B%E5%8A%A8%E6%89%93%E5%8C%85"></a>手动打包</h3><h4><a id="user-content-前端网页-1" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%89%8D%E7%AB%AF%E7%BD%91%E9%A1%B5-1"></a>前端网页</h4><p>1、修改根目录下 <code>.env</code> 文件中的 <code>VITE_GLOB_API_URL</code> 为你的实际后端接口地址</p><p>2、根目录下运行以下命令，然后将 <code>dist</code> 文件夹内的文件复制到你网站服务的根目录下</p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fcn.vitejs.dev%2Fguide%2Fstatic-deploy.html%23building-the-app">参考信息</a></p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pnpm build</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-常见问题" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"></a>常见问题</h2><p>Q: 为什么 <code>Git</code> 提交总是报错？</p><p>A: 因为有提交信息验证，请遵循 <a href="https://gitee.com/panday94/chat-master-web/blob/master/CONTRIBUTING.md">Commit 指南</a></p><p>Q: 如果只使用前端页面，在哪里改请求接口？</p><p>A: 根目录下 <code>.env</code> 文件中的 <code>VITE_GLOB_API_URL</code> 字段。</p><p>Q: 文件保存时全部爆红?</p><p>A: <code>vscode</code> 请安装项目推荐插件，或手动安装 <code>Eslint</code> 插件。</p><p>Q: 前端没有打字机效果？</p><p>A: 一种可能原因是经过 Nginx 反向代理，开启了 buffer，则 Nginx 会尝试从后端缓冲一定大小的数据再发送给浏览器。请尝试在反代参数后添加 <code>proxy_buffering off;</code>，然后重载 Nginx。其他 web server 配置同理。</p><h2><a id="user-content-参与贡献" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E5%8F%82%E4%B8%8E%E8%B4%A1%E7%8C%AE"></a>参与贡献</h2><p>贡献之前请先阅读 <a href="https://gitee.com/panday94/chat-master-web/blob/master/CONTRIBUTING.md">贡献指南</a></p><p>感谢所有做过贡献的人!</p><h2><a id="user-content-赞助" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E8%B5%9E%E5%8A%A9"></a>赞助</h2><p>如果你觉得这个项目对你有帮助，并且情况允许的话，可以给我一点点支持，总之非常感谢支持～</p><div><div><img src="https://gitee.com/panday94/chat-master-web/raw/master/docs/wechat.jpg" alt="微信" referrerpolicy="no-referrer"><p>WeChat Pay</p></div></div><h2><a id="user-content-联系我们" class="anchor" href="https://gitee.com/panday94/chat-master-web#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC"></a>联系我们</h2><div><div><img src="https://gitee.com/panday94/chat-master-web/raw/master/docs/mpqrcode.jpg" alt="公众号" referrerpolicy="no-referrer"><p>公众号</p></div><div><img src="https://gitee.com/panday94/chat-master-web/raw/master/docs/wxcode.jpg" alt="微信" referrerpolicy="no-referrer"><p>添加微信，加入交流群</p></div></div><h2><a id="user-content-license" class="anchor" href="https://gitee.com/panday94/chat-master-web#license"></a>License</h2><p>MIT © <a href="https://gitee.com/panday94/chat-master-web/blob/master/license">Master</a></p>]]>
            </description>
            <pubDate>Thu, 29 Feb 2024 02:00:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/panday94/chat-master-web</guid>
            <link>https://gitee.com/panday94/chat-master-web</link>
        </item>
        <item>
            <title>
                <![CDATA[零一万物发布 Yi 大模型 API 并启动公测，支持上下文 200K]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">零一万物通过其微信公众号<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FkSdGJh0xro9vm_LDDhMhgQ" target="_blank">宣布</a>，经过一段时间的开发和内测正式发布 Yi 大模型 API，同时启动邀测。目前，Yi 大模型 API 邀测名额限量开放中，申请成功即送 1000 万&nbsp;tokens。</span></p><p>此次邀测提供了两种模型：</p><ul><li>Yi-34B-Chat（0205）：支持聊天、问答、对话、写作、翻译等功能。</li><li>Yi-34B-Chat-200K：200K 上下文，多文档阅读理解、超长知识库构建小能手。</li></ul><h4><strong>模型优势</strong></h4><ul><li><strong>超长上下文</strong></li></ul><p>本次重磅出台 Yi-34B-Chat-200K API，加速大模型应用进入「长文本时代」。200K 支持处理约 20～30 万个中英文字符（例如，可以轻松处理整本《哈利•波特与魔法石》小说），适合用于多篇文档内容理解、海量数据分析挖掘和跨领域知识融合等，为各行各业提供了极大的便利。例如，金融分析师可以用它快速阅读报告并预测市场趋势、律师可以用它精准解读法律条文、科研人员可以用它高效提取论文要点、文学爱好者可以用它快速掌握作品精髓等，应用场景非常广泛。</p><p>例如，以下是 Yi-34B-Chat-200K 对经典文学作品《呼啸山庄》进行复杂角色和角色关系的归纳总结，该小说篇幅庞大（中文字数约 30 万字），且人物关系错综复杂，但它仍能精准地梳理和总结出人物之间的关系，展示了它在处理超长上下文时出色的复杂内容理解和分析能力。</p><p><img alt="" height="605" src="https://oscimg.oschina.net/oscnet/up-8036e1fcb930944e32404b4d2a35bd4bc33.png" width="300" referrerpolicy="no-referrer"></p><ul><li><strong>出色的指令遵循和创意内容生成能力</strong></li></ul><p>此前，零一万物发布并开源了 Yi-34B-Chat（1123），它的回复风格符合人类偏好，但在指令遵循上结果不够稳定。而此次新发布的 Yi-34B-Chat（0205）经过深度优化，性能得到大幅提升，不仅继承了符合人类偏好的回复风格，很擅长创意性内容创作，而且能够更好地理解复杂的用户需求，遵循多约束指令（指令遵循能力提升了近 30%），稳定生成指定格式的内容。</p><p>例如，以下是两个版本在指令遵循方面的测评对比。</p><p><strong>Prompt 1: 帮我输出一个俄国作家的书单，以 JSON 格式输出一个的 list，其中每一个 item 都要有两个 key，分别是书名和作家名字，请列出 3 本不同的书</strong></p><p><strong><img alt="" height="336" src="https://oscimg.oschina.net/oscnet/up-2d761ddabb83b7d03ee21cc74072f2431e1.jpg" width="500" referrerpolicy="no-referrer"></strong></p><p>Yi-34B-Chat（1123）输出的 JSON 文件格式略有不足（例如，第 8 行和第 12 行的引号），而 Yi-34B-Chat（0205）输出的 JSON 文件格式全部正确。</p><p><strong>Prompt 2: 判断下面这段话的情绪倾向，如果是正面的，回复数字 1；如果是负面的，回复数字 0：</strong></p><p><strong>这款手机真是物超所值，性能强大，电池续航长，外观设计也很有档次。我用了几个月，到现在还像新的一样。</strong></p><p><img height="169" src="https://oscimg.oschina.net/oscnet/up-e185cce9ff54b8737e02470009fe90a8d31.png" width="500" referrerpolicy="no-referrer"></p><p>Yi-34B-Chat（1123）虽然理解了问题，但是没有完全遵循指令，输出了较多冗余的分析。而 Yi-34B-Chat（0205）理解了问题，且正确遵循了用户指令。</p><h4><strong>API 优势</strong></h4><ul><li><strong>推理速度快</strong></li></ul><p>为了提升 API 性能，团队在 API 侧进行了推理优化，因此 Yi-34B-Chat 系列 API 具备较快的推理速度，这不仅缩短了处理时间，同时也保持了出色的模型效果。此外，优化的 API 接口显著降低了模型回复的延迟，进一步提高了用户体验的流畅性和响应速度。</p><ul><li><strong>兼容 OpenAI</strong></li></ul><p>Yi 大模型 API 与 OpenAI API 完全兼容，你只需修改少量代码，可以平滑迁移，即刻享受 Yi 大模型的超凡魅力。</p><pre><code>
import openai
from openai import OpenAI

API_BASE = "https://api.lingyiwanwu.com/v1"
API_KEY = "{{your key}}"

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key=API_KEY,
    base_url=API_BASE
)
completion = client.chat.completions.create(
    model="yi-34b-chat-200k",
    messages=[{"role": "user", "content": "Hi, who are you"}]
)
print(completion)</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 10:33:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280668</guid>
            <link>https://www.oschina.net/news/280668</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[WordPress 母公司 Automattic 计划出售数据给 OpenAI 等 AI 公司]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.404media.co%2Ftumblr-and-wordpress-to-sell-users-data-to-train-ai-tools%2F" target="_blank">404 Media 的一份报告显示</a></u>，Tumblr 和 WordPress.com 的所有者正在与人工智能公司 Midjourney 和 OpenAI 进行谈判，以提供从用户帖子中抓取的训练数据。</p><p><img height="1268" src="https://oscimg.oschina.net/oscnet/up-876500a00ea4d490602adc6cf3a01127f29.png" width="2282" referrerpolicy="no-referrer"></p><p>这份来自公司内部匿名消息人士的报告称，Automattic 与两家人工智能公司之间的交易「迫在眉睫」。过去一周，Tumblr 上流传着一些模糊的谣言，暗示与 Midjourney 的交易可能会为该网站带来新的收入来源。</p><p>根据报告，Automattic 计划在周三<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fautomattic.com%2F2024%2F02%2F27%2Fprotecting-user-choice%2F" target="_blank">推出一项新设置</a></u>，「允许用户选择不与包括人工智能公司在内的第三方共享数据」。</p><p>但它引用的内部帖子表明，该公司抓取了一份「初始数据转储」，其中包含「2014 年至 2023 年间 Tumblr 的所有公开帖子内容」，其中包括不会在博客上公开可见的内容——Automattic 称是错误抓取。目前尚不清楚这些数据做了什么，以及哪些数据已发送到 Midjourney 和 OpenAI。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 09:31:52 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280659/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools</guid>
            <link>https://www.oschina.net/news/280659/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[在 Zoom 会议中协作处理文档：适用于 Zoom 的 ONLYOFFICE 协作空间 app 现已发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">对 Zoom 用户的好消息！用于 Zoom 的应用程序现已发布，可使用 ONLYOFFICE 协作空间在 Zoom 中编辑和协作办公文件。请阅读本文了解详情。</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="" src="https://static-blog.onlyoffice.com/wp-content/uploads/2024/01/10155146/ONLYOFFICE-DocSpace-app-for-Zoom-available.png" referrerpolicy="no-referrer"></p><h2><strong>为</strong><strong>文档协作</strong><strong>而生</strong></h2><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">用于 Zoom&nbsp;的 ONLYOFFICE 协作空间 app 可以让您直接在 Zoom 会议中，与其他人一起处理文件：</p><ul><li>创建、上传和共享有编辑或实时查看权限的文档。</li><li>使用两种共同编辑模式（修订、评论）与其他参与者实时协同编辑文档、工作表、幻灯片。</li><li>使用集成的 AI 助手生成、总结和翻译文本。</li><li>讨论并填写复杂的在线表格，如销售协议或合同。</li><li>查看 PDF 文档，添加注释、注释、绘图。</li><li>将 Zoom 会议期间编辑的文件存储在协作房间中，并随时随地查看它们。</li></ul><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="在 Zoom 会议中协作处理文档：适用于 Zoom 的 ONLYOFFICE 协作空间 app 现已发布" src="https://static-blog.onlyoffice.com/wp-content/uploads/2024/02/28095320/16.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">此外，在第一次 app 授权后会创建&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.onlyoffice.com%2Fzh%2Fdocspace.aspx%3Futm_source%3Dblog%26utm_medium%3Darticle%26utm_campaign%3Ddocspace_zoom" target="_blank"><u>ONLYOFFICE 协作空间</u></a>账号，您可以用它来高效管理会议的文件：</p><ul><li>管理文件，并为工作文档创建协作或自定义房间。</li><li>设置灵活的访问权限：编辑、评论、审阅或仅查看。</li><li>创建公共房间，让外部人员也能访问您的文档。</li></ul><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="在 Zoom 会议中协作处理文档：适用于 Zoom 的 ONLYOFFICE 协作空间 app 现已发布" src="https://static-blog.onlyoffice.com/wp-content/uploads/2024/02/28095314/13.png" referrerpolicy="no-referrer"></p><h2><strong>完全免费</strong></h2><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">使用新发布的 app、创建协作空间帐户都是免费的。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">此外，在&nbsp;<strong>2024 年 8 月 1 日前</strong><strong>，</strong><strong>通过 Z</strong><strong>oom</strong><strong><span>&nbsp;</span>注册</strong><strong><span>&nbsp;</span>ONLYOFFICE 协作空间</strong>的用户都可以获得以下福利：</p><ul><li>100 个管理员用户</li><li>100 个房间</li><li>100 GB 磁盘空间</li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">注：本福利方案有效期为 6 个月（自注册之日起）。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">如果您有一个庞大的团队，需要更多的管理员或更大的存储空间，可以选择<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.onlyoffice.com%2Fzh%2Fdocspace-prices.aspx%3Futm_source%3Dblog%26utm_medium%3Darticle%26utm_campaign%3Ddocspace_zoom" target="_blank"><u>协作空间专业版</u></a>，它只需要为具有扩展权限的管理员和高级用户付费，可免费添加其他普通用户。</p><h2><strong>快速</strong><strong>上手</strong><strong>和使用</strong></h2><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>1.</strong>登录您的 Zoom 账户。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>2.</strong>在「应用程序」板块安装 ONLYOFFICE 协作空间，并允许请求的应用程序权限。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>3.</strong>开始会议，切换到「应用程序」选项卡，然后启动 ONLYOFFICE 协作空间 app。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>4.</strong>从列表中选择所需的文件来开启协作。您可以给其他参会者授予「编辑」或「实时查看」权限。</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="在 Zoom 会议中协作处理文档：适用于 Zoom 的 ONLYOFFICE 协作空间 app 现已发布" src="https://static-blog.onlyoffice.com/wp-content/uploads/2024/02/28095308/14-2.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>5.</strong>会议结束后，可在相应的 ONLYOFFICE 协作空间房间中查看文件的更改。</p><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:center"><img alt="在 Zoom 会议中协作处理文档：适用于 Zoom 的 ONLYOFFICE 协作空间 app 现已发布" src="https://static-blog.onlyoffice.com/wp-content/uploads/2024/02/28095325/15.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:center"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmarketplace.zoom.us%2Fapps%2FOW6rOq-nRgCihG5eps_p-g" target="_blank">免费获取应用程序</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>注</strong>：ONLYOFFICE 协作空间 app 与 Zoom 桌面应用程序也兼容。</p><h2><strong>常见问题</strong></h2><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>我需要在</strong><strong>哪里</strong><strong>注册才能使用该 app 吗？</strong></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">不用。当您开始使用该应用程序时，系统会根据您的 Zoom 数据（帐户 ID、姓名、电子邮件）自动创建一个免费的协作空间帐户。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>在创建的 ONLYOFFICE<span>&nbsp;</span></strong><strong>协作</strong><strong>空间帐户中可以做什么？</strong></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">您可以创建协作、自定义和公共房间、邀请用户、编辑各种文档、使用存储空间。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelpcenter.onlyoffice.com%2Fuserguides%2Fdocspace-index.aspx%3Futm_source%3Dblog%26utm_medium%3Darticle%26utm_campaign%3Ddocspace_zoom" target="_blank"><u>查看功能指南</u></a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>如果我已经有</strong><strong>协作</strong><strong>空间</strong><strong>帐户怎么办？</strong></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">您也可以使用该应用程序，但是月也会创建一个新的协作空间帐户，因为第一个应用程序版本不支持帐户同步。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>这种集成是免费的吗？</strong></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">用于 Zoom 的 ONLYOFFICE 协作空间的应用程序完全免费。创建的协作空间帐户也是免费版本，如有必要，可以切换到<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.onlyoffice.com%2Fzh%2Fdocspace-prices.aspx%3Futm_source%3Dblog%26utm_medium%3Darticle%26utm_campaign%3Ddocspace_zoom" target="_blank"><u>专业版协作空间</u></a>。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>会议结束后如何找到编辑</strong><strong>过</strong><strong>的文件？</strong></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">在协作会话期间对一个或多个文件所做的所有更改，都保存在自动创建的名为&nbsp;Zoom Collaboration_date_time 的协作房间中。</p><div><h3><strong>相关链接</strong></h3><p style="color:#333333; margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmarketplace.zoom.us%2Fapps%2FOW6rOq-nRgCihG5eps_p-g" target="_blank">用于 Zoom 的 ONLYOFFICE 协作空间 app</a></p><p style="color:#333333; margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelpcenter.onlyoffice.com%2Fintegration%2Fgettingstarted-zoom.aspx%3Futm_source%3Dblog%26utm_medium%3Darticle%26utm_campaign%3Ddocspace_zoom" target="_blank">相关指南</a></p><p style="color:#333333; margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.onlyoffice.com%2Fzh%2Fdocspace.aspx%3Futm_source%3Dblog%26utm_medium%3Darticle%26utm_campaign%3Ddocspace_zoom" target="_blank">ONLYOFFICE 协作空间</a></p><p style="color:#333333; margin-left:0; margin-right:0"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.onlyoffice.com%2Fzh%2Fall-connectors.aspx%3Futm_source%3Dblog%26utm_medium%3Darticle%26utm_campaign%3Ddocspace_zoom" target="_blank">查看所有的 ONLYOFFICE 集成</a></p></div></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 09:21:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280658</guid>
            <link>https://www.oschina.net/news/280658</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[华为发布通信行业首个大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">2 月 26 日至 2 月 29 日举行的世界移动通信大会（MWC24）期间，华为发布了由其自主研发的服务于通信行业的大模型。</span></p><p><span style="color:#000000"><img height="282" src="https://oscimg.oschina.net/oscnet/up-7be3f3a47ce7d8c9ff61ba6653dd16045d4.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">根据介绍，华为通信大模型是一款基于人工智能的商用大模型，提供关键的智能化技术能力，用于优化通信网络性能、智能调度资源等，旨在实现在 5G 技术基础上演进而来的 5G-A 时代智能化目标。</span></p><p><span style="color:#000000">华为董事、ICT 产品与解决方案总裁杨超斌介绍，华为通信大模型支撑运营商智能化目标，面向不同角色，提供智能语言交互能力，提升员工知识水平和工作效率；面向不同运营运维场景，提供智能体应用，分析拆解复杂流程，编排操作方案，确保用户体验和满意度。</span></p><p><span style="color:#000000">华为通信大模型具有众多典型场景实践。如在敏捷业务发放案例中，通过放号助手的多模态精准评估，实现了快速用户放号；在用户体验保障案例中，通过大模型的寻优能力，实现了多目标体验保障；在辅助排障场景下，跨流程的质差分析和对话辅助处理，显著改善了故障处理效率。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 08:31:36 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280646</guid>
            <link>https://www.oschina.net/news/280646</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenAI 称《纽约时报》曾雇人入侵 ChatGPT]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">OpenAI 要求驳回《纽约时报》于去年 12 月对其提起的版权诉讼的大部分内容，指控《纽约时报》花钱请人入侵 OpenAI 的产品，以在该案中制造误导性证据。《纽约时报》曾起诉 OpenAI 和微软公司，称这两家公司非法使用其受版权保护的材料来训练 AI 模型，分流了《纽约时报》网站的流量。</span></p><blockquote><p><span style="color:#000000">「《纽约时报》投诉中的指控不符合其著名的严格新闻标准。随着本案的进展，真相将水落石出，那就是《纽约时报》花钱雇人入侵了 OpenAI 的产品。他们花了数以万计的尝试，才产生了构成原告证据 J 的高度反常的结果。他们只有通过使用公然违反 OpenAI 使用条款的欺骗性提示，瞄准并利用一个漏洞（OpenAI 已承诺解决该漏洞），才能做到这一点。即便如此，他们还不得不向该工具提供他们想要获取的文章的逐字段落，而这些文章几乎都已出现在多个公共网站上。正常人不会以这种方式使用 OpenAI 的产品。」</span></p></blockquote><p><img height="259" src="https://oscimg.oschina.net/oscnet/up-3b5642691a2dd2818f5e24e4c20c0da1846.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">OpenAI 认为，与投诉中的指控相反的是，ChatGPT 无论如何都不能替代《纽约时报》的订阅。因为在现实世界中，人们不会也不能为此目的使用 ChatGPT 或任何其他 OpenAI 产品。</span></p><p><span style="color:#000000">且《纽约时报》近年来一直在跟进 OpenAI 聊天机器人的发展，却从未提出过任何有关版权侵权的担忧。OpenAI 声称，他们在 2020 年就披露了《纽约时报》的文章被用于训练其 AI 模型的这一事实，但该报却在 ChatGPT 于 2022 年首次亮相后人气爆棚时才开始关心此事，提起诉讼要求赔偿数十亿美元。</span></p><p><span style="color:#000000">OpenAI 还回应了《纽约时报》对 ChatGPT 提供付费文章访问权限的担忧；表示这种只是一个"罕见的错误"，目前正在努力修复中。并声称，"《纽约时报》没有完整阐述所有的全部故事。"</span></p><p><span style="color:#000000">对此，《纽约时报》的首席法律顾问 Ian Crosby 则<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farstechnica.com%2Ftech-policy%2F2024%2F02%2Fopenai-accuses-nyt-of-hacking-chatgpt-to-set-up-copyright-suit%2F" target="_blank">反驳称</a>，OpenAI 离奇地将《纽约时报》寻求证据的行为误解成了黑客行为。并补充称，OpenAI 的抄袭规模远远大于起诉书中 100 多个示例。「开发新产品不能成为违反版权法的借口。」</span></p><p><span style="color:#000000">详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffingfx.thomsonreuters.com%2Fgfx%2Flegaldocs%2Fbyvrkxbmgpe%2FOPENAI%2520MICROSOFT%2520NEW%2520YORK%2520TIMES%2520mtd.pdf" target="_blank">查看完整文件</a>。</span></p><p><strong><span style="color:#000000">相关阅读：</span></strong></p><ul><li><a href="https://www.oschina.net/news/274916/openai-and-journalism" target="_blank">OpenAI 称《纽约时报》的版权诉讼毫无根据</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 07:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280623/openai-new-york-times-lawsuit-hackin</guid>
            <link>https://www.oschina.net/news/280623/openai-new-york-times-lawsuit-hackin</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[蚂蚁百灵大模型推出 20 亿参数遥感模型 SkySense]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">蚂蚁百灵大模型推出了 20 亿参数多模态遥感基础模型 SkySense，这也是蚂蚁在多模态领域最新的研发成果。公开资料显示，SkySense 由蚂蚁 AI 创新研发部门 NextEvo 与武汉大学联合研发。</span></p><p><span style="color:#000000">SkySense 在总计 17 项国际权威公开数据集进行了测评，其测试任务类型包括了土地利用监测、高分辨率目标识别、地物变化检测等 7 种常见遥感感知任务，并与国际上已发布的包括 IBM 和 NASA 联合研发的 Prithvi 等共 18 个全球主流同类模型做了测试结果比较。</span></p><p><span style="color:#000000">数据显示，在 17 项测评中 SkySense 均名列第一。比如，在国际高清遥感地物检测榜单 FAIR1M 2.0 中，SkySense 平均精度（mAP）领先第二名超 3%。</span></p><p><span style="color:#000000"><img height="417" src="https://oscimg.oschina.net/oscnet/up-30d9098a9be808fb8043382fec2a6d88bcc.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">在蚂蚁百灵大模型多模态能力支持下，研发人员基于内部构建的 19 亿遥感影像数据集进行预训练，得到了 20.6 亿参数量的模型 SkySense。这也是迄今为止国际上参数规模最大、覆盖任务最全、识别精度最高的多模态遥感大模型。</span></p><p><span style="color:#000000">目前 SkySense 可广泛应用于城市规划、森林保护、应急救灾、绿色金融、农业监测等重要领域，目前通过蚂蚁内部 MEarth 平台提供数据与识别服务。&nbsp;</span></p><p><span style="color:#000000">据了解，蚂蚁集团正在计划开放 Skysense 模型参数，与行业共建，促进智能遥感技术与应用发展。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 06:15:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280605</guid>
            <link>https://www.oschina.net/news/280605</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[又一家硅谷明星公司误删库了]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>之前我们连续分析了两起误删库事件，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FShYpGMwrqige2bvmfO8ZRg" target="_blank">Linear 删库</a>，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FY7qAaYt2uIylqlPve9DGzg" target="_blank">GitLab 删库</a>。就在我们准备让这个主题告一段落时，业界又发生了一起删库事件。</p><p><img src="https://oscimg.oschina.net/oscnet/up-438613acc1f6667c828e7dbe14679e574cf.png" alt="file" referrerpolicy="no-referrer"></p><p>这次的主角是 Resend，也是最近硅谷冉冉升起的明星初创公司。想重塑邮件体验，挑战像 Mailchimp 这样的老牌玩家。</p><p><img src="https://oscimg.oschina.net/oscnet/up-59af844e75075e65e2d572ba7ba58a40bbd.png" alt="file" referrerpolicy="no-referrer"></p><p><strong>这次的删库事件依然是熟悉的配方，在执行数据库 schema 变更时，本来是针对本地环境执行，但结果命令发给了生产数据库，就这样把数据都删没了。</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-a35feb6af09f25ccb81dd4a7d6e38a21a59.png" alt="file" referrerpolicy="no-referrer"></p><p>而在恢复的过程中，第一次恢复使用了错误的备份，导致浪费了 6 个小时。又经过额外的 5 小时备份，才把数据库恢复过来，但还是有 5 分钟的数据丢失了。Resend 也列出了一些后续措施：</p><ul><li>恢复 5 分钟丢失的数据</li><li>收回所有用户对生产环境的写权限</li><li>改进本地开发流程，以降低数据库 schema 变更的风险</li><li>提高故障演练的频率</li></ul><p>也因为 Resend 小有名气，所以也引来了 Hacker News 上网友们的锐评：</p><p><img src="https://oscimg.oschina.net/oscnet/up-f09cddcfcfafcdbbf0bc92d56ac41656d0c.png" alt="file" referrerpolicy="no-referrer"></p><p>太业余了，像 email 这种核心组件，还是交给更加成熟的 AWS SES，Postmark，Sendgrid 这些吧。</p><p><img src="https://oscimg.oschina.net/oscnet/up-df0428cac45dece8b9cd009e4d6d2a3c10f.png" alt="file" referrerpolicy="no-referrer"></p><p>或许这家公司根本就不该存在。</p><h2>如何避免</h2><p>笔者认为这个故障虽然有点低级。但连错数据库这个事情，不算少见。备份过程碰到意外，也很常见。当然低级的问题，解决起来也不难。</p><p>针对第一点，引入像 Bytebase 这样的变更审核工具，所有针对生产环境的变更操作都要通过 Bytebase，经过人工审核后才能发布。</p><p><img src="https://oscimg.oschina.net/oscnet/up-ba37741baf890b551db462c259fd8f6b5d0.png" alt="file" referrerpolicy="no-referrer"></p><p>针对第二点，首先是采用云上的托管数据库服务，因为他们提供了完整的数据备份和恢复功能。另外就是定期做灾备演练。</p><p>大家也引以为戒吧。</p><hr><p>💡 更多资讯，请关注 Bytebase 公号：Bytebase</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 05:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6148470/blog/11045039</guid>
            <link>https://my.oschina.net/u/6148470/blog/11045039</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源日报 | MariaDB 消亡史；写代码我有三不沾；V 神建议马斯克用 Linux]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>欢迎阅读 OSCHINA 编辑部出品的开源日报，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.27</strong></span></h3><h2><strong><span style="color:#16a085">今日要点</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/280456/osi-delayed-open-source-publication-report" target="_blank">OSI 发布报告，研究 BSL 这样的 「延迟开源发布」</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Open Source Initiative（OSI）近期发布了一个报告《Delayed Open Source Publication: A Survey of Historical and Current Practices》（延迟开源发布：历史与当前实践调研）。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Delayed Open Source Publication，简称 DOSP，延迟开源发布的意思。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">结论：DOSP 自开源运动早期以来一直在使用，公司通常利用它来保持商业优势，同时尽可能保留开源的优势。报告强调，DOSP 的实验性和多样性比预期的要多，且这种趋势可能会继续。</p><h3><a href="https://www.oschina.net/news/280524" target="_blank">马斯克抱怨微软 Windows 难用，V 神：加入 Linux！</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img src="https://oscimg.oschina.net/oscnet/up-e5e820c2d6e02c58e0b272f4e8447623295.png" referrerpolicy="no-referrer"></p><hr><h2><strong><span style="color:#16a085">今日观察</span></strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-b918faeab5307706355aed13cc098be8979.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微信&nbsp;</span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkyMjQzOTkyMQ%3D%3D%26mid%3D2247484005%26idx%3D1%26sn%3D74c8079c36bc1a1a704a41f9d3dc48f9%26chksm%3Dc1f51bfbf68292ed01843d1451dc4170e814babd874c8b1661aa5c21e2bb9d99ad62da6ee348" target="_blank">IT 知识刺客</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-7376d45caae71d8c9a249f35fb238c25d26.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博&nbsp;</span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2689166291%2FO2vRh7Twn" target="_blank">归零归零归 ww</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推荐</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-505c94c6606e7aefd93a64ce5f75063c652.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>开源之声</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-ddca260d14744131b56cecc93028c61a364.png" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-271b8a5abd0cf2553fb727bb13de466f0c2.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日项目榜</strong></span></h2><p><strong><span style="background-color:#e67e22">每日 Gitee 精选</span></strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-618659c15ef289866a177fc7ccfe8e89e53.png" referrerpolicy="no-referrer"></p><blockquote><h4><strong><span style="background-color:#e67e22">在线阅读完整日报内容，访问：</span></strong><br><u><em><strong><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/7r8dkz3232v4e7a/17_maria_db_v_linux_GoyNoM85IZ.pdf">开源日报第 017 期：MariaDB 消亡史；写代码我有三不沾；V 神建议马斯克用 Linux</a></strong></em></u></h4></blockquote><hr><p><strong>往期回顾</strong></p><ul><li><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/6typ9w3u98f5mxn/16_1_8_2efTeNfFjN.pdf">开源日报第 016 期：鸿蒙程序员平均月薪超 1 万 8；中美 AI 差距有多大？</a></li><li><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/92n4c9ryegpcq1z/015_sora_KcAkRNX93Y.pdf">开源日报第 015 期：为什么挡不住英伟达；Sora 不靠蛮力</a></li><li><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/s7n800w84o6guyv/014_kyezhNxOGD.pdf">开源日报第 014 期：目前的人工智能技术连猫的智能水平都没达到</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC013%E6%9C%9F%EF%BC%9A%E7%AD%89%E5%88%B0%20Sora%20%E5%BC%80%E6%BA%90%E4%BA%86%E7%AB%8B%E5%88%BB%E6%8E%A8%E5%87%BA%E5%B1%9E%E4%BA%8E%E6%88%91%E4%BB%AC%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B.pdf">开源日报第 013 期：等到 Sora 开源了立刻推出属于我们自己的大模型</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">开源日报第 012 期：Sora 给中国 AI 带来的真实变化；Dart 3.3 发布</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">开源日报第 011 期：目前还没有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">开源日报第 010 期：Tauri v2 支持 Android 和 iOS，跨平台开发新选择</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">开源日报第 009 期：Vue.js 诞生 10 周年；扎克伯格解释 Meta 为什么要开源其 AI 技术</a></li><li><a href="https://www.oschina.net/news/277585">开源日报第 008 期：推动中国开源软硬件发展的经验与建议</a></li><li><a href="https://www.oschina.net/news/277415">开源日报第 007 期：「Linux 中国」 开源社区宣布停止运营</a></li><li><a href="https://www.oschina.net/news/277214">开源日报第 006 期：选择技术栈一定要选择开源的</a></li><li><a href="http://www.oschina.net/news/277040">开源日报第 005 期：RISC-V 万兆开源交换机发售；npm 存在大量武林外传视频</a></li><li><a href="https://www.oschina.net/news/276864">开源日报第 004 期：百度输入法在候选词区域植入广告；大神用 Excel 构建 CPU</a></li></ul><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 04:12:01 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280594</guid>
            <link>https://www.oschina.net/news/280594</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[国人独立开发的开源 Redis 客户端 ioredis 被 Redis 公司收购]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#333333">ioredis 作者&nbsp;</span><a href="https://my.oschina.net/u/1051352" target="_blank">@Luin</a><span style="background-color:#ffffff; color:#333333">&nbsp;宣布该项目已被 Redis 公司收购。</span>ioredis 是一个用于 Node.js 的 Redis 客户端，健壮、性能好、功能强大且全面。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-c7b9c050507e824beecc36c4767b126c712.png" referrerpolicy="no-referrer"></p></blockquote><p>目前&nbsp;ioredis 在 GitHub 的开源地址已迁移至&nbsp;Redis 公司旗下：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fredis%2Fioredis" target="_blank">https://github.com/redis/ioredis</a></u></em></p><p><img height="601" src="https://oscimg.oschina.net/oscnet/up-be0cf491613bb1c935a0bcfa6354508563c.jpg" width="1170" referrerpolicy="no-referrer"></p><p>两年前，ioredis <u><a href="https://www.oschina.net/news/208601">超过 </a></u>redis 成为了 Node.js 最流行的 Redis 客户端。当时&nbsp;<span style="background-color:#ffffff; color:#333333">ioredis 作者</span>还感叹&nbsp;redis 历经诸多波折终被 Redis 官方收购。</p><p><img src="https://oscimg.oschina.net/oscnet/up-b93256e806a38a0bede1296043170bf2947.png" referrerpolicy="no-referrer"></p><p><a href="https://my.oschina.net/u/1051352" target="_blank">@Luin</a><span style="background-color:#ffffff; color:#333333">&nbsp;曾表示&nbsp;</span>ioredis 是自己独立从零开发的项目，创建初衷也很「极客」——没找到满意的开源库，所以决定自己动手干。历经 9 年，从个人的&nbsp;side project 到被开源公司收购，吾辈楷模！</p><blockquote><p>2014 年底的时候我开始使用 Node.js 开发后端程序。为了连接 Redis ，所以研究了下市面上的 Redis 客户端库。当时最流行的库 redis 是由 Uber 的首席架构师 Matt Ranney 开发的。使用后发现这个库有一些让自己不满意的地方：</p><ol><li>不支持 Promise （当时 Promise 还是个非常新的概念）</li><li>命令语法不太美观（个人审美差异😄）</li><li>功能不齐全：缺少 Cluster 、Sentinel 等 Redis 新功能的支持。</li></ol><p>由于当时正好有点闲暇时间，就自己从零开发并开源了 ioredis 。</p></blockquote><p>延伸阅读：<em><u><a href="https://www.oschina.net/news/208601" target="_blank">ioredis 成为最流行的 Node.js Redis 库</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280584/redis-ltd-acquire-ioredis</guid>
            <link>https://www.oschina.net/news/280584/redis-ltd-acquire-ioredis</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[任天堂起诉 Switch 模拟器 Yuzu 开发者]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">美国任天堂公司正在对流行模拟器工具 Yuzu 背后的开发商 Tropic Haze LLC 提起诉讼，控告其「大规模侵犯任天堂和其他人版权作品知识产权」。</span></p><p><img height="315" src="https://oscimg.oschina.net/oscnet/up-7661078344ba837f79511462e7252c9b144.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">该公司在最新提交的一份法律<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdockets.justia.com%2Fdocket%2Frhode-island%2Fridce%2F1%3A2024cv00082%2F56980%3Fref%3Doverkill.wtf" target="_blank">文件</a>中指出，Tropic Haze LLC 非法规避了任天堂 Switch 游戏的软件加密和版权保护系统，从而助长了盗版行为，侵犯了《数字千年版权法案》（DMCA）规定的版权。并表示，其《塞尔达传说：王国之泪》在正式零售发售之前就因盗版被下载了超过一百万份。</span></p><p><span style="color:#000000">任天堂声称，Yuzu 的主要开发者已公开承认 Yuzu 网站向用户提供了指导，教他们如何入侵任天堂 Switch 游戏机，以及如何未经授权复制任天堂视频游戏。同时，任天堂还强调了 Yuzu 从中的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.patreon.com%2Fyuzuteam%3Fref%3Doverkill.wtf" target="_blank">获利</a></span><span style="color:#333333">；</span><span style="color:#000000">该项目目前已经得到了 7000 多名会员的支持，每月收入接近 3 万美元。</span></p><p><span style="color:#333333">因此，任天堂现在正在寻求获得损失赔偿，要求对每项违 </span><span style="color:#000000">DMCA&nbsp;</span><span style="color:#333333">反规避和反贩运条款的行为赔偿 2,500 美元，对每项侵犯版权的行为赔偿 150,000 美元。以及要求法院查封、扣押和销毁 Yuzu 模拟器的所有副本，和任天堂认为侵犯其版权的软件和硬件，并提出了"......立即将域名 yuzu-emu.org ......移交任天堂控制"的需求。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:39:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280580/nintendo-sue-yuzu-emulator</guid>
            <link>https://www.oschina.net/news/280580/nintendo-sue-yuzu-emulator</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[MFiX —— 开源多相流 CFD 软件]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>MFiX 是美国能源部开发的一款用于模拟颗粒流体多相流的开源软件，CFD 部分使用 SIMPLE 算法，而颗粒部分包含了 TFM、MPPIC 以及 DEM 等模型，且可以模拟连续相和离散相之间的传质传热。</p><p>MFiX 基于 fortran 语言开发，核心特性包括：并行、开源、跨平台、一键安装、用户图形界面、支持 TFM/DEM/PIC 多种模型。</p><ul><li>跨平台：该软件支持 Windows\Linux\macOS，使得用户既能在 windows 下学习、测试算例，也可以直接在超算平台上进行计算</li><li>一键安装：该开源软件可以通过 anaconda 方便的一键安装，解决了大规模开源软件难以编译安装的痛点</li><li>用户图形界面：该软件支持用户图形界面，可以方便的设置计算参数、监控计算结果。</li><li>Cutcell 网格：该软件采用 cutcell 网格处理复杂结构，可以通过 stl 文件或软件内置的几何结构生成器构建模拟计算区域。</li><li>物性数据库：软件自带物性数据库，方便传热、化学反应的计算</li><li>Stiff Chem Solver: 燃烧等化学反应的特征时间远远小于流动的特征时间，通过 stiff 化学求解器分步求解化学反应和流体流动，该软件可以方便的模拟多相反应流动</li><li>该软件自带多个演示算例，学习曲线平滑</li><li>模型丰富：采用 SIMPLE 算法的流体求解器、双流体 TFM 求解器、离散单元法 DEM 颗粒模拟、CFD-DEM 模拟、MP-PIC 模拟</li><li>MPI 多机并行，方便在超算平台上大规模计算</li></ul><p><strong>运行截图</strong></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-be3c669b3b0de4ac6c4bd620524919062f8.png" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:21:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/mfix</guid>
            <link>https://www.oschina.net/p/mfix</link>
        </item>
        <item>
            <title>
                <![CDATA[拓数派联手开源联盟 PG 分会，走进北京大学研究生公选课]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>为促进基础软件在中国高校的传播，进一步提高在校研究生对基础软件的学习和开发实践能力，培养数据库研发人才，<strong>拓数派联手开源联盟 PG 分会，走进北京大学</strong>，同国家特色化示范性软件学院：北京大学软件与微电子学院合作，进行了 2024 年《北京大学 PostgreSQL 内核开发：从入门到进阶》研究生公选课的打造与授课。</p><p style="text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-b22e29aa29909713ba2a7862c06e3909db1.png" referrerpolicy="no-referrer"></p><p style="text-align:center">公选课讲师及校方合影</p><p>本次课程由北京大学荆琦教授联合中国开源软件推进联盟（COPU）组织发起，面向国内一流技术企业收集优秀课程，已成功开展了 3 年。<strong>2024 年，拓数派联手 PG 开源分会精心组织了新学期的数据库内核开发从入门到进阶内容，经过评审成功进入北京大学研究生开源开发实践公选课框架。</strong> 公选课开课报告会已于 2 月 20 日成功举行。</p><p style="text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-fa96e68a8eefabc87ccf8c5eea797339cb6.png" referrerpolicy="no-referrer"></p><p style="text-align:center">公选课开课报告会合影</p><p>本次课程时长 16 周，共包括 32 个课时内容，针对数据库的数据加密、数据存取和优化器原理与实践三部分内容展开讲授。<strong>其中拓数派产品市场总监吴疆作为《优化器原理与实践》部分的讲师，结合云原生虚拟数仓 PieCloudDB Database 在云原生优化器的打造经验，</strong> 将以开源数据库 PostgreSQL 作为实操数据库，针对查询优化器的基本原理和工作流程展开授课。通过四周的学习，学生将学会如何使用统计信息和成本模型来评估不同的查询执行计划，并选择最佳的执行路径。我们还将讨论常见的查询优化技术，包括索引选择、连接算法和谓词下推等。</p><p style="text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-590815f6a6a18c08c611006284b9f326ed2.png" referrerpolicy="no-referrer"></p><p style="text-align:center">吴疆在开课报告会上发表演讲</p><p>拓数派一直致力于促进产学研合作，通过校园行系列活动「校园 Pie」的组织与打造、高校课程的合作、联合实验室的创建等多种方式，希望通过前沿技术、产业界案例和应用的分享，促进学术界与产业界的进一步融合，为数据库从业人才的培养和交流平台的打造提供更多的支持。新的一年，拓数派将不断努力，在产品、商业和生态的打造上继续前行！</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:16:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280576</guid>
            <link>https://www.oschina.net/news/280576</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 小红书自主研发的跨平台播放器 REDPlayer]]>
            </title>
            <description>
                <![CDATA[<h1><a id="redplayer" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#redplayer"></a>REDPlayer</h1><p><img src="https://gitee.com/rte-dev/RedPlayer/raw/main/redplayer.jpg" alt="示例图片" referrerpolicy="no-referrer"></p><p><img src="https://img.shields.io/badge/release-v1.0.0-blue" alt="GitHub release (latest by date including pre-releases)" referrerpolicy="no-referrer"><img src="https://img.shields.io/badge/license-LGPL2.1-blue" alt="GitHub license" referrerpolicy="no-referrer"></p><h2><a id="about-redplayer" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#about-redplayer"></a>About REDPlayer</h2><p>REDPlayer 是一款由小红书自主研发的跨平台 (支持 Android、iOS、HarmonyOS 等平台) 播放器。不同于行业其他播放器，REDPlayer 具有结构简单、耦合度低、功能边界清晰等特点，提供了多种接入方式，技术人员可根据需要灵活选择，既可快速集成 SDK 使用，也可基于源码进行定制开发。</p><p>REDPlayer 的宗旨是让开发者可以快速明确地了解播放器的基本构造，并可根据个人需求进行简单扩展，满足不同用户的多样需求，可作为学生学习的基础工具，也可作为企业的商用平台。</p><p>REDPlayer 支持点播、直播场景下的多种协议和格式 (如 HLS、MP4、FLV 等)，并可二次扩展更多协议 (如：RTC 等)。每个模块均是解耦的，开发者可以根据需要挂载自定义模块，如自研解码器、渲染器等。</p><table><thead><tr><th>Platform</th><th>Build Status</th></tr></thead><tbody><tr><td>Android</td><td>Done</td></tr><tr><td>iOS</td><td>Done</td></tr><tr><td>others</td><td>In Coming</td></tr></tbody></table><h3><a id="quickstartdemo" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#quickstartdemo"></a>Quickstart/Demo</h3><ul><li><p>Android <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/android/README.md">Quickstart</a>/ <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/android/app/README.md">Demo</a></p></li><li><p>IOS <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/ios/README.md">Quickstart</a>/ <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/ios/RedPlayerDemo">Demo</a></p></li><li><p>In coming...</p></li></ul><h3><a id="features" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#features"></a>Features</h3><table><thead><tr><th>Function</th><th>Function Description</th><th>Current Support Situation</th><th>Other Notes</th></tr></thead><tbody><tr><td>Rich Format</td><td>Supports rich audio and video formats such as FLV, HLS, MP4, MP3, and Vorbis</td><td>✅</td><td></td></tr><tr><td>DASH Protocol</td><td>Supports standard protocol DASH</td><td>✅</td><td>Optimized version of DASH for on-demand support in the later stage</td></tr><tr><td>HDR</td><td>Supports multiple HDR formats such as HDR10/HLG. Distribution and playback support are provided according to the model</td><td>✅</td><td></td></tr><tr><td>URL Playback</td><td>Supports playback of local and network videos via URL</td><td>✅</td><td></td></tr><tr><td>Log Reporting</td><td>Supports reporting player logs and statistics related to playback point information</td><td>✅</td><td></td></tr><tr><td>Abnormal Analysis</td><td>Supports obtaining corresponding abnormal information through log analysis</td><td>✅</td><td></td></tr><tr><td>H.264 Playback &amp; Hardware Decoding</td><td>Supports H.264 video sources and hardware decoding</td><td>✅</td><td></td></tr><tr><td>H.265 Playback &amp; Hardware Decoding</td><td>Supports H.265 video sources and hardware decoding</td><td>✅</td><td>Software decoding capabilities will be supported in the later stage</td></tr><tr><td>Automatic switching between software and hardware decoding</td><td>Automatically switches to software decoding when the terminal does not support hardware decoding</td><td>✅</td><td></td></tr><tr><td>Playback Control</td><td>Supports playback control functions such as start, end, pause, and resume</td><td>✅</td><td></td></tr><tr><td>Accurate Seeking</td><td>Supports accurate seeking to a specified position, which can be accurate to the frame level</td><td>✅</td><td></td></tr><tr><td>Dynamic Dropping</td><td>Start dynamic dropping when the frame rate exceeds 60 fps</td><td>✅</td><td></td></tr><tr><td>Replay</td><td>Supports manually triggered replay after the video ends</td><td>✅</td><td></td></tr><tr><td>Continue playing</td><td>Supports setting the continuous playing time point</td><td>✅</td><td></td></tr><tr><td>Loop Playback</td><td>Supports automatic replay after video playback ends</td><td>✅</td><td>Parameter configuration is required</td></tr><tr><td>Variable Speed Playback</td><td>Supports variable speed playback of 0.5-2 times, and the audio 实现 variable speed without changing the pitch</td><td>✅</td><td></td></tr><tr><td>Definition Adjustment</td><td>Supports switching between multiple definitions for on-demand and transcoding</td><td>✅</td><td></td></tr><tr><td>Seeking within the Cache</td><td>Supports seeking without clearing the buffer for cached video content</td><td>✅</td><td></td></tr><tr><td>Packing Mode</td><td>Supports picture cropping and filling</td><td>✅</td><td></td></tr><tr><td>Private DRM</td><td>Supports private DRM encryption schemes</td><td>✅</td><td></td></tr><tr><td>Adaptive Bitrate</td><td>When playing HLS/DASH, it supports automatically selecting the definition for playback through bandwidth prediction</td><td>✅</td><td>Currently only supports selection before playback, and does not support abr during playback</td></tr><tr><td>Volume Settings</td><td>Supports real-time adjustment of system volume and mute operation</td><td>✅</td><td></td></tr><tr><td>Pure Audio Playback</td><td>Supports playing audio only</td><td>✅</td><td></td></tr><tr><td>Preload</td><td>Supports setting the preload size to reduce the time spent on the first screen</td><td>✅</td><td></td></tr><tr><td>Play While Downloading</td><td>Supports playing while caching and downloading subsequent content, and you can set network policies</td><td>✅</td><td></td></tr><tr><td>Playback Callback</td><td>Supports playback status callback, first frame callback, playback completion or failure callback</td><td>✅</td><td></td></tr><tr><td>Retry on Playback Failure</td><td>Automatically retries on playback failure</td><td>✅</td><td>Only supports retries for non-4XX and 5XX classes</td></tr><tr><td>Real-time Download Speed</td><td>Supports getting real-time download speed</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>Encrypted Streaming PlayBack</td><td>Support for on-demand transcoding of encrypted streams</td><td>❌</td><td>Need for custom development</td></tr><tr><td>Screenshot Function</td><td>Support for capturing any frame of the playback picture</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>Thumbnail Preview</td><td>Support for previewing progress bar thumbnails (sprite map)</td><td>❌</td><td>Related to business, not currently supported</td></tr><tr><td>Set player size</td><td>Support for customizing the width and height of the player</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>External subtitles</td><td>Support for two docking modes of external subtitles: full-link solution and pure client solution</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>Client super-resolution</td><td>The client performs super-resolution enhancement on low-quality videos</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>H.266 playback</td><td>Support for video playback in H.266 encoding format</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>AV1 playback</td><td>Support for video playback in AV1 encoding format</td><td>❌</td><td>Will be supported in later versions</td></tr></tbody></table><h3><a id="open-content" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#open-content"></a>Open Content</h3><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">```bash</span><span id="LC2" class="line"># Describe the main contents of current open source and the estimated time and contents of the next open source</span><span id="LC3" class="line">```</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li><a href="https://gitee.com/rte-dev/RedPlayer/blob/main/CONTENTS.md">CONTENTS.md</a></li></ul><h3><a id="usage" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#usage"></a>Usage</h3><ul><li>You can directly integrate your project by calling the interface or compile independently.</li><li><a href="https://gitee.com/rte-dev/RedPlayer/blob/main/INTERFACES.md">INTERFACES.md</a></li></ul><h3><a id="build-environment" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#build-environment"></a>Build Environment</h3><ul><li><p><strong>Install Homebrew &amp; Git</strong></p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"> /bin/bash <span class="nt">-c</span><span class="s2">"</span><span class="si">$(</span>curl <span class="nt">-fsSL</span> https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span class="si">)</span><span class="s2">"</span></span><span id="LC2" class="line"> brew <span class="nb">install </span>git</span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li><li><p><strong>Build Android</strong></p><p><strong>Using Android SDK</strong></p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fdeveloper.android.com%2Fstudio%3Fhl%3Den">Andrioid SDK</a> is android project base dependency. You should download and then config with the following command:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># add this line to your ~/.bash_profile or ~/.profile, the android sdk will work</span></span><span id="LC2" class="line"><span class="nb">export </span><span class="nv">ANDROID_SDK</span><span class="o">=</span>&lt;your sdk path&gt;</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># My build environment:</span></span><span id="LC5" class="line"><span class="c"># macOS 14.0</span></span><span id="LC6" class="line"><span class="c"># Android Studio Flamingo | 2022.2.1 Patch 2</span></span><span id="LC7" class="line"><span class="c"># gradle version: 7.5.0</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li><li><p><strong>Build iOS</strong></p><p><strong>Using CocoaPods</strong></p><p><a href="https://gitee.com/link?target=http%3A%2F%2Fcocoapods.org">CocoaPods</a> is a dependency manager for Cocoa projects. You can install it with the following command:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nv">$ </span>gem <span class="nb">install </span>cocoapods</span><span id="LC2" class="line"></span><span id="LC3" class="line"><span class="c"># My build environment:</span></span><span id="LC4" class="line"><span class="c"># macOS 14.0</span></span><span id="LC5" class="line"><span class="c"># Xcode 15.2 (15C500b)</span></span><span id="LC6" class="line"><span class="c"># Cocoapods version: 1.10.2</span></span><span id="LC7" class="line"><span class="c"># Ruby 3.0.6p216</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li></ul><h3><a id="latest-changes" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#latest-changes"></a>Latest Changes</h3><ul><li><a href="https://gitee.com/rte-dev/RedPlayer/blob/main/NEWS.md">NEWS.md</a></li></ul><h3><a id="support" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#support"></a>Support</h3><ul><li>Please try to discuss technical issues (<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FRTE-Dev%2FRedPlayer%2Fissues">https://github.com/RTE-Dev/RedPlayer/issues</a>) publicly on github, and do not inquire privately by email. We will not reply one by one.</li></ul><h3><a id="licence" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#licence"></a>Licence</h3><h4><a id="self-licence" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#self-licence"></a>Self Licence</h4><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Copyright (c) 2024 xiaohongshu</span><span id="LC2" class="line">Licensed under LGPLv2.1 or later</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="dependence-licence" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#dependence-licence"></a>Dependence Licence</h4><ul><li>ffmpeg: LGPL v2.1+</li><li>soundtouch: LGPL v2.1</li><li>libcurl: MIT License</li><li>c-ares: MIT License</li><li>glide: MIT License</li><li>Masonry: MIT License</li><li>openssl: Apache License 2.0</li><li>PictureSelector: Apache License 2.0</li></ul><h3><a id="law-and-rule" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#law-and-rule"></a>Law And Rule</h3><p>All rights and explanations belong to Xiaohongshu，you should always ask your lawyer for these stuffs before use it in your product.</p>]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/rte-dev/RedPlayer</guid>
            <link>https://gitee.com/rte-dev/RedPlayer</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 语言大模型的浮点运算分配]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p><img height="352" src="https://oscimg.oschina.net/oscnet/203eda90-2b91-4ed0-9fbf-02459d2253d5.jpg" width="578" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px; text-align:left"><span>本文</span><span>通过实证分析展示了实际 LLM 模型的 FLOPS 分配情况，并与理论分析进行对比。</span><span>通过理论和实证相结合的方式，本文为理解和优化语言大模型的性能提供了有益见解。</span></p><p>&nbsp;</p><p><span>作者 Finbarr Timbers 是一名机器学习研究员，曾就职于 DeepMind。（以下内容由 OneFlow 编译发布，转载请联系授权。原文：<span style="background-color:#efefef">https://www.artfintel.com/p/where-do-llms-spend-their-flops</span></span><span><span>）</span></span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">作者 |&nbsp;Finbarr Timbers</span></strong></p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">OneFlow 编译</span></strong></p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">翻译｜宛子琳、杨婷</span></strong></p><p>&nbsp;</p><p><span style="color:#3f3f3f">本文对 LLM 的性能进行了理论分析，然后通过详细分析一个实际的 LLM，查看实证结果与理论之间的差异。首先是理论分析部分，我会借助</span><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247491309%26idx%3D1%26sn%3D407fefb7ec76a2c9fdfe1ae960f7de4d%26chksm%3Dfe4190dbc93619cd0b9bfecb979e142a125fd8548d323dd9bdc2cbeb619400202e6e1affced0%26scene%3D21%23wechat_redirect" target="_blank"><strong><span>Kipply 的优质博文</span></strong></a><span style="color:#3f3f3f">来填充细节。基本结论是：对于标准解码器模型，FLOPS（每秒浮点运算数）的分配如下（按每层计算）：</span></p><p>&nbsp;</p><ol><li><p><span style="color:#3f3f3f">6d^2 用于计算 QKV（Query（查询）、Key（键）和 Value（值））</span></p></li><li><p><span style="color:#3f3f3f">2d^2 用于计算注意力输出矩阵，softmax(Q @ K.T) @ V</span></p></li><li><p><span style="color:#3f3f3f">16d^2 用于运行前馈神经网络（FFN）&nbsp;</span></p></li></ol><p>&nbsp;</p><p><span style="color:#3f3f3f">总计 24d^2 FLOPS。从百分比看，25% 的时间用于计算 QKV，约 8% 的时间用于计算注意力输出矩阵，约 66% 的时间用于运行 FFN。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">那么用于注意力机制的时间呢？众所周知，注意力机制方程为：</span></p><p>&nbsp;</p><p style="text-align:center"><img height="84" src="https://oscimg.oschina.net/oscnet/fec217d2-600c-4ed3-850a-7407dd0b583b.jpg" width="315" referrerpolicy="no-referrer"></p><p style="text-align:left">&nbsp;</p><p style="text-align:left"><span style="color:#3f3f3f">假设你正在使用 KV 缓存，Q（查询）、K（键）和 V（值）都是 d 维向量（等价于（d，1）矩阵）。每个点积大约需要 2d 个 flops（</span><span style="color:#888888"><em><span>https://www.stat.cmu.edu/~ryantibs/convexopt-F18/scribes/Lecture_19.pdf</span></em></span><span style="color:#3f3f3f">），加上进行 d 次除法需要 d 个 flops，总计约为 5d 个 flops，四舍五入为零。</span></p><p>&nbsp;</p><p style="text-align:center"><img height="112" src="https://oscimg.oschina.net/oscnet/4c742fa6-51fa-4652-bcf4-06fcb9d9263e.png" width="253" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span style="color:#3f3f3f">当 d 等于 4096（在 Llama7b 中的取值），这仅为 0.005%，几乎可以忽略不计。这似乎表明注意力机制不重要，但事实并非如此。我们之所以使用 KV 缓存（以及 flash attention 等）正是因为它们非常重要，可以将其类比于米尔顿·弗里德曼的恒温器（</span><span style="color:#888888"><em><span>https://worthwhile.typepad.com/worthwhile_canadian_initi/2010/12/milton-friedmans-thermostat.html</span></em><span>，感谢 @bradchattergoon</span></span><span style="color:#3f3f3f">）：</span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span>假设一个房屋配备了一个运行良好的恒温器，那么我们能看到炉子燃烧的油量（M）与室外温度（V）之间存在强烈的负相关关系，同时炉子燃烧的油量（M）与室内温度（P）之间没有相关性，此外，室外温度（V）与室内温度（P）之间也没有相关性。</span></em></span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span>一位计量经济学家观察数据后得出结论：燃烧的油量对室内温度没有影响，室外温度对室内温度也没有影响。唯一的影响似乎是燃烧油量会降低室外温度。</span></em></span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span>观察相同的数据，第二位计量经济学家得出了完全相反的结论。他认为，室外温度（V）增加唯一的影响是会减少耗油量（M），而不会对室内温度（P）产生任何影响。</span></em></span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span style="color:#888888">尽管两位计量经济学家得出了不同的结论，但他们一致认为燃烧油量（M）和室外温度（V）对室内温度（P）没有影响。基于这一共识，他们决定关闭炉子，不再浪费金钱购买燃油。</span></em></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">KV 缓存需要 O(T) 的内存（其中 T 是我们希望生成的词元数），因此内存需求成本较高，这一点可以参考公司股票（$NVDA）情况。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">KV 缓存有多大呢？对于每个词元，需要存储以下数量的字节（第一个 2 是因为我们假设使用 bf16 精度，因此每个参数占用 2 个字节；第二个 2 是因为需要同时存储 K 和 V 张量）：</span></p><p style="text-align:center"><img height="77" src="https://oscimg.oschina.net/oscnet/64861c4b-a4cb-4b5a-94e8-85c4b0b6f198.jpg" width="415" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span style="color:#3f3f3f">注意，根据假设，n_heads*d_head=d_model=d，因此字节数为 4*层数*d。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">GPT-3 有 96 层，d_model 为 12288，每个词元需要 4.72MB。因此，生成 2048 个词元需要 5.6GB 的内存。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">尽管如此，要使用给定模型生成给定长度的序列，我们仍需使用与 KV 缓存相同的内存量，只是在每次前向传播结束时将其丢弃。因此，我们并不需要更多内存。从某种意义上说，KV 缓存不占用内存（至少在 Jax 中是如此，除了一些繁琐的 bookkeeping 工作）。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">对于一些新兴架构（例如 Mistral 7B）又有何不同呢？Mistral 7b 使用了分组查询注意力（Llama2 也使用了类似的注意力机制，就好像这两个模型的作者存在某种联系。）和滑动窗口注意力。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">在分组查询注意力中，我们可以在多头之间共享一个 KV 投影（MQA），具体而言，可以是所有注意力头之间共享一个 KV 投影（MQA，</span><span style="color:#888888"><em><span>https://arxiv.org/abs/1911.02150</span></em></span><span style="color:#3f3f3f">），或者将其分成多个组（GQA，</span><span style="color:#888888"><em><span>https://arxiv.org/abs/2305.13245v</span></em><span>3</span></span><span style="color:#3f3f3f">）。这两种方法都等同于具有较小 d_model 的标准多头注意力（MHA）。在之前的 KV 缓存计算中，我们假设注意力头的数量乘以头的维度等于模型维度，但是在 MQA/GQA 中，我们放宽了这一假设。KV 缓存公式如下：</span></p><p>&nbsp;</p><p style="text-align:center"><img height="66" src="https://oscimg.oschina.net/oscnet/5062dc02-0be8-4534-b2b4-39320bbfa8a9.jpg" width="345" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span style="color:#3f3f3f">可以转换为：</span></p><p style="color:#494949">&nbsp;</p><p style="text-align:center"><img height="71" src="https://oscimg.oschina.net/oscnet/96864198-3330-4aab-8f30-22d5cd408e8e.jpg" width="332" referrerpolicy="no-referrer"></p><p style="color:#494949">&nbsp;</p><p><span><span style="color:#3f3f3f">其中，注意力头的数量乘以头的维度就是模型的有效维度。因此，可以看到，随着 KV 头数量的减少，KV 缓存大小呈线性减小（ 这也是 GQA/MQA 方法背后的关键动机之一）。</span></span></p><p>&nbsp;</p><p><span><span style="color:#3f3f3f">Llama{1,2} 模型参数如下：</span></span></p><p style="color:#494949">&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/7a2e216e-dbda-47b6-96f4-17d8c0b5f9b6.png" width="1060" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span><span><span style="color:#3f3f3f">Llama 2 中，每个词元所需的 KV 缓存如下：</span></span></span></p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/0acb1a24-9589-4d7e-bc1f-6be3cbe3a123.png" width="auto" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">在没有分组查询注意力（GQA）的情况下，34B 模型需要的 KV 缓存内存是原来的 5 倍，而 70B 模型需要的 KV 缓存内存是原来的 8 倍。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">Llama/Mistral 的另一个改进是滑动窗口注意力，它保证我们可以将 KV 缓存限制在窗口大小，对于 Llama7B 来说，窗口大小为 4096。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><strong><span style="color:#f6ab00">1</span></strong></p><p style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">性能驱动的架构变化</span></strong></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">如前所述，LLM 每层使用了 24d^2 个 flops。增加的层数将线性扩展 flops 和参数数量，增加模型宽度会二次方扩展模型大小。需要注意的是，这是因为参数的数量与 d_model 的平方成正比，因为我们的大多数层是从一个 d_model 输入向量转变为一个 d_model 的输出向量，所以权重矩阵的尺寸为 (d_model, d_model)。换句话说，计算量与参数的数量呈正比，增加 d_model 会使参数数量呈二次方增加。模型深度增加一倍会使参数数量翻倍，但模型宽度增加一倍会使参数数量增加四倍。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">尽管如此，更宽的模型的优势是能够更好地并行化。要计算第 N 层，必须首先计算前面的 N-1 层。这在高效并行化上十分困难，尤其是在训练期间，而通过张量并行化方法，跨多个 GPU 拆分单个层要容易得多。如果你主要关心时延，那么选择更宽的模型可能更合适。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><strong><span style="color:#f6ab00">2</span></strong></p><p style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">实证分析</span></strong></span></p><p style="margin-left:8px; margin-right:8px; text-align:center">&nbsp;</p><p><span style="color:#3f3f3f">我使用 Colab 进行了这项分析。（</span><span style="color:#888888"><em>https://colab.research.google.com/drive/1TH6AKsICZqlFoW1ph8h3wsF7q7qVMF8T?usp=sharing</em></span><span style="color:#3f3f3f">）</span></p><p><span style="color:#3f3f3f">以下是单次前向传播的高级概要（我的网站上有交互式概要：</span><span style="color:#888888"><em>https://finbarr.ca/static/llama-profile.svg</em></span><span style="color:#3f3f3f">）：</span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/d803a059-2155-4d20-bdfe-d09119f0d0a4.png" width="2378" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">可以看到，本次运行的总时间中有 4.88% 用於单次前向传播。在前向传播中，有 1.98% 的时间用于注意力机制，有 2.58% 的时间用于多层感知机（MLP）。在前向传播的总时间中，有 40% 的时间用于注意力层，53% 用于 MLP。在注意力层内部，时间分配在 4 个不同的线性层上，其中有 2 个线性层花费的时间大致相同（linear_1、linear_2），一个花费的时间多 50%（linear_3），另一个则是前两者的两倍（linear_0）。我猜测 linear_0 正在计算查询嵌入，而 linear_1/2 正在计算键和值嵌入。请注意，由于 KV 头的数量较少，计算速度要快得多！GQA（Query-aware Attention）带来了明显的差异，即便所使用的注意力机制（</span><span style="color:#888888"><em>xformers.ops.memory_efficient_attention</em></span><span style="color:#3f3f3f">）要求 QKV 嵌入被广播到相同的大小。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">理论分析预测，2/3 的时间将用于计算 FFN，1/3 将用于计算注意力机制。这基本符合我们上面所看到的情况。我们花在计算注意力机制上的时间略多于 MLP，但我怀疑这是因为 MLP 正在为 Torch 执行一个经良好优化的路径。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><strong><span style="color:#f6ab00">3</span></strong></p><p style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">性能变化</span></strong></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">接下来，我对 Llama2 进行了一系列实验，涉及模型宽度和深度的调整。以下是实验结果：</span></span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/0d787e3e-65db-48e6-9f1b-241cb3529be4.png" width="567" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">结果非常有趣。可以看到，隐藏维度为 1024 和 1536 的两个模型的速度基本没有变化（1.10 秒 vs1.11 秒），隐藏维度为 1024 和 2048 的模型只发生了轻微变化（1.15 秒 vs1.10 秒）。然而，当我们比较隐藏维度为 2048（1.15 秒）、3072（1.41 秒）和 4096（1.82 秒）的模型时，可以看到速度类似于线性扩展！</span></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">对此，我的看法是，调度 kernel 和实际执行矩阵乘法中存在较大的开销。这是在 T4 上运行的，尽管按现在的标准来看有些过时，但仍具有 65 TFLOPS 的 bf16 计算能力。如果我们将两个 1024x1024 的矩阵相乘，这就需要 1G FLOP 的计算能力，因此，理论上，我们可以每秒乘以 65000 个 1024x1024 的矩阵。实际上，我们只能得到其 60-80% 的性能，但仍然是每秒 40000 次矩阵乘法。如今的 GPU 拥有大量核心，T4 有 2560 个 CUDA 核心，每个核心的运行频率在 585 到 1590 MHz 之间。因此，任何能够并行化的任务都表现良好，但是那些需要顺序计算的任务则不会得到优化。我认为，这就是图中所看到的的情况——没有足够的并行性来充分利用 GPU。</span></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">Transformer 的深度使性能与预期完全一致：推理时间与深度呈线性增长。最深的模型可能存在一些噪声，但它的性能表现相当稳定。</span></span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/64122776-737e-4d29-bc3e-bdd23f972cf0.png" width="567" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">接下来，我计算了生成更多词元所需的成本（对每个词元数量进行了 10 次运行，以减少噪音）：</span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/d3c4dac0-6e2f-4efa-b5d6-a3cfcc0740b7.png" width="auto" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">正如预期的那样，完全呈线性增长，因为 Llama2 使用了 KV 缓存。如果我们查看保留的内存，就会看到 KV 缓存与预期的一致（在某种程度上）：</span></span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/5e09bf5a-80bb-44e5-b570-fe8e6c840b03.png" width="571" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span><span><span><span style="color:#3f3f3f">可以看到，模型每增加 20 个词元，内存占用就会增加约 2.1MB。由于该模型的 d_model 为 1024，有 8 个隐藏层，因此需要 4 * num_layers * d_model 字节的内存，即 4*8*1024 字节=每词元 32KB 的内存。理论上我们只需要 640KB 的内存。目前还不清楚额外的 3 倍开销是从哪里产生的。我怀疑是因为执行还不够高效。</span></span></span></span></span></p><p>&nbsp;</p><span id="OSC_h3_1"></span><h3>&nbsp;</h3><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">【语言大模型推理最高<strong><span>加速 11</span></strong>倍】</span></strong><span style="color:#3f3f3f">SiliconLLM 是由硅基流动开发的高效、易用、可扩展的 LLM 推理加速引擎，旨在为用户提供开箱即用的推理加速能力，显著降低大模型部署成本，加速生成式 AI 产品落地。（技术合作、交流请添加微信：SiliconFlow01）</span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/838dbcc5-3a63-4cde-8f52-3b567a5f020a.png" referrerpolicy="no-referrer"></p><p style="text-align:left"><span style="color:#3f3f3f">SiliconLLM 的吞吐最高提升近<strong>4</strong>倍，时延最高降低近<strong>4</strong>倍</span></p><p style="text-align:center">&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/d7781a37-f9dd-4d5f-b5ef-7767cc2816af.png" referrerpolicy="no-referrer"></p><p style="text-align:left"><strong><span style="color:#3f3f3f">数据中心+PCIe</span></strong><span style="color:#3f3f3f">：SiliconLLM 的吞吐最高提升近<strong>5</strong>倍；<strong>消费卡场景</strong>：SiliconLLM 的吞吐最高提升近<strong>3</strong>倍</span></p><p style="text-align:center">&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/d42ca228-3463-4797-9dfb-454d4682d478.png" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">Sy</span><span style="color:#3f3f3f">stem Prompt 场景</span></strong><span style="color:#3f3f3f">：SiliconLLM 的吞吐最高提升<strong>11</strong>倍；<strong>MoE 模型</strong>：推理 SiliconLLM 的吞吐最高提升近<strong>10</strong>倍</span></p><p>&nbsp;</p><p><span style="color:#888888">其他人都在看</span></p><span id="OSC_h3_2"></span><h3>&nbsp;</h3><ul><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493221%26idx%3D1%26sn%3D0c75b8115f4d4a27c8a5d6505a0a4986%26chksm%3Dfe426853c935e145d21abd30e0ceb29486c9e306032dfcb3d071ce34d171425c11341a57d7bf%26scene%3D21%23wechat_redirect" target="_blank">800+页免费「大模型」电子书</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493108%26idx%3D1%26sn%3Db254dff8281096bf6f5462489e94658a%26chksm%3Dfe426bc2c935e2d404fb803985241ea05109aa9d7925a1876304a51c322fa5e6e57dd41e2e66%26scene%3D21%23wechat_redirect" target="_blank">语言大模型的推理技巧</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247491309%26idx%3D1%26sn%3D407fefb7ec76a2c9fdfe1ae960f7de4d%26chksm%3Dfe4190dbc93619cd0b9bfecb979e142a125fd8548d323dd9bdc2cbeb619400202e6e1affced0%26scene%3D21%23wechat_redirect" target="_blank">语言大模型的推理演算</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493321%26idx%3D1%26sn%3Dffc39c67080fefb01f1790e285a5085b%26chksm%3Dfe4268ffc935e1e9c8aeb5a095f08868a5eb8df90c1f46eb1e07044ec34ae6404d14b8b5926b%26scene%3D21%23wechat_redirect" target="_blank">语言大模型推理加速指南</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492811%26idx%3D1%26sn%3D916e330a2a4152dab3192635c3e475fa%26chksm%3Dfe426afdc935e3eb2f371ff5f56247c95800ce91a950a89bea871c26ddc4c3d13371acf03978%26scene%3D21%23wechat_redirect" target="_blank">语言大模型推理性能工程：最佳实践</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493149%26idx%3D1%26sn%3Dfd0369875ad89e8ad173935ec7b38126%26chksm%3Dfe42682bc935e13d91f1ae73cb0135cca0d815c0356baef29c20ab985c9279280e28ae956642%26scene%3D21%23wechat_redirect" target="_blank">迈向 100 倍加速：全栈 Transformer 推理优化</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493282%26idx%3D1%26sn%3D7f91a174ab4cccf16303aa3ce11afac7%26chksm%3Dfe426894c935e182d15b65dbf6d1e3d4d12398664ceae848f6fa79ee289dff1fad23a9c8aea5%26scene%3D21%23wechat_redirect" target="_blank">Mistral AI:LLM 推理的吞吐、时延及成本空间</a></p></li></ul><p><span style="color:#3f3f3f">试用 OneDiff:&nbsp;<strong><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgithub.com%2Fsiliconflow%2Fonediff" target="_blank">github.com/siliconflow/onediff</a></strong></span></p><span id="OSC_h2_3"></span><h2 style="margin-left:8px; margin-right:8px">&nbsp;</h2><p><img src="https://oscimg.oschina.net/oscnet/4d6d060f-6daf-4d9f-a3db-4de78e4b9745.png" width="578" referrerpolicy="no-referrer"></p><p>&nbsp;</p></div><p style="color:#858585">本文分享自微信公众号 - OneFlow（OneFlowTechnology）。<br> 如有侵权，请联系 support@oschina.cn 删除。<br> 本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/oneflow/blog/11030216</guid>
            <link>https://my.oschina.net/oneflow/blog/11030216</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[GreatSQL TPC-H 性能测试报告正式发布！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><img height="383" src="https://oscimg.oschina.net/oscnet/up-93f0563f25f5317db04df95a53c0bcc8c0f.png" width="900" referrerpolicy="no-referrer"></p><p style="text-align:center">GreatSQL TPC-H 性能测试报告 - （2024 年 2 月 28 日）</p><p>完整性能测试报告：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreatsql.cn%2Fdocs%2F8032-25%2Fuser-manual%2F10-optimze%2F3-3-benchmark-greatsql-tpch-report.html" target="_blank">https://greatsql.cn/docs/8032-25/user-manual/10-optimze/3-3-benchmark-greatsql-tpch-report.html</a></p><h2>1、概述</h2><p>本次测试针对 GreatSQL 数据库基于标准 TPC-H 场景的测试。</p><p>TPC-H（商业智能计算测试）是美国交易处理效能委员会（TPC，TransactionProcessing Performance Council）组织制定的用来模拟决策支持类应用的一个测试集。目前，学术界和工业界普遍采用 TPC-H 来评价决策支持技术方面应用的性能。这种商业测试可以全方位评测系统的整体商业计算综合能力，对厂商的要求更高，同时也具有普遍的商业实用意义，目前在银行信贷分析和信用卡分析、电信运营分析、税收分析、烟草行业决策分析中都有广泛的应用，TPC-H 查询包含八张数据表和 22 条复杂 SQL 查询，大多数查询包含多表联接（JOIN）、子查询和聚合查询等。</p><p>GreatSQL 数据库是一款<strong>开源免费</strong>数据库，可在普通硬件上满足金融级应用场景，具有<strong>高可用</strong>、<strong>高性能</strong>、<strong>高兼容</strong>、<strong>高安全</strong>等特性，可作为 MySQL 或 Percona Server for MySQL 的理想可选替换。</p><h2>2、测试环境</h2><table><tbody><tr><th>配置</th><th>备注</th></tr></tbody><tbody><tr><td>操作系统</td><td>OS：CentOS Linux release 7.9.2009 (Core)<br> 内核：3.10.0-1160.el7.x86_64</td></tr><tr><td>CPU</td><td>Intel(R) Xeon(R) Gold 6238 CPU @ 2.10GHz * 4</td></tr><tr><td>内存</td><td>251G</td></tr><tr><td>磁盘</td><td>INTEL SSDPE2KE032T8</td></tr><tr><td>数据库</td><td>GreatSQL 8.0.32-25, Release 25, Revision 79f57097e3f</td></tr></tbody></table><p><strong>提示</strong>：在下面运行 TPC-H 测试时，设置了 Rapid 引擎最大可使用的内存及线程数。</p><pre><code class="language-sql">greatsql&gt; SET GLOBAL rapid_memory_limit = 68719476736;
greatsql&gt; SET GLOBAL rapid_worker_threads = 32;
</code></pre><h2>3、测试表结构和数据量</h2><p>各表数据量对比：</p><table><tbody><tr><th>表名</th><th>TPC-H SF100 数据量</th><th>TPC-H SF300 数据量</th><th>备注</th></tr></tbody><tbody><tr><td>region</td><td>5</td><td>5</td><td>地区信息</td></tr><tr><td>nation</td><td>25</td><td>25</td><td>国家表</td></tr><tr><td>supplier</td><td>1000000</td><td>3000000</td><td>供应商信息</td></tr><tr><td>part</td><td>20000000</td><td>60000000</td><td>零件表</td></tr><tr><td>customer</td><td>15000000</td><td>45000000</td><td>消费者表</td></tr><tr><td>partsupp</td><td>80000000</td><td>240000000</td><td>配件供应表</td></tr><tr><td>orders</td><td>150000000</td><td>450000000</td><td>订单表</td></tr><tr><td>lineitem</td><td>600037902</td><td>1799989091</td><td>订单明细表</td></tr></tbody></table><p>Rapid 引擎表空间压缩率：</p><table><tbody><tr><th>库名</th><th>InnoDB 表空间文件总大小</th><th>Rapid 引擎表空间总大小</th><th>压缩率</th></tr></tbody><tbody><tr><td>TPC-H SF100</td><td>184570593436</td><td>28728373248</td><td>6.42</td></tr><tr><td>TPC-H SF300</td><td>591644573888</td><td>74334864443</td><td>7.96</td></tr></tbody></table><p>各表结构关系如下图所示：</p><p><img height="720" src="https://oscimg.oschina.net/oscnet/up-1c9a6376e22d965db5c040dd053da59ae6a.png" width="756" referrerpolicy="no-referrer"></p><h2>4、测试结果</h2><p>GreatSQL 8.0.32-25 中，采用全新的 Rapid 存储引擎，使得其在 TPC-H 性能测试中表现大大优于此前的其他版本，也大大优于 MySQL 社区版、Percona Server MySQL、MariaDB 等数据库。</p><p>在 TPC-H SF100 场景下，运行完全部 22 个 TPC-H 查询 SQL 总耗时为<strong>79.28 秒</strong>。在 TPC-H SF300 场景下，运行完全部 22 个 TPC-H 查询 SQL 总耗时为<strong>386.195 秒</strong>。</p><p>每条 SQL 详细耗时如下：</p><table><tbody><tr><th>TPC-H Query</th><th>GreatSQL TPC-H SF100（32C64G）耗时（秒）</th><th>GreatSQL TPC-H SF300（32C64G）耗时（秒）</th></tr></tbody><tbody><tr><td>Q1</td><td>1.184</td><td>3.537</td></tr><tr><td>Q2</td><td>0.924</td><td>3.865</td></tr><tr><td>Q3</td><td>1.324</td><td>4.167</td></tr><tr><td>Q4</td><td>3.678</td><td>22.712</td></tr><tr><td>Q5</td><td>1.287</td><td>4.119</td></tr><tr><td>Q6</td><td>0.344</td><td>0.959</td></tr><tr><td>Q7</td><td>5.48</td><td>50.217</td></tr><tr><td>Q8</td><td>1.13</td><td>3.534</td></tr><tr><td>Q9</td><td>7.311</td><td>31.872</td></tr><tr><td>Q10</td><td>2.885</td><td>15.301</td></tr><tr><td>Q11</td><td>0.477</td><td>0.921</td></tr><tr><td>Q12</td><td>0.799</td><td>2.294</td></tr><tr><td>Q13</td><td>3.758</td><td>10.997</td></tr><tr><td>Q14</td><td>0.966</td><td>2.471</td></tr><tr><td>Q15</td><td>2.831</td><td>11.898</td></tr><tr><td>Q16</td><td>1.194</td><td>3.487</td></tr><tr><td>Q17</td><td>8.537</td><td>27.523</td></tr><tr><td>Q18</td><td>13.007</td><td>108.237</td></tr><tr><td>Q19</td><td>1.892</td><td>4.046</td></tr><tr><td>Q20</td><td>4.21</td><td>10.668</td></tr><tr><td>Q21</td><td>11.965</td><td>60.084</td></tr><tr><td>Q22</td><td>2.513</td><td>3.286</td></tr><tr><td>总耗时</td><td><strong>77.696</strong></td><td><strong>386.195</strong></td></tr></tbody></table><p>GreatSQL SF100 vs SF300（32C64G）对比示意图如下</p><p><img height="400" src="https://oscimg.oschina.net/oscnet/up-a576e5fe8987d0e3b32a49b4122e603c754.png" width="981" referrerpolicy="no-referrer"></p><h2>5、测试步骤</h2><h3>5.1 安装 GreatSQL</h3><p>请参考 GreatSQL 手册内容：<strong>安装指南</strong> ➥https://greatsql.cn/docs/8032-25/user-manual/4-install-guide/0-install-guide.html，完成 GreatSQL 安装。</p><h3>5.2 生成 TPC-H 测试数据</h3><p>请参考 GreatSQL 手册内容：<strong>TPC-H 性能测试</strong> ➥https://greatsql.cn/docs/8032-25/user-manual/10-optimze/3-2-benchmark-tpch.html，完成 TPC-H 工具编译安装。</p><p>运行 TPC-H <code>dbgen</code> 工具，生成数据文件，一共会生成 8 个表对应的 tbl 数据文件，例如：</p><pre><code class="language-shell">$ ./dbgen -vf -s 100
...

$ ls -l *tbl
-rw-r--r-- 1 root root  2463490271 Sep 26 09:20 customer.tbl
-rw-r--r-- 1 root root 79579694556 Sep 26 09:20 lineitem.tbl
-rw-r--r-- 1 root root        2224 Sep 26 09:20 nation.tbl
-rw-r--r-- 1 root root 17793116301 Sep 26 09:20 orders.tbl
-rw-r--r-- 1 root root 12209211160 Sep 26 09:20 partsupp.tbl
-rw-r--r-- 1 root root  2453234158 Sep 26 09:20 part.tbl
-rw-r--r-- 1 root root         389 Sep 26 09:20 region.tbl
-rw-r--r-- 1 root root   142869803 Sep 26 09:20 supplier.tbl
</code></pre><p>也可以参考 <strong>duckdb_dbgen.py</strong> ➥ <a href="https://gitee.com/GreatSQL/GreatSQL-Doc/blob/master/tpch/3.0.1/duckdb_dbgen.py">https://gitee.com/GreatSQL/GreatSQL-Doc/blob/master/tpch/3.0.1/duckdb_dbgen.py</a> 脚本做法，利用 duckdb 并行生成测试数据。</p><h3>5.3 创建 TPC-H 测试数据库表并导入数据</h3><p>参考 GreatSQL 社区提供的 TPC-H 数据库表初始化脚本：<strong>tpch-create-table.sql</strong> ➥ <a href="https://gitee.com/GreatSQL/GreatSQL-Doc/blob/master/tpch/3.0.1/tpch-create-table.sql%EF%BC%8C%E5%AE%8C%E6%88%90TPC-H%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E5%88%9B%E5%BB%BA%E3%80%82">https://gitee.com/GreatSQL/GreatSQL-Doc/blob/master/tpch/3.0.1/tpch-create-table.sql，完成 TPC-H 测试数据库表创建。</a></p><pre><code class="language-shell">$ mysql -f &lt; tpch-create-table.sql
$ mysqlshow tpch100
Database: tpch100
+----------+
|  Tables  |
+----------+
| customer |
| lineitem |
| nation   |
| orders   |
| part     |
| partsupp |
| region   |
| revenue0 |
| supplier |
+----------+
</code></pre><p>利用 GreatSQL 的 <strong>parallel load data 特性</strong> ➥ <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreatsql.cn%2Fdocs%2F8032-25%2Fuser-manual%2F5-enhance%2F5-1-highperf-parallel-load.html" target="_blank">https://greatsql.cn/docs/8032-25/user-manual/5-enhance/5-1-highperf-parallel-load.html</a> 并行导入 TPC-H 测试数据。</p><p>需要先修改 GreatSQL 选项<code>secure_file_priv</code>设置，指向上述 workdir 所在目录，重启 GreatSQL 使之生效。</p><p>参考 GreatSQL 社区提供的并发导入脚本：<strong><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fload-data-parallel.sh" target="_blank">load-data-parallel.sh</a></strong> ➥ <a href="https://gitee.com/GreatSQL/GreatSQL-Doc/blob/master/tpch/3.0.1/load-data-parallel.sh%EF%BC%8C%E5%AE%8C%E6%88%90%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E3%80%82">https://gitee.com/GreatSQL/GreatSQL-Doc/blob/master/tpch/3.0.1/load-data-parallel.sh，完成数据导入。</a></p><p><strong>提示</strong>：运行 LOAD DATA 导入数据时，可能会在 <code>tmpdir</code> 产生临时文件，因此要保证 <code>tmpdir</code> 有足够的剩余可用磁盘空间。</p><h3>5.4 确认 Rapid 引擎设置，并加载数据到 secondary engine</h3><p>数据导入完成后，在开始运行 TPC-H 测试前，需要先将测试数据加载到 secondary engine 引擎中。</p><p>先执行下面命令，动态修改 Rapid 引擎最大可使用内存，其余相关选项均为默认值：</p><pre><code class="language-sql">greatsql&gt; SET GLOBAL rapid_memory_limit = 68719476736;
greatsql&gt; SET GLOBAL rapid_worker_threads = 32;
</code></pre><p>之后，执行以下命令加载测试数据到 secondary engine：</p><pre><code class="language-sql">greatsql&gt; alter table customer secondary_load;
alter table lineitem secondary_load;
alter table nation secondary_load;
alter table orders secondary_load;
alter table part secondary_load;
alter table partsupp secondary_load;
alter table region secondary_load;
alter table supplier secondary_load;
</code></pre><p>这个过程需要一定时间，请耐心等待。</p><h3>5.5 执行 TPC-H 测试</h3><p>参考 GreatSQL 社区提供的 TPC-H 性能测试脚本，完成测试，并记录各个 SQL 的耗时。</p><p>该测试脚本大概工作模式如下：</p><ol><li><p>先执行 22 个查询 SQL，进行数据预热，每条 SQL 各执行 2 次。</p></li><li><p>再分别执行 22 个查询 SQL，每个 SQL 各执行 3 次。</p></li><li><p>每次执行 SQL 都会记录其起止时间，及其耗时，如下面例所示：</p></li></ol><pre><code class="language-shell">[2023-09-27 01:38:45] BEGIN RUN TPC-H Q1 1 times
[2023-09-27 01:38:46] TPC-H Q1 END, COST: 1.301s


[2023-09-27 01:38:46] BEGIN RUN TPC-H Q1 2 times
[2023-09-27 01:38:47] TPC-H Q1 END, COST: 0.787s
</code></pre><p>上述结果中的 COST: 1.301s ，即为本 SQL 的运行耗时：1.301 秒。</p><p>完整性能测试报告：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreatsql.cn%2Fdocs%2F8032-25%2Fuser-manual%2F10-optimze%2F3-3-benchmark-greatsql-tpch-report.html" target="_blank">https://greatsql.cn/docs/8032-25/user-manual/10-optimze/3-3-benchmark-greatsql-tpch-report.html</a></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 01:49:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280571</guid>
            <link>https://www.oschina.net/news/280571</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[营销系统黑名单优化：位图的应用解析 | 京东云技术团队]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1>背景</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">营销系统中，客户投诉是业务发展的一大阻碍，一般会过滤掉黑名单高风险账号，并配合频控策略，来减少客诉，进而增加营销效率，减少营销成本，提升营销质量。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">营销系统一般是通过大数据分析建模，在 CDP（客户数据平台，以客户为核心，围绕数据融合、人群圈选、用户洞察等提供产品能力）创建营销目标客户群体，黑名单同样也是通过 CDP 维护。下面的图片简单描述了过滤黑名单的处理流程，流程是相对简单的。但是，测试过程中却发现一个问题，对于一个近 30 万的营销群体，整个触达流程需要处理一个多小时，而其中过滤黑名单就占用了近半个小时的时间，业务有点难以接受这个性能。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="黑名单处理流程" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-31-13-49JulpYPnbsooFF49.png" referrerpolicy="no-referrer"></p><span id="OSC_h1_2"></span><h1>性能优化</h1><span id="OSC_h2_3"></span><h2>引入多线程优化</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">其实很容易就能想到，对于调用 RPC 接口这种含有 I/O 操作的场景，可以引入多线程优化，将一个几十万的账号集合拆分为多个子任务提交给线程池处理，从而加快处理速度。从下图可以看出引入多线程后性能有很明显的改善，单线程处理 25 万、50 万个账号的群体分别需要近半小时、近一小时，改为 25 个线程处理后可以分别控制在 1 分钟、2 分钟左右。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="多线程处理" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-31-13-43TZ1WKDW1YbppzFJ.png" referrerpolicy="no-referrer"></p><span id="OSC_h2_4"></span><h2>引入位图优化</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">进一步了解 CDP 的底层原理后，会发现这个问题应该还有其他的解决方案，即通过位图优化。CDP 的群体都会有对应的位图文件，也就是说营销客户群体和黑名单群体都是以位图的数据结构存储的，通过 CDP 下载群体的 SDK 就可以获取到位图文件，营销群体的位图与黑名单群体位图进行与非操作（<code>andNot</code>，就是从一个位图中移除另一个位图中存在的元素，而保留不在另一个位图中的元素），得到的新的位图就是过滤掉黑名单账号后的目标客户的位图。代码实现很简单，使用 CDP SDK 的示例代码如下（也可以参考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fhowiefh%2Fbitmap-example%2Fblob%2Fmain%2Fsrc%2Fmain%2Fjava%2Fio%2Fgithub%2Fhowiefh%2Fbitmap%2FBitmapBlacklistOptimize.java" rel="nofollow" target="_blank">GitHub</a>示例代码，但不适用于 CDP 群体位图处理）：</p><pre><code><span style="color:#dd4a68">DataLoader</span> dataLoader <span style="background-color:rgba(255, 255, 255, 0.5)">=</span><span style="color:#0077aa">new</span><span style="color:#dd4a68">DataLoader</span><span style="color:#999999">(</span>token<span style="color:#999999">,</span> bitMapBaseUrl<span style="color:#999999">)</span><span style="color:#999999">;</span><span style="color:#dd4a68">ABitmap</span> customerBitmap <span style="background-color:rgba(255, 255, 255, 0.5)">=</span> dataLoader<span style="color:#999999">.</span><span style="color:#dd4a68">loadGroup</span><span style="color:#999999">(</span>customerGroupCode<span style="color:#999999">)</span><span style="color:#999999">;</span><span style="color:#dd4a68">ABitmap</span> blacklistBitmap <span style="background-color:rgba(255, 255, 255, 0.5)">=</span> dataLoader<span style="color:#999999">.</span><span style="color:#dd4a68">loadGroup</span><span style="color:#999999">(</span>blacklistGroupCode<span style="color:#999999">)</span><span style="color:#999999">;</span>
customerBitmap<span style="color:#999999">.</span><span style="color:#dd4a68">andNot</span><span style="color:#999999">(</span>blacklistBitmap<span style="color:#999999">)</span><span style="color:#999999">;</span></code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">位图存储相当节省空间，50 万群体的位图文件也就约 2MB 大小。同时位图的与非操作是相当快的，上边例子中的 25 万、50 万的群体都可以在 80 毫秒左右过滤掉黑名单账号。从近半小时、近一小时到几十毫秒这个对比非常惊人了，那么为什么位图的处理速度可以这么快呢？</p><span id="OSC_h1_5"></span><h1>位图简介</h1><span id="OSC_h2_6"></span><h2>位图原理</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">位图的基本思想是使用 bit 来标记一个数值，1 表示该数值存在，0 表示不存在。由于以位为单位存储数据，因此可以大大节省存储空间。通过这种方式，可以非常高效地表示和操作数值集合。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">举个直观的例子，有 40 亿个不重复的随机自然数，如果使用<code>long</code>型数值存储，一个<code>long</code><span>&nbsp;</span>型数值 8 个字节，40 亿个数值占用约 29.8GB，但如果是存储为 40 亿个 bit，则只需要约 0.47GB。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">在 Java 中一个<code>long</code>型数值占 64 位，可以用一个<code>long</code>型数组<span>&nbsp;</span><code>long[] words = new long[(nBits - 1) / 64 + 1]</code><span>&nbsp;</span>存储位图，其中<code>nBits</code>表示位图的初始大小。对于给定任意自然数<code>x</code>，<code>x / 64</code>就能得到<code>x</code>在数组中的下标，<code>x % 64</code>就能得到<code>x</code>在此下标的哪个位。数组的第一个下标<span>&nbsp;</span><code>words[0]</code><span>&nbsp;</span>可以表示数值<code>0~63</code>，第二个下标<span>&nbsp;</span><code>words[1]</code><span>&nbsp;</span>可以表示数值<code>64~127</code>，之后依此类推。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果将 3, 4, 6 几个数值存入位图，则如下图所示，对应数组的第一个下标的 3, 4, 6 位被标记为 1，其他位均为 0。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="位图" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-30-23-43yLHW11WCjBnYpArn.png" referrerpolicy="no-referrer"></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">对于添加操作，假设要添加数值<code>2</code>，可以计算出其在数组中的下标为<code>2 / 64</code>即<code>0</code>，在<code>words[0]</code>的位置为<code>2 % 64</code>即<span>&nbsp;</span><code>2</code>，只需将<code>1</code>按位左移<code>2</code>位，然后和<code>words[0]</code>进行按位或操作，将相应位置置为<code>1</code>。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="位图添加成员" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-30-23-43wpf43e7QCWuxdIFH.png" referrerpolicy="no-referrer"></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">对于移除操作，假设要移除刚添加的数值<code>2</code>，和添加操作一样，可以通过计算得到其在数组的下标为<code>0</code>, 在<code>words[0]</code>的位置为<span>&nbsp;</span><code>2</code>，只需将<code>1</code>按位左移<code>2</code>位再按位取反，然后和<code>words[0]</code>进行按位与操作，将相应位置置为<code>0</code>。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="位图移除成员" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-30-23-44MVUuAD0KAkT0gJF.png" referrerpolicy="no-referrer"></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">而对于查找操作，假设要查找数值<code>3</code>，可以计算得到其在数组的下标为<code>0</code>, 在<code>words[0]</code>的位置为<span>&nbsp;</span><code>3</code>，只需将<code>1</code>按位左移<code>3</code>位，然后和<code>words[0]</code>按位与操作不等于<code>0</code>即可判断数值是否存在。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="位图查询成员" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-02-02-19-347nQe2zYgnc49JhpM.png" referrerpolicy="no-referrer"></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">以上内容简单介绍了 Java 中的<span>&nbsp;</span><code>BitSet</code><span>&nbsp;</span>的实现原理，实际代码还会稍微复杂一些，比如会涉及到数组扩容，范围边界的检测等等。有意思的是<code>BitSet</code>中计算数组下标和位置并没有使用除法和取模，都是通过位移操作实现的，<code>x / 64</code><span>&nbsp;</span>是通过右移操作<span>&nbsp;</span><code>x &gt;&gt; 6</code>，<code>1</code>按位左移<code>x % 64</code>位是直接将<code>1</code>左移<code>x</code>位即<code>1 &lt;&lt; x</code>。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">位图对象还支持一些常用的位运算，如求交集 (<code>and</code>, 按位与操作)，求并集 (<code>or</code>, 按位或操作)，求差集 (<code>andNot</code>, 按位与非操作)。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">位图非常节省存储空间，位操作也非常高效，这也是为什么引入位图过滤黑名单能在毫秒级别处理完成的原因。</p><span id="OSC_h2_7"></span><h2>RoaringBitmap</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">遗憾的是，<code>BitSet</code>会占用过多内存。如果<code>BitSet</code>中只存储一个数值<span>&nbsp;</span><code>200000000</code>，通过 GraphLayout 发现<code>BitSet</code>会占用约 23MB 内存，这种情况对空间的浪费极其严重。为了弥补这一缺陷，通常使用压缩位图。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><code>RoaringBitmap</code>是一种压缩位图，其性能往往优于<span>&nbsp;</span><code>WAH</code>、<code>EWAH</code><span>&nbsp;</span>或<span>&nbsp;</span><code>Concise</code><span>&nbsp;</span>等传统压缩位图。在某些情况下，<code>RoaringBitmap</code>的速度可以快上数百倍，而且压缩效果往往要好得多。它们甚至比未压缩的位图更快。如果使用<code>RoaringBitmap</code>只存储一个数值<span>&nbsp;</span><code>200000000</code>，只需要 144B 的内存。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><code>RoaringBitmap</code>将一个<code>int</code>数值<code>x</code>划分为高 16 位和低 16 位，高 16 位下标可以通过<code>x &gt;&gt;&gt; 16</code>得到，高位 container 中维护了一个数组，数组的元素中存储了低位 container，低位 container 中的元素数量未达到 4096 时，使用<code>ArrayContainer</code>存储，其内部实现是一个<code>char</code>数组，数组中存放低位数值，达到 4096 后低位 container 会转换为<code>BitmapContainer</code>，其内部实现就是一个位图。此外还有一个<code>RunContainer</code>的实现，不过较少使用。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="RoaringBitmap" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-02-02-13-08TBP9hO2xQKOXitE.png" referrerpolicy="no-referrer"></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">为什么要使用 4096 这个阈值呢？是因为超过 4096 后，<code>BitmapContainer</code>会比<code>ArrayContainer</code>更节省空间。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="container" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-31-13-40l6cjXSuXEHafAvr.png" referrerpolicy="no-referrer"></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">存储<span>&nbsp;</span><code>long</code>型数值时可以使用<code>Roaring64NavigableMap</code>，区别是它会将数值分为高 32 位和低 32 位。CDP 存储人群的位图就是基于<code>Roaring64NavigableMap</code>实现的。</p><span id="OSC_h1_8"></span><h1>位图的应用场景</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">位图可以用较少的内存来表示大规模的布尔值集合，节省内存空间，并且支持高效的位操作，如<span>&nbsp;</span><code>AND</code>、<code>OR</code>、<code>XOR</code><span>&nbsp;</span>等，使得对集合进行复杂操作变得简单高效，对于存在性查询，位图可以在常数时间内完成，具有高效的查询性能。一些面试题中出现的几十亿数据的去重、排序、计数或者成员查询等问题，都可以通过位图解决，此外还有很多场景应用到了位图。</p><span id="OSC_h2_9"></span><h2>Java 中的位图应用</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><code>ArrayList</code>为了提升性能并节省空间，重写了<code>Collection</code>接口默认的<code>removeIf</code>方法，重写后的方法使用了位图，首先遍历一遍元素用位图标记待删除的元素位置，然后遍历第二遍才真正删除元素，通过这种方式实现，可以高效移除元素，减少不必要的数组复制和元素移动次数，并且使用位图标记待删除位置也没有过多浪费空间。</p><span id="OSC_h2_10"></span><h2>位图索引</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">位图索引是一种特别适合于处理具有较少唯一值的列（例如性别、婚姻状况等）查询的数据结构，它在数据仓库等场合中非常有用，因为这些环境通常包含大量的数据读取操作和复杂的布尔逻辑查询，同时数据更新的频率相对较低。位图索引通过将列值映射到位上，并利用位运算来快速完成查询，能够有效提高查询效率，但它不适合那些具有高基数值和频繁更新的场景，因为这些场景下位图索引会占用大量空间并且更新成本很高。</p><span id="OSC_h2_11"></span><h2>Redis 的位图</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">Redis 的位图非常适合于处理大量的布尔值数据，例如追踪用户的在线状态、记录用户每日签到或统计活跃用户数量等场景，因为位图通过每个位代表一个布尔值，可以极大地节省存储空间，并且 Redis 提供了丰富的位操作命令来高效地执行各种计算，如统计特定位上值为 1 的数量或者对多个位图进行位运算以实现快速的集合操作，这些特性使得位图在特征标记、实验分组以及 AB 测试等方面也非常有用；但是，需要注意的是，由于 Redis 将位图存储为字符串，因此其大小会受到字符串最大长度的限制，并且当数据量巨大时，对内存的使用效率也是一个需要考虑的因素。</p><span id="OSC_h2_12"></span><h2>布隆过滤器</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">数值可以很方便地使用位图处理，但是有些场景需要处理的可能是字符串，比如用户账号、URL 等，一般需要将字符串跟数值做一个映射，CDP 是将用户账号和偏移量 offset 做了一个映射表，再将偏移量 offset 存储到位图。布隆过滤器则是通过多个哈希函数将元素映射到了位图上，它是一种空间效率极高的概率型数据结构，它用于判断一个元素是否在一个集合中，并且能够非常快速地进行查询，常见的应用场景包括网络爬虫中避免重复爬取相同的 URL、数据库中快速判断某个元素是否存在以减少不必要的磁盘 IO 操作、防止缓存击穿，以及各种需要快速集合检测且可以容忍一定误报率的场合，误报是指布隆过滤器可能会错误地判断某个不存在集合中的元素为存在，但它绝不会错误地判断存在的元素为不存在，因此在不需要百分之百准确性的情况下，布隆过滤器是一种非常有用的工具。</p><span id="OSC_h1_13"></span><h1>总结</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">通过探讨营销系统中优化黑名单过滤的策略，本文引入了位图这一数据结构，并详细阐述了其背后的实现机制及适用场合。位图特别适用于那些对空间效率和查询速度有极高要求的场景。在处理大数据时，位图通过压缩和优化可以极大地减少内存占用，提升数据处理的性能，希望本文能为大家提供有益的参考和帮助。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">作者：京东科技，冯浩</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">来源：京东云开发者社区，转载请注明来源</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 19 Feb 2024 07:17:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/11043986</guid>
            <link>https://my.oschina.net/u/4090830/blog/11043986</link>
            <author>
                <![CDATA[京东云开发者]]>
            </author>
        </item>
    </channel>
</rss>
