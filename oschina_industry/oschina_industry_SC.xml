<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Fri, 23 Feb 2024 10:39:07 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[2024 年，只有搞颜色的 P 站真正关心网站性能]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>2024 年，大家觉得一个网站 JS 文件的平均大小应该是多少？1MB、5MB、10MB，还是更加大呢？</p><p>近年来，层出不穷的现代化前端技术让人眼花缭乱，再加上终端设备的配置越来越高，许多网站似乎不用再过分担心性能问题 —— 常常打开网站就要下载超过 10M 的&nbsp;<span style="font-family:-apple-system,BlinkMacSystemFont,&quot;Apple Color Emoji&quot;,&quot;Segoe UI Emoji&quot;,&quot;Segoe UI Symbol&quot;,&quot;Segoe UI&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei&quot;,&quot;Helvetica Neue&quot;,Helvetica,Arial,sans-serif">JS 文件。</span></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-25992e8199a790e84d4276053e0858e1b77.png" referrerpolicy="no-referrer"></p><p>知名开源开发者 Nikita Prokopov 对常见网站的 JS 文件大小进行了统计，结果有点令人出乎意料。</p><hr><h3><strong><span style="background-color:#e67e22">以静态页面为主的网站</span></strong></h3><ul><li><h4>Wikipedia, 0.2&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-1421231d12c5140f4dc29b93285f2916686.png" referrerpolicy="no-referrer"></p><ul><li><h4>Linear, 3&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e8a9b91df6faa5480f2103efd9cd244aa66.png" referrerpolicy="no-referrer"></p><ul><li><h4>Zoom, 6&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-63b072146754bdd245d2af6dcb63dc79c3e.png" referrerpolicy="no-referrer"></p><ul><li><h4>Vercel, 6&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-158a201b80b904688e2a4f9590f8345b4df.png" referrerpolicy="no-referrer"></p><ul><li><h4>Gitlab,<span style="background-color:#f1c40f"> 13&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3de67d966870abe2b064d536d3f770d66bc.png" referrerpolicy="no-referrer"></p><ul><li><h4>Medium, 3&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a132d36ae636b6435528e8d174ad233f7d2.png" referrerpolicy="no-referrer"></p><ul><li><h4>Quora, 4.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3a6c4a73ff80a50c4052aa7faeced43118b.png" referrerpolicy="no-referrer"></p><ul><li><h4>Pinterest, <span style="background-color:#f1c40f">10&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-bb825579fd8f6d787be8051e5b32e081fed.png" referrerpolicy="no-referrer"></p><hr><h3><strong><span style="background-color:#e67e22">以搜索功能为主的网站</span></strong></h3><ul><li><h4>StackOverflow, 3.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-860ef367515f1ea0ae55c567ed403a1a9c4.png" referrerpolicy="no-referrer"></p><ul><li><h4>NPM, 4&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e55316c24ebe2e2a885a576106ac801e0b9.png" referrerpolicy="no-referrer"></p><ul><li><h4>Airbnb, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a59cfa95b748bb2888d93b3b647a63ca416.png" referrerpolicy="no-referrer"></p><ul><li><h4>Booking.com, <span style="background-color:#f1c40f">12&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fee82c93858ef67b723612d9e1ae53314ed.png" referrerpolicy="no-referrer"></p><ul><li><h4>Google, 9&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-d677500f815f8156922beb9938670463a68.png" referrerpolicy="no-referrer"></p><h3><span style="background-color:#e67e22">具有简单交互的单应用网站</span></h3><ul><li><h4>Google Translate, 2.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-31a83dfa4eba4b19f6010f5479ce06fc03b.png" referrerpolicy="no-referrer"></p><ul><li><h4>ChatGPT, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-76109ab4826fd1e649be9e2d303e44be5ff.png" referrerpolicy="no-referrer"></p><h3><span style="background-color:#e67e22">视频/多媒体类网站</span></h3><ul><li><h4>Loom, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-35a3913d214fb0b4ee6edc2929f9cec5b77.png" referrerpolicy="no-referrer"></p><ul><li><h4>YouTube, <span style="background-color:#f1c40f">12&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-11c1827cfb0fcf18a4d1ab929ce533a8d42.png" referrerpolicy="no-referrer"></p><ul><li><h4>Pornhub, <span style="background-color:#16a085">&nbsp;1.4&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6531a939e4e613a4503e6bec46e4e5eb0eb.png" referrerpolicy="no-referrer"></p><p>目前看下来，维基百科网站的 JS 文件最小，仅有 0.2MB。Pornhub 次之，为 1.4MB。</p><p>但这俩在下面这个网站前面都是弟弟——</p><ul><li><h4><strong>jQuery, 0.1 MB</strong></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a30ca6a8e2eb8cbc214df1411e42772b4c4.png" referrerpolicy="no-referrer"></p><hr><p>最后看看本站：</p><p><img src="https://oscimg.oschina.net/oscnet/up-8c7490be3117d1e5730f1bb2d6ab1bb8e77.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 09:27:08 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279994/js-bloat-2024</guid>
            <link>https://www.oschina.net/news/279994/js-bloat-2024</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Oracle 致力解决 Java 虚拟线程「Pinning」问题]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>虚拟线程在 2023 年 9 月发布的 JDK 21 中正式成为一项稳定功能。该功能在 Java 生态系统中反响极佳，但仍存在一些痛点。Oracle 日前在&nbsp;Inside Java 网站上详细<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finside.java%2F2024%2F02%2F21%2Fquality-heads-up%2F" target="_blank">介绍</a>了虚拟线程的「Pinning」问题。</p><p>最常见的两种情况是：(a) 虚拟线程在 synchronized method 中驻留（如执行 socket I/O）；(b) <span style="color:#333333">虚拟线程阻塞进入&nbsp;synchronized method</span>，因为对象的相关监视器被另一个线程持有。</p><p>在这两种情况下，载体或本地线程都不会被释放去做其他工作。因此可能会影响性能和可扩展性，并可能<span style="color:#333333">在某些情况下</span>导致饥饿和死锁。官方最近发布的一个<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finside.java%2F2024%2F02%2F17%2Fvirtual-threads-next-steps%2F" target="_blank">Virtual Threads Next Steps</a>&nbsp;视频中则更详细地解释了其中的原因，并讨论了一些潜在的解决方案。</p><p><img height="196" src="https://oscimg.oschina.net/oscnet/up-6a2e93ce6802a570c1704258c9a589e998e.png" width="500" referrerpolicy="no-referrer"></p><p>项目团队正在努力解决这些问题。Java Project Loom 的新早期访问版本<span style="color:#4e4242">引入了对对象监视器实现的更改</span><span style="color:#333333">，但不适用这两种常见情况。因此 </span><span style="color:#4e4242">Loom&nbsp;</span><span style="color:#333333">团队正在寻求用户的帮助，以测试这些更新的对象监控器在使用虚拟线程的代码和大量同步的库中的可靠性和性能。可通过&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmail.openjdk.org%2Fpipermail%2Floom-dev%2F" target="_blank">Loom 邮件列表</a>&nbsp;<span style="color:#333333">报告或反馈问题。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 06:47:02 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279954/java-virtual-threads-pinning-issue</guid>
            <link>https://www.oschina.net/news/279954/java-virtual-threads-pinning-issue</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Koordinator v1.4 正式发布！为用户带来更多的计算负载类型和更灵活的资源管理机制]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h3_1"></span><h3>背景</h3><p style="text-align:justify">Koordinator 作为一个积极发展的开源项目，自 2022 年 4 月发布 v0.1.0 版本以来，经历了多次迭代，持续为 Kubernetes 生态系统带来创新和增强。项目的核心是提供混部工作负载编排、混部资源调度、混部资源隔离和混部性能调优的综合解决方案，帮助用户优化容器性能，并提升集群资源使用效率。</p><p style="text-align:justify">在过去的版本迭代中，Koordinator 社区不断壮大，已经得到了包括阿里巴巴、蚂蚁科技、Intel、小米、小红书、爱奇艺、360、有赞、趣玩、美亚柏科、PITS 等知名企业工程师的积极参与和贡献。每一个版本都是在社区共同努力下推进的，反映了项目在实际生产环境中解决问题的能力。</p><p style="text-align:justify"><strong>今天我们很高兴的向大家宣布，Koordinator v1.4.0 版本正式发布。</strong>在本次发布中，Koordinator 引入了 Kubernetes 与 YARN 负载混部、NUMA 拓扑对齐策略、CPU 归一化和冷内存上报等新特性，同时重点增强了弹性配额管理、宿主机非容器化应用的 QoS 管理、重调度防护策略等领域的功能。这些新增和改进点旨在更好地支持企业级 Kubernetes 集群环境，特别是对于复杂和多样化的应用场景。</p><p style="text-align:justify">v1.4.0 版本的发布，将为用户带来更多的计算负载类型支持和更灵活的资源管理机制，我们期待这些改进能够帮助用户应对更多企业资源管理挑战。在 v1.4.0 版本中，共有 11 位新加入的开发者参与到了 Koordinator 社区的建设，他们是&nbsp;<em>@shaloulcy，@baowj-678，@zqzten，@tan90github，@pheianox，@zxh326，@qinfustu，@ikaven1024，@peiqiaoWang，@bogo-y，@xujihui1985</em>，感谢期间各位社区同学的积极参与和贡献，也感谢所有同学在社区的持续投入。</p><span id="OSC_h3_2"></span><h3>版本功能特性解读</h3><span id="OSC_h4_3"></span><h4>1. 支持 K8s 与 YARN 混部</h4><p style="text-align:justify">Koordinator 已经支持了 K8s 生态内的在离线混部，然而在 K8s 生态外，仍有相当数量的大数据任务运行在传统的 Hadoop YARN 之上。YARN 作为发展多年的大数据生态下的资源管理系统，承载了包括 MapReduce、Spark、Flink 以及 Presto 等在内的多种计算引擎。</p><p style="text-align:justify">Koordinator 社区会同来自阿里云、小红书、蚂蚁金服的开发者们共同启动了 Hadoop YARN 与 K8s 混部项目 Koordinator YARN Copilot，支持将 Hadoop NodeManager 运行在 kubernetes 集群中，充分发挥不同类型负载错峰复用的技术价值。Koordinator YARN Copilot 具备以下特点：</p><ul><li><strong>面向开源生态</strong></li></ul><p style="text-align:justify">基于 Hadoop YARN 开源版本，不涉及对 YARN 的侵入式改造；</p><ul><li><strong>统一资源优先级和 QoS 策略</strong></li></ul><p style="text-align:justify">YARN NM 使用 Koordinator 的 Batch 优先级资源，遵循 Koordinator QoS 管理策略；</p><ul><li><strong>节点级别的资源共享</strong></li></ul><p style="text-align:justify">Koordinator 提供的混部资源，既可被 K8s Pod 使用，也可被 YARN task 使用，不同类型的离线应用可运行在同一节点。</p><p style="text-align:center"><img src="https://pic2.zhimg.com/80/v2-ac0aa93eb176110201a885bb0d474c31_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">关于 Koordinator YARN Copilot 的详细设计，以及在小红书生产环境的使用情况，请参考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttp%253A%2F%2Fmp.weixin.qq.com%2Fs%253Fspm%253Da2c6h.13046898.publish-article.4.5bb96ffaCJHZTt%2526__biz%253DMzUzNzYxNjAzMg%253D%253D%2526mid%253D2247559704%2526idx%253D1%2526sn%253D3aed8968e50c85f7af7d7e79387b9365%2526chksm%253Dfae7e1d7cd9068c10df63fa4cc9362ee259bcb9c5a4f7d68439d6f9779bb31492b072142582e%2526scene%253D21%2523wechat_redirect" target="_blank">往期文章：《Koordinator 助力云原生应用性能提升：小红书混部技术实践》</a>以及社区官方文档<strong>[1]</strong>。</p><span id="OSC_h4_4"></span><h4>2. 引入 NUMA 拓扑对齐策略</h4><p style="text-align:justify">运行在 Kubernetes 集群中的工作负载日益多样化。尤其是在机器学习等领域，对于高性能计算资源的需求持续上升。在这些领域中，不仅需要大量 CPU 资源，还经常需要 GPU 和 RDMA 等其他高速计算资源配合使用；并且，为了获得最佳的性能，这些资源往往需要在同一个 NUMA 节点，甚至同一个 PCIE 中。</p><p style="text-align:justify">Kubernetes 的 Kubelet 提供了 Topology Manager 来管理资源分配的 NUMA 拓扑，试图在 Kubelet 的 Admission 阶段从节点层面对齐多种资源的拓扑。然而，节点组件没有调度器的全局视角以及为 Pod 选择节点的时机，可能导致 Pod 被调度到无法满足拓扑对齐策略的节点上，从而导致 Pod 由于 Topology Affinity 错误无法启动。</p><p style="text-align:justify">为了解决这一问题，Koordinator 将 NUMA 拓扑选择和对齐的时机放在中心调度器中，从集群级别优化资源之间的 NUMA 拓扑。在本次发布的版本中，Koordinator 将 CPU 资源（包含 Batch 资源）的 NUMA 感知调度和 GPU 设备的 NUMA 感知调度作为 alpha 功能支持，整套 NUMA 感知调度快速演进中。</p><p style="text-align:justify">Koordinator 支持用户通过节点的 Label 配置节点上多种资源的 NUMA 拓扑对齐策略，可配置策略如下：</p><ul><li>None 是默认策略，不执行任何拓扑对齐。</li><li>BestEffort 表示节点不严格按照 NUMA 拓扑对齐来分配资源。只要节点的剩余总量满足 Pods 的需求，调度器总是可以将这样的节点分配给 Pods。</li><li>Restricted 表示节点严格按照 NUMA 拓扑对齐来分配资源，即调度器在分配多个资源时必须只选择相同的一个或多个 NUMA 节点，否则不应使用该节点；可以使用多个 NUMA 节点。例如，如果一个 Pod 请求 33C，并且每个 NUMA 节点有 32C，那么它可以被分配使用两个 NUMA 节点。如果这个 Pod 还需要请求 GPU/RDMA，那么它需要位于与 CPU 相同的 NUMA 节点上。</li><li>SingleNUMANode 与 Restricted 类似，也是严格按照 NUMA 拓扑对齐，但与 Restricted 不同的是，Restricted 允许使用多个 NUMA 节点，而 SingleNUMANode 只允许使用一个 NUMA 节点。</li></ul><p style="text-align:justify">举例，我们可以为 node-0 设置策略 SingleNUMANode：</p><pre><code>apiVersion: v1
kind: Node
metadata:
  labels:
    node.koordinator.sh/numa-topology-policy: "SingleNUMANode"
  name: node-0
spec:
  ...</code></pre><p style="text-align:justify">在生产环境中，用户可能已经开启了 Kubelet 的拓扑对齐策略，这个策略会由 koordlet 更新到 NodeResourceTopologyCRD 对象中的 TopologyPolicies 字段。当 Kubelet 的策略和用户在 Node 上设置的策略相冲突时，以 Kubelet 策略为准。Koordinator 调度器基本采用与 Kubelet Topology Manager 相同的 NUMA 对齐策略语义，Kubelet 策略 SingleNUMANodePodLevel 和 SingleNUMANodeContainerLevel 被映射为 SingleNUMANode。</p><p style="text-align:justify">在为节点配置好 NUMA 对齐策略的前提下，调度器可以为每个 Pod 选出许多个符合条件的 NUMA Node 分配结果。Koordinator 当前支持 NodeNUMAResource 插件配置 CPU 和内存资源的 NUMA Node 分配结果打分策略，包括 LeastAllocated 和 MostAllocated，默认为 LeastAllocated 策略，资源支持配置权重。调度器最终将选择得分最高的 NUMA Node 分配结果。如下例，我们配置 NUMA Node 分配结果打分策略为 MostAllocated：</p><pre><code>apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - pluginConfig:
      - name: NodeNUMAResource
        args:
          apiVersion: kubescheduler.config.k8s.io/v1beta2
          kind: NodeNUMAResourceArgs
          scoringStrategy:  # Here configure Node level scoring strategy
            type: MostAllocated
            resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              - name: "kubernetes.io/batch-cpu"
                weight: 1
              - name: "kubernetes.io/batch-memory"
                weight: 1
          numaScoringStrategy: # Here configure NUMA-Node level scoring strategy
            type: MostAllocated
            resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              - name: "kubernetes.io/batch-cpu"
                weight: 1
              - name: "kubernetes.io/batch-memory"
                weight: 1</code></pre><span id="OSC_h4_5"></span><h4>3. ElasticQuota 再进化</h4><p style="text-align:justify">为了充分地利用集群资源、降低管控系统成本，用户常常将多个租户的负载部署在一个集群中。在集群资源有限的情况下，不同租户之间必然会发生资源争抢。有的租户的负载可能一直被满足，而有的租户的负载一直无法得到执行。这就产生对公平性的诉求。配额机制是非常自然地保障租户间公平性的方式，给每个租户一个配额，租户可以使用配额内的资源，超过配额的任务将不被调度和执行。然而，简单的配额管理无法满足租户对云的弹性期待。用户希望除了配额之内的资源请求可以被满足外，配额之外的资源请求也可以按需地被满足。</p><p style="text-align:justify">在之前的版本中，Koordinator 复用了上游 ElasticQuota 的协议，允许租户设置 Min 表达其一定要满足的资源诉求，允许设置 Max 限制其最大可以使用的资源和表达在集群资源不足的情况下对集群剩余资源的使用权重。另外，Koordinator 观察到，一些租户可能通过 Min 申请了配额，但是实际的任务申请可能并没有充分利用该配额。由此，为了更近一步地提高资源利用率，Koordinator 允许租户间借用/归还资源。</p><p style="text-align:justify">除了提供弹性的配额机制满足租户按需诉求外，Koordinator 在 ElasticQuota 上增加注解将其组织成树的结构，方便用户表达树形的组织架构。</p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-0f15360828d04b31ebe10218c08a8758_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">上图是使用了 Koordinator 弹性配额的集群中常见的 Quota 结构树。Root Quota 是连接配额与集群中实际资源之间的桥梁。在之前的设计中，Root Quota 只在调度器逻辑中存在，在本次发布中，我们将 Root Quota 也通过 CRD 的形式暴露给用户，用户可以通过 koordinator-root-quota 这个 ElasticQuota CRD 查看 Root Quota 信息。</p><p style="text-align:justify"><strong>3.1 引入 Multi QuotaTree</strong></p><p style="text-align:justify">大型集群中的节点的形态是多样的，例如云厂商提供的 ECS VM 会有不同的架构，常见的是 amd64 和 arm64，相同架构又会有不同种类的机型，而且一般会把节点按可用区划分。不同类型的节点放到同一个 Quota Tree 中管理时，其特有的属性将丢失，当用户希望精细化管理机器的特有属性时，当前的 ElasticQuota 显得不够精确。为了满足用户灵活的资源管理或资源隔离诉求，Koordinator 支持用户将集群中的资源划分为多份，每一份由一个 Quota Tree 来管理，如下图所示：</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-e673d71606d710f3a447f0e504527d43_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">同时，为了帮助用户简化管理复杂性，Koordinator 在 v1.4.0 中，引入了 ElasticQuotaProfile 机制，用户可以通过 nodeSelector 快速的将节点关联到不同的 QuotaTree 中，如下实例所示：</p><pre><code>apiVersion: quota.koordinator.sh/v1alpha1
kind: ElasticQuotaProfile
metadata:
  labels:
    kubernetes.io/arch: amd64
  name: amd64-profile
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      kubernetes.io/arch: amd64 // 挑选 amd64 节点
  quotaName: amd64-root-quota   // 匹配的 root quota 名称
---
apiVersion: quota.koordinator.sh/v1alpha1
kind: ElasticQuotaProfile
metadata:
  labels:
    kubernetes.io/arch: arm64   
  name: arm64-profile
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      kubernetes.io/arch: arm64  // 挑选 arm64 节点
  quotaName: arm64-root-quota    // 匹配的 root quota 名称</code></pre><p style="text-align:justify">关联好 QuotaTree 之后，用户在每一个 QuotaTree 中与之前的 ElasticQuota 用法一致。当用户提交 Pod 到对应的 Quota 时，当前仍然需要用户完成 Pod NodeAffinity 的管理，以确保 Pod 运行在正确的节点上。未来，我们会增加一个特性帮助用户自动管理 Quota 到 Node 的映射关系。</p><p style="text-align:justify"><strong>3.2 支持 non-preemptible</strong></p><p style="text-align:justify">Koordinator ElasticQuota 支持把 ElasticQuota 中 min 未使用的部分共享给其他 ElasticQuota 使用从而提高资源利用效率，但当资源紧张时，会通过抢占机制把借用配额的 Pod 抢占驱逐走拿回资源。</p><p style="text-align:justify">在实际生产环境中，有一些在线服务如果从其他 ElasticQuota 中借用了这部分额度，后续又发生了抢占，是可能影响服务质量的。这类工作负载实质上是不能被抢占的。</p><p style="text-align:justify">为了实现这个机制，Koordinator v1.4.0 引入了新的 API，用户只需要在 Pod 上声明 quota.scheduling.koordinator.sh/preemptible: false 表示这个 Pod 不可以被抢占。</p><p style="text-align:justify">调度器调度时发现 Pod 声明了不可抢占，那么此类 Pod 的可用配额的上限不能超过 min，所以这里也需要注意的是，启用该能力时，一个 ElasticQuota 的 min 需要设置的合理，并且集群内有相应的资源保障。</p><p style="text-align:justify">这个特性不会破坏原有的行为。</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-example
  namespace: default
  labels:
    quota.scheduling.koordinator.sh/name: "quota-example"
    quota.scheduling.koordinator.sh/preemptible: false
spec:
...</code></pre><p style="text-align:justify"><strong>3.3 其它改进</strong></p><p style="text-align:justify">1. Koordinator Scheduler 过去支持跨 Namespace 使用同一个 ElasticQuota 对象，但有一些场景下，希望只被一个或者多个有限的 Namespace 可以共享同一个对象，为了支持这个场景，用户可以在 ElasticQuota 上增加 annotation quota.scheduling.koordinator.sh/namespaces，对应的值为一个 JSON 字符串数组。</p><p style="text-align:justify">2. 性能优化：过去的实现中，当 ElasticQuota 发生变化时，ElasticQuota 插件会重建整棵 Quota 树，在 v1.4.0 版本中做了优化。</p><p style="text-align:justify">3. 支持忽略 Overhead：当 Pod 使用一些安全容器时，一般是在 Pod 中声明 Overhead 表示安全容器自身的资源开销，但这部分资源成本最终是否归于终端用户承担取决于资源售卖策略。当期望不用用户承担这部分成本时，那么就要求 ElaticQuota 忽略 overhead。在 v1.4.0 版本中，可以开启 featureGate ElasticQuotaIgnorePodOverhead 启用该功能。</p><span id="OSC_h4_6"></span><h4>4. CPU 归一化</h4><p style="text-align:justify">随着 Kubernetes 集群中节点硬件的多样化，不同架构和代数的 CPU 之间性能差异显著。因此，即使 Pod 的 CPU 请求相同，实际获得的计算能力也可能大不相同，这可能导致资源浪费或应用性能下降。CPU 归一化的目标是通过标准化节点上可分配 CPU 的性能，来保证每个 CPU 单元在 Kubernetes 中提供的计算能力在异构节点间保持一致。</p><p style="text-align:justify">为了解决该问题，Koordinator 在 v1.4.0 版本中实现了一套支持 CPU 归一化机制，根据节点的资源放大策略，调整节点上可分配的 CPU 资源数量，使得集群中每个可分配的 CPU 通过缩放实现算力的基本一致。整体的架构如下图所示：</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-b0e335f164d52e400107a5294a2a7dbb_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">CPU 归一化分为两个步骤：</p><p style="text-align:justify">1. CPU 性能评估，计算不同 CPU 的性能基准，可以参考工业级性能评测标准 SPEC CPU<strong>[2]</strong>，这部分 Koordinator 项目未提供；</p><p style="text-align:justify">2. 配置 CPU 归一化系数到 Koordinator，调度系统基于归一化系数来调度资源，这部分 Koordinator 提供。</p><p style="text-align:justify">将 CPU 归一化比例信息配置到 koord-manager 的 slo-controller-config 中，配置示例如下：</p><pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: slo-controller-config
  namespace: koordinator-system
data:
  cpu-normalization-config: |
    {
      "enable": true,
      "ratioModel": {
         "Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz": {
           "baseRatio": 1.29,
           "hyperThreadEnabledRatio": 0.82,
           "turboEnabledRatio": 1.52,
           "hyperThreadTurboEnabledRatio": 1.0
         },
         "Intel Xeon Platinum 8369B CPU @ 2.90GHz": {
           "baseRatio": 1.69,
           "hyperThreadEnabledRatio": 1.06,
           "turboEnabledRatio": 1.91,
           "hyperThreadTurboEnabledRatio": 1.20
         }
      }
    }
  # ...</code></pre><p style="text-align:justify">对于配置了 CPU 归一化的节点，Koordinator 通过 Webhook 拦截 Kubelet 对 Node.Status.Allocatable 的更新以实现 CPU 资源的缩放，最终在节点上呈现出归一后的 CPU 资源可分配量。</p><span id="OSC_h4_7"></span><h4>5. 改进的重调度防护策略</h4><p style="text-align:justify">Pod 迁移是一个复杂的过程，涉及审计、资源分配、应用启动等步骤，并且与应用升级、扩展场景以及集群管理员的资源操作和维护操作混合在一起。因此，如果同时有大量 Pods 正在进行迁移，可能会对系统的稳定性产生影响。此外，如果同一工作负载的许多 Pods 同时被迁移，也会影响应用的稳定性。此外，如果同时迁移多个作业中的 Pods，可能会造成惊群效应。因此，我们希望顺序处理每个作业中的 Pods。</p><p style="text-align:justify">Koordinator 在之前提供的 PodMigrationJob 功能中已经提供了一些防护策略来解决上述问题。在 v1.4.0 版本中，Koordinator 将之前的防护策略增强为仲裁机制。当有大量的 PodMigrationJob 可以被执行时，由仲裁器通过排序和筛选，来决定哪些 PodMigrationJob 可以得到执行。</p><p style="text-align:justify">排序过程如下：</p><ul><li>根据迁移开始时间与当前时间的间隔进行排序，间隔越小，排名越高。</li><li>根据 PodMigrationJob 的 Pod 优先级进行排序，优先级越低，排名越高。</li><li>按照工作负载分散 Jobs，使得同一作业中的 PodMigrationJobs 靠近。</li><li>如果作业中已有 Pods 正在迁移，则该 PodMigrationJob 的排名更高。</li></ul><p style="text-align:justify">筛选过程如下：</p><ul><li>根据工作负载、节点、命名空间等对 PodMigrationJob 进行分组和筛选。</li><li>检查每个工作负载中正在运行状态的 PodMigrationJob 数量，达到一定阈值的将被排除。</li><li>检查每个工作负载中不可用副本的数量是否超出了最大不可用副本数，超出的将被排除。</li><li>检查目标 Pod 所在节点上正在迁移的 Pod 数量是否超过单个节点的最大迁移量，超出的将被排除。</li></ul><span id="OSC_h4_8"></span><h4>6. 冷内存上报</h4><p style="text-align:justify">为提升系统性能，内核一般尽可能不让应用程序请求的页面缓存空闲，而是尽可能将其分配给应用程序。虽然内核分配了这些内存，但是应用可能不再访问，这些内存被称为冷内存。</p><p style="text-align:justify">Koordinator 在 1.4 版本中引入冷内存上报功能，主要为未来冷内存回收功能打下基础。冷内存回收主要用于应对两个场景：</p><ul><li>对于标准的 Kubernetes 集群，当节点内存水位过高时，突发的内存请求容器导致系统直接内存回收，操作系统的直接内存回收触发时会影响已经运行容器的性能，如果回收不及时极端场景可能触发整机 oom。保持节点内存资源的相对空闲，对提升运行时稳定性至关重要。</li><li>在混部场景中，高优先级应用程序请求但未使用的资源可以被低优先级应用程序回收利用。对内存而言，操作系统未回收的内存，是不能被 Koordinator 调度系统看到的。为了提高混部资源效率，回收容器未使用的内存页面可以提高整机的资源利用效率。</li></ul><p style="text-align:justify">Koordlet 在 Collector Plugins 中添加了一个冷页面回收器，用于读取由 kidled（Anolis 内核）、kstaled（Google）或 DAMON（Amazon）导出的 cgroup 文件 memory.idle_stat。该文件包含页面缓存中的冷页面信息，并存在于 memory 的每个层次结构中。目前 koordlet 已经对接了 kidled 冷页面收集器并提供了其他冷页面收集器接口。</p><p style="text-align:justify">在收集冷页面信息后，冷页面回收器将把收集到的指标（例如节点、Pod 和容器的热页面使用量和冷页面大小）存到 metriccache 中，最后该数据会被上报到 NodeMetric CRD 中。</p><p style="text-align:justify">用户可以通过 NodeMetric 启用冷内存回收和配置冷内存收集策略，当前提供了 usageWithHotPageCache、usageWithoutPageCache 和 usageWithPageCache 三种策略，更多的细节详见社区设计文档<strong>[3]</strong>。</p><span id="OSC_h4_9"></span><h4>7. 非容器化应用的 QoS 管理</h4><p style="text-align:justify">在企业容器化过程中，除了已经运行在 K8s 上的应用，可能还会存在一些非容器化的应用运行在主机上。为了更好兼容企业在容器化过程这一过渡态，Koordinator 开发了节点资源预留机制，可以在尚未容器化的应用预留资源并赋予特定的 QoS 特性。与 Kubelet 提供的资源预留配置不同，Koordinator 主要目标是解决这些非容器化应用与容器化应用运行时的 QoS 问题，整体的方案如下图所示：</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-5edba11d4ab8bac0dcef2bc16d12d4f6_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">目前，应用程序需要按照规范将进程启动到对应的 cgroup 中，Koordinator 未实现自动的 cgroup 搬迁工具。针对宿主机非容器化应用，支持 QoS 如下：</p><ul><li><strong>LS (Latency Sensitive)</strong><ul><li>CPU QoS(Group Identity)：应用按照规范将进程运行在 cgroup 的 cpu 子系统中，koordlet 根据 CPU QoS 的配置 resource-qos-config 为其设置 Group Identity 参数；</li><li>CPUSet Allocation：应用按照规范将进程运行在 cgroup 的 cpu 子系统中，koordlet 将为其设置 cpu share pool 中的所有 CPU 核心。</li></ul></li><li><strong>BE (Best-effort)</strong><ul><li>CPU QoS(Group Identity)：应用按照规范将进程运行在 cgroup 的 cpu 子系统中，koordlet 根据 CPU QoS 的配置为其设置 Group Identity 参数。</li></ul></li></ul><p style="text-align:justify">关于宿主机应用 QoS 管理的详细设计，可以参考社区文档<strong>[4]</strong>，后续我们将陆续增加其他 QoS 策略对宿主机应用的支持。</p><span id="OSC_h4_10"></span><h4>8. 其它特性</h4><p style="text-align:justify">除了上述新特性和功能增强外，Koordinator 在 v1.4.0 版本还做了一些如下的 bugfix 和优化：</p><ul><li><strong>RequiredCPUBindPolicy</strong></li></ul><p style="text-align:justify">精细化 CPU 编排支持 Required 的 CPU 绑定策略配置，表示严格按照指定的 CPU 绑定策略分配 CPU，否则调度失败。</p><ul><li><strong>CICD</strong></li></ul><p style="text-align:justify">Koordinator 社区在 v1.4.0 提供了一套 e2e 测试的 Pipeline；提供了 ARM64 镜像。</p><ul><li><strong>Batch 资源计算策略优化</strong></li></ul><p style="text-align:justify">支持了 maxUsageRequest 的计算策略，用于更保守地超卖高优资源；优化了节点上短时间大量 Pod 启停时，Batch allocatable 被低估的问题；完善了对 hostApplication、thirdparty allocatable、dangling pod used 等特殊情况的考虑。</p><ul><li><strong>其它</strong></li></ul><p style="text-align:justify">利用 libpfm4&amp;perf group 优化 CPI 采集、SystemResourceCollector 支持自定义的过期时间配置、BE Pod 支持根据 evictByAllocatable 策略计算 CPU 满足度、Koordlet CPUSetAllocator 修复了对于 LS 和 None Qos 的 Pod 的过滤逻辑、RDT 资源控制支持获得 sandbox 容器的 task IDs 等。</p><p style="text-align:justify">通过 v1.4.0 Release<strong>[5]</strong>页面，可以看到更多包含在 v1.4.0 版本的新增功能。</p><span id="OSC_h3_11"></span><h3>未来计划</h3><p style="text-align:justify">在接下来的版本中，Koordinator 目前规划了以下功能：</p><ul><li><strong>Core Scheduling</strong></li></ul><p style="text-align:justify">在运行时侧，Koordinator 开始探索下一代 CPU QoS 能力，通过利用 Linux Core Scheduling 等内核机制，增强的物理核维度的资源隔离，降低混部的安全性风险，相关工作详见 Issue #1728<strong>[6]</strong>。</p><ul><li><strong>设备联合分配</strong></li></ul><p style="text-align:justify">在 AI 大模型分布式训练场景中，不同机器 GPU 之间通常需要通过高性能网卡相互通信，且 GPU 和高性能网卡就近分配的时候性能更好。Koordinator 正在推进支持多种异构资源的联合分配，目前已经在协议上和调度器分配逻辑上支持联合分配；单机侧关于网卡资源的上报逻辑正在探索中。</p><p style="text-align:justify">更多信息，敬请关注 Milestone v1.5.0<strong>[7]</strong>。</p><span id="OSC_h3_12"></span><h3>结语</h3><p style="text-align:justify">最后，我们十分感谢 Koordinator 社区的所有贡献者和用户，是您们的积极参与和宝贵意见让 Koordinator 不断进步。我们期待您继续提供反馈，并欢迎新的贡献者加入我们的行列。</p><p style="text-align:justify"><strong>相关链接：</strong></p><p style="text-align:justify">[1] 社区官方文档</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fkoordinator.sh%2Fzh-Hans%2Fdocs%2Fnext%2Fdesigns%2Fkoordinator-yarn%2F%253Fspm%253Da2c6h.13046898.publish-article.5.5bb96ffaCJHZTt" target="_blank">https://koordinator.sh/zh-Hans/docs/next/designs/koordinator-yarn/</a></em></u></p><p style="text-align:justify">[2] SPEC CPU</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fwww.spec.org%2Fcpu2017%2F%253Fspm%253Da2c6h.13046898.publish-article.6.5bb96ffaCJHZTt" target="_blank">https://www.spec.org/cpu2017/</a></em></u></p><p style="text-align:justify">[3] 设计文档</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fblob%2Fmain%2Fdocs%2Fproposals%2Fkoordlet%2F20230728-support-cold-memory-compute.md%253Fspm%253Da2c6h.13046898.publish-article.7.5bb96ffaCJHZTt%2526file%253D20230728-support-cold-memory-compute.md" target="_blank">https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/koordlet/20230728-support-cold-memory-compute.md</a></em></u></p><p style="text-align:justify">[4] 社区文档</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fkoordinator.sh%2Fzh-Hans%2Fdocs%2Fnext%2Fuser-manuals%2Fhost-application-qos%2F%253Fspm%253Da2c6h.13046898.publish-article.8.5bb96ffaCJHZTt" target="_blank">https://koordinator.sh/zh-Hans/docs/next/user-manuals/host-application-qos/</a></em></u></p><p style="text-align:justify">[5] v1.4.0 Release</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Freleases%2Ftag%2Fv1.4.0%253Fspm%253Da2c6h.13046898.publish-article.9.5bb96ffaCJHZTt%2526file%253Dv1.4.0" target="_blank">https://github.com/koordinator-sh/koordinator/releases/tag/v1.4.0</a></em></u></p><p style="text-align:justify">[6] Issue #1728</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fissues%2F1728%253Fspm%253Da2c6h.13046898.publish-article.10.5bb96ffaCJHZTt" target="_blank">https://github.com/koordinator-sh/koordinator/issues/1728</a></em></u></p><p style="text-align:justify">[7] Milestone v1.5.0</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fmilestone%2F14%253Fspm%253Da2c6h.13046898.publish-article.11.5bb96ffaCJHZTt" target="_blank">https://github.com/koordinator-sh/koordinator/milestone/14</a></em></u></p><p style="text-align:justify"><em>作者：乔普</em></p><p style="text-align:justify"><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1423263%3Futm_content%3Dg_1000390303" target="_blank">原文链接</a></strong></p><p style="text-align:justify"><strong>本文为阿里云原创内容，未经允许不得转载。</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 06:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/yunqi/blog/11044520</guid>
            <link>https://my.oschina.net/yunqi/blog/11044520</link>
            <author>
                <![CDATA[阿里云云栖号]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源日报 | 目前的人工智能技术连猫的智能水平都没达到]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>欢迎阅读 OSCHINA 编辑部出品的开源日报，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.22</strong></span></h3><h2><strong><span style="color:#16a085">今日要点</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/279713/google-gemma-open-models" target="_blank">谷歌发布轻量级开源大语言模型 Gemma</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Gemma 是一款轻量级、先进的开源模型，供开发者和研究人员用于 AI 构建。Gemma 模型家族包括 2B（20 亿参数）和 7B（70 亿参数）两种尺寸，能够在不同的设备类型上运行，包括笔记本电脑、桌面电脑、IoT 设备、移动设备和云端。</p><p><img src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><h3><a href="https://www.oschina.net/news/279741/nightingale-7-0-0-beta-0-released" target="_blank">夜莺监控 V7 第一个 beta 版本</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">夜莺项目从 2024 开始开发 V7 版本，重点做体验优化，V7 和 V6 版本兼容可以平滑升级<span style="background-color:#ffffff; color:#333333">（V6 升级到 V7 只需要替换一下二进制重启即可，如果是容器部署，只需要更新镜像并重启）</span>，第一个 beta 的优化项包括：</p><ul><li>全站暗黑主题</li><li>优化边缘机房机器失联告警的实现逻辑，真正做到边缘机房告警自闭环</li><li>优化内置大盘、内置告警规则的列表页面 UI</li><li>全局回调地址页面展示优化，增加详尽的文档提示信息</li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="20240221141801" src="https://download.flashcat.cloud/ulric/20240221141801.png" referrerpolicy="no-referrer"></p><hr><h2><strong><span style="color:#16a085">今日观察</span></strong></h2><p><img height="554" src="https://oscimg.oschina.net/oscnet/up-14e1acb77ab8633d225b3117ebbc7dc7920.png" width="1522" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博<span>&nbsp;</span></span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1233486457%2FO1MdFaqZm" target="_blank">高飞</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-a2ff0599a45c140a6bf468a7d85524102fa.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博<span>&nbsp;</span></span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1856404484%2FO1JFSAEg0" target="_blank">凤凰网科技</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推荐</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-da11d19579f9cc14d57a12c12f4479f067a.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>开源之声</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-33430582a60711cf56a923991e1fef04da9.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日项目榜</strong></span></h2><p>GitHub Trending</p><p><img src="https://oscimg.oschina.net/oscnet/up-291d0213846cc3efbd09b526da6bac5ae29.png" referrerpolicy="no-referrer"></p><hr><p><strong>往期回顾</strong></p><ul><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC013%E6%9C%9F%EF%BC%9A%E7%AD%89%E5%88%B0%20Sora%20%E5%BC%80%E6%BA%90%E4%BA%86%E7%AB%8B%E5%88%BB%E6%8E%A8%E5%87%BA%E5%B1%9E%E4%BA%8E%E6%88%91%E4%BB%AC%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B.pdf">开源日报第 013 期：等到 Sora 开源了立刻推出属于我们自己的大模型</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">开源日报第 012 期：Sora 给中国 AI 带来的真实变化；Dart 3.3 发布</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">开源日报第 011 期：目前还没有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">开源日报第 010 期：Tauri v2 支持 Android 和 iOS，跨平台开发新选择</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">开源日报第 009 期：Vue.js 诞生 10 周年；扎克伯格解释 Meta 为什么要开源其 AI 技术</a></li><li><a href="https://www.oschina.net/news/277585">开源日报第 008 期：推动中国开源软硬件发展的经验与建议</a></li><li><a href="https://www.oschina.net/news/277415">开源日报第 007 期：「Linux 中国」 开源社区宣布停止运营</a></li><li><a href="https://www.oschina.net/news/277214">开源日报第 006 期：选择技术栈一定要选择开源的</a></li><li><a href="http://www.oschina.net/news/277040">开源日报第 005 期：RISC-V 万兆开源交换机发售；npm 存在大量武林外传视频</a></li><li><a href="https://www.oschina.net/news/276864">开源日报第 004 期：百度输入法在候选词区域植入广告；大神用 Excel 构建 CPU</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 03:53:48 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279923</guid>
            <link>https://www.oschina.net/news/279923</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Altman 回应 7 万亿美元半导体计划：所需投资远超想象]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">在英伟达发布了强劲的 2024 财年第四季度财报之后的几小时，英特尔首席执行官 Pat Gelsinger 和 OpenAI 首席执行官 Sam Altman 在加利福尼亚州圣何塞的一个会议中心展开了一场对话，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fapnews.com%2Farticle%2Fintel-openai-nvidia-chips-boom-dbf20077caafc9b33870f9f6d32d3794" target="_blank">畅谈</a>半导体在 AI 时代塑造社会所扮演的角色。</span></p><p><span style="color:#000000"><img alt="" height="375" src="https://oscimg.oschina.net/oscnet/up-67bd113ff9160357624584468c53d04258a.webp" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">在活动中，Gelsinger 询问了 Altman 有关最近报道的计划从中东地区筹集高达 7 万亿美元的资金，以支持 OpenAI 的一项半导体计划，并与英伟达展开竞争的传闻。对此，Altman 则反驳称，这种匿名人士未经证实的说法比比皆是，「我的主要工作不是到处修正这些错误的文章」。</span></p><p><span style="color:#000000">但 Altman 同时也承认，AI 的发展需要大量的资金。「事实是，我们认为世界将需要更多的 AI 计算（芯片）。这将需要全球范围内大量的投入，超出我们的想象。我们现在还没有一个具体数字。」</span></p><p><span style="color:#000000">他还强调了过去一年加快人工智能发展的重要性。他认为人工智能的进步将为人类带来更美好的未来，不过奥特曼也承认，在前进的过程中会有不利的一面。"我们正在走向这样一个世界：人工智能生成的内容将多于人类生成的内容。这将不仅仅是一个 good story，而是一个 net good story。"</span></p><p><span style="color:#000000">此外，Altman 重申了在 AI 领域进行监管的必要性，强调了政府在制定框架以降低潜在风险方面的关键作用。</span></p><p><span style="color:#000000">Gelsinger 预测，</span><span style="background-color:#ffffff; color:#000000">到 2030 年，英特尔将成为全球第二大代工企业，仅次于台积电。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 03:12:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279911/intel-openai-chips</guid>
            <link>https://www.oschina.net/news/279911/intel-openai-chips</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[和 Gmail 基本 HTML 视图说再见]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌已开始<u><a href="https://www.oschina.net/news/259383">停止支持 Gmail </a><a href="https://www.oschina.net/news/259383">基本 HTML 视图</a></u>。自 2024 年 2 月起，Gmail 会自动将用户从基本 HTML 视图转换为标准视图。</p><p><img src="https://oscimg.oschina.net/oscnet/up-0cdd4e28b49bf3c37718b40baee96da1d75.png" referrerpolicy="no-referrer"></p><p><em>基本 HTML 视图允许用户以简陋的方式查看电子邮件，但对所有的浏览器提供了最大的兼容性。</em></p><p>Gmail 更新了其<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsupport.google.com%2Fmail%2Fanswer%2F15049%3Fhl%3Dzh-Hans" target="_blank">支持页面</a>，以反映 Gmail 将在截止日期后自动切换到标准视图。不仅如此，有用户在<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.ycombinator.com%2Fitem%3Fid%3D37558372" target="_blank">Hacker News</a>上发帖称，他们收到了一封来自 Google 的邮件，表明该功能已经结束。</p><blockquote><p>"我们写信通知您，从 2024 年 1 月初开始，桌面网页和移动网页的 Gmail Basic HTML 视图将被禁用。Gmail Basic HTML 视图是 Gmail 的旧版本，10 多年前就已被现代版本取代，不包含完整的 Gmail 功能。"</p></blockquote><p>即使在今天，当你尝试访问 HTML 版本时，Google 也会显示一条信息，称该版本是为"较慢的连接速度和传统浏览器"设计的，并要求你确认是否不想使用标准版本。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-cca03ff735c3d625769511865800215a87d.png" referrerpolicy="no-referrer"></p><p>HTML 版本缺少很多功能，如聊天、拼写检查、搜索过滤器、键盘快捷键和丰富的格式。但在连接性较差的地区或只想浏览电子邮件而不想使用任何额外功能的情况下，HTML 版还是很有用的。目前还不清楚 Google 是否计划添加低连接模式。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:41:03 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279900/goodbye-html-gmail</guid>
            <link>https://www.oschina.net/news/279900/goodbye-html-gmail</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[记一次 Rust 内存泄漏排查之旅]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在某次持续压测过程中，我们发现 GreptimeDB 的 Frontend 节点内存即使在请求量平稳的阶段也在持续上涨，直至被 OOM kill。我们判断 Frontend 应该是有内存泄漏了，于是开启了排查内存泄漏之旅。</p><h2>Heap Profiling</h2><p>大型项目几乎不可能只通过看代码就能找到内存泄漏的地方。所以我们首先要对程序的内存用量做统计分析。幸运的是，GreptimeDB 使用的 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjemalloc%2Fjemalloc%2Fwiki%2FUse-Case%253A-Heap-Profiling" target="_blank">jemalloc 自带 heap profiling</a>，我们也<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fblob%2Fdevelop%2Fsrc%2Fcommon%2Fmem-prof%2FREADME.md" target="_blank">支持了导出 jemalloc 的 profile dump 文件</a>。于是我们在 GreptimeDB 的 Frontend 节点内存达到 300MB 和 800MB 时，分别 dump 出了其内存 profile 文件，再用 jemalloc 自带的 <code>jeprof</code> 分析两者内存差异（<code>--base</code> 参数），最后用火焰图显示出来：</p><p><img src="https://oscimg.oschina.net/oscnet/up-002154d38e6da2e50485895918972b1b8a1.png" alt="" referrerpolicy="no-referrer"></p><p>显然图片中间那一大长块就是不断增长的 500MB 内存占用了。仔细观察，居然有 thread 相关的 stack trace。难道是创建了太多线程？简单用 <code>ps -T -p</code> 命令看了几次 Frontend 节点的进程，线程数稳定在 84 个，而且都是预知的会创建的线程。所以「线程太多」这个原因可以排除。</p><p>再继续往下看，我们发现了很多 Tokio runtime 相关的 stack trace，而 Tokio 的 task 泄漏也是常见的一种内存泄漏。这个时候我们就要祭出另一个神器：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ftokio-rs%2Fconsole" target="_blank">Tokio-console</a>。</p><h2>Tokio Console</h2><p>Tokio Console 是 Tokio 官方的诊断工具，输出结果如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-1772a99dbc1b17d9b0ab49532e083a4234f.png" alt="" referrerpolicy="no-referrer"></p><p>我们看到居然有 5559 个正在运行的 task，且绝大多数都是 Idle 状态！于是我们可以确定，内存泄漏发生在 Tokio 的 task 上。 现在问题就变成了：GreptimeDB 的代码里，哪里 spawn 了那么多的无法结束的 Tokio task？</p><p>从上图的 "Location" 列我们可以看到 task 被 spawn 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fblob%2Fdevelop%2Fsrc%2Fcommon%2Fruntime%2Fsrc%2Fruntime.rs%23L63" target="_blank">地方</a>：</p><pre><code class="language-rust">impl Runtime {
    /// Spawn a future and execute it in this thread pool
    ///
    /// Similar to Tokio::runtime::Runtime::spawn()
    pub fn spawn&lt;F&gt;(&amp;self, future: F) -&gt; JoinHandle&lt;F::Output&gt;
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        self.handle.spawn(future)
    }
}
</code></pre><p>接下来的任务是找到 GreptimeDB 里所有调用这个方法的代码。</p><h2><code>..Default::default()</code>！</h2><p>经过一番看代码的仔细排查，我们终于定位到了 Tokio task 泄漏的地方，并在 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fpull%2F1512" target="_blank">PR #1512</a> 中修复了这个泄漏。简单地说，就是我们在某个会被经常创建的 struct 的构造方法中，spawn 了一个可以在后台持续运行的 Tokio task，却未能及时回收它。对于资源管理来说，在构造方法中创建 task 本身并不是问题，只要在 <code>Drop</code> 中能够顺利终止这个 task 即可。而我们的内存泄漏就坏在忽视了这个约定。</p><p>这个构造方法同时在该 struct 的 <code>Default::default()</code> 方法当中被调用了，更增加了我们找到根因的难度。</p><p>Rust 有一个很方便的，可以用另一个 struct 来构造自己 struct 的方法，即 "<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdoc.rust-lang.org%2Fbook%2Fch05-01-defining-structs.html%23creating-instances-from-other-instances-with-struct-update-syntax" target="_blank">Struct Update Syntax</a>"。如果 struct 实现了 <code>Default</code>，我们可以简单的在 struct 的 field 构造中使用 <code>..Default::default()</code>。如果 <code>Default::default()</code> 内部有 「side effect」（比如我们本次内存泄漏的原因——创建了一个后台运行的 Tokio task），一定要特别注意：struct 构造完成后，<code>Default</code> 创建出来的临时 struct 就被丢弃了，一定要做好资源回收。</p><p>例如下面这个小例子：（<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplay.rust-lang.org%2F%3Fversion%3Dstable%26mode%3Ddebug%26edition%3D2021%26gist%3Dc121ffd32d2ff0fa8e1241a62809bcef" target="_blank">Rust Playground</a>）</p><pre><code class="language-rust">struct A {
    i: i32,
}

impl Default for A {
    fn default() -&gt; Self {
        println!("called A::default()");
        A { i: 42 }
    }
}

#[derive(Default)]
struct B {
    a: A,
    i: i32,
}

impl B {
    fn new(a: A) -&gt; Self {
        B {
            a,
            // A::default() is called in B::default(), even though "a" is provided here.
            ..Default::default()
        }
    }
}

fn main() {
    let a = A { i: 1 };
    let b = B::new(a);
    println!("{}", b.a.i);
}
</code></pre><p>struct A 的 <code>default</code> 方法是会被调用的，打印出 <code>called A::default()</code>。</p><h2>总结</h2><ul><li>排查 Rust 程序的内存泄漏，我们可以用 jemalloc 的 heap profiling 导出 dump 文件；再生成火焰图可直观展现内存使用情况。</li><li>Tokio-console 可以方便地显示出 Tokio runtime 的 task 运行情况；要特别注意不断增长的 idle tasks。</li><li>尽量不要在常用 struct 的构造方法中留下有副作用的代码。</li><li><code>Default</code> 只应该用于值类型 struct。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-3d33a6c61f7c37725020cbe267d0c2d0f01.jpg" alt="" referrerpolicy="no-referrer"></p><h3>关于 Greptime</h3><p>Greptime 格睿科技于 2022 年创立，目前正在完善和打造时序数据库 GreptimeDB 和格睿云 GreptimeCloud 这两款产品。</p><p>GreptimeDB 是一款用 Rust 语言编写的时序数据库，具有分布式、开源、云原生、兼容性强等特点，帮助企业实时读写、处理和分析时序数据的同时，降低长期存储的成本。</p><p>GreptimeCloud 基于开源的 GreptimeDB，为用户提供全托管的 DBaaS，以及与可观测性、物联网等领域结合的应用产品。利用云提供软件和服务，可以达到快速的自助开通和交付，标准化的运维支持，和更好的资源弹性。GreptimeCloud 已正式开放内测，欢迎关注公众号或官网了解最新动态！</p><p>官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreptime.com%2F" target="_blank">https://greptime.com/</a></p><p>公众号：GreptimeDB</p><p>GitHub: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb" target="_blank">https://github.com/GreptimeTeam/greptimedb</a></p><p>文档：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.greptime.com%2F" target="_blank">https://docs.greptime.com/</a></p><p>Twitter: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2FGreptime" target="_blank">https://twitter.com/Greptime</a></p><p>Slack: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreptime.com%2Fslack" target="_blank">https://greptime.com/slack</a></p><p>LinkedIn: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fgreptime%2F" target="_blank">https://www.linkedin.com/company/greptime/</a></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:23:03 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6839317/blog/11044292</guid>
            <link>https://my.oschina.net/u/6839317/blog/11044292</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Stability AI 发布 Stable Diffusion 3 早期预览版]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>AI 创业公司 Stability AI <u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fstable-diffusion-3" target="_blank">宣布</a></u>其最新一代的文本图像模型 Stable Diffusion 3 开放预览，该版本目前仅限部分用户参与测试，主要是为了在正式发布前收集与性能和安全性相关的用户反馈。感兴趣的用户可以申请加入等候名单。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3fa2fd390f1fbdd7808ee91a4ed1a0ef4fd.png" referrerpolicy="no-referrer"></p><p>Stable Diffusion 3 早期预览版相比前代产品在图片质量、多主题展示和文字展示方面有大幅提升。Stable Diffusion 3 模型的参数规模从 8 亿，到 80 亿不等，其架构组合了 diffusion transformer（扩散变换架构）和 flow matching（流匹配），技术报告将在晚些时候公布。</p><p><img height="582" src="https://oscimg.oschina.net/oscnet/up-25784ed1b6a0b80ca5fc50b3c842456064f.png" width="3052" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-377f7b831888b11ab490f50cafe8931753d.png" referrerpolicy="no-referrer"></p><p>性能的具体提升内容包括：</p><ol><li>多主题提示处理能力： 新模型对于包含多个主题或元素的提示具有更好的理解和处理能力。这意味着用户可以在一个提示中描述更复杂的场景，而模型能够更准确地根据这些描述生成图像。</li><li>图像质量： Stable Diffusion 3 在生成的图像质量上有显著提高，包括更细腻的细节表现、更准确的颜色匹配以及更自然的光影处理。这些改进使得生成的图像更加逼真，更能捕捉到用户的创意意图。</li><li>拼写和文本处理能力： 这个版本在处理文本元素，尤其是在图像中直接展现的文本（如标语、标签等）时，有更好的拼写能力和文本理解。这包括更准确地识别和渲染用户提示中的文字，甚至是在复杂的视觉背景中。</li></ol><p>Stable Diffusion 3 的性能提升不仅基于其先进的扩散变换架构，还包括了以下关键的技术创新和改进：</p><ol><li>新型扩散变换器： Stable Diffusion 3 采用了一种新型的扩散变换技术，与 Sora 类似，这种新技术为模型提供了更强大的图像生成能力。 Transformer 是一种深度学习模型，专门设计来逐步构建图像的细节，从而生成高质量的视觉内容。</li><li>流匹配与其他改进： 模型还整合了流匹配技术和其他技术改进，进一步增强了生成图像的质量和多样性。流匹配技术有助于模型更好地理解和模拟图像中的动态元素和结构，使得生成的图像在视觉上更加连贯和自然。</li><li>利用 Transformer 的改进： Stable Diffusion 3 充分利用了 Transformer 技术的最新进展，这不仅使模型能够进一步扩展其能力，还使其能够接受多模态输入。这意味着模型能够处理更复杂和多样化的数据类型，如结合文本和图像的输入，从而在理解和生成图像内容方面提供更大的灵活性和精确度。</li></ol><p>加入等候名单：<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fstablediffusion3" target="_blank">https://stability.ai/stablediffusion3</a></em></u></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:12:30 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279886/stable-diffusion-3-preview</guid>
            <link>https://www.oschina.net/news/279886/stable-diffusion-3-preview</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[「未来产业划定发展路线图」出炉]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span>近日，工业和信息化部、科技部、交通运输部、文化和旅游部等部门</span><span>联合印发</span><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMjM5OTUwMTc2OA%3D%3D%26mid%3D2650903280%26idx%3D1%26sn%3Db6edd0ce1c41aac431cd9f9a52eef8be%26chksm%3Dbccfb8578bb831412b6e96f801604afe81effbd68c11d7371507f00349b1b3accc4cc18e01b4%26scene%3D21%23wechat_redirect" target="_blank">《关于推动未来产业创新发展的实施意见》</a><span>，提出到 2025 年，未来产业技术创新、产业培育、安全治理等全面发展，部分领域达到国际先进水平，产业规模稳步提升；到 2027 年，未来产业综合实力显著提升，部分领域实现全球引领。</span></p><p>专家认为，《意见》充分把握全球科技创新和产业发展趋势，前瞻部署了生物制造、量子信息、氢能、核能、基因和细胞技术等多个细分赛道，将全面支撑推进新型工业化，加快形成新质生产力。</p><p style="margin-left:0; margin-right:0"><strong><span>全面布局新赛道</span></strong></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>未来产业由前沿技术驱动，尚处于孕育萌发阶段或产业化初期，是具有显著战略性、引领性、颠覆性和不确定性的前瞻性新兴产业。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">当前，新一轮科技革命和产业变革加速演进，重大前沿技术、颠覆性技术持续涌现，科技创新和产业发展融合不断加深，催生出元宇宙、人形机器人、脑机接口、量子信息等新产业发展方向。大力培育未来产业已成为引领科技进步、带动产业升级、开辟新赛道、塑造新质生产力的战略选择。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">我国具备工业体系完整、产业规模庞大、应用场景丰富等综合优势，为未来产业发展提供了丰厚的土壤。各省（区、市）积极培育未来产业，北京、上海、江苏、浙江等地出台了培育未来产业的政策文件。但我国未来产业发展也面临系统谋划不足、技术底座不牢等问题。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">针对这些问题，《意见》从技术创新、产品突破、企业培育、场景开拓、产业竞争力等方面提出到 2025 年和 2027 年的发展目标。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">赛迪研究院未来产业研究中心所长韩健介绍，到 2025 年要形成「一批+6 百」的目标体系，建设一批未来产业孵化器和先导区，突破百项前沿关键核心技术，形成百项标志性产品，打造百家领军企业，开拓百项典型应用场景，制定百项关键标准，培育百家专业服务机构，初步形成符合我国实际的未来产业发展模式。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">《意见》重在产业化落地。赛智产业研究院院长赵刚认为，《意见》提出以传统产业的高端化升级和前沿技术的产业化落地为主线，力争做到两年「打基础」，五年「大提升」，成为世界未来产业重要策源地。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">此外，《意见》还详细规划了六大方向超过 50 多个细分领域的未来产业发展，明确提出了下一代智能终端、信息服务产品、未来高端装备三类标志性产品发展路线。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">「设定未来产业发展目标既是我国推进新型工业化的自身现实需求，也是参与国际竞争的外部形势要求。从自身需求看，是我国引领科技进步、带动产业升级、培育新质生产力的战略选择；从外部需求看，是我国主动参与全球未来产业分工合作、深度融入全球创新网络的必然选择。」赵刚说。</p><p style="margin-left:0; margin-right:0"><strong><span>重点瞄准六大方向</span></strong></p><p><span>未来产业发展的核心是前沿技术创新突破。《意见》按照「技术创新—前瞻识别—成果转化」的思路，提出面向未来制造、未来信息、未来材料、未来能源、未来空间、未来健康六大重点方向，实施国家科技重大项目和重大科技攻关，发挥国家实验室、全国重点实验室等创新载体作用，鼓励龙头企业牵头成立创新联合体，体系化推进关键核心技术攻关。</span></p><p>赵刚分析，与优势产业、传统产业、战略性新兴产业相比，未来产业有 3 个明显特征。未来产业技术创新不是渐进式微创新，而是前瞻性、颠覆性重大创新，例如未来信息产业中的通用人工智能和量子信息、未来健康产业中的基因工程、未来材料产业中的超导材料等技术创新；未来产业生产要素配置不是传统要素线性叠加，而是现代要素相互融合和配置效率指数级提升，例如量子计算机能让计算能力实现成千上万倍增加；未来产业边界不是界限清晰，而是呈现出不同产业跨界融合和智能化、绿色化等发展特征，如智能制造、生物材料、人形机器人、脑机接口等。</p><p>对于这六大方向业内已有布局。早在 2016 年，字节跳动公司就成立了人工智能实验室，聚焦研究自然语言处理、机器学习、数据挖掘等方面。2023 年以来，字节跳动公司加码人工智能应用研究，旗下产品不断加入 AIGC（生成式人工智能）功能。比如，结合火山引擎智能创作云的 AIGC 能力，火山引擎视频云在商品营销、互动娱乐、在线教育、智能驾驶等场景引入数字人、虚拟直播间等，助力企业降本增效，提升用户体验。</p><p>「技术创新是经济长期持续增长的不竭动力，发展未来产业是高质量发展的前瞻性战略布局。今天对未来产业 20% 的投入和布局，将为以后带来 80% 的收益，从而建立起我国经济高质量发展的长效创新机制。」赵刚说。</p><p style="margin-left:0; margin-right:0"><strong><span>打造标志性创新产品</span></strong></p><p><span>《意见》提出，打造人形机器人、脑机接口、超大规模新型智算中心、第三代互联网等十大创新标志性产品。</span></p><p>赵刚分析，当颠覆式技术创新呈现出技术性能成倍提升、产品化成本大幅降低、应用场景广泛等特征后，创新产品就形成规模经济效应，具有巨大的市场前景。</p><p>当前，满足这 3 个特征的标志性产品主要有两类。一是通用人工智能产品。由于以 ChatGPT 为代表的通用人工智能技术取得重大进展，围绕通用人工智能技术创新形成的智能产品，如生成式人工智能产品、AI 手机和个人计算机、人形机器人、高级别智能网联汽车、智能装备、智能云服务、超大规模新型智算中心等智能产品和服务就具有较好前景。二是生物科技产品。由于细胞和基因工程等技术取得突破性进展，生物科技创新产品工程化能力加速提升，具有很好的市场前景，如基因编辑、合成生物等。其他一些前瞻性技术尽管在实验室获得了成功，但离大规模产品化和商业化还有很大差距，例如量子信息技术创新。</p><p>国际数据公司 IDC 预测，人工智能电脑在中国个人计算机市场中新机的装配比例将快速攀升，2027 年有望达 85%，成为市场主流。联想集团副总裁、中国区战略及业务拓展副总裁阿不力克木·阿不力米提表示，人工智能电脑是自然语言交互的个人 AI 助理。在过去 40 年发展历程中，联想不断推出变革用户体验的产品，未来还将和生态伙伴携手实现人工智能电脑快速普及，让 AI 惠及每一个人。</p><p>目前我国算力总规模排名全球第二位。但从结构看，通用算力占了大半，高性能算力占比有待提升。浪潮信息高级副总裁刘军表示，高质量算力采用先进的计算架构，具备高算效、高能效、可持续、可获得、可评估五大特征。其中，高算效是实测性能与资源利用率双重提升，是算力供需失衡、算力利用率低等矛盾的破解之道。而高能效是在最低碳排放前提下实现最大化算力输出，确保能源利用最优解。</p><p>脑机接口作为十大标志性产品之一，近年来在电极、算法、芯片等方面取得了重要进展。2023 年 9 月，中国信息通信研究院云计算与大数据研究所牵头在人工智能医疗器械创新合作平台成立脑机接口研究工作组。</p><p>中国信通院云计算与大数据研究所副所长闵栋介绍，脑机接口可应用于医疗、娱乐、智能生活、教育等领域。其中，医疗领域是主要阵地。脑机接口与医疗结合展现出广阔应用前景，为相关疾病诊疗和康复提供了全新手段。此外，脑机接口还可与虚拟现实、人机交互、人工智能等技术结合推动现有产业变革，如脑机接口应用于工业领域，可帮助人们通过意念操控机器人、无人车、工业产线等设备。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">未来产业潜在价值巨大，需要资本持续投入。赵刚建议，要推动制造业转型升级基金、国家中小企业发展基金等加大投入，也可适时组建国家未来产业发展基金，并引导地方设立未来产业专项资金，发挥政府引导基金的引导性作用，吸引社会资本共同投资未来产业。同时，完善金融财税支持政策。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 05:58:15 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279756</guid>
            <link>https://www.oschina.net/news/279756</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源日报：等到 Sora 开源了立刻推出属于我们自己的大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>欢迎阅读 OSCHINA 编辑部出品的开源日报，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.21</strong></span></h3><h2><strong><span style="color:#16a085">今日要点</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/279326/linux-kernel-is-a-cna" target="_blank">2023 年度 Rust 调查报告</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000">有 93% 的受访者称自己是 Rust 用户，其中 49% 的人每天（或几乎每天）都会使用 Rust，相较上一年小幅增加 2 个百分点。在没有使用 Rust 的用户中，31% 的人表示主要原因时使用 Rust 有难度；67% 的人表示他们还没有机会优先学习 Rust，这也是最常见的原因。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000"><img alt="" height="429" src="https://oscimg.oschina.net/oscnet/up-335afef97d65f03d34d0f08eb8831153557.png" width="500" referrerpolicy="no-referrer"></span></p><h3><a href="https://www.oschina.net/news/279389/dart-3-3-released" target="_blank">Go 1.22 正式发布</a></h3><p>Go 1.22 中新增的优化之一是改进了虚拟化，允许静态调度更多的接口方法调用。启用 PGO 后，大多数程序的性能将提高 2% 至 14%。 此外，Go 运行时中的内存优化可将 CPU 性能提高 1-3%，同时还可将大多数 Go 程序的内存开销减少约 1%。</p><p>新的 math/rand/v2 软件包提供了更简洁、更一致的应用程序接口，并使用了质量更高、速度更快的伪随机生成算法。&nbsp;</p><hr><h2><strong><span style="color:#16a085">今日观察</span></strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-9fc6f45ad8365450e0980fe7fe1855a56c5.png" referrerpolicy="no-referrer"></p><p>- 微博 <u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F6640496217%2FO1tM7BenZ" target="_blank">-赤犬龙之介-</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-d423a4b13075c233da7226ccd5235dc0a3e.png" referrerpolicy="no-referrer"></p><p>-&nbsp;<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fishare.ifeng.com%2Fc%2Fs%2F8XMQgC7LlaK" target="_blank">21 世纪经济报道</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推荐</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-25f6c0a5f5becd775a0ad6bd1080893da87.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>开源之声</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-bbc6216c930e22f859d514d8becc057daae.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日项目榜</strong></span></h2><p>Gitee 榜单：</p><p><img src="https://oscimg.oschina.net/oscnet/up-5ab4f59bdf01add8c61aac9c617fa074d2c.png" referrerpolicy="no-referrer"></p><hr><p><strong>往期回顾</strong></p><ul><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">开源日报第 012 期：Sora 给中国 AI 带来的真实变化；Dart 3.3 发布</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">开源日报第 011 期：目前还没有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">开源日报第 010 期：Tauri v2 支持 Android 和 iOS，跨平台开发新选择</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">开源日报第 009 期：Vue.js 诞生 10 周年；扎克伯格解释 Meta 为什么要开源其 AI 技术</a></li><li><a href="https://www.oschina.net/news/277585">开源日报第 008 期：推动中国开源软硬件发展的经验与建议</a></li><li><a href="https://www.oschina.net/news/277415">开源日报第 007 期：「Linux 中国」 开源社区宣布停止运营</a></li><li><a href="https://www.oschina.net/news/277214">开源日报第 006 期：选择技术栈一定要选择开源的</a></li><li><a href="http://www.oschina.net/news/277040">开源日报第 005 期：RISC-V 万兆开源交换机发售；npm 存在大量武林外传视频</a></li><li><a href="https://www.oschina.net/news/276864">开源日报第 004 期：百度输入法在候选词区域植入广告；大神用 Excel 构建 CPU</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:47:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279735</guid>
            <link>https://www.oschina.net/news/279735</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[数百位名人签署公开信，呼吁制定反深度伪造立法]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>数百名 AI 界人士签署了一封<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenletter.net%2Fl%2Fdisrupting-deepfakes" target="_blank">公开信</a>，呼吁严格监管 AI 生成的冒名顶替或深度伪造（Deepfake）内容。</p><p><img height="274" src="https://oscimg.oschina.net/oscnet/up-df56f62fff93b887bf1198aca4ccf4bcb5e.png" width="500" referrerpolicy="no-referrer"></p><p>公开信指出，"深度伪造"是指未经同意或严重误导的人工智能生成的声音、图像或视频，合理的人会误以为是真实的。这不包括对图像或声音的轻微改动，也不包括容易识别为合成的无害娱乐或讽刺。</p><p>如今，深度伪造通常涉及性图像、欺诈或政治虚假信息。由于人工智能发展迅速，使得深度伪造变得更加容易，因此我们需要为数字基础设施的运行和完整性提供保障。「Deepfakes 对社会的威胁日益严重，政府必须在整个供应链中施加义务，以阻止 Deepfakes 的扩散。」</p><p>信中呼吁：</p><ul><li>将深度伪造的儿童性虐待材料（CSAM，又名儿童色情制品）完全定为刑事犯罪，无论所描绘的人物是真实的还是虚构的。</li><li>在任何情况下，如果有人制造或传播有害的深度伪造品，都需要受到刑事处罚。</li><li>要求软件开发商和分销商防止其音频和视频产品被用于制造有害的深度伪造品，如果他们的预防措施不充分，就要承担责任接受处罚。</li></ul><p>他们认为，如果设计得当，这些法律可以在不会造成过重负担的同时，培育有社会责任感的企业。</p><p>这封信中较为知名的签名者包括：</p><ul><li>Jaron Lanier</li><li>Frances Haugen</li><li>Stuart Russell</li><li>Andrew Yang</li><li>Marietje Schaake</li><li>Steven Pinker</li><li>Gary Marcus</li><li>Oren Etzioni</li><li>Genevieve smith</li><li>Yoshua Bengio</li><li>Dan Hendrycks</li><li>Tim Wu</li></ul><p>事实上，这也不是首次出现相关的呼吁。在本月早些时候正式提出之前，欧盟已就此类措施进行了多年辩论。外媒 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2024%2F02%2F21%2Fhundreds-of-ai-luminaries-sign-letter-calling-for-anti-deepfake-legislation%2F" target="_blank">TechCrunch</a> 指出，也许正是欧盟愿意进行审议和落实，才激活了这些研究人员、创作者和管理人员的发言权。虽然此举不一定能推动真正的立法，但它确实是业界专家们如何看待这一争议问题的风向标。</p><p>更多详情可查看此处：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenletter.net%2Fl%2Fdisrupting-deepfakes" target="_blank">https://openletter.net/l/disrupting-deepfakes</a>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:32:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279733/disrupting-deepfakes</guid>
            <link>https://www.oschina.net/news/279733/disrupting-deepfakes</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[出门问问创始人李志飞点评谷歌开源大模型 Gemma：差点意思]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌在北京时间昨晚发布了<u><a href="https://www.oschina.net/news/279713/google-gemma-open-models">开源大模型 Gemma</a></u>，对标 Meta 旗下 Llama 2。</p><p><img src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><p>出门问问创始人李志飞发表文章点评，<strong>称 Gemma 推出时间有点晚、开源力度不够、未放下高贵的头颅</strong>。</p><p>李志飞在文章中表示，相比于去年上半年就开源，现在可能要花数倍的努力进行模型的差异化以及推广的投入，才有可能在众多开源模型中脱颖而出。「面对 OpenAI 的强力竞争，只有杀敌一千、自损一千五。」</p><p>以下为李志飞全文：</p><blockquote><p>看到 Google 开源了小的语言模型 Gemma，直接狙击 Llama 2，回顾去年 5 月对 Google 关于开源和竞争的看法，几点思考如下：</p><p>1. 时间有点晚：相比于去年上半年就开源，现在可能要花数倍的努力进行模型的差异化以及推广的投入，才有可能在众多开源模型中脱颖而出。</p><p>2. 开源力度不够：感觉这次开源还是被动防御和略显扭捏的应对之策，不是进攻。比如说，开个 7B 的模型实在是太小儿科了，一点杀伤力都没有。应该直接开源一个超越市场上所有开源的至少 100B 的模型、1M 的超长上下文、完善的推理 infra 方案、外加送一定的 cloud credit。是的，再不歇斯底里 Google 真的就晚了。面对 OpenAI 的强力竞争，只有杀敌一千、自损一千五。</p><p>3. 未放下高贵的头颅：有种感觉，Google 觉得自己还是 AI 王者，放不下高贵的头颅，很多发布都有点不痛不痒，还是沿着过去研发驱动的老路而不是产品和竞争驱动，比如不停发论文、取新名字（多模态相关模型过去半年就发了 Palme、RT-2、Gemini、VideoPoet、W.A.L.T 等）、发布的模型又完整度不够，感觉就没有一个绝对能打的产品。Google 可能要意识到在公众眼中，他在 AI 领域已经是廉颇老矣溃不成军，经常起大早赶晚集（比如说这次 Sora 借鉴的 ViT、ViViT、NaVit、MAGVit 等核心组件技术都是它家写的论文）。</p><p>4. 希望亡羊补牢未为晚也：Google 作为一个僵化的大公司，动作慢一点可以理解，但是如果再不努力是不是就是 PC 互联网的 IBM、移动互联网的 Microsoft？ 作为 Google 的铁粉，还是希望他能打起精神一战，AI 产业需要强力的竞争才能不停向前发展，也需要他在前沿研究和系统的开源才能帮助一大众「贫穷」的 AI 创业公司。</p><p>5. 另外，除了对外开源外，Google 应该组成三个方阵面对大模型的竞争，详见去年 3 月发文。</p><p>回顾科技竞争史，PC 互联网时代的 IBM、移动互联网时代的 Microsoft、AGI 时代的 Google，新时代来临后，难道上一个时代科技霸主都难逃衰落的宿命？</p><p>当然，Microsoft 靠 Office SaaS、云和 OpenAI 又翻盘了。</p><p>历史的铁律，有被改写的可能吗？</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:15:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279728</guid>
            <link>https://www.oschina.net/news/279728</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenAI 员工的一天]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Chain Of Thought (CoT) 第一作者、从谷歌跳槽到 OpenAI 的 Jason Wei 分享了自己在 OpenAI 的一天：</p><blockquote><p>[9:00am] 起床</p><p>[9:30am] 搭乘 Waymo 前往 Mission SF，途中在 Tartine 买个牛油果吐司</p><p>[9:45am] 背诵 OpenAI 章程。向优化之神致敬，学习《The Bitter Lession》（强化学习之父 Rich Sutton 著）</p><p>[10:00am] 在 Google Meet 上开会，讨论如何在更多数据上训练更大的模型</p><p>[11:00am] 敲代码，在更多数据上训练更大的模型。搭档是 Hyung Won Chung</p><p>[12:00pm] 去食堂吃午饭（纯素且无麸质）</p><p>[1:00pm] 真正开始在大量数据上训练大模型</p><p>[2:00pm] 处理基础设施问题（我是脑子被驴踢了吗为啥要从 master 分支拉代码？）</p><p>[3:00pm] 监控模型训练进度，玩 Sora</p><p>[4:00pm] 给刚才训练的大模型上提示工程</p><p>[4:30pm] 短暂休息，坐在牛油果椅子上，思考 Gemini Ultra 的性能到底有多强</p><p>[5:00pm] 头脑风暴模型可能的算法改进</p><p>[5:05pm] 修改算法风险太高，pass。最安全的策略还是力大砖飞（增加算力和数据规模）</p><p>[6:00pm] 晚餐时间，和 Roon 一起享用蛤蜊浓汤</p><p>[7:00pm] 回家</p><p>[8:00pm] 喝点小酒，继续写码。Ballmer’s peak（酒精带来的编码高效阶段）即将到来</p><p>[9:00pm] 分析实验结果，我对 wandb 又爱又恨</p><p>[10:00pm] 启动实验，让它自己跑一晚上，第二天查看结果</p><p>[1:00am] 实验真正开始运行</p><p>[1:15am] 上床睡觉。在纳德拉和黄仁勋的守护下入梦。心想：压缩才是真谛。晚安</p><p><img src="https://oscimg.oschina.net/oscnet/up-3041206e9fa5084776ebedb71c760828888.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2F_jasonwei%2Fstatus%2F1760032264120041684" target="_blank">https://twitter.com/_jasonwei/status/1760032264120041684</a></u></em></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:46:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279722</guid>
            <link>https://www.oschina.net/news/279722</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[StarkNet 撒钱，程序员在 Web3 领了 14 万 ​​​]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>StarkNet 启动空投活动，GitHub 排名前 5000 开源项目的贡献者可领取价值 $200 奖励。</p><h2>背景</h2><ul><li>StarkNet 公链项目为了激励开发者参与其平台建设，启动了空投活动。</li><li>如果曾向 GitHub 上获得较多 Star 的项目提交过 PR ，就有资格领取 111.1 STRK 的空投奖励。</li><li>只需要使用 OAuth 2.0 登录，就可以直接领取。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-44f62dafa84c34781d15d46c71c7c8c862c.jpg" referrerpolicy="no-referrer"></p><p>有程序员晒出自己在这次空投活动获得的奖励——近 14 万人民币。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-96e62f68c4891cdbfd1f43b86ddfcdcbf79.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2Fstrrlthedev%2Fstatus%2F1760182038504837287" target="_blank">https://twitter.com/strrlthedev/status/1760182038504837287</a></u></em></p></blockquote><h2>领取规则</h2><ol><li>截止到 2023 年 11 月 15 日，至少对全球排名前 5000 的仓库提交过三次代码贡献。</li><li>其中至少有一次贡献是在 2018 年或之后完成的。</li></ol><h2>领取步骤</h2><p>领取地址：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fprovisions.starknet.io%2F" target="_blank">https://provisions.starknet.io/</a></u></em></p><ol><li>访问奖励领取页面并连接钱包（推荐使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchromewebstore.google.com%2Fdetail%2Fargent-x-starknet-wallet%2Fdlcobpjiigpikoobohmabehhmhfoodbb" target="_blank">Argent X</a>）。</li><li>通过 GitHub 登录，采用 OAuth 2.0 验证方式。</li><li>直接领取奖励。后续的治理投票和问卷调查可忽略不计。</li></ol></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:30:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279720</guid>
            <link>https://www.oschina.net/news/279720</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[华为：2 月 26 日将首发华为通信大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">华为官方微信公众号消息<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F7dnrCXQoOqWSfKBrCJWl6Q" target="_blank">显示</a>， 在 2 月 26 日华为产品与解决方案发布会上，即将首发华为通信大模型。</span></p><p><img height="677" src="https://oscimg.oschina.net/oscnet/up-59659387ba8ade9e0cf71c6f84d476a728e.png" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">华为称，2024 年，5G-A 商用元年正式开启！万兆时代已来，共同见证将 5G-A 带入现实。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:23:42 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279717</guid>
            <link>https://www.oschina.net/news/279717</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[谷歌发布轻量级开源大语言模型 Gemma]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌发布了开源大语言模型 Gemma，这是一款轻量级、先进的开源模型，供开发者和研究人员用于 AI 构建。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><p>Gemma 模型家族包括 2B（20 亿参数）和 7B（70 亿参数）两种尺寸，能够在不同的设备类型上运行，包括笔记本电脑、桌面电脑、IoT 设备、移动设备和云端。</p><p><strong>性能和设计</strong></p><p>Gemma 模型在技术和基础设施组件上与 Gemini 共享，这使得 Gemma 2B 和 7B 在其大小范围内相比其他开放模型具有最佳性能。</p><p>Gemma 模型不仅可以直接在开发者的笔记本电脑或桌面电脑上运行，而且在关键基准测试中的表现超过了更大的模型，同时遵循严格的安全和负责任输出标准。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-50f9213a30f71b22bb88d2abb40a2f7a116.png" referrerpolicy="no-referrer"></p><p><strong>主要特点</strong></p><ol><li><strong>轻量级、高性能模型</strong>：Gemma 模型家族包括 Gemma 2B 和 Gemma 7B 两种尺寸，提供预训练和指令调优的变体，针对其大小范围内相比其他开放模型具有最佳性能。</li><li><strong>跨框架工具链支持</strong>：支持 JAX、PyTorch 和 TensorFlow 通过原生 Keras 3.0 进行推理和监督式微调（SFT），适应多种开发需求和环境。</li><li><strong>易于入门和集成</strong>：提供准备就绪的 Colab 和 Kaggle 笔记本，以及与 Hugging Face、MaxText、NVIDIA NeMo 和 TensorRT-LLM 等流行工具的集成，方便开发者快速上手。</li><li><strong>高效的运算能力</strong>：针对多个 AI 硬件平台上进行优化，确保在 NVIDIA GPU 和 Google Cloud TPU 上的行业领先性能。通过与 NVIDIA 的合作，无论是在数据中心、云端还是本地 RTX AI PC 上，都确保了行业领先的性能和与尖端技术的集成。</li></ol><p>Gemma 模型能够在不同的设备类型上运行，包括笔记本电脑、桌面电脑、IoT 设备、移动设备和云端。这种广泛的兼容性使得模型能够适应各种应用场景和需求。</p><ul><li><span>模型地址：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dgemma%26sort%3Dtrending%26search%3Dgoogle" target="_blank">https://huggingface.co/models?other=gemma&amp;sort=trending&amp;search=google…</a><span></span></li><li><span>博客：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgemma-open-models%2F" target="_blank">https://blog.google/technology/developers/gemma-open-models/</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:17:47 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279713/google-gemma-open-models</guid>
            <link>https://www.oschina.net/news/279713/google-gemma-open-models</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[你好，iLogtail 2.0！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>作者：张浩翔（笃敏）</p><h2>概述</h2><p>随着可观测数据采集需求的不断推陈出新，多样化的数据输入输出选项、个性化的数据处理能力组合、以及高性能的数据处理吞吐能力已经成为顶流可观测数据采集器的必备条件。然而，由于历史原因，现有的 iLogtail 架构和采集配置结构已经无法继续满足上述需求，逐渐成为制约 iLogtail 继续向前快速演进的瓶颈：</p><p>▶︎ iLogtail 设计之初完全面向文件日志采集至日志服务的场景：</p><p>1）简单地将日志分为多种格式，每种格式的日志仅支持一种处理方式（如正则解析、Json 解析等）；</p><p>2）功能实现与日志服务相关概念（如 Logstore 等）强绑定；</p><p>基于此设计思想，现有的 iLogtail 架构偏向於单体架构，导致模块间耦合严重，可扩展性和普适性较差，难以提供多个处理流程级联的能力。</p><p>▶︎ Golang 插件系统的引入极大地扩展了 iLogtail 的输入输出通道，且一定程度提升了 iLogtail 的处理能力。然而，囿于 C++ 部分的实现，输入输出与处理模块间的组合能力仍然严重受限：</p><p>1）C++ 部分原生的高性能处理能力仍然仅限于采集日志文件并投递至日志服务的场景使用；</p><p>2）C++ 部分的处理能力无法与插件系统的处理能力相结合，二者只能选其一，从而降低了复杂日志处理场景的性能。</p><p>▶︎ 与 iLogtail 整体架构类似，现有的 iLogtail 采集配置结构也采用平铺结构，缺乏处理流水线的概念，无法表达处理流程级联的语义。</p><p>基于上述原因，在 iLogtail 诞生 10 周年之际，日志服务启动对 iLogtail 的升级改造，寄希望于让 iLogtail 的易用性更佳，性能更优，可扩展性更强，从而更好地服务广大用户。</p><p>目前，经过半年多的重构与优化，iLogtail 2.0 已经呼之欲出。接下来，就让我们来抢先了解一下 iLogtail 2.0 的新特性吧！</p><h2>新特性</h2><h3>（一）【商业版】采集配置全面升级流水线结构</h3><p>为了解决旧版采集配置平铺结构无法表达复杂采集行为的问题，iLogtail 2.0 全面拥抱新版流水线配置，即每一个配置对应一条处理流水线，包括输入模块、处理模块和输出模块，每个模块由若干个插件组成，各模块的插件功能如下：</p><ul><li><strong>输入插件：</strong> 用于从指定输入源获取数据（各插件具体功能详见输入插件 <strong>[</strong><strong>1]</strong> ）</li><li><strong>处理插件：</strong> 用于对日志进行解析和处理（各插件具体功能详见处理插件 <strong>[</strong><strong>2]</strong> ），可进一步分为原生处理插件和扩展处理插件</li></ul><p>&lt;!----&gt;</p><ul><li>原生处理插件：性能较优，适用于大部分业务场景，推荐优先使用</li><li>扩展处理插件：功能覆盖更广，但性能劣于原生处理插件，建议仅在原生处理插件无法完成全部处理需求时使用</li></ul><p>&lt;!----&gt;</p><ul><li><strong>输出插件：</strong> 用于将处理后的数据发送至指定的存储</li></ul><p>我们可以用一个 JSON 对象来表示一个流水线配置：</p><p><img src="https://oscimg.oschina.net/oscnet/up-f6cc95f0ae9e478bd8c674f6fae4ee9ceb9.png" alt="" referrerpolicy="no-referrer"></p><p>其中，inputs、processors 和 flushers 即代表输入、处理和输出模块，列表中的每一个元素 {...} 即代表一个插件；global 代表流水线的一些配置。有关流水线配置结构的具体信息，可参见 iLogtail 流水线配置结构 <strong>[</strong><strong>3]</strong> 。</p><blockquote><p>示例：采集 /var/log 目录下的 test.log，对日志进行 json 解析后发送到日志服务。以下是实现该采集需求对应的旧版和新版配置，可以看到新版配置十分精炼，执行的操作一目了然。</p><p><strong>旧版配置：</strong></p><pre><code>{
&nbsp;&nbsp;&nbsp;&nbsp;"configName":&nbsp;"test-config",
&nbsp;&nbsp;&nbsp;&nbsp;"inputType":&nbsp;"file",
&nbsp;&nbsp;&nbsp;&nbsp;"inputDetail":&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"topicFormat":&nbsp;"none",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"priority":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logPath":&nbsp;"/var/log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"filePattern":&nbsp;"test.log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"maxDepth":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tailExisted":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"fileEncoding":&nbsp;"utf8",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logBeginRegex":&nbsp;".*",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerFile":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerIncludeLabel":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerExcludeLabel":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerIncludeEnv":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerExcludeEnv":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"preserve":&nbsp;true,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"preserveDepth":&nbsp;1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"delaySkipBytes":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"delayAlarmBytes":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logType":&nbsp;"json_log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"timeKey":&nbsp;"",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"timeFormat":&nbsp;"",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"adjustTimezone":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logTimezone":&nbsp;"",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"filterRegex":&nbsp;[],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"filterKey":&nbsp;[],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"discardNonUtf8":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"sensitive_keys":&nbsp;[],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"mergeType":&nbsp;"topic",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"sendRateExpire":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"maxSendRate":&nbsp;-1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"localStorage":&nbsp;true
&nbsp;&nbsp;&nbsp;&nbsp;},
&nbsp;&nbsp;&nbsp;&nbsp;"outputType":&nbsp;"LogService",
&nbsp;&nbsp;&nbsp;&nbsp;"outputDetail":&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logstoreName":&nbsp;"test_logstore"
&nbsp;&nbsp;&nbsp;&nbsp;}
}
</code></pre><p><strong>新版流水线配置：</strong></p><pre><code>{
&nbsp;&nbsp;&nbsp;&nbsp;"configName":&nbsp;"test-config",
&nbsp;&nbsp;&nbsp;&nbsp;"inputs":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"file_log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"FilePaths":&nbsp;"/var/log/test.log"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp;&nbsp;&nbsp;"processors":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_parse_json_native"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"SourceKey":&nbsp;"content"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp;&nbsp;&nbsp;"flushers":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"flusher_sls",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Logstore":&nbsp;"test_logstore"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;]
}
</code></pre><p>如果在执行 json 解析后需要进一步处理，在流水线配置中只需额外增加一个处理插件即可，但是在旧版配置中已经无法表达上述需求。</p></blockquote><p>有关新版流水线配置和旧版配置的兼容性问题，请参见文末兼容性说明板块。</p><h4>全新 API</h4><p>为了支持流水线配置，同时区分旧版配置结构，我们提供了全新的用于管理流水线配置的 API 接口，包括：</p><ul><li>CreateLogtailPipelineConfig</li><li>UpdateCreateLogtailPipelineConfig</li><li>GetLogtailPipelineConfig</li><li>DeleteLogtailPipelineConfig</li><li>ListLogtailPipelineConfig</li></ul><p>有关这些接口的详细信息，请参见 OpenAPI 文档 <strong>[</strong><strong>4]</strong> 。</p><h4>全新控制枱界面</h4><p>与流水线采集配置结构相对应，前端控制枱界面也进行了全新升级，分为了全局配置、输入配置、处理配置和输出配置。</p><p><img src="https://oscimg.oschina.net/oscnet/up-52a9a2935a7738f4ddd6cc8123ae3551f2d.png" alt="" referrerpolicy="no-referrer"></p><p>与旧版控制枱界面相比，新版控制枱具有如下特点：</p><p><strong>参数内聚：</strong> 某一功能相关的参数集中展示，避免了旧版控制枱参数散落各处出现漏配置。</p><blockquote><p>示例：最大目录监控深度与日志路径中的**密切相关，旧版界面中，二者分隔较远，容易遗忘；在新版界面中，二者在一起，便于理解。</p><p><strong>旧版控制枱：</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-04d5bc90063cbedb86c8049d54635e3023c.png" alt="" referrerpolicy="no-referrer"></p><p><strong>新版控制枱：</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-b559b0e4ad0b5877d16aa684f9ba4d50d00.png" alt="" referrerpolicy="no-referrer"></p></blockquote><p><strong>所有参数均为有效参数：</strong> 在旧版控制枱中，启用插件处理后，部分控制枱参数会失效，从而引起不必要的误解。新版控制枱所有参数均为有效参数。</p><h4>全新 CRD</h4><p>同样，与新版采集配置对应，K8s 场景中与采集配置对应的 CRD 资源也全新升级。与旧版 CRD 相比，新版 CRD 具有如下特点：</p><ul><li>支持新版流水线采集配置</li><li>CRD 类型调整为 Cluster 级别，且将 CRD 名称直接作为采集配置名称，避免同一集群多个不同的 CRD 资源指向同一个采集配置引起冲突</li><li>对所有操作的结果进行定义，避免出现多次操作旧版 CRD 后出现的行为未定义情况</li></ul><pre><code>apiVersion:&nbsp;log.alibabacloud.com/v1alpha1
kind:&nbsp;ClusterAliyunLogConfig
metadata:
&nbsp;&nbsp;name:&nbsp;test-config
spec:
&nbsp;&nbsp;project:
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;test-project
&nbsp;&nbsp;logstore:
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;test-logstore
&nbsp;&nbsp;machineGroup:
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;test-machine_group
&nbsp;&nbsp;config:
&nbsp;&nbsp;&nbsp;&nbsp;inputs:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;Type:&nbsp;input_file
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FilePaths:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;/var/log/test.log
&nbsp;&nbsp;&nbsp;&nbsp;processors:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;Type:&nbsp;processor_parse_json_native
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SourceKey:&nbsp;content
</code></pre><h3>（二）处理插件组合更加灵活</h3><p>对于文本日志采集场景，当您的日志较为复杂需要多次解析时，您是否在为只能使用扩展处理插件而困惑？是否为因此带来的性能损失和各种不一致问题而烦恼？</p><p>升级 iLogtail 2.0，以上问题都将成为过去！</p><p>iLogtail 2.0 的处理流水线支持全新级联模式，和 1.x 系列相比，有以下能力升级：</p><ul><li><p><strong>原生处理插件可任意组合：</strong></p><p>原有原生处理插件间的依赖限制不复存在，您可以随意组合原生处理插件以满足您的处理需求。</p></li><li><p><strong>原生处理插件和扩展处理插件可同时使用：</strong></p><p>对于复杂日志解析场景，如果仅用原生处理插件无法满足处理需求，您可进一步添加扩展处理插件进行处理。</p></li></ul><p><strong>🔔 注意：</strong> 扩展处理插件只能出现在所有的原生处理插件之后，不能出现在任何原生处理插件之前。</p><blockquote><p>示例：假如您的文本日志为如下内容：</p><p>{"time": "2024-01-22T14:00:00.745074", "level": "warning", "module": "box", "detail": "127.0.0.1 GET 200"}</p><p>您需要将 time、level 和 module 字段解析出来，同时还需要将 detail 字段做进一步正则解析，拆分出 ip、method 和 status 字段，最后丢弃 drop 字段，则您可以按顺序使用「Json 解析原生处理插件」、「正则解析原生处理插件」和「丢弃字段扩展处理插件」完成相关需求：</p><p>【商业版】</p><p><img src="https://oscimg.oschina.net/oscnet/up-6310f6b8a781ce3e256042e7f44ed09a75d.png" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-c5dea6cb6a41a50be2f5eb48e2dbeeda960.png" alt="" referrerpolicy="no-referrer"></p><p>【开源版】</p><pre><code>{
&nbsp;&nbsp;"configName":&nbsp;"test-config"
&nbsp;&nbsp;"inputs":&nbsp;[...],
&nbsp;&nbsp;"processors":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_parse_json_native",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"SourceKey":&nbsp;"content"
&nbsp;&nbsp;&nbsp;&nbsp;},
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_parse_regex_native",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"SourceKey":&nbsp;"detail",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Regex":&nbsp;"(\S)+\s(\S)+\s(.*)",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Keys":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"ip",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"method",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"status"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_drop",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"DropKeys":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"module"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;],
&nbsp;&nbsp;"flushers":&nbsp;[...]
}
</code></pre><p>采集结果如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-f272a5d2ade5c817461bf406d1a73836f89.png" alt="" referrerpolicy="no-referrer"></p></blockquote><h3>（三）新增 SPL 处理模式</h3><p>除了使用处理插件组合来处理日志，iLogtail 2.0 还新增了 SPL（SLS Processing Language）处理模式，即使用日志服务提供的用于统一查询、端上处理、数据加工等的语法，来实现端上的数据处理。使用 SPL 处理模式的优势在于：</p><ul><li>拥有丰富的工具和函数：支持多级管道操作，内置功能丰富的算子和函数</li><li>上手难度低：低代码，简单易学</li><li>【商业版】统一语法：一个语言玩转日志采集、查询、加工和消费</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-4513da15197b25b1099a5707d39a222214d.png" alt="" referrerpolicy="no-referrer"></p><h4>SPL 语法</h4><h5>整体结构：</h5><ul><li>指令式语句，支持结构化数据和非结构化数据统一处理</li><li>管道符（|）引导的探索式语法，复杂逻辑编排简便</li></ul><pre><code>&lt;data-source&gt;&nbsp;
|&nbsp;&lt;spl-cmd&gt;&nbsp;-option=&lt;option&gt;&nbsp;-option&nbsp;...&nbsp;&lt;expression&gt;,&nbsp;...&nbsp;as&nbsp;&lt;output&gt;,&nbsp;...
|&nbsp;&lt;spl-cmd&gt;&nbsp;...
|&nbsp;&lt;spl-cmd&gt;&nbsp;...
</code></pre><h5>结构化数据 SQL 计算指令：</h5><ul><li>where&nbsp;通过 SQL 表达式计算结果产生新字段</li><li>extend&nbsp;根据 SQL 表达式计算结果过滤数据条目</li></ul><pre><code>*
|&nbsp;extend&nbsp;latency=cast(latency&nbsp;as&nbsp;BIGINT)
|&nbsp;where&nbsp;status='200'&nbsp;AND&nbsp;latency&gt;100
</code></pre><h5>非结构化数据提取指令：</h5><ul><li>parse-regexp&nbsp;提取指定字段中的正则表达式分组匹配信息</li><li>parse-json&nbsp;提取指定字段中的第一层 JSON 信息</li><li>parse-csv&nbsp;提取指定字段中的 CSV 格式信息</li></ul><pre><code>*
|&nbsp;project-csv&nbsp;-delim='^_^'&nbsp;content&nbsp;as&nbsp;time,&nbsp;body
|&nbsp;project-regexp&nbsp;body,&nbsp;'(\S+)\s+(\w+)'&nbsp;as&nbsp;msg,&nbsp;user
</code></pre><h3>（四）日志解析控制更加精细</h3><p>对于原生解析类插件，iLogtail 2.0 提供了更精细的解析控制，包括如下参数：</p><ul><li>KeepingSourceWhenParseFail：解析失败时，是否保留原始字段。若不配置，默认不保留。</li><li>KeepingSourceWhenParseSucceed：解析成功时，是否保留原始字段。若不配置，默认不保留。</li><li>RenameSourceKey：当原始字段被保留时，用于存储原始字段的字段名。若不配置，默认不改名。</li></ul><blockquote><p>示例：假设需要在日志字段内容解析失败时在日志中保留该字段，并重命名为 raw，则可配置如下参数：</p><ul><li>KeepingSourceWhenParseFail：true</li><li>RenameSourceKey：raw</li></ul></blockquote><h3>（五）【商业版】日志时间解析支持纳秒级精度</h3><p>在 iLogtail 1.x 版本中，如果您需要提取日志时间字段到纳秒精度，日志服务只能在您的日志中额外添加「纳秒时间戳」字段。在 iLogtail 2.0 版本中，纳秒信息将直接附加至日志采集时间（<strong>time</strong>）而无需额外添加字段，不仅减少了不必要的日志存储空间，也为您在 SLS 控制枱根据纳秒时间精度对日志进行排序提供方便。</p><p>如果需要在 iLogtail 2.0 中提取日志时间字段到纳秒精度，您需要首先配置时间解析原生处理插件，并在「源时间格式（SourceFormat）」的末尾添加「.%f」，然后在全局参数中增加"EnableTimestampNanosecond": true。</p><blockquote><p>示例：假设日志中存在字段 time，其值为 2024-01-23T14:00:00.745074，时区为东 8 区，现在需要解析该时间至纳秒精度并将 <strong>time</strong> 置为该值。</p><p><img src="https://oscimg.oschina.net/oscnet/up-443ebb6650ab6fb7334c3b3b1c5527ac60c.png" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-6237e2882b874738edda03a89819297d75b.png" alt="" referrerpolicy="no-referrer"></p><p>采集结果如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-519f2570442d18bdb7f276c108ff1112546.png" alt="" referrerpolicy="no-referrer"></p></blockquote><p><strong>🔔 注意：</strong> iLogtail 2.0 不再支持 1.x 版本中提取纳秒时间戳的方式，如果您在 1.x 版本中已经使用了提取纳秒时间戳功能，在升级 iLogtail 2.0 后，需要按照上述示例手动开启新版纳秒精度提取功能，详细信息参见文末兼容性说明。</p><h3>（六）【商业版】状态观测更加清晰</h3><p>相比于 iLogtail 1.x 暴露的简单指标，iLogtail 2.0 极大地完善了自身可观测性的建设：</p><ul><li>所有采集配置都有完整指标，可以在 Project/Logstore 等维度上进行不同采集配置的统计与比较</li><li>所有插件都有自己的指标，可以构建完整流水线的拓扑图，每个插件的状态可以进行清楚的观测</li><li>C++ 原生插件提供更加详细的指标，可以用来监控与优化插件的配置参数</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-1eafd646874b01b9014f3feb5edee33dd27.png" alt="" referrerpolicy="no-referrer"></p><h3>（七）运行更快更安全</h3><p>iLogtail 2.0 支持 C++ 17 语法，C++ 编译器升级至 gcc 9，同时更新了 C++ 依赖库的版本，使得 iLogtail 的运行更快更安全。</p><p>表：iLogtail 2.0 单线程处理日志的性能（以单条日志长度 1KB 为例）</p><table><thead><tr><th align="left"><strong>场景</strong></th><th align="left"><strong>CPU（核）</strong></th><th align="left"><strong>内存（MB）</strong></th><th align="left"><strong>处理速率（MB/s）</strong></th></tr></thead><tbody><tr><td align="left">单行日志采集</td><td align="left">1.06</td><td align="left">33</td><td align="left">400</td></tr><tr><td align="left">多行日志采集</td><td align="left">1.04</td><td align="left">33</td><td align="left">150</td></tr></tbody></table><h2>兼容性说明</h2><h3>（一）采集配置</h3><h4>商业版</h4><ul><li>新版流水线采集配置是完全向前兼容旧版采集配置的，因此：</li></ul><p>&lt;!----&gt;</p><ul><li>在您升级 iLogtail 至 2.0 版本的过程中，日志服务会在下发配置时自动将您的旧版配置转换为新版流水线配置，您无需执行任何额外操作。您可以通过 GetLogtailPipelineConfig 接口直接获取旧版配置对应的新版流水线配置</li></ul><p>&lt;!----&gt;</p><ul><li>旧版采集配置并不完全向后兼容新配流水线配置</li></ul><p>&lt;!----&gt;</p><ul><li>如果流水线配置描述的采集处理能力可用旧版配置表达，则该流水线配置依然可以被 iLogtail 0.x 和 1.x 版本使用，日志服务会在向 iLogtail 下发配置时自动将新版流水线配置转换为旧版配置</li><li>反之，该流水线配置会被 iLogtail 0.x 和 1.x 版本忽略</li></ul><h4>开源版</h4><p>新版采集配置与旧版采集配置存在少量不兼容情况，详见 iLogtail 2.0 版本采集配置不兼容变更说明 <strong>[</strong><strong>5]</strong> 。</p><h3>（二）iLogtail 客户端</h3><p><strong>1. 使用扩展处理插件时的 Tag 存储位置</strong></p><p>当您使用扩展插件处理日志时，iLogtail 1.x 版本由于实现原因会将部分 tag 存放在日志的普通字段中，从而为您后续在 SLS 控制枱使用查询、搜索和消费等功能时带来诸多不便。为了解决这一问题，iLogtail 2.0 将默认将所有 tag 归位，如果您仍希望保持 1.x 版本行为，您可以在配置的全局参数中增加"UsingOldContentTag": true。</p><ul><li>对于通过旧版控制枱界面和旧版 API 创建的采集配置，在您升级 iLogtail 2.0 后，tag 的存储位置仍然与 1.x 版本一致；</li><li>对于通过新版控制枱界面和新版 API 创建的采集配置，在您升级 iLogtail 2.0 后，tag 的存储位置将默认归位。</li></ul><p><strong>2. 高精度日志时间提取</strong></p><p>2.0 版本不再支持 1.x 版本的 PreciseTimestampKey 和 PreciseTimestampUnit 参数，当您升级 iLogtail 2.0 版本后，原有纳秒时间戳提取功能将失效，如果您仍需解析纳秒精度时间戳，您需要参照日志时间解析支持纳秒精度板块对配置进行手动更新。</p><p><strong>3. 飞天格式日志微秒时间戳时区调整</strong></p><p>2.0 版本的飞天解析原生处理插件将不再支持 1.x 版本的 AdjustingMicroTimezone 参数，默认微秒时间戳也会根据配置的时区进行正确的时区调整。</p><p><strong>4. 日志解析控制</strong></p><p>对于原生解析类插件，除了日志解析控制更加精细板块中提到的 3 个参数，还存在 CopyingRawLog 参数，该参数仅在 KeepingSourceWhenParseFail 和 KeepingSourceWhenParseSucceed 都为 true 时有效，它将在日志解析失败时，在日志中额外增加 <strong>raw_log</strong> 字段，字段内容为解析失败的内容。</p><p>该参数的存在是为了兼容旧版配置，当您升级 iLogtail 2.0 版本后，建议您及时删去该参数以减少不必要的重复日志上传。</p><h2>总结</h2><p>为用户提供更舒适便捷的用户体验一直是日志服务的宗旨。相比于 iLogtail 1.x 时代，iLogtail 2.0 的变化是比较明显的，但这些转变只是 iLogtail 迈向现代可观测数据采集器的序曲。我们强烈建议您在条件允许的情况下尝试 iLogtail 2.0，也许您在转换之初会有些许的不适应，但我们相信，您很快会被 iLogtail 2.0 更强大的功能和更出色的性能所吸引。</p><p><strong>相关链接：</strong></p><p>[1]&nbsp;输入插件</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fsls%2Fuser-guide%2Foverview-19%3Fspm%3Da2c4g.11186623.0.0.2a755c0dN5uxv4" target="_blank">https://help.aliyun.com/zh/sls/user-guide/overview-19?spm=a2c4g.11186623.0.0.2a755c0dN5uxv4</a></em></p><p>[2]&nbsp;处理插件</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fsls%2Fuser-guide%2Foverview-22%3Fspm%3Da2c4g.11186623.0.0.2f2d1279yGXSce" target="_blank">https://help.aliyun.com/zh/sls/user-guide/overview-22?spm=a2c4g.11186623.0.0.2f2d1279yGXSce</a></em></p><p>[3]&nbsp;iLogtail 流水线配置结构</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnext.api.aliyun.com%2Fstruct%2FSls%2F2020-12-30%2FLogtailPipelineConfig%3Fspm%3Dapi-workbench.api_explorer.0.0.65e61a47jWtoir" target="_blank">https://next.api.aliyun.com/struct/Sls/2020-12-30/LogtailPipelineConfig?spm=api-workbench.api_explorer.0.0.65e61a47jWtoir</a></em></p><p>[4]&nbsp;OpenAPI 文档</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnext.api.aliyun.com%2Fdocument%2FSls%2F2020-12-30%2FCreateLogtailPipelineConfig%3Fspm%3Dapi-workbench.api_explorer.0.0.65e61a47jWtoir" target="_blank">https://next.api.aliyun.com/document/Sls/2020-12-30/CreateLogtailPipelineConfig?spm=api-workbench.api_explorer.0.0.65e61a47jWtoir</a></em></p><p>[5]&nbsp;iLogtail 2.0 版本采集配置不兼容变更说明</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Filogtail%2Fdiscussions%2F1294" target="_blank">https://github.com/alibaba/ilogtail/discussions/1294</a></em></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:08:47 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/11044307</guid>
            <link>https://my.oschina.net/u/3874284/blog/11044307</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[程序员因 bug 事故被公司强制要求归还 4 万年终奖]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>某程序员在 V2EX 发帖称，因线上流量异常事故，自己被公司进行处罚。处罚的结果是被要求将去年发的 4 万多年终奖归还给公司，如果逾期不还，将以每天万分之 5 的利息收取滞纳金。</p><p>该程序员还称，公司 hr 还扬言三个月内还是不还就免费开除。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-11365064309a71d744e9abe207d19996d40.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.v2ex.com%2Ft%2F1016302" target="_blank">https://www.v2ex.com/t/1016302</a></u></em></p></blockquote><p>最新后续：</p><blockquote><p><img height="4236" src="https://oscimg.oschina.net/oscnet/up-0b14b51439371608fa1837e2222c753a465.png" width="1504" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.v2ex.com%2Ft%2F1017164" target="_blank">https://www.v2ex.com/t/1017164</a></u></em></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Wed, 21 Feb 2024 10:31:56 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279644</guid>
            <link>https://www.oschina.net/news/279644</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[我国 5G 基站总数超 337 万个，5G 移动电话用户达 8.05 亿户]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>工业和信息化部数据指出：截至 2023 年底，我国累计建成 5G 基站 337.7 万个，5G 移动电话用户达 8.05 亿户。</p><p>网络基础日益完备。我国已建成全球最大的光纤和移动宽带网络，全国行政村通 5G 比例超 80%。通信杆塔资源与社会杆塔资源双向共享取得显著成效，目前 90% 以上的基站实现共建共享，5G 基站单站址能耗相较于商用初期降低 20% 以上。</p><p>创新能力不断增强。我国 5G 技术产业在技术标准、网络设备、终端设备等方面创新能力不断增强。轻量化 5G 核心网、定制化基站等实现商用部署。5G 工业网关、巡检机器人等一批新型终端成功研发。5G 标准必要专利声明量全球占比超 42%，持续保持全球领先。</p><p>赋能效应持续凸显。融合应用广度和深度不断拓展，5G 应用已融入 71 个国民经济大类，应用案例数超 9.4 万个，5G 行业虚拟专网超 2.9 万个。5G 应用在工业、矿业、电力、港口、医疗等行业深入推广。「5G+工业互联网」项目数超 1 万个。</p><p>赋值效应更加显著。5G 移动电话用户持续增长、5G 流量消费快速提升，有效拓展了移动通信市场的发展空间。截至 2023 年底，我国 5G 网络接入流量占比达 47%。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 21 Feb 2024 09:01:13 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279630</guid>
            <link>https://www.oschina.net/news/279630</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[法国电信公司 Orange 违反 GPL 许可协议，被罚 65 万欧元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>根据 2024 年 2 月 14 日下达的判决，法国上诉法院判定当地电信公司 <strong>Orange 因未遵守 GNU GPL v2 许可证条款而侵权</strong>，并且需要向 Entr'Ouvert 支付&nbsp;<strong>50 万欧元的经济损失赔偿和 15 万欧元的精神损失赔偿</strong>。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-aba84da216106bb416ed362f90f6181cab6.png" referrerpolicy="no-referrer"></p><p><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweb.archive.org%2Fweb%2F20240216164701%2Fhttps%3A%2F%2Fwww.legalis.net%2Factualite%2Forange-condamne-a-650-000-e-pour-non-respect-de-la-licence-gpl%2F" target="_blank">https://web.archive.org/web/20240216164701/https://www.legalis.net/actualite/orange-condamne-a-650-000-e-pour-non-respect-de-la-licence-gpl/</a></u></em></p></blockquote><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6928315e1d404157b8404126515b42d68a7.png" referrerpolicy="no-referrer"></p><p><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.orange.com%2Fen" target="_blank">Orange</a></u><span>&nbsp;是一家法国电信运营商。根据上文的判决，</span>Orange 的 IDMP 平台使用了名叫 Lasso 的库，该库的版权所有者为 Entr'Ouvert 公司。<strong>Lasso 采用 GPLv2 License —— 并为私有项目提供了商业许可证</strong>。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-7e59b5839901ae3f8dc0da42fd32d14c3d1.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flasso.entrouvert.org%2F" target="_blank">https://lasso.entrouvert.org/</a></u></em></p></blockquote><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-2d0135e65a7b51a798b2692a65007c31a65.png" referrerpolicy="no-referrer"></p><p>2005 年年底，Orange 参与了电子管理局关于实施 My Public Service 门户网站的招标活动，为身份管理提供一套 IT 解决方案，其中包括<strong>通过软件接口将 IDMP 平台与 Entr'ouvert 公司发布的 Lasso 软件库连接起来，Lasso 库采用 GPL 许可证</strong>。</p><p><img src="https://oscimg.oschina.net/oscnet/up-7d0ef557e10594da586685534b7728c372e.png" referrerpolicy="no-referrer"></p><p>法院认为，Entr'Ouvert 首先蒙受了与其在公共市场 Mon.service-public.fr 上收益受损相关的经济损失，「因为如果 Orange 公司遵守许可合同，并签订付费许可，他们就应该向对方支付版税。」</p><p>此外，上诉法院特别指出，「Orange 免费使用 Lasso 软件为这个持续 7 年的大规模公开市场带来了利润，此外还有 Lasso 给这个门户网站在形象方面带来的好处。<strong>这番操作让 Orange 得以节省投入从而受益，因为通过免费使用 Lasso 软件，Orange 公司可以满足通信安全和隐私管理局 (ADAE) 要求的安全标准，从而能够节省研发成本</strong>。」</p><p>Entr'ouvert 着重批评 Orange 违反了 Lasso 程序的许可合同条款，这些条款涉及它作为该程序版权持有人所拥有的知识产权，因此它以涉嫌侵犯其权利为由起诉了 Orange。</p><p>上诉法院首先要求 Entr'ouvert 证明 Lasso 软件具有原创性。</p><p>根据 Entr'ouvert 出示的证据，上诉法院得出的结论是，<strong>Lasso 「在软件组成、结构和表达方面都是原创的，符合版权保护的条件」</strong>。</p><p>上诉法院随后审查了 Entr'Ouvert 援引的三起违反 GNU GPL v2 许可协议的行为。</p><p><strong>首先，法院认为 Orange 违反了许可合同第 2 条，因为它对 IDMP 所基于的 Lasso 进行了修改，却没有将 IDMP 作为一个免费整体授予政府。</strong></p><p><strong>其次，法院判定 Orange 没有进一步遵守第 3 条，因为没有提供修改后的源代码。</strong></p><p><strong>最后，法院特别指出，Orange 在没有遵守许可合同的所有条件、特别是第 4 条的情况下，复制、修改和分发了 Lasso。</strong></p><p><strong>法院还认为 Lasso 被整合到 IDMP 平台中，而 IDMP 平台的发行条件不一样，且没有征得 Entr'Ouvert 公司的授权，此举也违反了许可合同第 10&nbsp;条。</strong></p><p>总而言之，Orange 的 IDMP 产品使用了 Lasso，要么按照 GPLv2 许可证要求公布它的源代码，要么从版权所有者 Entr'Ouvert 购买许可证。Orange 没有付费也没有遵守 GPL 许可协议。法国上诉法庭判决 Orange 侵权行为成立。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 21 Feb 2024 08:09:45 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279616/orange-condamne-a-650-000-e-pour-non-respect-de-la-licence-gpl</guid>
            <link>https://www.oschina.net/news/279616/orange-condamne-a-650-000-e-pour-non-respect-de-la-licence-gpl</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
