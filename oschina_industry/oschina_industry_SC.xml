<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-综合资讯]]>
        </title>
        <link>https://www.oschina.net/news/industry</link>
        <atom:link href="https://rsshub.app/oschina/news/industry" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-综合资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 11 Oct 2023 00:13:27 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[GNOME 移除对 X.Org 会话支持]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>由于 Fedora 40 希望禁用 GNOME 的 X11 会话支持，并且还让 KDE Plasma 6 Wayland 仅适用于 Fedora。目前上游 GNOME 正在评估禁用以及移除对 X.Org 会话的支持，未来转变为<strong>只支持 Wayland 的桌面环境</strong>。</p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitlab.gnome.org%2FGNOME%2Fgnome-session%2F-%2Fmerge_requests%2F98" target="_blank">根据 GNOME 的仓库动态</a>，开发者递交了一组合并请求，准备移除对 X.Org 会话支持。移除这些支持将直接使 gnome-session 减少 3.6k 行代码。</p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgitlab.gnome.org%2FGNOME%2Fgnome-session%2F-%2Fmerge_requests%2F98" target="_blank"><img height="1099" src="https://static.oschina.net/uploads/space/2023/1010/184926_ndoo_2720166.png" width="2187" referrerpolicy="no-referrer"></a></p><p>开发者称，X11 的测试越来越少，自 2016 年以来 GNOME 就默认使用 Wayland 会话，现在是时候完全移除 X.Org 会话了。可能到明年 GNOME 项目将只支持 Wayland 会话。</p><p>与此同时，有人提出了一些担忧，这可能会影响 Budgie 和 Pantheon 等尚未完全过渡到 Wayland 的下游桌面。</p><hr><p>延伸阅读</p><ul><li><a href="https://www.oschina.net/news/241063/asahi-linux-stop-x-org" target="_blank">Asahi Linux 致用户：停止使用 X.Org，Wayland 才是未来</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 11:01:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261201/gnome-mr-drop-x11-session</guid>
            <link>https://www.oschina.net/news/261201/gnome-mr-drop-x11-session</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Captum —— PyTorch 模型解释和理解库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Captum 是为 PyTorch 设计的模型解释性和理解库。在拉丁语中，「Captum」意味着理解。这个库为 PyTorch 模型提供了通用的实现方法，包括集成渐变、显著性图、smoothgrad、vargrad 等。它可以快速集成使用像 torchvision、torchtext 等特定领域的库构建的模型。目前，Captum 仍处于测试阶段，并在积极开发中。</p><p>随着模型复杂性的增加以及透明度的缺失，模型解释性方法变得越来越重要。模型理解已经成为研究的活跃领域，并在各个使用机器学习的行业中获得了实际应用的关注。Captum 提供了最先进的算法，包括集成渐变，以使研究者和开发者更容易理解哪些特征对模型的输出起了作用。</p><p>对于模型开发者来说，Captum 可以帮助他们通过识别不同的特征来改进和排查模型，从而设计出更好的模型，并排查意外的模型输出。</p><p>Captum 还帮助机器学习研究者更轻松地实现可以与 PyTorch 模型互动的解释性算法。此外，研究者还可以使用 Captum 快速地将他们的工作与库中的其他现有算法进行对比。</p><p><strong>属性算法概述</strong></p><p><strong>目标受众</strong></p><p>Captum 的主要受众是希望改进他们的模型并了解哪些特征重要的模型开发者，以及专注于识别能够更好解释许多模型类型的算法的解释性研究者。</p><p>应用工程师也可以使用 Captum。他们在生产中使用经过训练的模型时，Captum 能提供更容易的排错方式，并有可能为最终用户提供更好的解释，例如为什么他们会看到某个特定的内容，如电影推荐。</p><p>总体而言，Captum 为 PyTorch 提供了一个强大的工具，不仅对模型进行深入的解释和理解，还可以为模型开发者和研究人员提供有关模型工作原理的洞察，从而促进更好的模型设计和应用。</p></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 08:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/captum</guid>
            <link>https://www.oschina.net/p/captum</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | Redis 流量镜像的实现]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h2_1"></span><h2>背景</h2><p>对 Redis 场景降本增效，涉及到将部分 Redis 实例迁移到类似社区 pika 这种支持 Redis 协议的基于 SSD 磁盘存储的项目（阿里云 Tair），降低存储成本。迁移过程需要进行性能验证，除了基本的选型压测之外，还必须对每个业务场景做全指令的性能覆盖，才能确保业务迁移的性能以及指令兼容稳定性。常规的做法是需要业务开发配合在工程里进行流量双发，或者小范围流量灰度。</p><p>以上这个问题，不管哪种方式都需要投入更多的人力和时间，对降本增效本身这件事情来说，大大降低了 roi 。如果能够做到直接将原 Redis 的所有读流量重放到目标 Redis SSD 的实例，则迁移整件事件 SRE 可以完成 99% ，而且将大大缩短迁移实例的时间，所以 Redis 流量镜像这个需求就应运而生了。</p><p>tips：我们的数据迁移方案采用阿里云的 DTS ，DTS 是基于 Redis 主从复制的原理实现的，所以写流量性能在数据同步过程就可以直接验证了</p><span id="OSC_h2_2"></span><h2>调研</h2><p>Google 上、Github 上逛了一圈，没有十分契合的东西，所以最后决定自研。查到的一些相关信息如下：</p><ul><li>阿里云的 SLS Redis 审计日志</li></ul><p>阿里云的 Redis 实例支持将 Redis 的执行日志丢到 SLS（一个日志记录查询的产品） 记录，但是只有写流量的记录，达不到 Redis 读流量回放的需求。</p><ul><li>pika redis-copy 工具：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenAtomFoundation%2Fpika%2Fissues%2F2044" target="_blank">https://github.com/OpenAtomFoundation/pika/issues/2044</a></li></ul><p>这个工具已经被 pika 项目丢弃了。目前只有文档，仓库里已经没有相关的代码了。不过本文实现也是基于和 pika 的实现原理一样的</p><ul><li>istio 实现 Redis 流量镜像：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcloudnativeto%2Fcloudnativeto.github.io%2Fissues%2F76" target="_blank">https://github.com/cloudnativeto/cloudnativeto.github.io/issues/76</a></li></ul><p>基于 istio 做 Redis 的镜像流量，必须接入 istio 才行，局限性太大了，而且引入一个新的 istio 组件需要做很多的稳定性测试，所以这个路线就直接否了</p><span id="OSC_h2_3"></span><h2>实现</h2><p>直接接入正题，采用 Redis 的 monitor 指令来实现这个需求。</p><pre><code class="language-java">/**
 * @author kl (http://kailing.pub)
 * @since 2023/9/27
 */
public class RedisMonitorTest {

    static final JedisPool targetRedisPool = new JedisPool("127.0.0.1", 6398);
    static final JedisPool sourceRedisPool = new JedisPool("127.0.0.1", 6379);
    static final Set&lt;String&gt; redisReadCommands = new HashSet&lt;&gt;(Arrays.asList(
            "get", "strlen", "exists", "getbit", "getrange", "substr", "mget", "llen", "lindex",
            "lrange", "sismember", "scard", "srandmember", "sinter", "sunion", "sdiff", "smembers",
            "sscan", "zrange", "zrangebyscore", "zrevrangebyscore", "zrangebylex", "zrevrangebylex",
            "zcount", "zlexcount", "zrevrange", "zcard", "zscore", "zrank", "zrevrank", "zscan", "hget",
            "hmget", "hlen", "hstrlen", "hkeys", "hvals", "hgetall", "hexists", "hscan", "randomkey",
            "keys", "scan", "dbsize", "type", "sync", "psync", "ttl", "touch", "pttl", "dump", "object",
            "memory", "bitcount", "bitpos", "georadius_ro", "georadiusbymember_ro", "geohash",
            "geopos", "geodist", "pfcount", "xrange", "xrevrange", "xlen", "xread", "xpending",
            "xinfo", "lolwut"
    ));
    
    public static void main(String[] args) {
            try (Jedis jedis = sourceRedisPool.getResource()) {
                jedis.monitor(new JedisMonitor() {
                    @Override
                    public void onCommand(String command) {
                        sendCommand(command);
                    }
                });
            }
    }
    
    public static void sendCommand(String commandStr) {
        String[] parts = commandStr.split("\"");
        if (parts.length &lt; 2) {
            return;
        }
        String cmd = parts[1];
        List&lt;String&gt; args = new ArrayList&lt;&gt;();
        for (int i = 3; i &lt; parts.length; i += 2) {
            args.add(parts[i]);
        }
        if (redisReadCommands.contains(cmd.toLowerCase())) {
            ProtocolCommand command = () -&gt; cmd.getBytes(StandardCharsets.UTF_8);

            try (Jedis jedis = targetRedisPool.getResource()) {
                try {
                    long startTime = System.currentTimeMillis();
                    jedis.sendCommand(command, args.toArray(new String[0]));
                    System.out.println(cmd + ":" + (System.currentTimeMillis() - startTime));
                } catch (Exception e) {
                    System.err.println(cmd + e.getMessage());
                }
            }
        }
    }
}</code></pre><p>以上是一段可以直接执行的伪代码，将&nbsp;sourceRedis 的所有读流量转发到&nbsp;targetRedis 执行</p><span id="OSC_h2_4"></span><h2>实现解析</h2><p>上面采用的 Java 的 Redis 客户端 jedis 来开发，首先调用了 monitor 这个指令，这个指令是一个阻塞指令，会一直订阅 Redis 服务端的指令执行记录，记录的格式如下：</p><pre><code>1695869359.747056 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869359805247076" "LIMIT" "0" "1"
1695869359.748040 [0 127.0.0.1:64257] "EXISTS" "asynq:{sys}:paused"
1695869359.748259 [0 127.0.0.1:64257] "EXISTS" "asynq:{sync}:paused"
1695869359.748578 [0 127.0.0.1:64257] "EXISTS" "asynq:{default}:paused"
1695869359.748916 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869359869190783" "LIMIT" "0" "1"
1695869359.749154 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869359877625076" "LIMIT" "0" "1"
1695869359.749348 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869359878760313" "LIMIT" "0" "1"
1695869359.749530 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869359882064571" "LIMIT" "0" "1"
1695869359.779048 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869360024586886" "LIMIT" "0" "1"
1695869359.785898 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869360031603858" "LIMIT" "0" "1"
1695869359.786092 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869360031656719" "LIMIT" "0" "1"
1695869359.786923 [0 127.0.0.1:64257] "ZRANGEBYSCORE" "delayed_tasks" "0" "1695869360031666910" "LIMIT" "0" "1"</code></pre><p>所以我们只要解析出指令，然后发送到目标实例就好了</p><p>这里还涉及到一个问题，怎么过滤出只有读指令的记录？</p><p>我尝试过问 chatGPT ，但是他一点都不靠谱，不是少了读的指令，就是用其他写指令凑数。所以不可信。好在 Redis 服务端对每个指令都进行了打标，区分了读指令还是写指令。</p><p><img height="2124" src="https://oscimg.oschina.net/oscnet/up-44f7a2eac1677679c388b29b1cbb6516f98.png" width="2932" referrerpolicy="no-referrer"></p><p>所以只需要把所有的只读指令打印到控制枱复制出来就解决这个问题了。</p><span id="OSC_h2_5"></span><h2>注意事项</h2><p>Redis monitor 指令是一个对 Redis 性能有损的指令，官方测试会对单实例 Redis 降低 50% 左右的性能，我实际测试在 Redis 实际负载不高的情况下,这个影响可以忽略不计（特别高 QPS 的实例谨慎使用）。因为 Redis 单机 QPS 能支撑 10W 。比如线上实时 QPS 1W ，使用 monitor 的时候，QPS 和 RT 几乎没有变化。</p><p>另外需要注意，monitor 长时间运行会增加 Redis 的内存消耗，所以如果做性能验证，最好控制下时间，不要一直跑。</p><p><img height="1532" src="https://oscimg.oschina.net/oscnet/up-ec5ada01401a210a0401bba6bb508c8d109.png" width="1580" referrerpolicy="no-referrer"></p><ul><li>monitor 文档：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fredis.io%2Fcommands%2Fmonitor%2F" target="_blank">https://redis.io/commands/monitor/</a></li></ul><span id="OSC_h2_6"></span><h2>结语</h2><p style="color:#000000; margin-left:0; margin-right:0; text-align:start">在本篇博客中，我们探讨了 Redis 流量镜像的实现方法和其在降本增效方面的重要性。我们了解到，传统的验证方式在迁移 Redis 实例时需要大量的人力和时间投入，降低了降本增效的 ROI。为了解决这一问题，引入了 Redis 流量镜像的需求。</p><p style="color:#000000; margin-left:0; margin-right:0; text-align:start">通过将原 Redis 的所有读流量直接重放到目标 Redis SSD 实例，我们可以高效地完成迁移实例的过程，减少了 SRE 的工作量，并显著缩短了迁移时间。这种方法不仅提高了迁移过程的效率，还降低了成本和风险，使得降本增效的目标更加可行和实现。通过采用这种方法，我们可以更高效地迁移 Redis 实例，并在降低成本、提高效率的同时保持业务的性能和稳定性。</p><p style="color:#000000; margin-left:0; margin-right:0; text-align:start">谢谢您的阅读！如果您有任何问题或想法，请随时参与评论留言。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 08:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/klblog/blog/10115744</guid>
            <link>https://my.oschina.net/klblog/blog/10115744</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[九章云极 DataCanvas 公司完成 D1 轮融资！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="text-align:center"><img height="383" src="https://oscimg.oschina.net/oscnet/up-bbbb9a1d74fefff63912dffe366a6a8cdd1.jpg" width="900" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0"><strong>近日，九章云极 DataCanvas 公司完成总融资额 3 亿元 D1 轮融资。</strong>中国电子集团旗下中电智慧基金、华民投、中国太平旗下太平创新、浙江东方旗下东方嘉富等央国企旗下投资机构，以及卓源资本等专注人工智能赛道的知名财务投资机构参与本轮融资。</p><p style="margin-left:0; margin-right:0">九章云极 DataCanvas 公司作为国家专精特新小巨人企业，其「人工智能」技术创新能力和「基础软件」产品商业化能力备受市场认可。2020 年被 Gartner 列入全球 AutoML 关键供应商库，并连续四年入选 IDC 中国机器学习开发平台厂商全国 Top3。在全球人工智能开源领域，自主研发的多项全球首个开源项目，填补 AI 领域技术空白，引领人工智能行业跨时代发展。作为中国信通院的战略合作单位、标准核心参编单位，共同编订发布全球首个 AI 模型开发管理标准、全国首个商用人工智能开发平台等多项人工智能基础软件领域、大模型领域的标准。作为大模型生态共同体中的通用大模型代表企业，模型伙伴成员单位，九章云极 DataCanvas 公司与相关政府部门共建经济高效的智算中心以及大模型产业集聚区，并率先落地规模化、可复制的大模型应用——企业知识管家，充分破除企业知识管理过程中的痛点难点，全面释放企业业务价值。</p><p style="margin-left:0; margin-right:0">投资方表示，九章云极 DataCanvas 公司包含大模型在内的前沿人工智能技术成果、长效优势显著的 AI 基础软件商业化策略，充分展现了我国科技创新企业的实力和潜力。基础软件是人工智能的底座，人工智能的基础软件的发展决定了人工智能发展的深度、高度、广度，拥有商业化的广阔市场。在大算力时代，充分发挥算法+算力的优势，作为赛道领头企业实现规模化行业应用能力，看好公司未来发展。</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>AI 基础软件</span>正为企业创新和社会进步提供核心驱动力</strong></p><p style="margin-left:0; margin-right:0"><span>近年来，AI 基础软件成为政府、技术服务厂商和市场客户的重点关注和布局方向，各部门政府相继发布措施条例来鼓励、支持、培养基础软件产品服务的开发以及对本地现有产业的支持赋能，云服务厂商、技术创新企业、传统企业都正在利用自身优势打造 AI 基础软件服务内核和外延，为垂直业务场景创新落地和跨行业智能决策提供源源不断的活力。</span>IDC 数据显示，截止到 2022 年底，中国人工智能软件市场规模超过 307 亿元人民币。根据中国信息通信研究院测算，2022 年我国算力核心产业规模达到 1.8 万亿元，其中政府、金融、电信领域保持高速增长，连同其他专业 AI 服务领域的增长共同促进了中国人工智能基础软件的发展和应用落地。</p><p style="margin-left:0; margin-right:0">多年来，凭借行业领先的人工智能基础软件，九章云极 DataCanvas 为各行各业的头部企业提供自主 AI 能力。在今年 6 月的产品发布会上，九章云极 DataCanvas 公司宣布产品体系升级，重磅发布业界前沿技术水准的 DataCanvas Alaya 九章元识大模型、DingoDB 多模向量数据库等多款产品，共同构成<span>&nbsp;</span><strong>「AIFS 人工智能基础软件」和「DataPilot 数据领航员」</strong>核心产品体系。至此，九章云极 DataCanvas 公司成功实现构建企业「大+小」模型能力的产品布局。</p><p style="margin-left:0; margin-right:0">DataCanvas Alaya 九章元识大模型（以下简称「九章元识」）不仅包含配置和参数的多样选择，还以「白盒」形态提供给用户，这使得用户获用大模型技术能力的同时，还获得了最大化的自主能力。九章元识这一开放性无疑将大模型的自主应用门槛降到最低。</p><p style="margin-left:0; margin-right:0">此外，九章元识还具有「多模态」的重要属性。支持各类结构化和非结构化数据（包括数字、文本、图像、音频、视频等），将成倍转化和输出数据资产价值。同时，为了支撑多模态数据的对齐，九章云极 DataCanvas 公司推出的开源产品 DingoDB 多模向量数据库，配合元识大模型，可以为向量数据提供存储和分析，为多模态数据的利用和管理提供必备工具载体。</p><p style="margin-left:0; margin-right:0">当前，九章云极 DataCanvas 公司已推出基于九章元识和 DingoDB 的大模型解决方案「企业知识管家」，破除私域多模态数据管理和应用难题，赋能企业构建高度自动化与智能化的企业知识库，加速多模态大模型落地应用。</p><p style="margin-left:0; margin-right:0; text-align:center"><strong><span>从 AI 基础软件到 AI 基础服务</span>，&nbsp;「云中云」战略再进阶</strong></p><p style="margin-left:0; margin-right:0">从终端大模型应用，到模型构建平台、算法平台，到向量数据库，再到最底层的算力建设，大模型技术的火热为 AI 模型产业链提供层层机遇。在将底层核心技术视为发展命脉的市场态势下，人工智能基础设施再度引爆。基于对未来趋势的研判和为用户提供全生命周期服务的初衷，<strong>九章云极 DataCanvas 公司正式成立智算中心，将在人工智能基础软件产品优势的基础上，为广大用户提供「算法+算力」一体化服务，构建高效计算底座的基础设施。</strong></p><p style="margin-left:0; margin-right:0">九章云极 DataCanvas 公司董事长方磊表示，自提出「云中云」战略以来，公司以 AI 基础软件为载体，持续将 AI 能力嵌入千行百业的云生态中，成果显著。此次推出的「算法+算力」一体化服务，是抓住市场中算力的巨大需求，深化与云厂商、智算中心等伙伴的生态合作，共同构建高质量的「AI 基础服务」。&nbsp;</p><p style="margin-left:0; margin-right:0">未来，九章云极 DataCanvas 公司将围绕客户的人工智能升级需求，践行「云中云」战略，充分布局算力建设，加速多模态大模型行业落地，以先进技术创新引领行业，为数字经济建设助力筑基。</p><p style="margin-left:0; margin-right:0"><strong>投资方简介</strong></p><p style="margin-left:0; margin-right:0"><strong>中电智慧基金</strong>作为中国电子的产业投资基金，坚持服务于中国电子主责主业，投向战略性新兴产业，推动重构计算产业体系，在集成电路、网络安全、数据产业、高新电子等领域开展投资布局。充分发挥产业背景优势，推动被投企业与集团产业充分协同，对集团补链强链，对企业投后赋能。</p><p style="margin-left:0; margin-right:0"><strong>华民投</strong>是股权投资领域的国家队、央企投资协会会员单位。借助权威股东的资源背景，公司广泛链接资本市场和 5000 多家上市公司，紧跟国家战略，围绕战略新兴产业开展投资，投资的项目涉及半导体、新材料、新能源、智能汽车、人工智能、数字经济和生物医药等多个方向，全面助力国家产业升级转型和经济发展。</p><p style="margin-left:0; margin-right:0"><strong>太平创新</strong>是国内首批经原中国银保监会批准设立的保险系私募基金管理公司之一，作为中国太平保险集团旗下专业私募股权投资平台，充分发挥保险资金的专业优势与中国太平的央企担当，布局医疗健康、科技金融、先进制造、乡村振兴等投资赛道。</p><p style="margin-left:0; margin-right:0"><strong>东方嘉富</strong>是浙江省属国有上市金控平台——浙江东方 (SH.600120) 旗下「市场化运作专业化管理」的混合所有制私募基金管理机构，重点关注智能制造和新材料、新一代信息技术及生命科学领域早期及成长期企业的投资，并已建立起私募股权投资加私募证券投资「一二级投研联动」的资产管理体系。截至目前，东方嘉富累积管理基金总规模超 150 亿元人民币，累积投资企业超 90 家。</p><p style="margin-left:0; margin-right:0"><strong>卓源资本</strong>创始于 2016 年，是一家专注于集成电路及 AI 的专业投资机构，起源于清华大学计算机系 FIT 楼实验室，由清华大学校友共同创办。卓源资本致力于打造中国最领先的泛集成电路及 AI 领域的投资机构，创始团队累计管理与投资规模超过 30 亿人民币。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 06:50:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261144</guid>
            <link>https://www.oschina.net/news/261144</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[首个图文混合创作大模型「书生·浦语灵笔」开源]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">上海人工智能实验室（上海 AI 实验室）<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FbXz_BRiv8CskONSWQR44WA" target="_blank">宣布</a>推出首个图文混合创作大模型书生·浦语灵笔（InternLM-XComposer，简称「浦语灵笔」），依托强大的多模态性能，解锁「一键生成」图文混合文章的创作能力，为大模型落地应用提供更多可能。</span></p><p><span style="color:#000000">目前，浦语灵笔已开源其中的智能创作和对话（InternLM-XComposer-7B）及多任务预训练（InternLM-XComposer-VL-7B）版本，并提供免费商用。</span></p><p><span style="color:#000000">此前，上海 AI 实验室曾陆续开源了书生·浦语大语言模型的 7B（InterLM-7B）及 20B（InternLM-20B）版本。基于书生·浦语大语言模型（InternLM），浦语灵笔接受视觉和语言模态输入，不仅在图文对话方面表现优秀，更具备图文并茂文章的「一键生成」能力。</span></p><p><span style="color:#000000">浦语灵笔能够进行流利的中英文图文对话，准确理解图像内容；并解锁了图文并茂文章创作的全新能力。除自动配图能力外，浦语灵笔还提供了配图推荐和更换功能，根据用户实际需求定制图文内容。</span></p><p><span style="color:#000000"><img alt="" height="284" src="https://oscimg.oschina.net/oscnet/up-ec14d16110bb87f0afcb2b74f79d7e39691.png" width="500" referrerpolicy="no-referrer"></span></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-f6052a5051fe3cacf90b67197e4a6aca666.gif" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">目前，浦语灵笔已支持科普文稿、营销广告、新闻稿件、影视评论、生活指南等类型文章的图文并茂生成，并将逐渐开放更多能力。</span></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-65a9432b417fb971eade0876788010d6752.gif" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">浦语灵笔为图文文章创作设计了「三步走」的算法流程：</span></p><ul><li><span style="color:#000000">理解用户指令，创作符合主题要求的长文章。</span></li><li><span style="color:#000000">智能分析文章，模型自动规划插图的理想位置，并生成所需图像的内容要求。</span></li><li><span style="color:#000000">多层次智能筛选，利用多模态大模型的图像理解能力，从图库中锁定最完美的图片。</span></li></ul><p><img alt="" height="249" src="https://oscimg.oschina.net/oscnet/up-b14e63c7394663e682d3500915895eaf962.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">研究人员采用 5 个主流的多模态大模型评测对 InternLM-XComposer-VL-7B 的能力进行了详细测试。</span></p><p><img height="384" src="https://oscimg.oschina.net/oscnet/up-6d115621e76f671c92c4cd8b35d201bd0c4.png" width="500" referrerpolicy="no-referrer"></p><p><img height="233" src="https://oscimg.oschina.net/oscnet/up-221640dab71f38f3d96610b4a6b94d5671d.png" width="500" referrerpolicy="no-referrer"></p><p><img height="353" src="https://oscimg.oschina.net/oscnet/up-fd2769a9128cb307d94d946f31bb4ff3bc2.png" width="500" referrerpolicy="no-referrer"></p><p><img height="345" src="https://oscimg.oschina.net/oscnet/up-fee648bef717b06a7d28b20b5d0ed2625e7.png" width="500" referrerpolicy="no-referrer"></p><p><img height="355" src="https://oscimg.oschina.net/oscnet/up-9c29359b6718e111b5a2c03074cc6ed6b97.png" width="500" referrerpolicy="no-referrer"></p><p><img height="315" src="https://oscimg.oschina.net/oscnet/up-b748c9bb6f4b6b3948fb921b630992b227f.png" width="500" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 06:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261135</guid>
            <link>https://www.oschina.net/news/261135</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[ChatGPT 移动应用 9 月收入高达 458 万美元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">市场情报公司 Appfigures 的最新分析数据<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fappfigures.com%2Fresources%2Finsights%2F20231006%3Ff%3D2" target="_blank">显示</a>，OpenAI 的官方 ChatGPT 应用程序安装数量和收入持续增长，9 月份创下了两项新纪录：全球 iOS 和 Android 应用程序的下载量达到 1560 万次，其中 Google Play 的下载量为 900 万，App Store 的下载量为 660 万。以及总收入接近 460 万美元，净收入 320 万美元；其中 300 万美元，自 iPhone 用户，其余来自 Google Play。</span></p><p><span style="color:#000000">ChatGPT 官方应用自 5 月起在 App Store 上架，7 月正式登录 Google Play 平台。自推出以来，ChatGPT 应用的总安装量已达到 5220 万次。该应用在 6、7、8 月份的总营收分别为 210 万美元、274 万美元以及 381 万美元。</span></p><p><span style="color:#000000">目前为止，美国仍是 ChatGPT 的最大市场，贡献了该应用程序生命周期收入的约 60%。</span></p><p><img height="285" src="https://oscimg.oschina.net/oscnet/up-fd19ce9c3118494505c250065e7d0da18c7.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">但数据也显示，移动端 ChatGPT 应用的收入增长已经开始放缓。前几个月的收入增长率高达 30% (7 月为 31%，8 月为 39%)，但 9 月份已降至 20%，是迄今为止最低数值。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2023%2F10%2F09%2Fchatgpts-mobile-app-hit-record-4-58m-in-revenue-last-month-but-growth-is-slowing%2F" target="_blank">TechCrunch</a> 指出，收入增长放缓可能是 ChatGPT 接近饱和的第一个迹象，表明愿意为每月 19.99 美元的 ChatGPT+ 订阅服务付费的用户可能已经达到了上限。</span></p><p><span style="color:#000000">不过 ChatGPT 并不是收入最高的 AI 应用。Appfigures 数据显示，一款名为 Ask AI 的竞争对手由于大量的广告支出而获得了更多的收入；从 ChatGPT 移动版推出时的 5 月份的 648 万美元上升到了 8 月份的峰值 655 万美元。9 月份略有下降，降至 551 万美元，但仍高于 ChatGPT。其他竞争者如 Genie 和 AI Chat Smith 的增长幅度都不及 Ask AI。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 03:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261118/chatgpts-mobile-app-revenue-4-58m</guid>
            <link>https://www.oschina.net/news/261118/chatgpts-mobile-app-revenue-4-58m</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[curl 8.4.0 将修复高危安全漏洞]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>curl 创始人&nbsp;Daniel Stenberg（社区称号 bagder）表示将于 10 月 11 日发布的 curl 8.4.0 会修复高危安全漏洞，并称该漏洞可能是很长一段时间以来<strong> curl 遇到的最严重漏洞 (the worst curl security flaw in a long time)</strong>，同时影响到 libcurl 库和 curl 工具。</p><p>此外还会修复被评级为"LOW"的安全漏洞——仅 libcurl 受影响。</p><ul><li>CVE-2023-38545: severity HIGH (affects both libcurl and the curl tool)</li><li>CVE-2023-38546: severity LOW (affects libcurl only, not the tool)</li></ul><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1010/105819_w1di_2720166.png" referrerpolicy="no-referrer"></p><p>via&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fcurl%2Fcurl%2Fdiscussions%2F12026" target="_blank">https://github.com/curl/curl/discussions/12026</a></p></blockquote><p>bagder 表示目前无法透露有关受影响的版本范围，因为这会导致漏洞被高精确地针对性利用。他只说到「过去几年」发布的版本都会受影响。</p><p>bagder 已在邮件列表宣布该消息，建议使用 curl 的开发者密切关注即将发布的 curl 8.4.0，同时梳理在项目中使用该库的情况，以便及时安装更新。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 03:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261110/curl-8-4-coming</guid>
            <link>https://www.oschina.net/news/261110/curl-8-4-coming</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Unity 首席执行官 John Riccitiello 离职]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>根据 Unity 官方消息，Unity 首席执行官 John Riccitiello 已宣布离职，该事宜即日生效。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6c10a9521e4d9916a0e7e7d57f5f97d17cb.png" referrerpolicy="no-referrer"></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fwww.businesswire.com%2Fnews%2Fhome%2F20231009494331%2Fen%2FUnity-Announces-Leadership-Transition" target="_blank">Unity 在一份声明中表示</a>，已任命 James Whitehurst 为临时首席执行官，Roelof Botha 为董事长。Riccitiello 将继续为 Unity 提供建议，以确保平稳过渡，董事会将启动任命一位永久首席执行官的流程。</p><p><img src="https://static.oschina.net/uploads/space/2023/1010/103434_WHy2_2720166.png" referrerpolicy="no-referrer"></p><p>此前 Unity 宣布了新的收费规则，引起业内人士的强烈不满。之后 Unity 向公众和业内人士道歉，并调整了收费规则。</p><p><strong>延伸阅读</strong></p><ul><li><a href="https://www.oschina.net/news/257929/unity-runtime-fee">Unity 引擎明年起根据游戏安装量收费 (runtime fee)</a></li><li><a href="https://www.oschina.net/news/258513/unity-apologize-for-runtime-fee">Unity 道歉：将修改 "runtime fee" 收费政策</a></li><li><a href="https://www.oschina.net/news/258477/wait-is-unity-allowed-to-just-change-its-fee-structure-like-that">走近「收费门」：互相矛盾的服务条款导致 Unity 面临被起诉的风险</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 02:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261101/unity-ceo-john-riccitiello-is-retiring</guid>
            <link>https://www.oschina.net/news/261101/unity-ceo-john-riccitiello-is-retiring</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[【SIG 月报】9 月 openKylin 社区 SIG 组最新进展分享]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#222222; margin-left:0px; margin-right:0px; text-align:justify"><span><span style="color:#000000">为推动社区繁荣发展，打造开源操作系统创新生态，openKylin 社区围绕</span><strong><span style="color:#000000">创新硬件、人机交互、智能支撑、终端安全、互联协同、云端融合</span></strong><span style="color:#000000">等多个技术领域，以技术小组的形式开展深入研究和技术创新。接下来，让我们一起盘点 9 月份 openKylin 社区 SIG 组的最新进展：</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:center"><span><strong><span style="color:#ffffff"><span style="background-color:#4f9eff">9 月社区新增 SIG</span></span></strong></span></p><h1>9 月社区新增 1 个 SIG 组，目前已累计成立 86 个 SIG 组，新增 SIG 组信息如下：</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:left"><span><strong><span style="color:#ff9b0e"><span style="background-color:#f5faff">01</span></span><span style="color:#1c9cee"><span style="background-color:#f5faff">AI4OS SIG</span></span></strong></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000"><span style="background-color:#f5faff">操作系统智能化（Artificial Intelligence for Operating System）由社区爱好者发起成立，致力于将人工智能（AI）与操作系统（OS）相结合，以实现操作系统的智能化和性能优化。将大模型为代表的 AI 技术嵌入 openKylin 操作系统，让 AI 深扎底层操作系统，可以在没有任何应用作为中介的情况下，直接调用 AI 大模型能力完成任务。</span></span></span></p><ul><li><span><span style="color:#000000">SIG 主页：</span></span></li><li><span><span style="color:#0052ff">https://gitee.com/openkylin/community/tree/master/sig/AI4OS</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:center"><span><strong><span style="color:#ffffff"><span style="background-color:#4f9eff">9 月社区 SIG 活跃度汇总</span></span></strong></span></p><p>9 月社区新增有效 PR 数 559 个、仓库 Fork 数新增 104 个、SIG 组公开例会召开 7 次。截至目前，社区累计有效 PR 数 11122 个、仓库 Fork 数 4364 个、SIG 组公开例会召开 93 次，其中：</p><ul><li><span><span style="color:#000000">9 月社区 SIG 组 PR 贡献 top15 如下：</span></span></li></ul><div><p style="text-align:center"><img alt="" height="556" src="https://oscimg.oschina.net/oscnet/up-f6da435acfce936b5ba91e340785173187f.png" width="794" referrerpolicy="no-referrer"></p><p style="margin-left:0px; margin-right:0px">&nbsp;</p></div><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">9 月社区 SIG 组活跃地图分布情况（颜色越深代表越活跃，参考维度：PR、issue、SIG 会议）如下：</span></span></p><div><p style="text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-13b61252308faea98672ac9805bdafca274.png" referrerpolicy="no-referrer"></p><p style="margin-left:0px; margin-right:0px">&nbsp;</p></div><p style="color:#222222; margin-left:0; margin-right:0; text-align:center"><span><strong><span style="color:#ffffff"><span style="background-color:#4f9eff">9 月社区技术进展与成果</span></span></strong></span></p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>一、UKUI SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">UKUI(Ultimate Kylin User Interface) SIG 小组致力于桌面环境相关软件包的规划、维护和升级工作，满足各种设备和用户需求的桌面环境程序，主要包含程序启动器（开始菜单）、用户配置、文件管理、登录锁屏、桌面、网络工具、快捷配置等，为用户提供基本的图形化操作平台。桌面核心组件开发工具以 Qt、C++为主，宗旨是始终如一地提升系统的操作体验，提供集稳定性、美观性、流畅性和便捷性为一体的桌面环境。9 月进展如下：</span></span></p><ul><li><span><span style="color:#000000">系统监视器新增后台运行功能并注册托盘图标；</span></span></li><li><span><span style="color:#000000">搜索、开始菜单适配多 Display 场景下；</span></span></li><li><span><span style="color:#000000">优化搜索设置项显示策略、优化内存操作逻辑和若干 bug；</span></span></li><li><span><span style="color:#000000">修复文件管理器操作相关的若干问题；</span></span></li><li><span><span style="color:#000000">修复切换语言后立刻重启计算机后桌面出现多余图标问题；</span></span></li><li><span><span style="color:#000000">修复桌面目录后出现「锁状」图标问题；</span></span></li><li><span><span style="color:#000000">修复任务栏预览窗口操作显示逻辑问题；</span></span></li><li><span><span style="color:#000000">修复控制面板关于模块部分名词拼写问题；</span></span></li><li><span><span style="color:#000000">修复系统安装完成后应用通知按钮为关闭状态的问题；</span></span></li><li><span><span style="color:#000000">完成 3 篇桌面协议相关翻译；</span></span></li><li><span><span style="color:#000000">UKUI-Lite 技术方案评审；</span></span></li><li><span><span style="color:#000000">UKUI Framework 后续规划及统一接口方案评审。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎各位感兴趣的社区开发者加入我们，一起打造 openKylin 桌面系统稳定易用的桌面环境！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>二、RISC-V SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#333333">本</span>SIG 组主要负责 RISC-V 架构开源软件包的维护，发布 openKylin 的 RISC-V 版本，进行软件包构建、系统构建等工作。9 月 RISC-V SIG 组进展如下：</span></p><ul><li><span><span style="color:#000000">解决了 VisionFive2 开发板在 wayland 模式下鼠标指针不显示的问题；</span></span></li><li><span><span style="color:#000000">制作 roma 笔记本新版 sdk3.6.1 的镜像；</span></span></li><li><span><span style="color:#000000">更新并发布 VisionFive2 和荔枝派的 openKylin1.0.1 版本镜像；</span></span></li><li><span><span style="color:#000000">获取 electron 的包，通过安装高版本依赖的方式解决了 electron 的启动问题，目前可以在 xorg 模式下正常运行 electron；</span></span></li><li><span><span style="color:#000000">尝试在荔枝派开发板中适配 gpu，和厂商沟通联调；</span></span></li><li><span><span style="color:#000000">调试并进行 box64 代码梳理。主要学习动态重编译部分，梳理了运行微信从模拟运行入口，到 opcode 翻译，到汇编指令执行的流程。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 RISC-V 开发平台技术方向感兴趣的爱好者加入到 RISC-V SIG！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>三、Release SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">Release SIG 主要负责协调各个 SIG 组，把控版本开发进度和风险，制定版本发布计划，完成版本发布工作等。Release SIG 9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">推动 openKylin 1.0.1 各架构版本集成、测试验收等工作，完成 1.0.1 版本发布；</span></span></li><li><span><span style="color:#000000">推动搜狗输入法 NG 版本和 openKylin 1.0.1 适配；</span></span></li><li><span><span style="color:#000000">编写 openKylin 1.0.1 版本更新日志；</span></span></li><li><span><span style="color:#000000">openKylin 2.0 版本规划，需求讨论、推进，启动基础库选型等工作；</span></span></li><li><span><span style="color:#000000">和儒特科技讨论 QSFramework 在社区合作落地、代码持续集成等事宜。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 社区版本集成、版本管理、版本发行等工作感兴趣的爱好者加入到 Release SIG！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>四、Packaging SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">Packaging SIG 负责维护 openKylin 社区的软件包打包规范，维护公共软件包，以及协调和决策社区版本发布过程中的包依赖问题。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">《openKylin 生态应用自主选型构建》任务第一阶段选型报告评审；</span></span></li><li><span><span style="color:#000000">cme 程序无法使用（Compilation failed），执行报错问题分析修改；</span></span></li><li><span><span style="color:#000000">解决 arm64 架构基础构建工具 cmake 运行报符号未定义的问题；</span></span></li><li><span><span style="color:#000000">软件包源码信息整改。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 社区软件自主选型、编译打包工作感兴趣的社区爱好者加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>五、QA SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">QA SIG 组致力于提升 openKylin 社区版本质量，包括社区版本测试、质量保障等。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">openKylin-1.0-2309-beta-0901 版本测试；</span></span></li><li><span><span style="color:#000000">窗管替换 wlcom 专项测试；</span></span></li><li><span><span style="color:#000000">openKylin-1.0-2309-beta-0908 版本测试；</span></span></li><li><span><span style="color:#000000">openKylin-1.0.1-0918 版本测试。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 社区版本测试、质量管理感兴趣的社区爱好者加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>六、SecurityGovernance SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">openKylin SecurityGovernance SIG 通过接收和响应 openKylin 社区的产品安全问题报告、提供社区安全指导，开展安全治理等活动提升社区产品的安全性。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">Genmai 开发：新增安全漏洞自动化用例 poc40 个；解决某些架构下返回值异常导致崩溃问题；适配龙芯架构,适配 risc-v 架构，进度 80%；解决漏洞用例 yaml 文件格式错误问题；解决 kysec、Selinux 导致 poc 及基线扫描出错问题；增加「将 root 权限传入基线脚本」的功能；开发根据服务端版本自动更新功能；解决 C/S 架构漏洞检测传输时长过长造成超时问题；新增原创安全漏洞 5 个；</span></span></li><li><span><span style="color:#000000">参加在西班牙毕尔巴鄂由 Linux 基金会主办的 2023 Linux 安全峰会，并发表主题演讲。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 版本安全全漏洞挖掘/验证、安全漏洞修复等安全方面工作感兴趣的社区爱好者加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>七、OpenSDK SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">OpenSDK SIG 组负责开发者套件（base、system、applications）规划、开发、维护等工作，致力于解决应用在多操作系统中的兼容性问题。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">完成配置化模块的 conf2 表结构设计、xml2yaml 特定格式转换工具开发工作；后端服务支持数据写入以及键值对变更信号发送；</span></span></li><li><span><span style="color:#000000">优化并协助其他组件排查解决问题共 21 个：优化开发手册易用性，包括 man 手册以及开发指南中 API 引入的 sdk 版本号；优化应用埋点接口功能。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对开发者套件感兴趣的社区爱好者们加入 OpenSDK SIG 组！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:left">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>八、CompatWinApp SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">CompatWinApp SIG 组致力于将大量的 Windows 系统应用程序引入到 openKylin 系统。SIG 组将通过研究应用兼容技术和指令翻译技术，研制完善的 windows 应用兼容方案，让更多的 windows 应用能兼容运行于 openKylin 系统，不断繁荣 openKylin 软件生态。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">修改 wine 助手下载流程，直接下载应用修改为先跳转到应用程序下载页面；在当前应用配置文件中增加应用下载页参数；</span></span></li><li><span><span style="color:#000000">下载应用时增加对用户的操作提示；修复当应用下载链接更新时无法下载的问题；修复下载链接重定向时无法下载的问题；</span></span></li><li><span><span style="color:#000000">研究解决了 wine riched20 模块中导致的微信编辑输入框右键不显示菜单的问题，初步解决了编辑框光标位置错乱的问题；</span></span></li><li><span><span style="color:#000000">在 wine-program 仓库 wiki 界面增加 wine 助手的使用说明。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对应用兼容技术和指令翻译技术感兴趣的爱好者加入到 CompatWinApp SIG！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>九、Infrastructure SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">Infrastructure SIG 负责 openKylin 社区的基础平台系统功能的开发、维护。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">优化 openKylin 看板结合 SIG 状态自动更新 SIG 数量问题；</span></span></li><li><span><span style="color:#000000">openKylin 看板增加任务平台积分更新功能；</span></span></li><li><span><span style="color:#000000">优化 openKylin 看板前端，增加页面缓存 TAB；</span></span></li><li><span><span style="color:#000000">CLA 开放企业管理员手动添加签署员工限制；</span></span></li><li><span><span style="color:#000000">CLA 修复企业管理员后台一些内容未国际化问题；</span></span></li><li><span><span style="color:#000000">openKylin 看板增加 commit 信息统计，支持 commit 记录导出。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 社区基础设施平台开发维护感兴趣的爱好者加入到 Infrastructure SIG！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>十、Connectivity SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">本 SIG 组致力于 openKylin 社区的互联互通基础能力开发与维护，9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">优化文件管理其插件相关能力，解决线程安全问题。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎各位感兴趣的社区开发者加入 Connectivity SIG 小组，一起共建 openKylin 桌面系统互联互通能力！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>十一、I18n SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">I18N SIG 组负责 openKylin 社区国际化和本地化相关工作，包括多语言开发框架、多语言平台开发和维护，以及社区、版本内文档的翻译管理相关工作。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">翻译官网新闻 18 篇；</span></span></li><li><span><span style="color:#000000">翻译 openKylin 基于 RISC-V 的主要工作介绍；</span></span></li><li><span><span style="color:#000000">校验 openKylin 个人信息保护及隐私政策声明。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对操作系统、网站网页、文档等翻译工作感兴趣的社区爱好者们加入 I18n SIG 组！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:left">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>十二、InputMethod SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">本 SIG 组致力于组建输入法开源社区，推进开源输入法框架及开源输入法在社区维护。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">定位分析搜狗输入法问题，包括 wayland 环境输入窗口显示异常、托盘菜单图标显示异常和候选词上屏异常等问题；</span></span></li><li><span><span style="color:#000000">讨论 OK 输入法进展，完成 OK 输入法设计文档；</span></span></li><li><span><span style="color:#000000">分析拼音输入法右键菜单显示异常问题，与 fcitx 社区讨论修改情况；</span></span></li><li><span><span style="color:#000000">完成虚拟键盘支持动画效果开发，提高 UI 美观度；</span></span></li><li><span><span style="color:#000000">完成虚拟键盘支持多语言输入开发，其中包括哈萨克语、维吾尔语、柯尔克孜语和藏语等。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 社区 fcitx 输入法框架、桌面虚拟键盘开发工作感兴趣的社区爱好者加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>十三、Kernel SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">Kernel SIG 组致力于新硬件适配、新功能、新特性开发。不断提升内核健壮性、稳定性，能更好的为 openKylin 系统和应用程序提供底层技术支持。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">6.1 内核从 6.1.43 更新到 6.1.55。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对内核感兴趣的社区小伙伴加入 openKylin 社区 Kernel SIG 组！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="margin-left:0px; margin-right:0px; text-align:justify"><strong>十四、Virtualization SIG</strong></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">Virtualization SIG 组致力于构建 openKylin 社区系统虚拟化技术，打造面向端、边、云的全场景虚拟化解决方案。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">qemu:修复了 CVE-2023-0330 漏洞。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对虚拟化组件或软件包技术感兴趣的社区小伙伴加入 openKylin 社区 Virtualization SIG 组！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><em><strong>十五、Framework SIG</strong></em></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">本 SIG 组致力于为 openKylin 社区提供集程序编辑、编译、调试、发布、分析等全套开发功能的编程环境，涵盖通用集成开发环境、编译工具链、运行时环境、类库等，9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">Kylin-Code 发布 v0.1.3，修复诸多问题，并上架应用商店；</span></span></li></ul><ul><li><span><span style="color:#000000">C/C++调试，Java 调试，插件依赖管理，死锁检测等插件发布新版本。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎对集成开发环境研发感兴趣的社区开发者和爱好者加入 Framework SIG！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><em><strong>十六、Cutefish SIG</strong></em></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">Cutefish SIG 负责移植 Cutefish 桌面环境及其组件，专注于打造美观易用、极简操作的桌面环境。9 月进展如下：</span></span></p><ul><li><span><span style="color:#000000">完成 RISC-V 架构开发板 VisionFive2 的适配工作；</span></span></li><li><span><span style="color:#000000">完成 ARM 架构开发板 CoolPi 4B 的适配工作。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">如果您对移植桌面环境有兴趣，或者有相关打包经验，欢迎加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><em><strong>十七、KernelBuilder SIG</strong></em></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">KernelBuilder SIG 组负责 openKylin 内核预览版本的自动化构建，构建工具（kernel-builder）的规划、开发、维护等工作。同时积极维护了 openkylin-rootfs 和 openkylin-wsl 仓库，为 openKylin 提供了可用的根文件系统、wsl 开发环境为 openKylin 在 docker 容器化创造了条件。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">rootfs 根文件系统迭代更新；</span></span></li><li><span><span style="color:#000000">docker 镜像打包制作；</span></span></li><li><span><span style="color:#000000">利用 github actions 自动打包制作 rootfs 根文件系统和 docker 镜像，并更新内部环境；</span></span></li><li><span><span style="color:#000000">distcc 软件适配进行中；</span></span></li><li><span><span style="color:#000000">为内核自动化构建创造 docker 预运行环境；</span></span></li><li><span><span style="color:#000000">同时本月联合 opendde sig 开展共同开发计划；</span></span></li><li><span><span style="color:#000000">香橙派 kernel 适配中、根文件系统适配中；</span></span></li><li><span><span style="color:#000000">内部测试 apt 源已搭建完成、目前可以小范围通过 apt 分发测试版内核。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 社区内核构建及应用、docker 容器化、根文件系统、wsl 开发环境感兴趣的社区爱好者加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><em><strong>十八、RTHypervisor SIG</strong></em></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">RTHypervisor SIG 小组致力于实时虚拟化技术的研究，目前主要包括 Jailhouse，提供工控、车载等领域实时控制的虚拟化解决方案，Jailhouse 项目 9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">新增 arm64 平台上通过将 pcie rc 和 its 隔离给 non root cel 的方式，将 pcie 设备隔离给 non root cell，实现 pcie 设备直通功能。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有对 openKylin 社区实时虚拟化技术感兴趣的社区爱好者加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><em><strong>十九、FAQ SIG</strong></em></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">FAQ SIG 小组致力于收集各渠道社区开发者、爱好者等用户反馈的问题，并建立相关标准化流程推动问题解答或解决同时，在这一过程中不断为 openKylin 社区积累 FAQ 知识库。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">召开 1 次 SIG 例会，和 UKUI SIG，Docs SIG，Community SIG 组交流和讨论当前待解决的问题；</span></span></li><li><span><span style="color:#000000">收集论坛、社群高频问题并提交 issue 指派给开发，解决 10+高频问题；</span></span></li><li><span><span style="color:#000000">社群用户答疑与指导，指导用户解决系统下载安装、软件商店、桌面环境等相关问题。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">欢迎所有感兴趣的社区爱好者加入我们！</span></span></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify">&nbsp;</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><em><strong>二十、OpenDDE SIG</strong></em></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">OpenDDE SIG 致力于维护 openKylin 的 DDE 桌面环境以及相关组件，专注于打造美观易用、极简操作的桌面环境。9 月主要进展如下：</span></span></p><ul><li><span><span style="color:#000000">联合 KernelBuilder SIG 研究了 VisionFive2 EDK2 UEFI 镜像制作流程；</span></span></li><li><span><span style="color:#000000">DDE 桌面软件包更新工作。</span></span></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:justify"><span><span style="color:#000000">如果您对移植桌面环境有兴趣，或者有相关打包经验，欢迎加入我们！</span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 02:25:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261098</guid>
            <link>https://www.oschina.net/news/261098</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[搞流式计算，大厂也没有什么神话]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p>抖音、今日头条，是字节跳动旗下最受用户欢迎的两款产品，也是字节跳动的门面。而在这背后，是众多技术团队在支撑，流式计算就是其中一支。</p><p>不过，即使是在字节跳动，搞流式计算也没有神话。只有一群年轻人，花了六年时间，一步一个脚印，从一开始的「不懂技术不懂业务」，最后承载起了字节内部流式计算平台以及应用场景的构建，支撑了机器学习平台、推荐、数仓、搜索、广告、流媒体、安全和风控等众多核心业务。2022 年，该团队完成了对 Flink 计算引擎的云原生化改造，并通过火山引擎正式对外提供云上能力。</p><p>这不是一个挽狂澜于既倒的英雄故事，没有什么跌宕起伏的情节，也没有耀眼的鲜花与掌声。而是千千万万个普通开发者中的一小群人，一边在业务中被动接受成长，一边在开源中主动寻求突破的一段记录。</p><span id="OSC_h3_1"></span><h3><span style="color:#2980b9">01 <strong>代码要写，业务也要拉</strong></span></h3><p>2019 年，随着抖音的爆发，字节跳动站在了高速增长的起点，直播、短视频，广告等业务也都乘势而起。这些业务，都需要流式计算来支撑。</p><p><strong>字节流式计算团队负责人张光辉，正面临诸多棘手的问题。</strong></p><p>先把时间线往前推两年，彼时张光辉刚加入字节跳动，计算引擎用的还是 Apache<em></em>Storm——诞生于 2011 年的、Twitter 开发的第一代流处理系统，只支持一些 low level 的 API。</p><p>「所有的 Storm 任务都是在开发机上用脚本提交，运维平台处于非常原始的状态。如果 Storm 集群故障，作业都无法自动恢复，甚至无法找到所有存量作业。」张光辉对此记忆犹新。</p><p>话虽这么说，但谁也别嫌弃谁。那时张光辉的履历上，并没有流式计算产品的经验，不过有些「沾亲带故」——参与过流式计算的上下游产品开发，比如数据采集、消息队列。</p><p>好在趁着字节的业务场景偏单一，主要聚焦在机器学习场景，张光辉和其团队将流式计算引擎从 Apache Storm 切换到了 Apache Flink。所谓团队，其实连他在内，也仅有两人。之后又在 2018 年与数据流团队合作完成了流式计算平台化的构建，包括任务的监控、报警，日志采集，异常诊断等工具体系。</p><p>来到 2019 年，流式计算要支撑的业务场景已经相当丰富，扩展到了实时数仓、安全和风控等，并且还在不断增加。单个场景需求也变得更加复杂：推荐业务越来越大，单个作业超过 5 万 Cores；实时数仓业务场景需要 SQL 来开发，且对数据准确性有了更高要求。</p><p>然而，由于团队人手严重不足，工作进展很是缓慢。「只有两个人，Oncall 轮流值周。不用值周的时候，往往都在解决上一周 Oncall 遗留的问题。」张光辉如此形容。</p><p>张光辉不得不一边扩充人员，一边与数据集成团队着手构建 SQL 平台。李本超正是这个时候加入了流式计算团队，并且在不久之后，就成为了 Flink SQL 方向的技术负责人。</p><p><strong>然而，用 </strong><strong>SQL</strong><strong> 来开发</strong><strong>流式计算</strong><strong>任务</strong><strong>，李本超也没有太多经验：「一开始，技术也不懂，业务也不懂。」</strong></p><p>在此之前，他在一家中小型企业任职，工作范围涉及广泛，流式计算只能算其中一个方向。加入字节后，李本超这才意识到，字节的流式计算规模远超自己的想象。之前只能看到 1 个并发的任务，而在字节，一个任务的并发却可以上万，仅单个任务使用的计算资源就比其上家公司所有任务加起来都多。</p><p>但李本超不能不懂。一周五天上班时间，其中有三天，张光辉早上第一件事情就逮着他问，跟哪个业务聊了，能新建几个 SQL 任务。</p><p><strong>指标每天都在头顶打转，李本超不得不给团队「拉业务」。</strong>用的话术就跟在大街上拦住路人卖产品一样，只不过地点换成了字节在北京的各个工区。</p><p>「哎，这个流式计算我们可以通过 SQL 开发，你们感不感兴趣？想不想了解一下？」李本超没事就联系电商、直播、广告、游戏、教育等业务部门负责人。只要人家点头，李本超二话不说，马上坐班车跑去工区现场交流。</p><p>张光辉评价：「那个时候，真的是‘无所不用其极’。」</p><p>有了 SQL 平台，开发及维护效率飞速提升。「原来一个人开发一个任务，需要一两天。而现在，一个人一天直接就能搞定十个任务。此外，业务方与我们的沟通方式也更简单了，对方写的代码我们也都能看懂，优化起来很方便。」</p><p>除此之外，字节在 Flink 稳定性方面做了大量的工作，比如支持黑名单机制，单点故障恢复，Gang 调度，推测执行等功能。由于业务对数据的准确性要求更高了，团队支持作业开启 Checkpoint 机制来保证数据不丢失，并在字节得到了大面积的推广和落地。</p><p>在这个过程中，李本超也发现，Flink 可能没有想象得那么强大、易用，比如随便改一改 SQL 状态就没法兼容。针对这类尚未被社区解决的问题，字节内部也进行了大量的优化方案探索。</p><p style="text-align:center"><img height="600" src="https://oscimg.oschina.net/oscnet/up-e8d825d23950c5e2d58dc9a101db0d82ab1.png" width="600" referrerpolicy="no-referrer"></p><p style="text-align:center"><span style="color:#8f959e"><em>字节跳动 </em></span><span style="color:#8f959e"><em>Flink</em></span><span style="color:#8f959e"><em></em></span><span style="color:#8f959e"><em>SQL</em></span><span style="color:#8f959e"><em> 任务占比</em></span></p><span id="OSC_h3_2"></span><h3><span style="color:#2980b9">02 <strong>Flink</strong><strong>，原来不止于流式计算</strong></span></h3><p>字节跳动选用 Flink 作为流式计算处理引擎后，每天有数万个 Flink 作业运行在内部集群上，峰值流量高达每秒 100 亿条数据。单个作业的规模也非常大，每个计算节点使用 3 万左右的并发，整个作业使用 300 多台物理机。Flink 集群的稳定性和性能优化，以及单个超大作业的部署、执行和 Failover 等优化，面临的问题在整个业界都难觅第二。</p><p>由于 Flink 是一个流批一体计算引擎，字节跳动内部也在积极推动 Flink 流批一体落地，上线了 2 万多个 Flink 批式作业，在这个过程中解决了很多稳定性和性能问题，比如 Hive 语法兼容、慢节点、推测执行等。</p><p>同时，字节跳动内部启动了 ByteHTAP 项目，结合字节内部的 OLTP 系统，已经能够支持数据延时低（亚秒级）、数据一致性要求高的分析型计算，但还缺一个计算引擎来支持 OLAP 计算。由于字节在 Flink 做了大量的深入优化，最终将其作为 ByteHTAP 的 OLAP 引擎。</p><p style="text-align:center"><img height="630" src="https://oscimg.oschina.net/oscnet/up-0bdaccfa52987511e6780144f12d4450ee4.png" width="772" referrerpolicy="no-referrer"></p><p><strong>然而，</strong><strong>在 ByteHTAP </strong><strong>开始给业务方提供线上 </strong><strong>OLAP</strong><strong> 服务时，新的问题又出现了。</strong>业务方不仅对单并发查询的 latency （延迟）有要求，还希望团队提供的 OLAP 服务能够支持高并发度。</p><p>正值 2021 年年初，方勇加入了字节跳动，担任流式计算架构师。为了支撑线上业务，方勇和团队要尽快把这块的能力给补齐。</p><p>「整个开发过程非常煎熬，压力非常大。」方勇说：」ByteHTAP 已经提供了线上服务，我们需要快速迭代，使 Flink 支持更高的并发查询。」</p><p>每次团队开周会，方勇都会盯着 QPS 指标。用了近半年的时间，「总算把 QPS 从个位数优化到十几、几十，直到线上单集群支持几百 QPS」。</p><p>近两年，字节正在将 Flink OLAP 诸多优化贡献回社区。 Flink OLAP 的相关内容也加入了到了 Apache Flink 2. 0 的 Roadmap 中。</p><p>一条完整的数据生产链路，分为三个计算场景，分别是流式、批式和 OLAP 计算。在实时数仓场景，需要 Storm 或 Flink 来支撑流式计算；在批式场景，则要依靠 Hive 或 Spark。当计算语义不一样时，两套引擎会导致流式结果和批式结果不一致。而且，流批一体数据计算完成之后，还需导入数仓或者离线存储，此时还要引入一套新的 OLAP 引擎去探查、分析，这就更加无法保证正确性和一致性。</p><p>而且，优化及维护也颇为麻烦。三套系统就意味着，要建三个团队去分别维护。一旦遇到需要优化或者解决 bug 等情况，还要分别到三个社区提 issue 讨论。</p><p>Flink 社区提出了 Streaming Warehouse 解决这个问题，字节调研了目前流式计算发展方向和 Streaming Warehouse 系统，基于 Flink 和 Paimon 构建了 Streaming Warehouse 系统，分别统一流批一体的计算和存储，增加了作业和数据血缘管理、数据一致性管理、流式数据订正和回溯等核心功能，解决流式计算的准确性和数据运维等问题。</p><p style="text-align:center"><img height="259" src="https://oscimg.oschina.net/oscnet/up-fca6618852b7122974aca81d2377233a202.png" width="600" referrerpolicy="no-referrer"></p><p><strong>最终，「三套引擎，三个团队」变成「一套引擎，一个团队」。</strong>用方勇的话来说，使用 Flink 作为整个数据生产链路统一的流式、批式和 OLAP 一体的计算引擎，已经完全就不用担心数据的实时性、业务分析的复杂性。</p><p>至于 Flink 的未来，方勇已经有了设想。他希望能够集合社区的研发能力，一起完善整个 Flink 的计算生态，将 Flink 打造成统一流、批和 OLAP 的 Streaming Warehouse 系统。</p><span id="OSC_h3_3"></span><h3><span style="color:#2980b9">03 新业务，新场景，新挑战</span></h3><p>2022 年，字节流式计算团队支撑研发的计算引擎商业化产品「流式计算 Flink 版」上线火山引擎，正式对外提供云上计算能力，而不是仅仅服务于字节内部业务。</p><p>在字节，这款产品被称为「Serverless Flink」。Serverless Flink 依托于字节跳动在业内最大规模实时计算集群实践，基于火山引擎容器服务（VKE/VCI），提供 Serverless 极致弹性，是开箱即用的新一代云原生全托管实时计算平台。</p><p><strong>事实上，将</strong><strong> Serverless </strong><strong>Flink</strong><strong> 称之为一款</strong><strong>新上线的产品</strong><strong>可能并不合适。</strong>李本超解释，所谓「流式计算 flink 版」，其实就是团队在六年时间里，让 Apache Flink 在字节内部实现了大规模应用，并把积累的大量的产品经验和技术能力「包装」了一下，而不是重新做了一个产品。</p><p>它是基于 Apache Flink 衍生出来的，可以理解为 Apache Flink 增强版，并且 100% 兼容 Apache Flink，包含诸多特性：</p><ul><li><p>开发效率提升。 流式计算 Flink 版支持算子级别 Debug 输出、Queryable State、Temporal Table Function DDL，在开发效率上对开源版本 Flink 有显著提升。</p></li><li><p>可靠性提升。 流式计算 Flink 版针对单个 Task 进行 Checkpoint，提高了大并发下的 Checkpoint 成功率。单点任务恢复和节点黑名单机制功能，保障了对故障节点的快速响应，避免业务整体重启。</p></li><li><p>Serverless 云原生架构。 极致弹性，1‰ 核精细调度。</p></li><li><p>易用性增强。 极简 SQL 开发，开箱即用、免运维、支持流式数据全生命周期管理。</p></li><li><p>高性能低价格。 高性价比、高 SLA 保证、超低 TCO。</p></li></ul><p style="text-align:center"><img height="312" src="https://oscimg.oschina.net/oscnet/up-94e4c01fd23fdb7e7327a975d5f2575b6dd.png" width="800" referrerpolicy="no-referrer"></p><p style="text-align:center"><span style="color:#8f959e"><em>流式计算</em></span><span style="color:#8f959e"><em>Flink</em></span><span style="color:#8f959e"><em>版，架构图</em></span></p><p><strong>在 Serverless </strong><strong>Flink</strong><strong> 上线火山引擎之后，方勇发现，外部客户需求与内部业务需求很是不同。</strong>比如有的客户还在使用 Storm、Samza 等相对较为早期的流式技术栈。因此，团队不仅要对客户进行技术培训和技术支持，还要帮助技术支持人员理解客户的作业逻辑，以更好地服务其业务。</p><p>这意味着，流式计算团队面临的是新的场景与挑战，有时甚至要从零开始构建一个新的系统。</p><p>不过，一切工作都在有条不紊地展开。近两年，团队成员已经扩展到 30 人，并对 Serverless Flink 在调度、运行时、SQL 等各个方面都进行了全方面的优化，极大提升性能。</p><p>此外，基于 Apache Flink 及 Apache Paimon，团队在 Streaming Warehouse 实时数仓场景也有了新的突破，实现了支持数据的一致性、血缘，以及数据回溯等实时数仓的产品能力。</p><p>截止目前，基于流式计算 Flink 构建的实时业务场景已经涉及到字节几乎所有的业务和产品，包括实时数仓、实时风控、商业化、电商、游戏、小说、教育、房产、财经等，日常实时峰值超 100 亿 QPS。与此同时，流批一体在特征工程，数据同步，计数服务，电商等场景均得到了广泛的使用和落地，已上线将近 2 万 Flink Batch SQL 任务。</p><p><strong>此刻，张光辉才终于敢说：「 经历了从 0 到 1 的过程之后，今天字节的流式计算平台，</strong><strong>已经可以打 8 分了。</strong><strong>」</strong></p><p>方勇提到，未来，团队将在可用性、稳定性、性能等方面持续优化流式计算平台，并继续深入 Flink OLAP 生产实践，建设和完善稳定性和可用性等周边系统，比如 Debug 能力、 Auto Scaling 系统。</p><p>除了流式计算之外，团队在 Flink 批式方面也做了很多的优化和尝试， 比如兼容 Hive SQL 语法等。同时，一些超大规模的作业，也在往 Flink 批式方向上去尝试。在 Flink 批式场景积累经验之后，团队将会持续推动 Flink 流批一体的应用和实践，同时结合社区需求，贡献一些新的能力。</p><p>Native Engine 也将成为团队探索的一大方向。Flink 以 Java 语言为主，部分技术涉及行式计算，导致它并不能很好地利用 CPU，以及更新迭代的一些新功能。而如何利用 Native Engine 提升性能及运算能力，降低成本，是大势所趋。</p><span id="OSC_h3_4"></span><h3><span style="color:#2980b9">04 开源是一件自然而然的事</span></h3><p>从服务内部业务到服务外部客户，字节对 Apache Flink 的应用愈加深入。当然，字节之于 Apache Flink，并非只停留在「用」的层面，而是源源不断地将其创新成果贡献到开源社区中，更是成为了研发 Flink OLAP 等方向的主要牵头企业。在众多为 Flink 社区贡献的国内企业中，字节参与度能排到第二。</p><p><strong>除了 </strong><strong>Apache Flink</strong><strong>，流式计算团队还为 </strong><strong>Apache</strong><strong> Calcite 、Apache Paimon 这两个项目做出了不小的贡献，并在社区构建了一定的影响力。</strong></p><p>Apache Calcite 是一个动态的数据管理框架，它可以实现 SQL 的解析、验证、优化和执行。当前，字节是该项目核心贡献公司之一，参与 plan 优化、方言生态增强、运行时优化等工作。Apache Paimon (incubating) 则是一项流式数据湖存储技术，可以为用户提供高吞吐、低延迟的数据摄入、流式订阅以及实时查询能力。字节是该项目的创始贡献公司之一。</p><p>截至目前，字节流式计算团队培养了包括李本超、方勇等在内共 8 名 Apache 项目 committer，为 Flink 社区贡献了 174 个 commits，为 Calcite 社区贡献了 47 个 commits，以及为 Paimon 社区贡献了 107 个 commits。</p><p><strong>虽说开源成果丰硕，但在流式计算团队，并没有安排专门的人去贡献开源。于他们而言，开源是一个自然而然的过程。</strong></p><p>「我们用开源的组件来搭建产品，鼓励组员在日常开发过程中，将新增的功能特性、bug 修复以及一些优化，贡献到社区。这就是我们日常的工作模式。希望大家在社区交流中，可以提升代码质量以及保持对技术的探索。」张光辉补充说：「当然，最开始，也没有什么开源的氛围，每个人都忙着业务。不过，李本超和方勇这两个开源积极分子起了带头作用，其他团队成员在其影响下，也逐渐接触开源。」</p><p>李本超也提到，社区和公司之间没有明显界限。「上游项目 Apache Flink 跟我们的 Serverless Flink 其实是一个项目，只不过我们在用 Serverless Flink 来支撑一个更具体的公司业务场景。公司非常鼓励我们把成果贡献给社区。但如果内部需求更着急，或者说很难有一个非常快速且完整通用的方案，就会在内部先上线试用。」</p><p>团队也不强制要求研发人员一定要参加社区。</p><p>「参与开源是一件比较偏个人的事情，看他自己个人兴趣，以及对职业生涯、技术方面的规划。不过为了保证内外系统的一致性，以及我们系统后续发展的兼容性，增进研发同学之间的技术交流及合作，我们非常鼓励大家把遇到的问题提交到社区。有一些需要内部讨论或支持方案，如果刚好也是外部开源社区所需要的，我们都会考虑把这些需求引进到内部。这样可以做到内部统一开发，然后统一推进。」方勇解释。</p><span id="OSC_h3_5"></span><h3><span style="color:#2980b9">05 要贡献开源，其实并不容易</span></h3><p>如李本超所言，所有有利于社区变的更好的事情，都是一种贡献，比如用户问答、代码 review、文档的维护、不稳定测试的修复、build 系统的提升、技术讨论、release 等等。</p><p><strong>但对于从未参与过开源的人来说，开始可能是最困难的一步。</strong></p><p>「在 Apache Calcite 、Apache Flink 以及 Apache Paimon 等社区，开发者非常活跃，很多人提 issue 都会得到解答。但没参与之前，去哪里去找 issue，怎么写代码，怎么提 PR，怎么新建 feature，整个流程完全是陌生的。这个过程其实听起来比较简单，但真正去实践的时候，发现它还是有一定门槛。」方勇提到。</p><p><strong>即使已经有了一些重大的开发成果，要贡献给社区，也并不是简单地把代码从内部拿到外部。</strong></p><p>一些针对专项业务定制化开发的功能，在开源社区可能会被认为不够通用。李本超说，一些新开发的功能特性，即使已经在业务上验证过，但在回馈开源社区时，往往需要重新思考和设计，使其在满足业务诉求的基础上，又能抽象出更通用的能力。</p><p>当然，对李本超和方勇而言，字节业务的优先级自然是更高的，技术架构的普适性、能力的通用性方面的优先级稍微低一点。但在面对业务和社区共同的需求时，他们还是尽可能做到同时兼顾。也许最后开发出来的解决方案并不是最完美的，但已经能解决 80% 的问题。</p><p><strong>再进一步，如果已经抽象出一些功能特性，想把代码贡献到社区，也不代表这个过程会很顺利。</strong>由于社区对核心组件的代码要求比较高，在代码被合并之前，包括 API 设计、PR 合理性等在内的各方面问题，都需要经过社区讨论。</p><p>方勇曾向 Flink 提了一个 PR :在 job manager 节点进行内存优化。 一位德国的项目成员 review 代码后，认为原理上可以。但他还问了几个问题：为什么要提交这个 PR，你们遇到了什么问题，为什么要采用这种方式修复它？ 因为 Flink JVM 的 Java 代码从实现上来看，并没有内存问题。</p><p>由于该部分涉及到 JVM 层的 classloader 和 full GC 优化，在此之前，方勇就曾与 JVM 系统组有过深入研究探讨。他们发现，JVM 不仅有 Java 代码实现， 还有 C++ 代码实现，而 C++ 实现的代码如果有一些复用情况，会出现内存泄露，导致 job manager 节点的 full GC 变多，处理性能下降。当方勇把这一分析过程以及 Benchmark 测试贴到社区后，最终获得了认可。PR 也很快就被合并了。</p><p><strong>此外，贡献开源还要从代码架构的角度来思考，是否与现有系统兼容。</strong>李本超说，要获得业务部门的认可，要求开发人员对业务有深入的理解，帮助业务部门解决问题，达到预期收益就可以。但在开源社区，想要贡献代码，不仅要考虑事件本身的合理性，还要考虑其通用性是否够强，是否会跟已有功能冲突，未来怎么维护，如何演进等等。</p><p>方勇就曾遇到过一个案例。一个容灾体系，要先靠外部的数据流生成容灾 ID，Flink 再通过该 ID 实现整个作业容灾。社区为了支持这一功能，做了特定的 API 的开发。方勇在将部分功能代码提交到仓库时，就要考虑是否兼容特定的 API 。「不能让这个 API 受到干扰，否则 Flink 用户升级版本之后，原先功能就运行不起来了。这对 Flink 的稳定性以及后续发展都是不利的。」</p><span id="OSC_h3_6"></span><h3><span style="color:#2980b9">06 加入 PMC/PPMC，责任更大了</span></h3><p>今年，李本超、方勇先后分别成为了 Apache Calcite、Apache Paimon（incubating）的 PMC member（项目管理委员会成员）、PPMC member（孵化器项目管理委员会成员）。</p><p>这意味着，二人为开源社区做出的贡献，得到了认可。这并不容易。在社区想要获得认可，不仅仅只看代码，还要看技术能力、沟通能力、持续贡献的意愿，以及行为是否符合社区文化（比如 Apache 之道）。如何在日常参与社区事务的过程中，将能力和素质展现出来，是很有挑战性的。</p><p><strong>李本超有一次印象深刻的经历。</strong>在 Apache Calcite 社区，他从创建 issue，讨论 issue，写代码，提交 PR ，到最终合入代码，前后一共用了五个月的时间。</p><p>「不是说这个 issue 复杂到需要五个月，也不是说这五个月只做这一件事。社区成员都是异步沟通，大部分人都在利用业余时间来讨论问题和贡献代码。加上社区对代码质量要求高，项目本身很复杂，在讨论过程中，经常会产生不同的想法和建议。」这个过程对李本超来说，还是挺煎熬的。</p><p>「因为实现方案可能会换好几次，不同方案要写不同的代码。最后讨论来讨论去，可能还是原来的好，又要在原来的基础上再改。反反复复。最终代码量也没多大。」他补充说：「但这个过程能够很好地展现出个人素质和能力。」</p><p>具备耐心，并且长期坚持，在开源社区是很必要的。方勇认为，成为 PPMC 需要持续关注和投入，保持在自己在社区的活跃度。「持续关注新的 issue，把自己的工作整理出来，再去社区提 issue，然后在社区发起讨论，一起评估方案。最终把任务分解后，再开发代码，再把它合进去，花费的时间周期可能会非常长。在 Flink 社区，如果要合入一些对公共 API 有修改的代码，从设计讨论，到投票，再到开发以及推进，整个过程至少需要 3 个月。」</p><p><strong>对李本超来说，成为 </strong><strong>Apache</strong><strong> Calcite 的 </strong><strong>PMC</strong><strong> member 就意味着肩负了起更大的责任。</strong>「我在社区没有任何角色的时候，只关心是不是把问题解决了。而且社区里一定会有更资深的人去帮我去确认代码有没有问题，有人在后背托着我。」</p><p>现在，他的身份变了，不再只是贡献代码，而是辅助他人贡献。「我就是把好最后那道关的那个人，压力和责任更大了。心里会想着，这个东西我一定得想清楚，一定得为了社区健康、长远的发展去思考，而不只是把代码快速合入，把问题快速地解决掉。总之，更小心，更谨慎，思考维度也更多了。」</p><p>李本超还需要承担很多非代码的工作，比如代表社区跟 Apache 软件基金会的基础设施团队讨论社区机制、流程等问题，让新加入的开发者在为项目做贡献的时候，更容易、更方便。「很多时候，付出就不只是单纯贡献的维度了。投入到社区，需要自己有很大的兴趣和足够的精力。」</p><p><strong>同样深感责任重大的，还有方勇。</strong>他见到过一些开源社区，在成员慢慢变少后，不再活跃，对项目造成了严重打击。每年都有很多新的项目开源，也有很多老的开源项目死掉。</p><p>「既然给了 PPMC 这么大的权限，作为成员之一，我对整个项目的发展方向就要有更多的考量。 PPMC 最主要的责任，就是对开源项目的把控，包括方向的抉择、重大 feature 的演进。」</p><p>不同公司及团队源源不断地加入社区，会贡献很多新的功能，有些甚至能够拿来就用，无需重新开发，在加快项目和社区发展的同时，也在帮助字节完善其内部相关系统功能。当然，字节在将项目用于实际生产业务中时，也会为项目开发很多新的功能特性，并且经过了大量场景的验证和锤炼。因此，在规划项目演进方向时，方勇除了考量外部需求以及其他 PPMC 的提议，会更加主动地去展现公司内部开发的功能，这样既可以让内外系统保持一致，也能推动项目更好地往前走。</p><span id="OSC_h3_7"></span><h3><span style="color:#2980b9">07 有了开源社区，好像有了靠山</span></h3><p>大部分人对开源的认知，都发端于使用开源项目。「但如果只是使用一个开源软件，对社区的感知几乎是没有的。」张光辉虽然并未获得 Apache Flink 的 committer 或 PMC member 的身份，但他和很多人一样，也在为项目贡献着自己的力量。</p><p>「以前觉得开源离我们很远。尤其是在中国，这种感受更加明显。但是在参与 Apache Flink 社区以及线下大会之后，跟开发者有了交流和接触，自然而然就产生了情感上的链接，促使你主动回馈，促使你去成长。」<strong>张光辉自己也是从开源社区中成长起来的。</strong>此前在将流式计算引擎迁移到 Flink 时，张光辉就曾遇到过不少问题，经常会跟阿里、美团等公司的开发者在 Apache Flink 社区中讨论。</p><p><strong>开源给李本超带来了很强的踏实感。</strong>「我和这个领域最优秀的一群人站在了一起。有什么问题一起讨论，一起解决，好像多了一个非常强大的虚拟团队，即使是在处理内部业务的时候，也感觉自己有靠山了。社区里边有很多写代码超过 30 年的资深专家，不管是在技术领域，还是非技术领域，我都能从他们身上学到很多。」</p><p>在 Flink 社区，很多用户都会在邮件组提一些涉及使用、调优等方面的问题。也许问题本身并不复杂，但因为 Flink 系统很复杂，对初学者而言，门槛还是有点高。</p><p><strong>同样经历过新手期的方勇，很能理解这种情况。</strong>「有时候，对方邮件提到的 bug 或者 issue，我们没有遇到过，但我们仍然会去研究了相关的代码，帮他解答，甚至直接提交代码。问题解决之后，我经常会收到感谢邮件，这时候就觉得付出有回报，非常有成就感。」</p><p style="text-align:left">参与社区之后，方勇与开源社区成员见面的机会也更多了，经常与其深入交流项目演进、技术发展、行业趋势等各方面的想法，同时也在促进各开源项目、社区之间的结合。现在，他正琢磨着，怎么把 Apache Flink 和 Apache Paimon 更好地结合，做下一代流式计算解决方案。</p></div></div>
                                    ]]>
            </description>
            <pubDate>Mon, 09 Oct 2023 01:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/oscpyaqxylk/blog/10116398</guid>
            <link>https://my.oschina.net/oscpyaqxylk/blog/10116398</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源框架 NanUI 作者转行卖钢材，项目暂停开发]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>NanUI 作者在国庆节发布了停更公告，称该项目将暂停开发，原因是去年被裁员失业后，<strong>他已转行销售钢材</strong>，现在很难腾出时间来开发和维护 NanUI 项目。</p><p>他说道：</p><blockquote><p>为了生存，本人只能花费更多的时间和精力去谈单，去销售，去收款，因此已经很难再腾出时间来开发和维护 NanUI 项目，对此我深感无奈，也希望后面生活和工作稳定后能腾出时间来继续维护 NanUI。</p></blockquote><p>NanUI 作者表示，他所在公司因疫情于去年（2022 年）初彻底宣布裁减所有开发岗位，因此他也只能顺应大流在 36 岁这个尴尬的年纪失业。</p><p><img height="1285" src="https://static.oschina.net/uploads/space/2023/1009/173727_oVGe_2720166.png" width="1980" referrerpolicy="no-referrer"></p><p><em>via&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FXuanchenLin%2FNanUI%2Fdiscussions%2F367" target="_blank">https://github.com/XuanchenLin/NanUI/discussions/367</a></em></p><blockquote><p>NanUI 界面组件是一个开放源代码的 .NET / .NET Core 窗体应用程序（WinForms）界面框架。它适用于希望使用 HTML5/CSS3 等前端技术来构建 Windows 窗体应用程序用户界面的 .NET 开发人员。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-060f8d602f0ac9de0bb5953b554b233f62e.png" referrerpolicy="no-referrer"></p><p>NanUI 基于谷歌可嵌入的浏览器框架 Chromium Embedded Framework (CEF)，因此用户可以使用各种前端技术 HTML5/CSS3/JavaScript 和流行前端框架 React/Vue/Angular/Blazor 设计和开发 .NET 桌面应用程序的用户界面。</p><p>同时，NanUI 独创的 JavaScript Bridge 可以方便地实现浏览器端与 .NET 之间的通信和数据交换。</p><p>使用 NanUI 界面框架将为传统的 WinForm 应用程序的用户界面设计和开发工作带来无限种可能！</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Sun, 08 Oct 2023 09:50:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261033</guid>
            <link>https://www.oschina.net/news/261033</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[学习 DevOps 落地实践，全面提升技术水平]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在我们高速发展的技术时代，DevOps 已经成为企业持续交付和优化业务的关键。但是目前市场寒冬、经济下行，各大企业纷纷裁员。作为天选打工人的我们，收入锐减、就业困难，普通人如何应对？大龄 IT 从业者，职业迷茫、焦虑恐慌，如何加强学习快速成长？</p><p style="text-align:center"><img height="249" src="https://static.oschina.net/uploads/space/2023/1009/165530_PSgw_2720166.png" width="400" referrerpolicy="no-referrer"></p><p>为了帮助社区成员解决以上问题，我们特别推出了一套针对技术社区的《DevOps 落地实践训练营》。这个训练营将带您全面了解 DevOps 的核心理念和实践方法，从产品设计、需求管理、代码管理、持续集成、部署发布、数据度量、业务运营等各个领域，带您端到端的学习 DevOps 相关知识。</p><p style="text-align:center"><img src="https://static.oschina.net/uploads/space/2023/1009/165548_jSD5_2720166.png" referrerpolicy="no-referrer"></p><p>参加我们的 DevOps 落地实践训练营，您将会有以下收获：</p><ol><li>提升技术能力：通过系统的培训，您将对 DevOps 有更深入的理解，提升您的技术实力。</li><li>提高沟通能力：训练营将帮助您更好地与开发、测试和运维团队沟通，提高团队协作效率。</li><li>拓宽知识视野：训练营将邀请业内专家分享经验，帮助您掌握最新的 DevOps 工具和技术。</li><li>发掘最佳实践：通过案例分析和实践操作，您将了解到更多的 DevOps 最佳实践，为您的工作带来更多灵感。</li></ol><p style="text-align:center"><img src="https://static.oschina.net/uploads/space/2023/1009/165712_fZMA_2720166.png" referrerpolicy="no-referrer"></p><p>我们的训练营有以下亮点：</p><p>1、现在大多数的培训都是线上，效果如何，相信大家都有自己的判断。小编在以前学习线上课程时，超不过半小时就走神犯困。<strong>《DevOps 落地实践训练营》采用线下培训形式，确保学员能够亲身体验和实践所学内容</strong>。</p><p style="text-align:center"><img src="https://static.oschina.net/uploads/space/2023/1009/165732_9QiG_2720166.png" referrerpolicy="no-referrer"></p><p>2、术业有专攻，单一讲师的培训并不能做到全栈贯通赋能。本训练营邀请多位专业讲师联袂推出端到端的实践技能培训，内容包含 25 个章节，<strong>全面讲解软件开发过程中产品设计、项目管理、开发、测试、架构等多个领域的知识实践</strong>。</p><p style="text-align:center"><img src="https://static.oschina.net/uploads/space/2023/1009/165752_0tuV_2720166.png" referrerpolicy="no-referrer"></p><p>3、职业技能培训，最终的目的是学以致用。有别于其他认证类培训，本次训练营更为重要的是，<strong>每个环节都有动手练习环节，确保学员真正掌握所学技能</strong>。而且能够将理论知识与实际工作紧密结合，为学员提供贯通全栈赋能的培训。</p><p style="text-align:center"><img src="https://static.oschina.net/uploads/space/2023/1009/165814_dmE9_2720166.png" referrerpolicy="no-referrer"></p><p>4、「天下苦高价培训久矣。」本次训练营，以 480 元的价格提供了一种高性价比的培训方式，让学员不再因价格而犹豫。如果你想在学习技能的同时，还想获得一个职业技能证书，980 的价格，你就能获得由」中国管理科学院「颁发的《专业人才培训证书》。此处注意，培训和证书并不强制绑定，本土培训，仅此一家。</p><p style="text-align:center"><img src="https://static.oschina.net/uploads/space/2023/1009/165830_KvDB_2720166.png" referrerpolicy="no-referrer"></p><p>DevOps 落地实践训练营是一个全面提升技术水平的绝佳机会，不仅能帮助您更深入地理解 DevOps，还能提升您的团队协作能力，拓宽知识视野，并让您有机会接触到更多的最佳实践。我们邀请您积极参与，与业内专家和同行一起共创。我们期待在未来的交流和合作中与您共同成长，创造更多的可能性！</p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 08 Oct 2023 09:01:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261024</guid>
            <link>https://www.oschina.net/news/261024</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Reddit Programming 板块的未来]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">Reddit 管理员 ketralnis 发布并置顶了一篇名为「<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.reddit.com%2Fr%2Fprogramming%2Fcomments%2F173viwj%2Fmeta_the_future_of_rprogramming%2F" target="_blank">The future of r/programming</a>」的帖子，就 r/programming 板块的规则制订征集用户意见。</span></p><p><span style="color:#000000">根据介绍，目前&nbsp;r/programming 板块中允许发布的内容类型包括：实际的编程内容、编程新闻、程序员职业内容、程序员感兴趣但与编程无关的文章/新闻、以及一些带代码的演示。禁止的内容类型包括：政治和八卦讨论、没有代码的演示、一些低质量无营养的提问和内容、发布调查或招聘启事以及 Meta posts 等</span></p><p><span style="color:#000000">此外，像一些宽泛的行业相关新闻、低质量的复制粘贴类内容、同一话题的重复讨论帖、标题过于社论化或阴谋论的内容，一般情况下都会进行删除。但如这该贴已经具有了一定的热度，管理员则会视情况进行保留。</span></p><p><img height="395" src="https://oscimg.oschina.net/oscnet/up-16b207c45a0b6d76821ddb5ef82e35c5545.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">ketralnis 表示，目前 r/programming 板块的相关规则非常模糊，且实际的版主管理方式与这些规则的联系也很松散。</span></p><p><span style="color:#000000">譬如，就算是一些被禁止发布的帖子类型，如果评论线程中已经出现了健康的讨论内容，那么这些帖子大多数也都会被保留下来。且相关的评论审核也非常宽松，导致有很多机器人刷评以及一些引战的激烈言论出现。「然而，GPT 评论机器人数量正在明显上升，以我们现有的活跃版主人数而言，很快就会难以承受。」</span></p><p><span style="color:#000000">因此，ketralnis 在公告中也透露正在招募更多的 mods。「如果我们想让它变得更好，就需要更多的人力。」</span></p><p><span style="color:#000000">「我们知道世界上存在对不受监管的空间的需求，但&nbsp;r/programming&nbsp;并不是这样的空间......就我个人而言，我的梦想是让&nbsp;r/programming&nbsp;成为拥有最高质量编程内容的地方，在这里我每天都能读到有趣的内容、学到新的知识。」</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 08 Oct 2023 08:19:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261161/reddit-future-of-rprogramming</guid>
            <link>https://www.oschina.net/news/261161/reddit-future-of-rprogramming</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[联想计划推出 Android PC]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">联想于近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.techradar.com%2Fcomputing%2Fdesktop-pcs%2Flenovo-shifts-direction-with-new-android-based-pcs-and-they-look-powerful" target="_blank">宣布</a>计划生产&nbsp;Android PC。该公司将于&nbsp;<span style="background-color:#ffffff">Esper&nbsp;</span>合作，<span style="background-color:#ffffff">重新设计其台式一体机 ThinkCentre M70a，「</span><span style="background-color:#ffffff">这台一体机将代表着联想进军 Android 领域的第一步」。Esper 是一家专门提供 Android 定制服务以及设备管理产品的公司。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">ThinkCentre M70a 采用 21.5 英寸 FHD 无边框显示屏，</span>现有版本采用英特尔处理器，可以从入门级 i3 一直配置到功能强大的 i9 芯片。M70a 目前采用的是 Windows 11 操作系统，但据透露新版本将采用 Android 系统。</span></p><p><img height="401" src="https://oscimg.oschina.net/oscnet/up-5eda0c1eee499fc80e95d2f72bb2c76eeee.png" width="500" referrerpolicy="no-referrer"></p><p><img height="399" src="https://oscimg.oschina.net/oscnet/up-98d83cf5f92a9b40a586a0f0345868a918a.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">联想方面表示，ThinkCentre M70a 主要面向企业客户，希望它能吸引零售和酒店业的企业使用。除此之外，联想还计划与 Esper 合作推出基于 Android 系统的 ThinkCentre M70q，以及基于 Windows 系统的 ThinkEdge SE30 和 ThinkCentre M90n-1 IoT。</span></p><p><span style="color:#000000">目前在台式电脑领域最接近 Android 系统的是惠普的 Chromebase AIO 等产品，由于采用了 ChromeOS，它可以通过谷歌 Play 商店运行 Android 应用程序。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sun, 08 Oct 2023 07:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260995/lenovo-esper-android-pc</guid>
            <link>https://www.oschina.net/news/260995/lenovo-esper-android-pc</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 易语言开发的服务器软件 MODHTTP SERVER]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-modhttp-server" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#modhttp-server"></a>MODHTTP SERVER</h1><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/fw.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><p>项目地址：<a href="https://gitee.com/wxgshuju/modhttp-server">https://gitee.com/wxgshuju/modhttp-server</a></p><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/oschina.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><p>开源中国：<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.oschina.net%2Fp%2Fmodhttp-server">https://www.oschina.net/p/modhttp-server</a></p><h4><a id="user-content-介绍" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#%E4%BB%8B%E7%BB%8D"></a>介绍</h4><p>MODHTTP SERVER 是采用国产化编程易语言开发的网站服务器软件。</p><p>该程序集成 Nginx+ASP+PHP+MySQL+Openssl+HOSTS+MYSQL-FORM+Sqlite 数据库管理器+Access 数据库管理器;</p><p>支持 HTTP1.1 协议、HTTP2 协议、HTTP3 协议，该工具实现 NGINX 配置可视化编辑、PHP 可视化配置可视化编辑，</p><p>该程序不仅包括 ASP、PHP、Modhttp 调试环境，还包括了 MODHTTP 网页视图模块开发工具、开发手册等</p><h4><a id="user-content-声明" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#%E5%A3%B0%E6%98%8E"></a>声明</h4><p>本软件基于开源协议 Apache 发布，允许转发，允许第三方修改，本软件永久免费，终身免费</p><p>软件开发者：魔帝本尊</p><p>支持平台：Windows</p><p>优点：绿色服务解压即可安装，一键可视化配置即可使用</p><p>项目状态：持续更新维护中...</p><p>本软件已经开源，如需要程序源码请在 OPEN-SOURCE 中查看.</p><h4><a id="user-content-软件架构" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84"></a>软件架构</h4><p>本软件需要安装以下运行库库
Microsoft Visual C++ 2015-2022 Redistributable 14.38.32919.0 (2023-08-09)</p><p>x64 <a href="https://gitee.com/link?target=https%3A%2F%2Fdownload.visualstudio.microsoft.com%2Fdownload%2Fpr%2F02a6d5c5-3e10-47de-8025-d97a1321d3e3%2F5F60592799FAE0C82578112D4B621438FFC976AB39D848D8F7623F5705A83E27%2FVC_redist.x64.exe">https://download.visualstudio.microsoft.com/download/pr/02a6d5c5-3e10-47de-8025-d97a1321d3e3/5F60592799FAE0C82578112D4B621438FFC976AB39D848D8F7623F5705A83E27/VC_redist.x64.exe</a></p><p>x86 <a href="https://gitee.com/link?target=https%3A%2F%2Fdownload.visualstudio.microsoft.com%2Fdownload%2Fpr%2F02a6d5c5-3e10-47de-8025-d97a1321d3e3%2FAD573D3198853FC71137A88E51ABDE844B84F29B0CE6DD91BBEC661BC0143B36%2FVC_redist.x86.exe">https://download.visualstudio.microsoft.com/download/pr/02a6d5c5-3e10-47de-8025-d97a1321d3e3/AD573D3198853FC71137A88E51ABDE844B84F29B0CE6DD91BBEC661BC0143B36/VC_redist.x86.exe</a></p><h4><a id="user-content-下载路径" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#%E4%B8%8B%E8%BD%BD%E8%B7%AF%E5%BE%84"></a>下载路径</h4><p>最新版本：32.02</p><p>Gitee（含源码）：<a href="https://gitee.com/wxgshuju/modhttp-server/raw/master/MODHTTP_SERVER32.02.7z">https://gitee.com/wxgshuju/modhttp-server/raw/master/MODHTTP_SERVER32.02.7z</a></p><p>代码库中所有文件经过病毒扫描查杀</p><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/t0.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><p>.e 文件，是易语言源代码</p><p>.api 文件，是 MODHTTP 专属的易语言网页视图模块文件，作用等同 PHP、ASP</p><p>内存加速 1.7.ec  来源为汇编大神，白银大佬 (2962946246) 无偿提供</p><p>e2ee.fne e2ee_staticlib e2ee_static.res 来源为 E2EE 网站迅捷开发群 (536544662)</p><p>2.3.2 免费版支持库</p><p>源码中用到了编码转换类，动态内存库类等...</p><p>本地调试器.exe   是 MODHTTP 专属视图模块的调试工具，可以清晰看到浏览器请求到 MODHTTP 8081 端口的各种信息</p><p>如果仅用 PHP、ASP、MySQL 等集成环境，不使用 MODHTTP 网页视图模块文件则无需打开本地调试器.exe</p><h4><a id="user-content-安装教程" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B"></a>安装教程</h4><ol><li><p>解压后打开&lt;&lt;MODHTTP.exe&gt;&gt;</p></li><li><p>解压路径中不允许有空格，建议解压到根目录，例如 C:\ D：\等</p></li></ol><h4><a id="user-content-使用说明" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"></a>使用说明</h4><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/t1.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><ol><li>打开&lt;&lt;MODHTTP.exe&gt;&gt;</li></ol><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/t2.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><ol start="2"><li><p>首次使用打开后点击左上角菜单&gt;网站管理;打开网站管理器增填、修改网站目录，修改完成后点击【保存配置】按钮，关闭此窗口</p></li><li><p>点击启动,在首页右侧找到扩展项&gt;Nginx&gt;配置调试，点击【配置调试】按钮，页面显示以下内容则配置成功，可以启动服务。</p></li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">nginx: the configuration </span><span id="LC2" class="line">conf/nginx.conf syntax is ok</span><span id="LC3" class="line">nginx: configuration file D:\modhttp32.02A202310072215\modhttp-server\nginx/conf</span><span id="LC4" class="line">/nginx.conf test is successful</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/t3.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><p>4.按需要勾选组件，如 Nginx，Mysql，ASP，PHP 等，在选项前面打勾启动服务，如需关闭请再次点击取消√则停止服务</p><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/t4.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><p>5.启动服务后点击右上角【访问 nginx 页面】按钮，开始尽情的写 BUG 吧</p><p><img src="https://gitee.com/wxgshuju/modhttp-server/raw/master/t5.png" alt="输入图片说明" referrerpolicy="no-referrer"></p><h4><a id="user-content-帮助" class="anchor" href="https://gitee.com/wxgshuju/modhttp-server#%E5%B8%AE%E5%8A%A9"></a>帮助</h4><p>鼠标光标移动到功能，文字标题会显示帮助提示和信息</p>]]>
            </description>
            <pubDate>Sun, 08 Oct 2023 06:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/wxgshuju/modhttp-server</guid>
            <link>https://gitee.com/wxgshuju/modhttp-server</link>
        </item>
        <item>
            <title>
                <![CDATA[内核维护者回应缩短 LTS 内核支持期限，原因就是没人用]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Linux 基金会上个月宣布将 LTS 内核的支持时间<a href="https://www.oschina.net/news/258970/linux-gives-up-on-6-year-lts" target="_blank"><strong>从六年缩短到两年</strong></a>。当时给出的原因是缺乏使用和缺乏支持。</p><blockquote><p>维持这么久确实没有意义，因为人们已经不再使用它们了。</p><p>还有一个很大的问题是，Linux 代码维护人员的倦怠。他们在完成工作时面临着许多障碍。一方面，维护人员需要在日常工作之余维护代码，但维护工作通常没有报酬。最重要的是，由于人手不足等问题，维护人员的工作量也越来越大。</p></blockquote><p>稳定版内核维护者 Greg Kroah-Hartman <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.ycombinator.com%2Fitem%3Fid%3D37749846" target="_blank">近日的回应</a>再度印证上述原因——<strong>没人用 LTS 内核</strong>。</p><p><img src="https://static.oschina.net/uploads/space/2023/1010/161155_p7kt_2720166.png" referrerpolicy="no-referrer"></p><p>对于 Linux 基金会减少对内核开发的批评，Greg 表示基金会对内核社区的投入和支持一直在增加。仅仅因为 Linux 基金会成员公司的需要而不断引入新项目新用户，并不意味着它减少对内核社区的投入。这不是零和游戏，不是 Linux 基金会的运作方式。</p><p><img src="https://static.oschina.net/uploads/space/2023/1010/162752_spIB_2720166.png" referrerpolicy="no-referrer"></p><p>他强调 Linux 基金会对内核的支持一直在增加，根本不存在放弃的情况。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 08:29:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261164/greg-says-lf-strongly-supports-kernel-developers</guid>
            <link>https://www.oschina.net/news/261164/greg-says-lf-strongly-supports-kernel-developers</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[非凸科技受邀出席源创会，探讨数据技术的未来发展]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="text-align:center"><img height="466" src="https://oscimg.oschina.net/oscnet/up-c551fe1ce356a89e7e8b392055164817d4f.jpg" width="700" referrerpolicy="no-referrer"></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span>9</span></span><span><span>月</span></span><span><span>23</span></span><span><span>日</span></span><span><span>，</span></span><span><span>由开源中国联合腾讯云</span></span><span><span>TVP</span></span><span><span>开展的「数据与前沿技术」源创会活动在成都顺利举行</span></span><span><span>，</span></span><span><span>非凸科技受邀出席</span></span><span><span>，</span></span><span><span>与业界专家们共同探讨了数据存储</span></span><span><span>、</span></span><span><span>数据分析</span></span><span><span>、</span></span><span><span>数据挖掘等前沿技术</span></span><span><span>。</span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span>会上</span></span><span><span>，</span></span><span><span>非凸科技成都分公司研发总监赵海峰以「量化交易的数据驱动」为主题进行了分享</span></span><span><span>。</span></span><span><span>在量化交易领域如何高效地获取行情数据</span></span><span><span>，</span></span><span><span>如何将行情数据转发到需要的服务器</span></span><span><span>，</span></span><span><span>如何处理大量历史行情数据的存放和读取</span></span><span><span>，</span></span><span><span>又是如何通过行情数据进行模型的训练</span></span><span><span>，</span></span><span><span>赵海峰老师一一做出了精彩的解答</span></span><span><span>。</span></span><span><span>活动后</span></span><span><span>，</span></span><span><span>引发线上热烈交流讨论</span></span><span><span>。</span></span></span></span></p><p style="text-align:center"><img height="358" src="https://oscimg.oschina.net/oscnet/up-dd23c7e0f8444597f06f020859cc3e800bb.png" width="700" referrerpolicy="no-referrer"></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span>量化交易主要通过行情数据进行交易决策</span></span><span><span>，</span></span><span><span>那么如何通过券商获取行情</span></span><span><span>，</span></span><span><span>进行行情低延迟接收的软硬件方案呢</span></span><span><span>？</span></span><span><span>交易所发布</span></span><span><span>的</span></span><span><span>行情</span></span><span><span>会</span></span><span><span>经过券商的处理再转发给交易机构</span></span><span><span>，</span></span><span><span>其转发途径主要有</span></span><span><span>TCP、UDP、FPGA</span></span><span><span>加速的 UDP 和</span></span><span><span>ASIC</span></span><span><span>加速的 UDP</span></span><span><span>行情</span></span><span><span>等</span></span><span><span>。</span></span><span><span>然而</span></span><span><span>，</span></span><span><span>券商通过</span></span><span><span>TCP</span></span><span><span>连接将处理后的行情数据转发给交易机构</span></span><span><span>，</span></span><span><span>会存在延迟大</span></span><span><span>、</span></span><span><span>应用层</span></span><span><span>丢包</span></span><span><span>（非 TCP 协议丢包）、发送端负载大</span></span><span><span>等问题</span></span><span><span>。</span></span><span><span>为了解决这些问题</span></span><span><span>，</span></span><span><span>券商又通过</span></span><span><span>UDP</span></span><span><span>组播或广播的方式</span></span><span><span>，</span></span><span><span>将处理后的行情或交易所原始行情转发给交易机构</span></span><span><span>。</span></span><span><span>为了达到极致的低延迟，</span></span><span><span>券商端将会通过多种方式来解决</span></span><span><span>，</span></span><span><span>其中一个</span></span><span><span>特别有效</span></span><span><span>的方式是使用</span></span><span><span>L1</span></span><span><span>交换机</span></span><span><span>，在一层转发光或电信号给客户，其转发延迟可以低至 4ns。</span></span><span><span>需要注意的是</span></span><span><span>，</span></span><span><span>虽然 UDP 不是一个可靠传输协议，但</span></span><span><span>在同一个交换机连接的服务器</span></span><span><span>之间使用 UDP 进行通信</span></span><span><span>，</span></span><span><span>正常情况下</span></span><span><span>在网络上几乎不</span></span><span><span>会</span></span><span><span>丢包</span></span><span><span>。然而，</span></span><span><span>在客户端程序和服务器的网卡上可能</span></span><span><span>会</span></span><span><span>丢包</span></span><span><span>。</span></span><span><span>因此</span></span><span><span>，</span></span><span><span>客户在接收行情时</span></span><span><span>，</span></span><span><span>可以使用</span></span><span><span>无锁的</span></span><span><span>ring buffer</span></span><span><span>转发数据到处理线程</span></span><span><span>，</span></span><span><span>以并</span></span><span><span>行处理</span></span><span><span>不同股票的行情，</span></span><span><span>然后</span></span><span><span>将处理结果</span></span><span><span>写入共享内存</span></span><span><span>，</span></span><span><span>以供交易系统读取</span></span><span><span>。</span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span>收到行情后</span></span><span><span>，</span></span><span><span>如何将行情数据转发给内部的其他消费者呢</span></span><span><span>？</span></span><span><span>如果对延迟要求没有太高</span></span><span><span>，</span></span><span><span>可以使用</span></span><span><span>TCP</span></span><span><span>转发行情</span></span><span><span>，</span></span><span><span>能够自己控制丢包率</span></span><span><span>，为了降低延迟和增加吞吐，也可以使用 UDP 转发行情。</span></span><span><span>由于逐笔行情不允许丢包</span></span><span><span>，</span></span><span><span>所以在使用</span></span><span><span>UDP</span></span><span><span>转发行情时</span></span><span><span>，</span></span><span><span>可以搭配</span></span><span><span>TCP</span></span><span><span>行情重传服务</span></span><span><span>，</span></span><span><span>通过</span></span><span><span>多路行情汇聚</span></span><span><span>、R</span></span><span><span>oc</span></span><span><span>ksdb</span></span><span><span>持久化</span></span><span><span>等方式对 UDP 转发行情进行补充</span></span><span><span>。</span></span><span><span>如果</span></span><span><span>转发行情前</span></span><span><span>进行数据压缩</span></span><span><span>，</span></span><span><span>那么延迟</span></span><span><span>和吞吐量可能会更优秀</span></span><span><span>。</span></span><span><span>行情压缩主要有两种方式</span></span><span><span>：</span></span><span><span>行情消息的压缩</span></span><span><span>、</span></span><span><span>消息内部</span></span><span><span>字段</span></span><span><span>的压缩</span></span><span><span>（</span></span><span><span>股票代码</span></span><span><span>、</span></span><span><span>价格</span></span><span><span>）。</span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span>行情转发之后</span></span><span><span>，</span></span><span><span>如何使用行情数据分析交易执行情况</span></span><span><span>，</span></span><span><span>又该如何训练模型呢</span></span><span><span>？</span></span><span><span>收取到行情后</span></span><span><span>，</span></span><span><span>其中一种应用场景是训练量化交易模型</span></span><span><span>，</span></span><span><span>将收取到的行情数据进行特征处理</span></span><span><span>，</span></span><span><span>提取因子</span></span><span><span>，并利用 AI</span></span><span><span>进行模型训练</span></span><span><span>，</span></span><span><span>然后将训练好的模型解析出来以备高效地计算</span></span><span><span>实时</span></span><span><span>信号</span></span><span><span>，</span></span><span><span>在接收到实时信号值之后</span></span><span><span>，</span></span><span><span>再</span></span><span><span>极速</span></span><span><span>推送到交易系统</span></span><span><span>，</span></span><span><span>就可以根据不同的策略配置触发交易</span></span><span><span>；</span></span><span><span>另一种场景应用是把收取到的行情数据与</span></span><span><span>C</span></span><span><span>l</span></span><span><span>ickHouse</span></span><span><span>集成</span></span><span><span>，</span></span><span><span>这</span></span><span><span>不仅能提供高效的聚合和分析查询</span></span><span><span>功能</span></span><span><span>，</span></span><span><span>还能使用流式聚合表自动计算</span></span><span><span>交易数据，如实时</span></span><span><span>交易盈亏</span></span><span><span>，风险指标等</span></span><span><span>。</span></span></span></span></p><p style="text-align:center"><img height="466" src="https://oscimg.oschina.net/oscnet/up-14ae8244232acc42aea0b9bf638cf1632ff.jpg" width="700" referrerpolicy="no-referrer"></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span>非凸科技正在加大对金融科技研究的投入</span></span><span><span>，</span></span><span><span>持续以行业技术交流与合作的方式</span></span><span><span>，</span></span><span><span>整合行业生态优势资源</span></span><span><span>，</span></span><span><span>加快创新技术在实际业务场景中的落地</span></span><span><span>。</span></span></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 09:31:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/261029</guid>
            <link>https://www.oschina.net/news/261029</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 聊聊前端框架的未来 Signals]]>
            </title>
            <description>
                <![CDATA[<div class="content"><blockquote><p>Signals 在目前前端框架的选型中遥遥领先！</p></blockquote><p>国庆节前最后一周在 Code Review 新同学的 React 代码，发现他想通过 memo 和 useCallback 只渲染被修改的子组件部分。事实上该功能在 React 中是难以做到的。因为 React 状态变化后，会重新执行 render 函数。也就是在组件中调用 setState 之后，整个函数将会重新执行一次。</p><p>React 本身做不到。但是基于 Signals 的框架却不会这样，它通过自动状态绑定和依赖跟踪使得当前状态变化后仅仅只会重新执行用到该状态代码块。</p><p>个人当时没有过多的解释这个问题，只是匆匆解释了一下 React 的渲染机制。在这里做一个 Signals 的梳理。</p><h2>优势</h2><p>对比 React，基于 Signals 的框架状态响应粒度非常细。这里以 Solid 为例：</p><pre><code class="language-js">import { createSignal, onCleanup } from "solid-js";

const CountingComponent = () =&gt; {
  // 创建一个 signal
  const [count, setCount] = createSignal(0);

  // 创建一个 signal
  const [count2] = createSignal(666);

  // 每一秒递增 1
  const interval = setInterval(() =&gt; {
    setCount((c) =&gt; c + 1);
  }, 1000);

  // 组件销毁时清除定时器
  onCleanup(() =&gt; clearInterval(interval));

  return (
    &lt;div&gt;
      &lt;div&gt;
        count: {count()}
        {console.log("count is", count())}
      &lt;/div&gt;
      &lt;div&gt;
        count2: {count2()}
        {console.log("count2 is", count2())}
      &lt;/div&gt;
    &lt;/div&gt;
  );
};
</code></pre><p>上面这段代码在 count 单独变化时，只会打印 count，压根不会打印 count2 数据。</p><p>控制枱打印如下所示：</p><ul><li>count is 0</li><li>count2 is 666</li><li>count is 1</li><li>count is 2</li><li>...</li></ul><p>从打印结果来看，Solid 只会在最开始执行一次渲染函数，后续仅仅只会渲染更改过的 DOM 节点。这在 React 中是不可能做到的，React 是基于视图驱动的，状态改变会重新执行整个渲染函数，并且 React 完全无法识别状态是如何被使用的，开发者甚至可以通过下面的代码来实现 React 的重新渲染。</p><pre><code class="language-js">const [, forceRender] = useReducer((s) =&gt; s + 1, 0);
</code></pre><p>除了更新粒度细之外，使用 Signals 的框架心智模型也更加简单。其中最大的特点是：开发者完全不必在意状态在哪定义，也不在意对应状态在哪渲染。如下所示：</p><pre><code class="language-js">import { createSignal } from "solid-js";

// 把状态从过组件中提取出来
const [count, setCount] = createSignal(0);
const [count2] = createSignal(666);

setInterval(() =&gt; {
  setCount((c) =&gt; c + 1);
}, 1000);

// 子组件依然可以使用 count 函数
const SubCountingComponent = () =&gt; {
  return &lt;div&gt;{count()}&lt;/div&gt;;
};

const CountingComponent = () =&gt; {
  return (
    &lt;div&gt;
      &lt;div&gt;
        count: {count()}
        {console.log("count is", count())}
      &lt;/div&gt;
      &lt;div&gt;
        count2: {count2()}
        {console.log("count2 is", count2())}
      &lt;/div&gt;
      &lt;SubCountingComponent /&gt;
    &lt;/div&gt;
  );
};
</code></pre><p>上述代码依然可以正常运行。因为它是基于状态驱动的。开发者在组件内使用 Signal 是本地状态，在组件外定义 Signal 就是全局状态。</p><p>Signals 本身不是那么有价值，但结合派生状态以及副作用就不一样了。代码如下所示：</p><pre><code class="language-js">import {
  createSignal,
  onCleanup,
  createMemo,
  createEffect,
  onMount,
} from "solid-js";

const [count, setCount] = createSignal(0);

setInterval(() =&gt; {
  setCount((c) =&gt; c + 1);
}, 1000);

// 计算缓存
const doubleCount = createMemo(() =&gt; count() * 2);

// 基于当前缓存
const quadrupleCount = createMemo(() =&gt; doubleCount() * 2);

// 副作用
createEffect(() =&gt; {
  // 在 count 变化时重新执行 fetch
  fetch(`/api/${count()}`);
});

const CountingComponent = () =&gt; {
  // 挂载组件时执行
  onMount(() =&gt; {
    console.log("start");
  });

  // 销毁组件时执行
  onCleanup(() =&gt; {
    console.log("end");
  });

  return (
    &lt;div&gt;
      &lt;div&gt;Count value is {count()}&lt;/div&gt;
      &lt;div&gt;doubleCount value is {doubleCount()}&lt;/div&gt;
      &lt;div&gt;quadrupleCount value is {quadrupleCount()}&lt;/div&gt;
    &lt;/div&gt;
  );
};
</code></pre><p>从上述代码可以看到，派生状态和副作用都不需要像 React 一样填写依赖项，同时也将副作用与生命周期分开 (代码更好阅读)。</p><h2>实现机制</h2><p>细粒度，高性能，同时还没有什么限制。不愧被誉为前端框架的未来。那么它究竟是如何实现的呢？</p><p>本质上，Signals 是一个在访问时跟踪依赖、在变更时触发副作用的值容器。</p><p>这种基于响应性基础类型的范式在前端领域并不是一个特别新的概念：它可以追溯到十多年前的 Knockout observables 和 Meteor Tracker 等实现。Vue 的选项式 API 也是同样的原则，只不过将基础类型这部分隐藏在了对象属性背后。依靠这种范式，Vue2 基本不需要优化就有非常不错的性能。</p><h3>依赖收集</h3><p>React useState 返回当前状态和设置值函数，而 Solid 的 createSignal 返回两个函数。即：</p><pre><code class="language-TypeScript">type useState = (initial: any) =&gt; [state, setter];

type createSignal = (initial: any) =&gt; [getter, setter];
</code></pre><p>为什么 createSignal 要传递 getter 方法而不是直接传递对应的 state 值呢？这是因为框架为了具备响应能力，Signal 必须要收集谁对它的值感兴趣。仅仅传递状态是无法提供 Signal 任何信息的。而 getter 方法不但返回对应的数值，同时执行时创建一个订阅，以便收集所有依赖信息。</p><h3>模版编译</h3><p>要保证 Signals 框架的高性能，就不得不结合模版编译实现该功能，框架开发者通过模版编译实现动静分离，配合依赖收集，就可以做到状态变量变化时点对点的 DOM 更新。所以目前主流的 Signals 框架没有使用虚拟 DOM。而基于虚拟 DOM 的 Vue 目前依靠编译器来实现类似的优化。</p><p>下面我们先看看 Solid 的模版编译：</p><pre><code class="language-js">const CountingComponent = () =&gt; {
  const [count, setCount] = createSignal(0);
  const interval = setInterval(() =&gt; {
    setCount((c) =&gt; c + 1);
  }, 1000);

  onCleanup(() =&gt; clearInterval(interval));
  return &lt;div&gt;Count value is {count()}&lt;/div&gt;;
};
</code></pre><p>对应编译后的的组件代码。</p><pre><code class="language-js">const _tmpl$ = /*#__PURE__*/ _$template(`&lt;div&gt;Count value is `);

const CountingComponent = () =&gt; {
  const [count, setCount] = createSignal(0);
  const interval = setInterval(() =&gt; {
    setCount((c) =&gt; c + 1);
  }, 1000);

  onCleanup(() =&gt; clearInterval(interval));
  return (() =&gt; {
    const _el$ = _tmpl$(),
      _el$2 = _el$.firstChild;
    _$insert(_el$, count, null);
    return _el$;
  })();
};
</code></pre><ul><li>执行 _tmpl$ 函数，获取对应组件的静态模版</li><li>提取组件中的 count 函数，通过 _$insert 将状态函数和对应模版位置进行绑定</li><li>调用 setCount 函数更新时，比对一下对应的 count，然后修改对应的 _el$ 对应数据</li></ul><h2>其他</h2><p>大家可以看一看使用 Signals 的主流框架：</p><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcn.vuejs.org%2Fapi%2Freactivity-core.html%23ref" target="_blank">Vue Ref</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fangular.io%2Fguide%2Fsignals" target="_blank">Angular Signals</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpreactjs.com%2Fguide%2Fv10%2Fsignals%2F" target="_blank">Preact Signals</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.solidjs.com%2Fdocs%2Flatest%2Fapi%23createsignal" target="_blank">Solid Signals</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fqwik.builder.io%2Fdocs%2Fcomponents%2Fstate%2F%23usesignal" target="_blank">Qwik Signals</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsvelte.dev%2Fblog%2Frunes" target="_blank">Svelte 5(即将推出)</a></li></ul><p>不过目前来看 React 团队可能不会使用 Signals。</p><ul><li>Signals 性能很好，但不是编写 UI 代码的好方式</li><li>计划通过编译器来提升性能</li><li>可能会添加类似 Signals 的原语</li></ul><p>PREACT 作者编写了 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2F%40preact%2Fsignals-react" target="_blank">@preact/signals-react </a> 为 React 提供了 Signals。不过个人不建议在生产环境使用。</p><p>篇幅有限，后续个人会解读 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2F%40preact%2Fsignals-core" target="_blank">@preact/signals-core</a> 的源码。</p><h2>参考资料</h2><ul><li><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjuejin.cn%2Fpost%2F7137100589208436743%3FsearchId%3D2023100323265799EF4CF92C95049F6276" target="_blank">精读《SolidJS》</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.solidjs.com%2F" target="_blank">Solid.js</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsvelte.dev%2Fblog%2Frunes" target="_blank">Introducing runes</a></p></li></ul><h2>鼓励一下</h2><p>如果你觉得这篇文章不错，希望可以给与我一些鼓励，在我的 github 博客下帮忙 star 一下。</p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwsafight%2FpersonBlog" target="_blank">博客地址</a></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 06:52:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/wsafight/blog/10115779</guid>
            <link>https://my.oschina.net/wsafight/blog/10115779</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[两行代码解决大语言模型对话局限！港中文贾佳亚团队联合 MIT 发布超长文本扩展技术]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">中途迷失、模型偷懒、上下文越长大模型越笨......如果体验过大语言模型产品,用户多少会对文本输入长度带来的限制有所感触，比如当想和大模型讨论一些稍长的内容，需要拆分输入，而前面输入的要点，很快就会被大模型忘记。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">这是典型的大语言模型对话缺陷！就像先天有注意力缺陷的儿童，难以专注看完一本新书。而缺陷的关键，在于模型缺乏长文本处理能力。这个局面如今被打破。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">近日，贾佳亚团队联合 MIT 发布的新技术和新模型悄然登上各大开源网站的热榜：hugging face 热榜第一、paperwithcode 热度第一，Github 全部 python 项目热度第五、github stars 一周内破千，Twitter 上的相关技术帖子浏览量近 18 万......</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p7.itc.cn/q_70/images01/20231009/02802a3fa205413abde6f1ea42885d02.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">github stars 已达 1.3k</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p5.itc.cn/q_70/images01/20231009/598154ce3152480c8164e0cfab8efabb.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">Twitter 上的相关技术帖子浏览量近 18 万</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">这项名为 LongLoRA 的技术实用但却简单得令人惊讶：只需两行代码、一台 8 卡 A100 机器，便可将 7B 模型的文本长度拓展到 100k tokens，70B 模型的文本长度拓展到 32k tokens；同时，该研究团队还发布了首个拥有 70B 参数量的长文本对话大语言模型 LongAlpaca。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><strong>全球首个 70B 长文本大语言模型发布</strong></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">LongLoRA 的提出，让全球大语言模型的对话缺陷第一次得到解决，自此，几十页的论文、几百页的报告、鸿篇巨制不再成为大模型盲区。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">对此，有专业人士激动地表示，LongLoRA 是大语言模型迷宫中的希望之灯！它代表着业界对长文本大语言模型的重新思考和关注，有效扩展了大语言模型的上下文窗口，允许模型考虑和处理较长的文本序列，是大语言模型的革新性发明。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p9.itc.cn/q_70/images01/20231009/cbe10d2967664215bffee1abef01d462.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">除了技术革新外，大语言模型处理长文本问题的一大难点还在于缺少公开的长文本对话数据。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">为此，研究团队特意收集了 9k 条长文本问答语料对，包含针对名著、论文、深度报道甚至财务报表的各类问答。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">光会回答长问题还不够，该团队又挑选了 3k 的短问答语料与 9K 的长问答语料混合训练，让长文本大模型同时具备短文本对话能力。这个完整的数据集被称为 LongAlpaca-12k，目前已经开源。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">在 LongAlpaca-12k 数据集基础上，研究团队对不同参数大小 7B、13B、70B 进行了训练和评测，开源模型包括 LongAlpaca-7B, LongAlpaca-13B 和 LongAlpaca-70B。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><strong>看小说、改论文、指点经济堪称全能王</strong></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">话不多说，盲选几个 demo,一起看看应用了 LongLoRA 技术叠加 12K 问答语料的大模型 LongAlpaca 效果。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p3.itc.cn/q_70/images01/20231009/e0807c4fbd0e428da6cc72b9e4c566dc.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">让系统新读一篇论文，并根据 ICLR 的审查指南，对其提出修改意见，从而提升该论文的接收率。LongAlpaca 的意见是：通过更精确地阐明新颖性，提供更严格和更有对比性的实验结果 (包括具体的数据集和指标)、更广泛的应用和未来发展方向，重点呈现关键贡献和影响，论文被接受的机会将得到提高。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p3.itc.cn/q_70/images01/20231009/783214e5577d4580a8f89721e98f148e.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">现在，让系统读两篇新的不同的论文，让 LongAlpaca 概括 ICLR 和 CVPR 两个会议之间的风格区别。LongAlpaca 总结认为，CVPR 论文倾向更具结构性和实验性的风格，专注于实用性和技术性。而 ICLR 的论文风格更加灵活，侧重关键的理论分析和数学推导，而非标准格式。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">可以看出，经过训练的 LongAlpaca 模型已经可以很轻松地接受新的长篇学术论文，在学术相关问题的回答上相当精准。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">接下来，再看看 LongAlpaca 模型在颇高阅读和理解门槛的经济领域的解读表现。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p3.itc.cn/q_70/images01/20231009/e6eb1abcb4944627935707701425aa11.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p5.itc.cn/q_70/images01/20231009/a6f419f9eb03497e9aa357e147b61085.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">根据国际货币基金组织从 2012 年到 2023 年的全球经济展望的概述合集，对经济形势进行逐年总结以及对未来经济趋势进行预测。LongAlpaca 告诉我们，2023 年全球经济形势不明朗，跟 2022 年相比，增长预计放缓;未来几年，全球经济扩张速度偏温和并将低于预期，而地缘政治紧张、金融市场波动等大的挑战依然存在，需通过多边合作和结构性改革小心应对。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">LongAlpaca 还可以读新的长篇小说，读完后对内容进行分析。下面选取的对比模型是 13B 参数的 LongAlpaca 和 Llama2，且看各自表现。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p1.itc.cn/q_70/images01/20231009/72aeb31905df4635a8f3001fced32892.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">名著《西游记》中，为什么孙悟空在大闹天宫时很厉害，但取，经路上却屡屡受挫?LongAlpaca 给出了 5 点原因，大概可总结为 「孙悟空的不成熟、强大的对手、力量的限制、敌人的欺骗、最初，缺少同伴等因素导致了他在旅途中的失败。但随着时间推移，他获，得了智慧、经验和战友，使他最终战胜了最强大的敌人。」</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">而 Llama2 的回答较为简单。「虽然他确实大闹天宫，但说他代表着不败的力量并不一定准确。事实上，他最终在西游记中被强大的唐三藏打败。」Llama2 没有给出正确答案，并且还提出需要更多的内容信息。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p0.itc.cn/q_70/images01/20231009/a29c650d3a144a18afaf5e7b4d374559.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">这一次让系统新读一次《三体》，然后问第一部中，叶文洁为什么要联系外星人，而后期又为什么后悔？LongAlpaca 认为，叶文洁最初接触外星人是出于理想主义和复仇，但随着时间的推移，她对人类的看法发生了变化，意识到自己可能给全人类带来的危险，她的悔恨与日俱增。回答得十分清晰。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">而 Llama2 的回答则是，「通过与外星人的互动，她希望能更深入地了解宇宙的本质，以及自己在宇宙中的位置。叶文洁逐渐意识到，外星人和他们的技术并不能解决她的问题。」回答笼统，之后开始对小说整体进行评论，答非所问。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">从模型给出的答案可发现，一些模型如 Llama2 [2] 可能在预训练过程中见过相关小说，但如果在提问时进行仅根据小说题目进行短文本提问的话，回答并不理想。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">两个模型的回答对比，高下立见。LongAlpaca 改学术论文、点评全球经济大势和读小说，都是一把好手，完胜 Llama2。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><strong>两行代码和三个关键结论</strong></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">Llama2 可以说是 AI 社区内最强大的开源大模型之一，行业位置领先，LongAlpaca 居然可以完胜。其背后的 LongLoRA 技术成功引起网友们的注意，到底是怎么做到的？</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">原来大语言模型对长文本处理过程中，计算量的主要开销集中在自注意力机制 (self-attention)，其开销随着文本长度成平方次地增加。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">针对这个问题，研究团队提出 LongLoRA 技术，并用分组和偏移的方式来对全局自注意力机制进行模拟。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p0.itc.cn/q_70/images01/20231009/e9c924b5c8564afa9f24a208a98eeb8c.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">简单来说，就是将长文本对应的 tokens 拆分成不同的组，在每组内部做自注意力计算，而分组的方式在不同注意力头 (attention head) 上有所偏移。这样的方式既可以大幅度节约计算量，又可以维持全局感受野的传递。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">而这个实现方法也非常简洁，仅两行代码即可完成！</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p9.itc.cn/q_70/images01/20231009/096cdf76e3394df796ed04057a1cb6c2.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">LongLoRA 还探索了低秩训练的方式。原有的低秩训练方式，如 LoRA [5]，无法在文本长度迁移上取得良好的效果。而 LongLoRA 在低秩训练的基础上，引入嵌入层 (Embedding layer 和 Normalization layers) 进行微调，从而达到可以和全参数微调 (Full fine-tune) 逼近的效果。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em"><img src="https://p7.itc.cn/q_70/images01/20231009/c684b6990a394aacb2859a1f064e09b9.png" referrerpolicy="no-referrer"></p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">进行不同长度文本扩展和训练时，LongLoRA、LoRA 和全参数微调不同技术的具体效果如何，可以参考三个维度表现：</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">在 Perplexity-困惑度上，原有 LoRA 方法的性能在不断恶化，而 LongLoRA 和全参数微调都能在各种文本长度下维持很好的效果；</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">在显存消耗上，相比于全参数微调，LongLoRA 和原有 LoRA 都有大幅度的节省。例如，对于 8k 长度的模型训练，相比于全参数微调，LongLoRA 将显存消耗从 46.3GB 降低到 25.6GB；</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">在训练时间上，对于 64k 长度的模型训练，相比于常规 LoRA，LongLoRA 将训练时间从 90～100 小时左右降低到 52.4 小时，而全参数微调超过 1000 小时。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">极简的训练方法、极少的计算资源和时间消耗，以及极佳的准确性，令 LongLoRA 大规模推广成为可能。目前，相关技术与模型已全部开源，感兴趣的用户们可以自己部署感受。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">值得一提的是，这是贾佳亚团队继 8 月 9 日发布的「可以分割一切」的多模态大模型 LISA 后的又一力作。相距不过短短两个月，不得不说，这研究速度和能力跟 LongLoRA 一样惊人。</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">代码和 Demo 地址：https://github.com/dvlab-research/LongLoRA</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">论文地址：https://arxiv.org/pdf/2309.12307.pdf</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">参考文献</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">[1] LLaMA team. Llama: Open and efficient foundation language models. Arxiv, 2302.13971, 2023a.</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">[2] Llama2 team. Llama 2: Open foundation and fine-tuned chat models. Arxiv, 2307.09288, 2023b.</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">[3] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. Arxiv, 2306.15595, 2023.</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">[4] Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. Focused transformer: Contrastive training for context scaling. Arxiv, 2307.03170, 2023.</p><p style="color:#191919; margin-left:1.8em; margin-right:0.63em">[5] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.</p><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 05:26:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260968</guid>
            <link>https://www.oschina.net/news/260968</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[百度加紧训练文心大模型 4.0，或将于 10 月 17 日发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>据科创板日报报道，百度正在加紧训练文心大模型 4.0，或将在 10 月 17 日百度世界大会上发布。</p><p>据消息人士透露，文心大模型 4.0 的进展比预期快很多，将是基础模型的大升级，<strong>理解、生成、逻辑、记忆</strong>四大核心能力都将提升，尤其在逻辑推理、代码和数学等方面提升最明显。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-741c4810e55393b362a8f9a60b8c98a65dd.png" referrerpolicy="no-referrer"></p><p>今年 8 月，<a href="https://www.oschina.net/news/256156" target="_blank">百度宣布文心一言率先向全社会全面开放</a>，所有用户都能下载文心一言 App 或在官网体验。</p><hr><p>延伸阅读：<a href="https://www.oschina.net/news/256949">挑战 ChatGPT，国产有这 8 款 AI 大模型产品</a></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 04:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260957</guid>
            <link>https://www.oschina.net/news/260957</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
