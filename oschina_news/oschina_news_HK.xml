<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[開源中國-最新資訊]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[開源中國-最新資訊 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Fri, 23 Feb 2024 14:59:14 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[2024 年，只有搞顏色的 P 站真正關心網站性能]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>2024 年，大家覺得一個網站 JS 文件的平均大小應該是多少？1MB、5MB、10MB，還是更加大呢？</p><p>近年來，層出不窮的現代化前端技術讓人眼花繚亂，再加上終端設備的配置越來越高，許多網站似乎不用再過分擔心性能問題 —— 常常打開網站就要下載超過 10M 的&nbsp;<span style="font-family:-apple-system,BlinkMacSystemFont,&quot;Apple Color Emoji&quot;,&quot;Segoe UI Emoji&quot;,&quot;Segoe UI Symbol&quot;,&quot;Segoe UI&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei&quot;,&quot;Helvetica Neue&quot;,Helvetica,Arial,sans-serif">JS 文件。</span></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-25992e8199a790e84d4276053e0858e1b77.png" referrerpolicy="no-referrer"></p><p>知名開源開發者 Nikita Prokopov 對常見網站的 JS 文件大小進行了統計，結果有點令人出乎意料。</p><hr><h3><strong><span style="background-color:#e67e22">以靜態頁面為主的網站</span></strong></h3><ul><li><h4>Wikipedia, 0.2&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-1421231d12c5140f4dc29b93285f2916686.png" referrerpolicy="no-referrer"></p><ul><li><h4>Linear, 3&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e8a9b91df6faa5480f2103efd9cd244aa66.png" referrerpolicy="no-referrer"></p><ul><li><h4>Zoom, 6&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-63b072146754bdd245d2af6dcb63dc79c3e.png" referrerpolicy="no-referrer"></p><ul><li><h4>Vercel, 6&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-158a201b80b904688e2a4f9590f8345b4df.png" referrerpolicy="no-referrer"></p><ul><li><h4>Gitlab,<span style="background-color:#f1c40f"> 13&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3de67d966870abe2b064d536d3f770d66bc.png" referrerpolicy="no-referrer"></p><ul><li><h4>Medium, 3&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a132d36ae636b6435528e8d174ad233f7d2.png" referrerpolicy="no-referrer"></p><ul><li><h4>Quora, 4.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3a6c4a73ff80a50c4052aa7faeced43118b.png" referrerpolicy="no-referrer"></p><ul><li><h4>Pinterest, <span style="background-color:#f1c40f">10&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-bb825579fd8f6d787be8051e5b32e081fed.png" referrerpolicy="no-referrer"></p><hr><h3><strong><span style="background-color:#e67e22">以搜索功能為主的網站</span></strong></h3><ul><li><h4>StackOverflow, 3.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-860ef367515f1ea0ae55c567ed403a1a9c4.png" referrerpolicy="no-referrer"></p><ul><li><h4>NPM, 4&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e55316c24ebe2e2a885a576106ac801e0b9.png" referrerpolicy="no-referrer"></p><ul><li><h4>Airbnb, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a59cfa95b748bb2888d93b3b647a63ca416.png" referrerpolicy="no-referrer"></p><ul><li><h4>Booking.com, <span style="background-color:#f1c40f">12&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fee82c93858ef67b723612d9e1ae53314ed.png" referrerpolicy="no-referrer"></p><ul><li><h4>Google, 9&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-d677500f815f8156922beb9938670463a68.png" referrerpolicy="no-referrer"></p><h3><span style="background-color:#e67e22">具有簡單交互的單應用網站</span></h3><ul><li><h4>Google Translate, 2.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-31a83dfa4eba4b19f6010f5479ce06fc03b.png" referrerpolicy="no-referrer"></p><ul><li><h4>ChatGPT, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-76109ab4826fd1e649be9e2d303e44be5ff.png" referrerpolicy="no-referrer"></p><h3><span style="background-color:#e67e22">視頻/多媒體類網站</span></h3><ul><li><h4>Loom, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-35a3913d214fb0b4ee6edc2929f9cec5b77.png" referrerpolicy="no-referrer"></p><ul><li><h4>YouTube, <span style="background-color:#f1c40f">12&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-11c1827cfb0fcf18a4d1ab929ce533a8d42.png" referrerpolicy="no-referrer"></p><ul><li><h4>Pornhub, <span style="background-color:#16a085">&nbsp;1.4&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6531a939e4e613a4503e6bec46e4e5eb0eb.png" referrerpolicy="no-referrer"></p><p>目前看下來，維基百科網站的 JS 文件最小，僅有 0.2MB。Pornhub 次之，為 1.4MB。</p><p>但這倆在下面這個網站前面都是弟弟——</p><ul><li><h4><strong>jQuery, 0.1 MB</strong></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a30ca6a8e2eb8cbc214df1411e42772b4c4.png" referrerpolicy="no-referrer"></p><hr><p>最後看看本站：</p><p><img src="https://oscimg.oschina.net/oscnet/up-8c7490be3117d1e5730f1bb2d6ab1bb8e77.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 09:27:08 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279994/js-bloat-2024</guid>
            <link>https://www.oschina.net/news/279994/js-bloat-2024</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Oracle 致力解決 Java 虛擬線程「Pinning」問題]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>虛擬線程在 2023 年 9 月發佈的 JDK 21 中正式成為一項穩定功能。該功能在 Java 生態系統中反響極佳，但仍存在一些痛點。Oracle 日前在&nbsp;Inside Java 網站上詳細<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finside.java%2F2024%2F02%2F21%2Fquality-heads-up%2F" target="_blank">介紹</a>了虛擬線程的「Pinning」問題。</p><p>最常見的兩種情況是：(a) 虛擬線程在 synchronized method 中駐留（如執行 socket I/O）；(b) <span style="color:#333333">虛擬線程阻塞進入&nbsp;synchronized method</span>，因為對象的相關監視器被另一個線程持有。</p><p>在這兩種情況下，載體或本地線程都不會被釋放去做其他工作。因此可能會影響性能和可擴展性，並可能<span style="color:#333333">在某些情況下</span>導致飢餓和死鎖。官方最近發佈的一個<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finside.java%2F2024%2F02%2F17%2Fvirtual-threads-next-steps%2F" target="_blank">Virtual Threads Next Steps</a>&nbsp;視頻中則更詳細地解釋了其中的原因，並討論了一些潛在的解決方案。</p><p><img height="196" src="https://oscimg.oschina.net/oscnet/up-6a2e93ce6802a570c1704258c9a589e998e.png" width="500" referrerpolicy="no-referrer"></p><p>項目團隊正在努力解決這些問題。Java Project Loom 的新早期訪問版本<span style="color:#4e4242">引入了對對象監視器實現的更改</span><span style="color:#333333">，但不適用這兩種常見情況。因此 </span><span style="color:#4e4242">Loom&nbsp;</span><span style="color:#333333">團隊正在尋求用户的幫助，以測試這些更新的對象監控器在使用虛擬線程的代碼和大量同步的庫中的可靠性和性能。可通過&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmail.openjdk.org%2Fpipermail%2Floom-dev%2F" target="_blank">Loom 郵件列表</a>&nbsp;<span style="color:#333333">報告或反饋問題。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 06:47:02 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279954/java-virtual-threads-pinning-issue</guid>
            <link>https://www.oschina.net/news/279954/java-virtual-threads-pinning-issue</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Koordinator v1.4 正式發佈！為用户帶來更多的計算負載類型和更靈活的資源管理機制]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h3_1"></span><h3>背景</h3><p style="text-align:justify">Koordinator 作為一個積極發展的開源項目，自 2022 年 4 月發佈 v0.1.0 版本以來，經歷了多次迭代，持續為 Kubernetes 生態系統帶來創新和增強。項目的核心是提供混部工作負載編排、混部資源調度、混部資源隔離和混部性能調優的綜合解決方案，幫助用户優化容器性能，並提升集羣資源使用效率。</p><p style="text-align:justify">在過去的版本迭代中，Koordinator 社區不斷壯大，已經得到了包括阿里巴巴、螞蟻科技、Intel、小米、小紅書、愛奇藝、360、有贊、趣玩、美亞柏科、PITS 等知名企業工程師的積極參與和貢獻。每一個版本都是在社區共同努力下推進的，反映了項目在實際生產環境中解決問題的能力。</p><p style="text-align:justify"><strong>今天我們很高興的向大家宣佈，Koordinator v1.4.0 版本正式發佈。</strong>在本次發佈中，Koordinator 引入了 Kubernetes 與 YARN 負載混部、NUMA 拓撲對齊策略、CPU 歸一化和冷內存上報等新特性，同時重點增強了彈性配額管理、宿主機非容器化應用的 QoS 管理、重調度防護策略等領域的功能。這些新增和改進點旨在更好地支持企業級 Kubernetes 集羣環境，特別是對於複雜和多樣化的應用場景。</p><p style="text-align:justify">v1.4.0 版本的發佈，將為用户帶來更多的計算負載類型支持和更靈活的資源管理機制，我們期待這些改進能夠幫助用户應對更多企業資源管理挑戰。在 v1.4.0 版本中，共有 11 位新加入的開發者參與到了 Koordinator 社區的建設，他們是&nbsp;<em>@shaloulcy，@baowj-678，@zqzten，@tan90github，@pheianox，@zxh326，@qinfustu，@ikaven1024，@peiqiaoWang，@bogo-y，@xujihui1985</em>，感謝期間各位社區同學的積極參與和貢獻，也感謝所有同學在社區的持續投入。</p><span id="OSC_h3_2"></span><h3>版本功能特性解讀</h3><span id="OSC_h4_3"></span><h4>1. 支持 K8s 與 YARN 混部</h4><p style="text-align:justify">Koordinator 已經支持了 K8s 生態內的在離線混部，然而在 K8s 生態外，仍有相當數量的大數據任務運行在傳統的 Hadoop YARN 之上。YARN 作為發展多年的大數據生態下的資源管理系統，承載了包括 MapReduce、Spark、Flink 以及 Presto 等在內的多種計算引擎。</p><p style="text-align:justify">Koordinator 社區會同來自阿里雲、小紅書、螞蟻金服的開發者們共同啓動了 Hadoop YARN 與 K8s 混部項目 Koordinator YARN Copilot，支持將 Hadoop NodeManager 運行在 kubernetes 集羣中，充分發揮不同類型負載錯峯複用的技術價值。Koordinator YARN Copilot 具備以下特點：</p><ul><li><strong>面向開源生態</strong></li></ul><p style="text-align:justify">基於 Hadoop YARN 開源版本，不涉及對 YARN 的侵入式改造；</p><ul><li><strong>統一資源優先級和 QoS 策略</strong></li></ul><p style="text-align:justify">YARN NM 使用 Koordinator 的 Batch 優先級資源，遵循 Koordinator QoS 管理策略；</p><ul><li><strong>節點級別的資源共享</strong></li></ul><p style="text-align:justify">Koordinator 提供的混部資源，既可被 K8s Pod 使用，也可被 YARN task 使用，不同類型的離線應用可運行在同一節點。</p><p style="text-align:center"><img src="https://pic2.zhimg.com/80/v2-ac0aa93eb176110201a885bb0d474c31_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">關於 Koordinator YARN Copilot 的詳細設計，以及在小紅書生產環境的使用情況，請參考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttp%253A%2F%2Fmp.weixin.qq.com%2Fs%253Fspm%253Da2c6h.13046898.publish-article.4.5bb96ffaCJHZTt%2526__biz%253DMzUzNzYxNjAzMg%253D%253D%2526mid%253D2247559704%2526idx%253D1%2526sn%253D3aed8968e50c85f7af7d7e79387b9365%2526chksm%253Dfae7e1d7cd9068c10df63fa4cc9362ee259bcb9c5a4f7d68439d6f9779bb31492b072142582e%2526scene%253D21%2523wechat_redirect" target="_blank">往期文章：《Koordinator 助力雲原生應用性能提升：小紅書混部技術實踐》</a>以及社區官方文檔<strong>[1]</strong>。</p><span id="OSC_h4_4"></span><h4>2. 引入 NUMA 拓撲對齊策略</h4><p style="text-align:justify">運行在 Kubernetes 集羣中的工作負載日益多樣化。尤其是在機器學習等領域，對於高性能計算資源的需求持續上升。在這些領域中，不僅需要大量 CPU 資源，還經常需要 GPU 和 RDMA 等其他高速計算資源配合使用；並且，為了獲得最佳的性能，這些資源往往需要在同一個 NUMA 節點，甚至同一個 PCIE 中。</p><p style="text-align:justify">Kubernetes 的 Kubelet 提供了 Topology Manager 來管理資源分配的 NUMA 拓撲，試圖在 Kubelet 的 Admission 階段從節點層面對齊多種資源的拓撲。然而，節點組件沒有調度器的全局視角以及為 Pod 選擇節點的時機，可能導致 Pod 被調度到無法滿足拓撲對齊策略的節點上，從而導致 Pod 由於 Topology Affinity 錯誤無法啓動。</p><p style="text-align:justify">為瞭解決這一問題，Koordinator 將 NUMA 拓撲選擇和對齊的時機放在中心調度器中，從集羣級別優化資源之間的 NUMA 拓撲。在本次發佈的版本中，Koordinator 將 CPU 資源（包含 Batch 資源）的 NUMA 感知調度和 GPU 設備的 NUMA 感知調度作為 alpha 功能支持，整套 NUMA 感知調度快速演進中。</p><p style="text-align:justify">Koordinator 支持用户通過節點的 Label 配置節點上多種資源的 NUMA 拓撲對齊策略，可配置策略如下：</p><ul><li>None 是默認策略，不執行任何拓撲對齊。</li><li>BestEffort 表示節點不嚴格按照 NUMA 拓撲對齊來分配資源。只要節點的剩餘總量滿足 Pods 的需求，調度器總是可以將這樣的節點分配給 Pods。</li><li>Restricted 表示節點嚴格按照 NUMA 拓撲對齊來分配資源，即調度器在分配多個資源時必須只選擇相同的一個或多個 NUMA 節點，否則不應使用該節點；可以使用多個 NUMA 節點。例如，如果一個 Pod 請求 33C，並且每個 NUMA 節點有 32C，那麼它可以被分配使用兩個 NUMA 節點。如果這個 Pod 還需要請求 GPU/RDMA，那麼它需要位於與 CPU 相同的 NUMA 節點上。</li><li>SingleNUMANode 與 Restricted 類似，也是嚴格按照 NUMA 拓撲對齊，但與 Restricted 不同的是，Restricted 允許使用多個 NUMA 節點，而 SingleNUMANode 只允許使用一個 NUMA 節點。</li></ul><p style="text-align:justify">舉例，我們可以為 node-0 設置策略 SingleNUMANode：</p><pre><code>apiVersion: v1
kind: Node
metadata:
  labels:
    node.koordinator.sh/numa-topology-policy: "SingleNUMANode"
  name: node-0
spec:
  ...</code></pre><p style="text-align:justify">在生產環境中，用户可能已經開啓了 Kubelet 的拓撲對齊策略，這個策略會由 koordlet 更新到 NodeResourceTopologyCRD 對象中的 TopologyPolicies 字段。當 Kubelet 的策略和用户在 Node 上設置的策略相沖突時，以 Kubelet 策略為準。Koordinator 調度器基本採用與 Kubelet Topology Manager 相同的 NUMA 對齊策略語義，Kubelet 策略 SingleNUMANodePodLevel 和 SingleNUMANodeContainerLevel 被映射為 SingleNUMANode。</p><p style="text-align:justify">在為節點配置好 NUMA 對齊策略的前提下，調度器可以為每個 Pod 選出許多個符合條件的 NUMA Node 分配結果。Koordinator 當前支持 NodeNUMAResource 插件配置 CPU 和內存資源的 NUMA Node 分配結果打分策略，包括 LeastAllocated 和 MostAllocated，默認為 LeastAllocated 策略，資源支持配置權重。調度器最終將選擇得分最高的 NUMA Node 分配結果。如下例，我們配置 NUMA Node 分配結果打分策略為 MostAllocated：</p><pre><code>apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - pluginConfig:
      - name: NodeNUMAResource
        args:
          apiVersion: kubescheduler.config.k8s.io/v1beta2
          kind: NodeNUMAResourceArgs
          scoringStrategy:  # Here configure Node level scoring strategy
            type: MostAllocated
            resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              - name: "kubernetes.io/batch-cpu"
                weight: 1
              - name: "kubernetes.io/batch-memory"
                weight: 1
          numaScoringStrategy: # Here configure NUMA-Node level scoring strategy
            type: MostAllocated
            resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              - name: "kubernetes.io/batch-cpu"
                weight: 1
              - name: "kubernetes.io/batch-memory"
                weight: 1</code></pre><span id="OSC_h4_5"></span><h4>3. ElasticQuota 再進化</h4><p style="text-align:justify">為了充分地利用集羣資源、降低管控系統成本，用户常常將多個租户的負載部署在一個集羣中。在集羣資源有限的情況下，不同租户之間必然會發生資源爭搶。有的租户的負載可能一直被滿足，而有的租户的負載一直無法得到執行。這就產生對公平性的訴求。配額機制是非常自然地保障租户間公平性的方式，給每個租户一個配額，租户可以使用配額內的資源，超過配額的任務將不被調度和執行。然而，簡單的配額管理無法滿足租户對雲的彈性期待。用户希望除了配額之內的資源請求可以被滿足外，配額之外的資源請求也可以按需地被滿足。</p><p style="text-align:justify">在之前的版本中，Koordinator 複用了上游 ElasticQuota 的協議，允許租户設置 Min 表達其一定要滿足的資源訴求，允許設置 Max 限制其最大可以使用的資源和表達在集羣資源不足的情況下對集羣剩餘資源的使用權重。另外，Koordinator 觀察到，一些租户可能通過 Min 申請了配額，但是實際的任務申請可能並沒有充分利用該配額。由此，為了更近一步地提高資源利用率，Koordinator 允許租户間借用/歸還資源。</p><p style="text-align:justify">除了提供彈性的配額機制滿足租户按需訴求外，Koordinator 在 ElasticQuota 上增加註解將其組織成樹的結構，方便用户表達樹形的組織架構。</p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-0f15360828d04b31ebe10218c08a8758_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">上圖是使用了 Koordinator 彈性配額的集羣中常見的 Quota 結構樹。Root Quota 是連接配額與集羣中實際資源之間的橋樑。在之前的設計中，Root Quota 只在調度器邏輯中存在，在本次發佈中，我們將 Root Quota 也通過 CRD 的形式暴露給用户，用户可以通過 koordinator-root-quota 這個 ElasticQuota CRD 查看 Root Quota 信息。</p><p style="text-align:justify"><strong>3.1 引入 Multi QuotaTree</strong></p><p style="text-align:justify">大型集羣中的節點的形態是多樣的，例如雲廠商提供的 ECS VM 會有不同的架構，常見的是 amd64 和 arm64，相同架構又會有不同種類的機型，而且一般會把節點按可用區劃分。不同類型的節點放到同一個 Quota Tree 中管理時，其特有的屬性將丟失，當用户希望精細化管理機器的特有屬性時，當前的 ElasticQuota 顯得不夠精確。為了滿足用户靈活的資源管理或資源隔離訴求，Koordinator 支持用户將集羣中的資源劃分為多份，每一份由一個 Quota Tree 來管理，如下圖所示：</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-e673d71606d710f3a447f0e504527d43_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">同時，為了幫助用户簡化管理複雜性，Koordinator 在 v1.4.0 中，引入了 ElasticQuotaProfile 機制，用户可以通過 nodeSelector 快速的將節點關聯到不同的 QuotaTree 中，如下實例所示：</p><pre><code>apiVersion: quota.koordinator.sh/v1alpha1
kind: ElasticQuotaProfile
metadata:
  labels:
    kubernetes.io/arch: amd64
  name: amd64-profile
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      kubernetes.io/arch: amd64 // 挑選 amd64 節點
  quotaName: amd64-root-quota   // 匹配的 root quota 名稱
---
apiVersion: quota.koordinator.sh/v1alpha1
kind: ElasticQuotaProfile
metadata:
  labels:
    kubernetes.io/arch: arm64   
  name: arm64-profile
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      kubernetes.io/arch: arm64  // 挑選 arm64 節點
  quotaName: arm64-root-quota    // 匹配的 root quota 名稱</code></pre><p style="text-align:justify">關聯好 QuotaTree 之後，用户在每一個 QuotaTree 中與之前的 ElasticQuota 用法一致。當用户提交 Pod 到對應的 Quota 時，當前仍然需要用户完成 Pod NodeAffinity 的管理，以確保 Pod 運行在正確的節點上。未來，我們會增加一個特性幫助用户自動管理 Quota 到 Node 的映射關係。</p><p style="text-align:justify"><strong>3.2 支持 non-preemptible</strong></p><p style="text-align:justify">Koordinator ElasticQuota 支持把 ElasticQuota 中 min 未使用的部分共享給其他 ElasticQuota 使用從而提高資源利用效率，但當資源緊張時，會通過搶佔機制把借用配額的 Pod 搶佔驅逐走拿回資源。</p><p style="text-align:justify">在實際生產環境中，有一些在線服務如果從其他 ElasticQuota 中借用了這部分額度，後續又發生了搶佔，是可能影響服務質量的。這類工作負載實質上是不能被搶佔的。</p><p style="text-align:justify">為了實現這個機制，Koordinator v1.4.0 引入了新的 API，用户只需要在 Pod 上聲明 quota.scheduling.koordinator.sh/preemptible: false 表示這個 Pod 不可以被搶佔。</p><p style="text-align:justify">調度器調度時發現 Pod 聲明瞭不可搶佔，那麼此類 Pod 的可用配額的上限不能超過 min，所以這裏也需要注意的是，啓用該能力時，一個 ElasticQuota 的 min 需要設置的合理，並且集羣內有相應的資源保障。</p><p style="text-align:justify">這個特性不會破壞原有的行為。</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-example
  namespace: default
  labels:
    quota.scheduling.koordinator.sh/name: "quota-example"
    quota.scheduling.koordinator.sh/preemptible: false
spec:
...</code></pre><p style="text-align:justify"><strong>3.3 其它改進</strong></p><p style="text-align:justify">1. Koordinator Scheduler 過去支持跨 Namespace 使用同一個 ElasticQuota 對象，但有一些場景下，希望只被一個或者多個有限的 Namespace 可以共享同一個對象，為了支持這個場景，用户可以在 ElasticQuota 上增加 annotation quota.scheduling.koordinator.sh/namespaces，對應的值為一個 JSON 字符串數組。</p><p style="text-align:justify">2. 性能優化：過去的實現中，當 ElasticQuota 發生變化時，ElasticQuota 插件會重建整棵 Quota 樹，在 v1.4.0 版本中做了優化。</p><p style="text-align:justify">3. 支持忽略 Overhead：當 Pod 使用一些安全容器時，一般是在 Pod 中聲明 Overhead 表示安全容器自身的資源開銷，但這部分資源成本最終是否歸於終端用户承擔取決於資源售賣策略。當期望不用用户承擔這部分成本時，那麼就要求 ElaticQuota 忽略 overhead。在 v1.4.0 版本中，可以開啓 featureGate ElasticQuotaIgnorePodOverhead 啓用該功能。</p><span id="OSC_h4_6"></span><h4>4. CPU 歸一化</h4><p style="text-align:justify">隨着 Kubernetes 集羣中節點硬件的多樣化，不同架構和代數的 CPU 之間性能差異顯著。因此，即使 Pod 的 CPU 請求相同，實際獲得的計算能力也可能大不相同，這可能導致資源浪費或應用性能下降。CPU 歸一化的目標是通過標準化節點上可分配 CPU 的性能，來保證每個 CPU 單元在 Kubernetes 中提供的計算能力在異構節點間保持一致。</p><p style="text-align:justify">為瞭解決該問題，Koordinator 在 v1.4.0 版本中實現了一套支持 CPU 歸一化機制，根據節點的資源放大策略，調整節點上可分配的 CPU 資源數量，使得集羣中每個可分配的 CPU 通過縮放實現算力的基本一致。整體的架構如下圖所示：</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-b0e335f164d52e400107a5294a2a7dbb_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">CPU 歸一化分為兩個步驟：</p><p style="text-align:justify">1. CPU 性能評估，計算不同 CPU 的性能基準，可以參考工業級性能評測標準 SPEC CPU<strong>[2]</strong>，這部分 Koordinator 項目未提供；</p><p style="text-align:justify">2. 配置 CPU 歸一化係數到 Koordinator，調度系統基於歸一化係數來調度資源，這部分 Koordinator 提供。</p><p style="text-align:justify">將 CPU 歸一化比例信息配置到 koord-manager 的 slo-controller-config 中，配置示例如下：</p><pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: slo-controller-config
  namespace: koordinator-system
data:
  cpu-normalization-config: |
    {
      "enable": true,
      "ratioModel": {
         "Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz": {
           "baseRatio": 1.29,
           "hyperThreadEnabledRatio": 0.82,
           "turboEnabledRatio": 1.52,
           "hyperThreadTurboEnabledRatio": 1.0
         },
         "Intel Xeon Platinum 8369B CPU @ 2.90GHz": {
           "baseRatio": 1.69,
           "hyperThreadEnabledRatio": 1.06,
           "turboEnabledRatio": 1.91,
           "hyperThreadTurboEnabledRatio": 1.20
         }
      }
    }
  # ...</code></pre><p style="text-align:justify">對於配置了 CPU 歸一化的節點，Koordinator 通過 Webhook 攔截 Kubelet 對 Node.Status.Allocatable 的更新以實現 CPU 資源的縮放，最終在節點上呈現出歸一後的 CPU 資源可分配量。</p><span id="OSC_h4_7"></span><h4>5. 改進的重調度防護策略</h4><p style="text-align:justify">Pod 遷移是一個複雜的過程，涉及審計、資源分配、應用啓動等步驟，並且與應用升級、擴展場景以及集羣管理員的資源操作和維護操作混合在一起。因此，如果同時有大量 Pods 正在進行遷移，可能會對系統的穩定性產生影響。此外，如果同一工作負載的許多 Pods 同時被遷移，也會影響應用的穩定性。此外，如果同時遷移多個作業中的 Pods，可能會造成驚羣效應。因此，我們希望順序處理每個作業中的 Pods。</p><p style="text-align:justify">Koordinator 在之前提供的 PodMigrationJob 功能中已經提供了一些防護策略來解決上述問題。在 v1.4.0 版本中，Koordinator 將之前的防護策略增強為仲裁機制。當有大量的 PodMigrationJob 可以被執行時，由仲裁器通過排序和篩選，來決定哪些 PodMigrationJob 可以得到執行。</p><p style="text-align:justify">排序過程如下：</p><ul><li>根據遷移開始時間與當前時間的間隔進行排序，間隔越小，排名越高。</li><li>根據 PodMigrationJob 的 Pod 優先級進行排序，優先級越低，排名越高。</li><li>按照工作負載分散 Jobs，使得同一作業中的 PodMigrationJobs 靠近。</li><li>如果作業中已有 Pods 正在遷移，則該 PodMigrationJob 的排名更高。</li></ul><p style="text-align:justify">篩選過程如下：</p><ul><li>根據工作負載、節點、命名空間等對 PodMigrationJob 進行分組和篩選。</li><li>檢查每個工作負載中正在運行狀態的 PodMigrationJob 數量，達到一定閾值的將被排除。</li><li>檢查每個工作負載中不可用副本的數量是否超出了最大不可用副本數，超出的將被排除。</li><li>檢查目標 Pod 所在節點上正在遷移的 Pod 數量是否超過單個節點的最大遷移量，超出的將被排除。</li></ul><span id="OSC_h4_8"></span><h4>6. 冷內存上報</h4><p style="text-align:justify">為提升系統性能，內核一般儘可能不讓應用程序請求的頁面緩存空閒，而是儘可能將其分配給應用程序。雖然內核分配了這些內存，但是應用可能不再訪問，這些內存被稱為冷內存。</p><p style="text-align:justify">Koordinator 在 1.4 版本中引入冷內存上報功能，主要為未來冷內存回收功能打下基礎。冷內存回收主要用於應對兩個場景：</p><ul><li>對於標準的 Kubernetes 集羣，當節點內存水位過高時，突發的內存請求容器導致系統直接內存回收，操作系統的直接內存回收觸發時會影響已經運行容器的性能，如果回收不及時極端場景可能觸發整機 oom。保持節點內存資源的相對空閒，對提升運行時穩定性至關重要。</li><li>在混部場景中，高優先級應用程序請求但未使用的資源可以被低優先級應用程序回收利用。對內存而言，操作系統未回收的內存，是不能被 Koordinator 調度系統看到的。為了提高混部資源效率，回收容器未使用的內存頁面可以提高整機的資源利用效率。</li></ul><p style="text-align:justify">Koordlet 在 Collector Plugins 中添加了一個冷頁面回收器，用於讀取由 kidled（Anolis 內核）、kstaled（Google）或 DAMON（Amazon）導出的 cgroup 文件 memory.idle_stat。該文件包含頁面緩存中的冷頁面信息，並存在於 memory 的每個層次結構中。目前 koordlet 已經對接了 kidled 冷頁面收集器並提供了其他冷頁面收集器接口。</p><p style="text-align:justify">在收集冷頁面信息後，冷頁面回收器將把收集到的指標（例如節點、Pod 和容器的熱頁面使用量和冷頁面大小）存到 metriccache 中，最後該數據會被上報到 NodeMetric CRD 中。</p><p style="text-align:justify">用户可以通過 NodeMetric 啓用冷內存回收和配置冷內存收集策略，當前提供了 usageWithHotPageCache、usageWithoutPageCache 和 usageWithPageCache 三種策略，更多的細節詳見社區設計文檔<strong>[3]</strong>。</p><span id="OSC_h4_9"></span><h4>7. 非容器化應用的 QoS 管理</h4><p style="text-align:justify">在企業容器化過程中，除了已經運行在 K8s 上的應用，可能還會存在一些非容器化的應用運行在主機上。為了更好兼容企業在容器化過程這一過渡態，Koordinator 開發了節點資源預留機制，可以在尚未容器化的應用預留資源並賦予特定的 QoS 特性。與 Kubelet 提供的資源預留配置不同，Koordinator 主要目標是解決這些非容器化應用與容器化應用運行時的 QoS 問題，整體的方案如下圖所示：</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-5edba11d4ab8bac0dcef2bc16d12d4f6_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">目前，應用程序需要按照規範將進程啓動到對應的 cgroup 中，Koordinator 未實現自動的 cgroup 搬遷工具。針對宿主機非容器化應用，支持 QoS 如下：</p><ul><li><strong>LS (Latency Sensitive)</strong><ul><li>CPU QoS(Group Identity)：應用按照規範將進程運行在 cgroup 的 cpu 子系統中，koordlet 根據 CPU QoS 的配置 resource-qos-config 為其設置 Group Identity 參數；</li><li>CPUSet Allocation：應用按照規範將進程運行在 cgroup 的 cpu 子系統中，koordlet 將為其設置 cpu share pool 中的所有 CPU 核心。</li></ul></li><li><strong>BE (Best-effort)</strong><ul><li>CPU QoS(Group Identity)：應用按照規範將進程運行在 cgroup 的 cpu 子系統中，koordlet 根據 CPU QoS 的配置為其設置 Group Identity 參數。</li></ul></li></ul><p style="text-align:justify">關於宿主機應用 QoS 管理的詳細設計，可以參考社區文檔<strong>[4]</strong>，後續我們將陸續增加其他 QoS 策略對宿主機應用的支持。</p><span id="OSC_h4_10"></span><h4>8. 其它特性</h4><p style="text-align:justify">除了上述新特性和功能增強外，Koordinator 在 v1.4.0 版本還做了一些如下的 bugfix 和優化：</p><ul><li><strong>RequiredCPUBindPolicy</strong></li></ul><p style="text-align:justify">精細化 CPU 編排支持 Required 的 CPU 綁定策略配置，表示嚴格按照指定的 CPU 綁定策略分配 CPU，否則調度失敗。</p><ul><li><strong>CICD</strong></li></ul><p style="text-align:justify">Koordinator 社區在 v1.4.0 提供了一套 e2e 測試的 Pipeline；提供了 ARM64 鏡像。</p><ul><li><strong>Batch 資源計算策略優化</strong></li></ul><p style="text-align:justify">支持了 maxUsageRequest 的計算策略，用於更保守地超賣高優資源；優化了節點上短時間大量 Pod 啓停時，Batch allocatable 被低估的問題；完善了對 hostApplication、thirdparty allocatable、dangling pod used 等特殊情況的考慮。</p><ul><li><strong>其它</strong></li></ul><p style="text-align:justify">利用 libpfm4&amp;perf group 優化 CPI 採集、SystemResourceCollector 支持自定義的過期時間配置、BE Pod 支持根據 evictByAllocatable 策略計算 CPU 滿足度、Koordlet CPUSetAllocator 修復了對於 LS 和 None Qos 的 Pod 的過濾邏輯、RDT 資源控制支持獲得 sandbox 容器的 task IDs 等。</p><p style="text-align:justify">通過 v1.4.0 Release<strong>[5]</strong>頁面，可以看到更多包含在 v1.4.0 版本的新增功能。</p><span id="OSC_h3_11"></span><h3>未來計劃</h3><p style="text-align:justify">在接下來的版本中，Koordinator 目前規劃了以下功能：</p><ul><li><strong>Core Scheduling</strong></li></ul><p style="text-align:justify">在運行時側，Koordinator 開始探索下一代 CPU QoS 能力，通過利用 Linux Core Scheduling 等內核機制，增強的物理核維度的資源隔離，降低混部的安全性風險，相關工作詳見 Issue #1728<strong>[6]</strong>。</p><ul><li><strong>設備聯合分配</strong></li></ul><p style="text-align:justify">在 AI 大模型分佈式訓練場景中，不同機器 GPU 之間通常需要通過高性能網卡相互通信，且 GPU 和高性能網卡就近分配的時候性能更好。Koordinator 正在推進支持多種異構資源的聯合分配，目前已經在協議上和調度器分配邏輯上支持聯合分配；單機側關於網卡資源的上報邏輯正在探索中。</p><p style="text-align:justify">更多信息，敬請關注 Milestone v1.5.0<strong>[7]</strong>。</p><span id="OSC_h3_12"></span><h3>結語</h3><p style="text-align:justify">最後，我們十分感謝 Koordinator 社區的所有貢獻者和用户，是您們的積極參與和寶貴意見讓 Koordinator 不斷進步。我們期待您繼續提供反饋，並歡迎新的貢獻者加入我們的行列。</p><p style="text-align:justify"><strong>相關鏈接：</strong></p><p style="text-align:justify">[1] 社區官方文檔</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fkoordinator.sh%2Fzh-Hans%2Fdocs%2Fnext%2Fdesigns%2Fkoordinator-yarn%2F%253Fspm%253Da2c6h.13046898.publish-article.5.5bb96ffaCJHZTt" target="_blank">https://koordinator.sh/zh-Hans/docs/next/designs/koordinator-yarn/</a></em></u></p><p style="text-align:justify">[2] SPEC CPU</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fwww.spec.org%2Fcpu2017%2F%253Fspm%253Da2c6h.13046898.publish-article.6.5bb96ffaCJHZTt" target="_blank">https://www.spec.org/cpu2017/</a></em></u></p><p style="text-align:justify">[3] 設計文檔</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fblob%2Fmain%2Fdocs%2Fproposals%2Fkoordlet%2F20230728-support-cold-memory-compute.md%253Fspm%253Da2c6h.13046898.publish-article.7.5bb96ffaCJHZTt%2526file%253D20230728-support-cold-memory-compute.md" target="_blank">https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/koordlet/20230728-support-cold-memory-compute.md</a></em></u></p><p style="text-align:justify">[4] 社區文檔</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fkoordinator.sh%2Fzh-Hans%2Fdocs%2Fnext%2Fuser-manuals%2Fhost-application-qos%2F%253Fspm%253Da2c6h.13046898.publish-article.8.5bb96ffaCJHZTt" target="_blank">https://koordinator.sh/zh-Hans/docs/next/user-manuals/host-application-qos/</a></em></u></p><p style="text-align:justify">[5] v1.4.0 Release</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Freleases%2Ftag%2Fv1.4.0%253Fspm%253Da2c6h.13046898.publish-article.9.5bb96ffaCJHZTt%2526file%253Dv1.4.0" target="_blank">https://github.com/koordinator-sh/koordinator/releases/tag/v1.4.0</a></em></u></p><p style="text-align:justify">[6] Issue #1728</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fissues%2F1728%253Fspm%253Da2c6h.13046898.publish-article.10.5bb96ffaCJHZTt" target="_blank">https://github.com/koordinator-sh/koordinator/issues/1728</a></em></u></p><p style="text-align:justify">[7] Milestone v1.5.0</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fmilestone%2F14%253Fspm%253Da2c6h.13046898.publish-article.11.5bb96ffaCJHZTt" target="_blank">https://github.com/koordinator-sh/koordinator/milestone/14</a></em></u></p><p style="text-align:justify"><em>作者：喬普</em></p><p style="text-align:justify"><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1423263%3Futm_content%3Dg_1000390303" target="_blank">原文鏈接</a></strong></p><p style="text-align:justify"><strong>本文為阿里雲原創內容，未經允許不得轉載。</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 06:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/yunqi/blog/11044520</guid>
            <link>https://my.oschina.net/yunqi/blog/11044520</link>
            <author>
                <![CDATA[阿里云云棲號]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[開源日報 | 目前的人工智能技術連貓的智能水平都沒達到]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>歡迎閲讀 OSCHINA 編輯部出品的開源日報，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.22</strong></span></h3><h2><strong><span style="color:#16a085">今日要點</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/279713/google-gemma-open-models" target="_blank">谷歌發佈輕量級開源大語言模型 Gemma</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Gemma 是一款輕量級、先進的開源模型，供開發者和研究人員用於 AI 構建。Gemma 模型家族包括 2B（20 億參數）和 7B（70 億參數）兩種尺寸，能夠在不同的設備類型上運行，包括筆記本電腦、桌面電腦、IoT 設備、移動設備和雲端。</p><p><img src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><h3><a href="https://www.oschina.net/news/279741/nightingale-7-0-0-beta-0-released" target="_blank">夜鶯監控 V7 第一個 beta 版本</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">夜鶯項目從 2024 開始開發 V7 版本，重點做體驗優化，V7 和 V6 版本兼容可以平滑升級<span style="background-color:#ffffff; color:#333333">（V6 升級到 V7 只需要替換一下二進制重啓即可，如果是容器部署，只需要更新鏡像並重啓）</span>，第一個 beta 的優化項包括：</p><ul><li>全站暗黑主題</li><li>優化邊緣機房機器失聯告警的實現邏輯，真正做到邊緣機房告警自閉環</li><li>優化內置大盤、內置告警規則的列表頁面 UI</li><li>全局回調地址頁面展示優化，增加詳盡的文檔提示信息</li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="20240221141801" src="https://download.flashcat.cloud/ulric/20240221141801.png" referrerpolicy="no-referrer"></p><hr><h2><strong><span style="color:#16a085">今日觀察</span></strong></h2><p><img height="554" src="https://oscimg.oschina.net/oscnet/up-14e1acb77ab8633d225b3117ebbc7dc7920.png" width="1522" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博<span>&nbsp;</span></span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1233486457%2FO1MdFaqZm" target="_blank">高飛</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-a2ff0599a45c140a6bf468a7d85524102fa.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博<span>&nbsp;</span></span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1856404484%2FO1JFSAEg0" target="_blank">鳳凰網科技</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推薦</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-da11d19579f9cc14d57a12c12f4479f067a.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>開源之聲</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-33430582a60711cf56a923991e1fef04da9.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日項目榜</strong></span></h2><p>GitHub Trending</p><p><img src="https://oscimg.oschina.net/oscnet/up-291d0213846cc3efbd09b526da6bac5ae29.png" referrerpolicy="no-referrer"></p><hr><p><strong>往期回顧</strong></p><ul><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC013%E6%9C%9F%EF%BC%9A%E7%AD%89%E5%88%B0%20Sora%20%E5%BC%80%E6%BA%90%E4%BA%86%E7%AB%8B%E5%88%BB%E6%8E%A8%E5%87%BA%E5%B1%9E%E4%BA%8E%E6%88%91%E4%BB%AC%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B.pdf">開源日報第 013 期：等到 Sora 開源了立刻推出屬於我們自己的大模型</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">開源日報第 012 期：Sora 給中國 AI 帶來的真實變化；Dart 3.3 發佈</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">開源日報第 011 期：目前還沒有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">開源日報第 010 期：Tauri v2 支持 Android 和 iOS，跨平台開發新選擇</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">開源日報第 009 期：Vue.js 誕生 10 週年；扎克伯格解釋 Meta 為什麼要開源其 AI 技術</a></li><li><a href="https://www.oschina.net/news/277585">開源日報第 008 期：推動中國開源軟硬件發展的經驗與建議</a></li><li><a href="https://www.oschina.net/news/277415">開源日報第 007 期：「Linux 中國」 開源社區宣佈停止運營</a></li><li><a href="https://www.oschina.net/news/277214">開源日報第 006 期：選擇技術棧一定要選擇開源的</a></li><li><a href="http://www.oschina.net/news/277040">開源日報第 005 期：RISC-V 萬兆開源交換機發售；npm 存在大量武林外傳視頻</a></li><li><a href="https://www.oschina.net/news/276864">開源日報第 004 期：百度輸入法在候選詞區域植入廣告；大神用 Excel 構建 CPU</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 03:53:48 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279923</guid>
            <link>https://www.oschina.net/news/279923</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Altman 迴應 7 萬億美元半導體計劃：所需投資遠超想象]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">在英偉達發佈了強勁的 2024 財年第四季度財報之後的幾小時，英特爾首席執行官 Pat Gelsinger 和 OpenAI 首席執行官 Sam Altman 在加利福尼亞州聖何塞的一個會議中心展開了一場對話，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fapnews.com%2Farticle%2Fintel-openai-nvidia-chips-boom-dbf20077caafc9b33870f9f6d32d3794" target="_blank">暢談</a>半導體在 AI 時代塑造社會所扮演的角色。</span></p><p><span style="color:#000000"><img alt="" height="375" src="https://oscimg.oschina.net/oscnet/up-67bd113ff9160357624584468c53d04258a.webp" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">在活動中，Gelsinger 詢問了 Altman 有關最近報道的計劃從中東地區籌集高達 7 萬億美元的資金，以支持 OpenAI 的一項半導體計劃，並與英偉達展開競爭的傳聞。對此，Altman 則反駁稱，這種匿名人士未經證實的説法比比皆是，「我的主要工作不是到處修正這些錯誤的文章」。</span></p><p><span style="color:#000000">但 Altman 同時也承認，AI 的發展需要大量的資金。「事實是，我們認為世界將需要更多的 AI 計算（芯片）。這將需要全球範圍內大量的投入，超出我們的想象。我們現在還沒有一個具體數字。」</span></p><p><span style="color:#000000">他還強調了過去一年加快人工智能發展的重要性。他認為人工智能的進步將為人類帶來更美好的未來，不過奧特曼也承認，在前進的過程中會有不利的一面。"我們正在走向這樣一個世界：人工智能生成的內容將多於人類生成的內容。這將不僅僅是一個 good story，而是一個 net good story。"</span></p><p><span style="color:#000000">此外，Altman 重申了在 AI 領域進行監管的必要性，強調了政府在制定框架以降低潛在風險方面的關鍵作用。</span></p><p><span style="color:#000000">Gelsinger 預測，</span><span style="background-color:#ffffff; color:#000000">到 2030 年，英特爾將成為全球第二大代工企業，僅次於台積電。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 03:12:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279911/intel-openai-chips</guid>
            <link>https://www.oschina.net/news/279911/intel-openai-chips</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Apache ECharts 5.5.0 引入服務端渲染的新利器：1KB 的客户端輕量運行時]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Apache ECharts 5.5.0 版本已於 2024.2.18 正式發佈。</p><p><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fecharts%2Freleases%2Ftag%2F5.5.0" target="_blank">https://github.com/apache/echarts/releases/tag/5.5.0</a></u></em></p><p><strong>主要變化</strong></p><ul><li><p>增強了代碼的 ESM 識別，對&nbsp;Node.js&nbsp;環境開發更加友好；</p></li><li><p>為服務端渲染方案提供了一個 gzip 後僅 1KB 的輕量運行時，極大地降低了加載時間；</p></li><li><p>為數據下鑽支持了過渡動畫，開發者可以方便地實現多級數據的動畫效果；</p></li><li><p>為餅圖和極座標系圖表增加了更多配置項，可以實現更豐富的樣式；</p></li><li><p>新增阿拉伯語和荷蘭語兩種語言的翻譯</p></li><li><p>……</p></li></ul><p>以下內容轉自：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FIpGQS1GspyXzNe-u9F4B-A" target="_blank">https://mp.weixin.qq.com/s/IpGQS1GspyXzNe-u9F4B-A</a></u></em></p><hr><h2>增強的 ESM 支持</h2><p>為了讓開發者在測試和 Node.js 環境使用更方便，我們在這個版本中對 ESM 的識別問題進行了優化。</p><p>以前，ECharts 只在 npm（npm 包的 lib 目錄中）導出&nbsp;<code>*.esm</code>&nbsp;文件。雖然這在 bundlers 環境表現良好，但 Node.js 環境和一些基於 Node.js 的測試框架（如 vitest 和 jest）中的表現並不理想。</p><p>有了這個新功能，我們做了幾個改變以改善這個問題：</p><ul><li><p>在&nbsp;<code>package.json</code>&nbsp;中添加了&nbsp;<code>"type": "module"</code></p></li><li><p>在&nbsp;<code>package.json</code>&nbsp;中添加了&nbsp;<code>"exports": {...}</code></p></li><li><p>在子目錄中添加了一些只包含&nbsp;<code>"type": "commonjs"</code>&nbsp;的&nbsp;<code>package.json</code>&nbsp;文件</p></li></ul><p>這些改變意味着，像&nbsp;<code>echarts/core.js</code>&nbsp;這樣的文件現在可以在像純 Node.js、vitest、jest 和 create-react-app 這樣的環境中解析為 ESM。</p><p>我們還確保了這個新功能與各種環境兼容，包括運行時（Node.js/vitest/jest（create-react-app）/ssr/…）和打包器（webpack/rollup/vite/esbuild/…）。</p><p>我們非常期待這一新功能，並相信它將極大地改善開發者的體驗。</p><h2>服務端渲染 + 客户端輕量運行時</h2><p>Apache ECharts 功能強大，相應地，包體積也比較大。我們在之前的版本中也做了各種努力來改進這一點。開發者可以使用 TreeShaking 按需加載部分代碼，以減少加載的代碼量。從 Apache ECharts 5.3 版本起，我們支持了零依賴的服務端 SVG 字符串渲染方案，並支持圖表的初始動畫。這樣，使用服務端渲染的結果作為首屏渲染的畫面，可以大大減少首屏加載時間。</p><p>服務端渲染雖然是一種很有效減少包體積的解決方案，但如果需要在客户端實現一些交互，那麼不得不仍舊加載 echarts.js，這可能會增加更多的加載時間。對於一些對頁面加載速度要求較高的場景，這可能不是一個理想的選擇。</p><p><strong>在 5.5.0 版本中，我們新增了客户端輕量運行時</strong>，客户端無需加載完整 ECharts 即可實現部分交互。這樣，我們可以在服務端渲染圖表，然後在客户端加載輕量運行時，實現一些常見的交互。這意味着，<strong>只需要加載&nbsp;4KB 的輕量運行時（gzip 後 1KB），即可實現帶初始動畫和部分常用交互形式的圖表</strong>。這一改進將極大地提升頁面加載速度，特別是對於移動端的體驗。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-5c9857523986e92155d29f453c6307014c1.png" referrerpolicy="no-referrer"></p><p>以這個帶標題的餅圖為例，如果按客户端僅打包餅圖和標題組件的方案，gzip 後需要 135KB；如果按服務端渲染的方案，渲染結果 SVG gzip 後 1 KB、客户端運行時 gzip 後 1KB，僅為前者體積的 1.5%。交互方面，後者也可以做到初始動畫、鼠標移動到圖表元素後的高亮，並且獲取到點擊事件，能夠滿足大部分的常見交互需求。</p><p>如需使用客户端輕量運行時方案，服務端代碼和之前一樣，但需要保證 ECharts 版本號在 5.5.0 以上。</p><pre><code>//&nbsp;服務端代碼
const&nbsp;echarts&nbsp;=&nbsp;require('echarts');

//&nbsp;在&nbsp;SSR&nbsp;模式下第一個參數不需要再傳入&nbsp;DOM&nbsp;對象
const&nbsp;chart&nbsp;=&nbsp;echarts.init(null,&nbsp;null,&nbsp;{
&nbsp;&nbsp;renderer:&nbsp;'svg',&nbsp;//&nbsp;必須使用&nbsp;SVG&nbsp;模式
&nbsp;&nbsp;ssr:&nbsp;true,&nbsp;//&nbsp;開啓&nbsp;SSR
&nbsp;&nbsp;width:&nbsp;400,&nbsp;//&nbsp;需要指明高和寬，如果是根據客户端容器大小動態的，該值需要從客户端得到
&nbsp;&nbsp;height:&nbsp;300
});

//&nbsp;像正常使用一樣&nbsp;setOption
chart.setOption({
&nbsp;&nbsp;//...
});

//&nbsp;輸出字符串
const&nbsp;svgStr&nbsp;=&nbsp;chart.renderToSVGString();

//&nbsp;調用&nbsp;dispose&nbsp;以釋放內存
chart.dispose();
chart&nbsp;=&nbsp;null;

//&nbsp;通過 HTTP Response 返回 svgStr 給前端或者緩存到本地（這裏以 Express.js 為例）：
res.writeHead(200,&nbsp;{
&nbsp;&nbsp;'Content-Type':&nbsp;'application/xml'
});
res.write(svgStr);
res.end();

</code></pre><p>客户端將得到的 SVG 字符串添加到容器中，並綁定輕量運行時：</p><pre><code>&lt;div&nbsp;id="chart-container"&nbsp;style="width:800px;height:600px"&gt;&lt;/div&gt;

&lt;script&nbsp;src="https://cdn.jsdelivr.net/npm/echarts/ssr/client/dist/index.js"&gt;&lt;/script&gt;
&lt;script&gt;
const&nbsp;ssrClient&nbsp;=&nbsp;window['echarts-ssr-client'];

let&nbsp;isSeriesShown&nbsp;=&nbsp;{
&nbsp;&nbsp;a:&nbsp;true,
&nbsp;&nbsp;b:&nbsp;true
};

function&nbsp;updateChart(svgStr)&nbsp;{
&nbsp;&nbsp;const&nbsp;container&nbsp;=&nbsp;document.getElementById('chart-container');
&nbsp;&nbsp;container.innerHTML&nbsp;=&nbsp;svgStr;

&nbsp;&nbsp;//&nbsp;使用輕量運行時賦予圖表交互能力
&nbsp;&nbsp;ssrClient.hydrate(main,&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;on:&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;click:&nbsp;(params)&nbsp;=&gt;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(params.ssrType&nbsp;===&nbsp;'legend')&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;點擊圖例元素，請求服務器進行二次渲染
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isSeriesShown[params.seriesName]&nbsp;=&nbsp;!isSeriesShown[params.seriesName];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fetch('...?series='&nbsp;+&nbsp;JSON.stringify(isSeriesShown))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.then(res&nbsp;=&gt;&nbsp;res.text())
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.then(svgStr&nbsp;=&gt;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;updateChart(svgStr);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;});
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;});
}

//&nbsp;通過&nbsp;AJAX&nbsp;請求獲取服務端渲染的&nbsp;SVG&nbsp;字符串
fetch('...')
&nbsp;&nbsp;.then(res&nbsp;=&gt;&nbsp;res.text())
&nbsp;&nbsp;.then(svgStr&nbsp;=&gt;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;updateChart(svgStr);
&nbsp;&nbsp;});
&lt;/script&gt;
</code></pre><p>客户端輕量運行時必須配合 SVG 形式的服務端渲染結果使用，支持以下交互：</p><ul><li><p>圖表初始動畫（實現原理：服務端渲染的 SVG 帶有 CSS 動畫）</p></li><li><p>高亮樣式（實現原理：服務端渲染的 SVG 帶有 CSS 動畫）</p></li><li><p>動態改變數據（實現原理：輕量運行時請求服務器進行二次渲染）</p></li><li><p>點擊圖例切換系列是否顯示（實現原理：輕量運行時請求服務器進行二次渲染）</p></li></ul><p>可以發現，這能夠滿足大部分的交互場景需求。如果需要更復雜的交互，則客户端需要加載&nbsp;<code>echarts.js</code>&nbsp;實現完整功能。</p><p>完整的介紹請參見官網使用手冊的「應用篇 - 跨平台方案 - 服務端渲染」。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279907/echarts-5-5-0</guid>
            <link>https://www.oschina.net/news/279907/echarts-5-5-0</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[和 Gmail 基本 HTML 視圖説再見]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌已開始<u><a href="https://www.oschina.net/news/259383">停止支持 Gmail </a><a href="https://www.oschina.net/news/259383">基本 HTML 視圖</a></u>。自 2024 年 2 月起，Gmail 會自動將用户從基本 HTML 視圖轉換為標準視圖。</p><p><img src="https://oscimg.oschina.net/oscnet/up-0cdd4e28b49bf3c37718b40baee96da1d75.png" referrerpolicy="no-referrer"></p><p><em>基本 HTML 視圖允許用户以簡陋的方式查看電子郵件，但對所有的瀏覽器提供了最大的兼容性。</em></p><p>Gmail 更新了其<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsupport.google.com%2Fmail%2Fanswer%2F15049%3Fhl%3Dzh-Hans" target="_blank">支持頁面</a>，以反映 Gmail 將在截止日期後自動切換到標準視圖。不僅如此，有用户在<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.ycombinator.com%2Fitem%3Fid%3D37558372" target="_blank">Hacker News</a>上發帖稱，他們收到了一封來自 Google 的郵件，表明該功能已經結束。</p><blockquote><p>"我們寫信通知您，從 2024 年 1 月初開始，桌面網頁和移動網頁的 Gmail Basic HTML 視圖將被禁用。Gmail Basic HTML 視圖是 Gmail 的舊版本，10 多年前就已被現代版本取代，不包含完整的 Gmail 功能。"</p></blockquote><p>即使在今天，當你嘗試訪問 HTML 版本時，Google 也會顯示一條信息，稱該版本是為"較慢的連接速度和傳統瀏覽器"設計的，並要求你確認是否不想使用標準版本。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-cca03ff735c3d625769511865800215a87d.png" referrerpolicy="no-referrer"></p><p>HTML 版本缺少很多功能，如聊天、拼寫檢查、搜索過濾器、鍵盤快捷鍵和豐富的格式。但在連接性較差的地區或只想瀏覽電子郵件而不想使用任何額外功能的情況下，HTML 版還是很有用的。目前還不清楚 Google 是否計劃添加低連接模式。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:41:03 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279900/goodbye-html-gmail</guid>
            <link>https://www.oschina.net/news/279900/goodbye-html-gmail</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[記一次 Rust 內存泄漏排查之旅]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在某次持續壓測過程中，我們發現 GreptimeDB 的 Frontend 節點內存即使在請求量平穩的階段也在持續上漲，直至被 OOM kill。我們判斷 Frontend 應該是有內存泄漏了，於是開啓了排查內存泄漏之旅。</p><h2>Heap Profiling</h2><p>大型項目幾乎不可能只通過看代碼就能找到內存泄漏的地方。所以我們首先要對程序的內存用量做統計分析。幸運的是，GreptimeDB 使用的 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjemalloc%2Fjemalloc%2Fwiki%2FUse-Case%253A-Heap-Profiling" target="_blank">jemalloc 自帶 heap profiling</a>，我們也<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fblob%2Fdevelop%2Fsrc%2Fcommon%2Fmem-prof%2FREADME.md" target="_blank">支持了導出 jemalloc 的 profile dump 文件</a>。於是我們在 GreptimeDB 的 Frontend 節點內存達到 300MB 和 800MB 時，分別 dump 出了其內存 profile 文件，再用 jemalloc 自帶的 <code>jeprof</code> 分析兩者內存差異（<code>--base</code> 參數），最後用火焰圖顯示出來：</p><p><img src="https://oscimg.oschina.net/oscnet/up-002154d38e6da2e50485895918972b1b8a1.png" alt="" referrerpolicy="no-referrer"></p><p>顯然圖片中間那一大長塊就是不斷增長的 500MB 內存佔用了。仔細觀察，居然有 thread 相關的 stack trace。難道是創建了太多線程？簡單用 <code>ps -T -p</code> 命令看了幾次 Frontend 節點的進程，線程數穩定在 84 個，而且都是預知的會創建的線程。所以「線程太多」這個原因可以排除。</p><p>再繼續往下看，我們發現了很多 Tokio runtime 相關的 stack trace，而 Tokio 的 task 泄漏也是常見的一種內存泄漏。這個時候我們就要祭出另一個神器：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ftokio-rs%2Fconsole" target="_blank">Tokio-console</a>。</p><h2>Tokio Console</h2><p>Tokio Console 是 Tokio 官方的診斷工具，輸出結果如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-1772a99dbc1b17d9b0ab49532e083a4234f.png" alt="" referrerpolicy="no-referrer"></p><p>我們看到居然有 5559 個正在運行的 task，且絕大多數都是 Idle 狀態！於是我們可以確定，內存泄漏發生在 Tokio 的 task 上。 現在問題就變成了：GreptimeDB 的代碼裏，哪裏 spawn 了那麼多的無法結束的 Tokio task？</p><p>從上圖的 "Location" 列我們可以看到 task 被 spawn 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fblob%2Fdevelop%2Fsrc%2Fcommon%2Fruntime%2Fsrc%2Fruntime.rs%23L63" target="_blank">地方</a>：</p><pre><code class="language-rust">impl Runtime {
    /// Spawn a future and execute it in this thread pool
    ///
    /// Similar to Tokio::runtime::Runtime::spawn()
    pub fn spawn&lt;F&gt;(&amp;self, future: F) -&gt; JoinHandle&lt;F::Output&gt;
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        self.handle.spawn(future)
    }
}
</code></pre><p>接下來的任務是找到 GreptimeDB 裏所有調用這個方法的代碼。</p><h2><code>..Default::default()</code>！</h2><p>經過一番看代碼的仔細排查，我們終於定位到了 Tokio task 泄漏的地方，並在 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fpull%2F1512" target="_blank">PR #1512</a> 中修復了這個泄漏。簡單地説，就是我們在某個會被經常創建的 struct 的構造方法中，spawn 了一個可以在後台持續運行的 Tokio task，卻未能及時回收它。對於資源管理來説，在構造方法中創建 task 本身並不是問題，只要在 <code>Drop</code> 中能夠順利終止這個 task 即可。而我們的內存泄漏就壞在忽視了這個約定。</p><p>這個構造方法同時在該 struct 的 <code>Default::default()</code> 方法當中被調用了，更增加了我們找到根因的難度。</p><p>Rust 有一個很方便的，可以用另一個 struct 來構造自己 struct 的方法，即 "<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdoc.rust-lang.org%2Fbook%2Fch05-01-defining-structs.html%23creating-instances-from-other-instances-with-struct-update-syntax" target="_blank">Struct Update Syntax</a>"。如果 struct 實現了 <code>Default</code>，我們可以簡單的在 struct 的 field 構造中使用 <code>..Default::default()</code>。如果 <code>Default::default()</code> 內部有 「side effect」（比如我們本次內存泄漏的原因——創建了一個後台運行的 Tokio task），一定要特別注意：struct 構造完成後，<code>Default</code> 創建出來的臨時 struct 就被丟棄了，一定要做好資源回收。</p><p>例如下面這個小例子：（<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplay.rust-lang.org%2F%3Fversion%3Dstable%26mode%3Ddebug%26edition%3D2021%26gist%3Dc121ffd32d2ff0fa8e1241a62809bcef" target="_blank">Rust Playground</a>）</p><pre><code class="language-rust">struct A {
    i: i32,
}

impl Default for A {
    fn default() -&gt; Self {
        println!("called A::default()");
        A { i: 42 }
    }
}

#[derive(Default)]
struct B {
    a: A,
    i: i32,
}

impl B {
    fn new(a: A) -&gt; Self {
        B {
            a,
            // A::default() is called in B::default(), even though "a" is provided here.
            ..Default::default()
        }
    }
}

fn main() {
    let a = A { i: 1 };
    let b = B::new(a);
    println!("{}", b.a.i);
}
</code></pre><p>struct A 的 <code>default</code> 方法是會被調用的，打印出 <code>called A::default()</code>。</p><h2>總結</h2><ul><li>排查 Rust 程序的內存泄漏，我們可以用 jemalloc 的 heap profiling 導出 dump 文件；再生成火焰圖可直觀展現內存使用情況。</li><li>Tokio-console 可以方便地顯示出 Tokio runtime 的 task 運行情況；要特別注意不斷增長的 idle tasks。</li><li>儘量不要在常用 struct 的構造方法中留下有副作用的代碼。</li><li><code>Default</code> 只應該用於值類型 struct。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-3d33a6c61f7c37725020cbe267d0c2d0f01.jpg" alt="" referrerpolicy="no-referrer"></p><h3>關於 Greptime</h3><p>Greptime 格睿科技於 2022 年創立，目前正在完善和打造時序數據庫 GreptimeDB 和格睿雲 GreptimeCloud 這兩款產品。</p><p>GreptimeDB 是一款用 Rust 語言編寫的時序數據庫，具有分佈式、開源、雲原生、兼容性強等特點，幫助企業實時讀寫、處理和分析時序數據的同時，降低長期存儲的成本。</p><p>GreptimeCloud 基於開源的 GreptimeDB，為用户提供全託管的 DBaaS，以及與可觀測性、物聯網等領域結合的應用產品。利用雲提供軟件和服務，可以達到快速的自助開通和交付，標準化的運維支持，和更好的資源彈性。GreptimeCloud 已正式開放內測，歡迎關注公眾號或官網瞭解最新動態！</p><p>官網：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreptime.com%2F" target="_blank">https://greptime.com/</a></p><p>公眾號：GreptimeDB</p><p>GitHub: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb" target="_blank">https://github.com/GreptimeTeam/greptimedb</a></p><p>文檔：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.greptime.com%2F" target="_blank">https://docs.greptime.com/</a></p><p>Twitter: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2FGreptime" target="_blank">https://twitter.com/Greptime</a></p><p>Slack: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreptime.com%2Fslack" target="_blank">https://greptime.com/slack</a></p><p>LinkedIn: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fgreptime%2F" target="_blank">https://www.linkedin.com/company/greptime/</a></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:23:03 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6839317/blog/11044292</guid>
            <link>https://my.oschina.net/u/6839317/blog/11044292</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Stability AI 發佈 Stable Diffusion 3 早期預覽版]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>AI 創業公司 Stability AI <u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fstable-diffusion-3" target="_blank">宣佈</a></u>其最新一代的文本圖像模型 Stable Diffusion 3 開放預覽，該版本目前僅限部分用户參與測試，主要是為了在正式發佈前收集與性能和安全性相關的用户反饋。感興趣的用户可以申請加入等候名單。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3fa2fd390f1fbdd7808ee91a4ed1a0ef4fd.png" referrerpolicy="no-referrer"></p><p>Stable Diffusion 3 早期預覽版相比前代產品在圖片質量、多主題展示和文字展示方面有大幅提升。Stable Diffusion 3 模型的參數規模從 8 億，到 80 億不等，其架構組合了 diffusion transformer（擴散變換架構）和 flow matching（流匹配），技術報告將在晚些時候公佈。</p><p><img height="582" src="https://oscimg.oschina.net/oscnet/up-25784ed1b6a0b80ca5fc50b3c842456064f.png" width="3052" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-377f7b831888b11ab490f50cafe8931753d.png" referrerpolicy="no-referrer"></p><p>性能的具體提升內容包括：</p><ol><li>多主題提示處理能力： 新模型對於包含多個主題或元素的提示具有更好的理解和處理能力。這意味着用户可以在一個提示中描述更復雜的場景，而模型能夠更準確地根據這些描述生成圖像。</li><li>圖像質量： Stable Diffusion 3 在生成的圖像質量上有顯著提高，包括更細膩的細節表現、更準確的顏色匹配以及更自然的光影處理。這些改進使得生成的圖像更加逼真，更能捕捉到用户的創意意圖。</li><li>拼寫和文本處理能力： 這個版本在處理文本元素，尤其是在圖像中直接展現的文本（如標語、標籤等）時，有更好的拼寫能力和文本理解。這包括更準確地識別和渲染用户提示中的文字，甚至是在複雜的視覺背景中。</li></ol><p>Stable Diffusion 3 的性能提升不僅基於其先進的擴散變換架構，還包括了以下關鍵的技術創新和改進：</p><ol><li>新型擴散變換器： Stable Diffusion 3 採用了一種新型的擴散變換技術，與 Sora 類似，這種新技術為模型提供了更強大的圖像生成能力。 Transformer 是一種深度學習模型，專門設計來逐步構建圖像的細節，從而生成高質量的視覺內容。</li><li>流匹配與其他改進： 模型還整合了流匹配技術和其他技術改進，進一步增強了生成圖像的質量和多樣性。流匹配技術有助於模型更好地理解和模擬圖像中的動態元素和結構，使得生成的圖像在視覺上更加連貫和自然。</li><li>利用 Transformer 的改進： Stable Diffusion 3 充分利用了 Transformer 技術的最新進展，這不僅使模型能夠進一步擴展其能力，還使其能夠接受多模態輸入。這意味着模型能夠處理更復雜和多樣化的數據類型，如結合文本和圖像的輸入，從而在理解和生成圖像內容方面提供更大的靈活性和精確度。</li></ol><p>加入等候名單：<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fstablediffusion3" target="_blank">https://stability.ai/stablediffusion3</a></em></u></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:12:30 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279886/stable-diffusion-3-preview</guid>
            <link>https://www.oschina.net/news/279886/stable-diffusion-3-preview</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[「未來產業劃定發展路線圖」出爐]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span>近日，工業和信息化部、科技部、交通運輸部、文化和旅遊部等部門</span><span>聯合印發</span><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMjM5OTUwMTc2OA%3D%3D%26mid%3D2650903280%26idx%3D1%26sn%3Db6edd0ce1c41aac431cd9f9a52eef8be%26chksm%3Dbccfb8578bb831412b6e96f801604afe81effbd68c11d7371507f00349b1b3accc4cc18e01b4%26scene%3D21%23wechat_redirect" target="_blank">《關於推動未來產業創新發展的實施意見》</a><span>，提出到 2025 年，未來產業技術創新、產業培育、安全治理等全面發展，部分領域達到國際先進水平，產業規模穩步提升；到 2027 年，未來產業綜合實力顯著提升，部分領域實現全球引領。</span></p><p>專家認為，《意見》充分把握全球科技創新和產業發展趨勢，前瞻部署了生物製造、量子信息、氫能、核能、基因和細胞技術等多個細分賽道，將全面支撐推進新型工業化，加快形成新質生產力。</p><p style="margin-left:0; margin-right:0"><strong><span>全面佈局新賽道</span></strong></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>未來產業由前沿技術驅動，尚處於孕育萌發階段或產業化初期，是具有顯著戰略性、引領性、顛覆性和不確定性的前瞻性新興產業。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">當前，新一輪科技革命和產業變革加速演進，重大前沿技術、顛覆性技術持續湧現，科技創新和產業發展融合不斷加深，催生出元宇宙、人形機器人、腦機接口、量子信息等新產業發展方向。大力培育未來產業已成為引領科技進步、帶動產業升級、開闢新賽道、塑造新質生產力的戰略選擇。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">我國具備工業體系完整、產業規模龐大、應用場景豐富等綜合優勢，為未來產業發展提供了豐厚的土壤。各省（區、市）積極培育未來產業，北京、上海、江蘇、浙江等地出台了培育未來產業的政策文件。但我國未來產業發展也面臨系統謀劃不足、技術底座不牢等問題。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">針對這些問題，《意見》從技術創新、產品突破、企業培育、場景開拓、產業競爭力等方面提出到 2025 年和 2027 年的發展目標。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">賽迪研究院未來產業研究中心所長韓健介紹，到 2025 年要形成「一批+6 百」的目標體系，建設一批未來產業孵化器和先導區，突破百項前沿關鍵核心技術，形成百項標誌性產品，打造百家領軍企業，開拓百項典型應用場景，制定百項關鍵標準，培育百家專業服務機構，初步形成符合我國實際的未來產業發展模式。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">《意見》重在產業化落地。賽智產業研究院院長趙剛認為，《意見》提出以傳統產業的高端化升級和前沿技術的產業化落地為主線，力爭做到兩年「打基礎」，五年「大提升」，成為世界未來產業重要策源地。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">此外，《意見》還詳細規劃了六大方向超過 50 多個細分領域的未來產業發展，明確提出了下一代智能終端、信息服務產品、未來高端裝備三類標誌性產品發展路線。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">「設定未來產業發展目標既是我國推進新型工業化的自身現實需求，也是參與國際競爭的外部形勢要求。從自身需求看，是我國引領科技進步、帶動產業升級、培育新質生產力的戰略選擇；從外部需求看，是我國主動參與全球未來產業分工合作、深度融入全球創新網絡的必然選擇。」趙剛説。</p><p style="margin-left:0; margin-right:0"><strong><span>重點瞄準六大方向</span></strong></p><p><span>未來產業發展的核心是前沿技術創新突破。《意見》按照「技術創新—前瞻識別—成果轉化」的思路，提出面向未來製造、未來信息、未來材料、未來能源、未來空間、未來健康六大重點方向，實施國家科技重大項目和重大科技攻關，發揮國家實驗室、全國重點實驗室等創新載體作用，鼓勵龍頭企業牽頭成立創新聯合體，體系化推進關鍵核心技術攻關。</span></p><p>趙剛分析，與優勢產業、傳統產業、戰略性新興產業相比，未來產業有 3 個明顯特徵。未來產業技術創新不是漸進式微創新，而是前瞻性、顛覆性重大創新，例如未來信息產業中的通用人工智能和量子信息、未來健康產業中的基因工程、未來材料產業中的超導材料等技術創新；未來產業生產要素配置不是傳統要素線性疊加，而是現代要素相互融合和配置效率指數級提升，例如量子計算機能讓計算能力實現成千上萬倍增加；未來產業邊界不是界限清晰，而是呈現出不同產業跨界融合和智能化、綠色化等發展特徵，如智能製造、生物材料、人形機器人、腦機接口等。</p><p>對於這六大方向業內已有佈局。早在 2016 年，字節跳動公司就成立了人工智能實驗室，聚焦研究自然語言處理、機器學習、數據挖掘等方面。2023 年以來，字節跳動公司加碼人工智能應用研究，旗下產品不斷加入 AIGC（生成式人工智能）功能。比如，結合火山引擎智能創作雲的 AIGC 能力，火山引擎視頻雲在商品營銷、互動娛樂、在線教育、智能駕駛等場景引入數字人、虛擬直播間等，助力企業降本增效，提升用户體驗。</p><p>「技術創新是經濟長期持續增長的不竭動力，發展未來產業是高質量發展的前瞻性戰略佈局。今天對未來產業 20% 的投入和佈局，將為以後帶來 80% 的收益，從而建立起我國經濟高質量發展的長效創新機制。」趙剛説。</p><p style="margin-left:0; margin-right:0"><strong><span>打造標誌性創新產品</span></strong></p><p><span>《意見》提出，打造人形機器人、腦機接口、超大規模新型智算中心、第三代互聯網等十大創新標誌性產品。</span></p><p>趙剛分析，當顛覆式技術創新呈現出技術性能成倍提升、產品化成本大幅降低、應用場景廣泛等特徵後，創新產品就形成規模經濟效應，具有巨大的市場前景。</p><p>當前，滿足這 3 個特徵的標誌性產品主要有兩類。一是通用人工智能產品。由於以 ChatGPT 為代表的通用人工智能技術取得重大進展，圍繞通用人工智能技術創新形成的智能產品，如生成式人工智能產品、AI 手機和個人計算機、人形機器人、高級別智能網聯汽車、智能裝備、智能雲服務、超大規模新型智算中心等智能產品和服務就具有較好前景。二是生物科技產品。由於細胞和基因工程等技術取得突破性進展，生物科技創新產品工程化能力加速提升，具有很好的市場前景，如基因編輯、合成生物等。其他一些前瞻性技術儘管在實驗室獲得了成功，但離大規模產品化和商業化還有很大差距，例如量子信息技術創新。</p><p>國際數據公司 IDC 預測，人工智能電腦在中國個人計算機市場中新機的裝配比例將快速攀升，2027 年有望達 85%，成為市場主流。聯想集團副總裁、中國區戰略及業務拓展副總裁阿不力克木·阿不力米提表示，人工智能電腦是自然語言交互的個人 AI 助理。在過去 40 年發展歷程中，聯想不斷推出變革用户體驗的產品，未來還將和生態夥伴攜手實現人工智能電腦快速普及，讓 AI 惠及每一個人。</p><p>目前我國算力總規模排名全球第二位。但從結構看，通用算力佔了大半，高性能算力佔比有待提升。浪潮信息高級副總裁劉軍表示，高質量算力採用先進的計算架構，具備高算效、高能效、可持續、可獲得、可評估五大特徵。其中，高算效是實測性能與資源利用率雙重提升，是算力供需失衡、算力利用率低等矛盾的破解之道。而高能效是在最低碳排放前提下實現最大化算力輸出，確保能源利用最優解。</p><p>腦機接口作為十大標誌性產品之一，近年來在電極、算法、芯片等方面取得了重要進展。2023 年 9 月，中國信息通信研究院雲計算與大數據研究所牽頭在人工智能醫療器械創新合作平台成立腦機接口研究工作組。</p><p>中國信通院雲計算與大數據研究所副所長閔棟介紹，腦機接口可應用於醫療、娛樂、智能生活、教育等領域。其中，醫療領域是主要陣地。腦機接口與醫療結合展現出廣闊應用前景，為相關疾病診療和康復提供了全新手段。此外，腦機接口還可與虛擬現實、人機交互、人工智能等技術結合推動現有產業變革，如腦機接口應用於工業領域，可幫助人們通過意念操控機器人、無人車、工業產線等設備。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">未來產業潛在價值巨大，需要資本持續投入。趙剛建議，要推動製造業轉型升級基金、國家中小企業發展基金等加大投入，也可適時組建國家未來產業發展基金，並引導地方設立未來產業專項資金，發揮政府引導基金的引導性作用，吸引社會資本共同投資未來產業。同時，完善金融財税支持政策。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 05:58:15 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279756</guid>
            <link>https://www.oschina.net/news/279756</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[夜鶯監控 V7 第一個 beta 版本來了]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>夜鶯項目從 2024 開始開發 V7 版本，重點做體驗優化，V7 和 V6 版本兼容可以平滑升級<span style="background-color:#ffffff; color:#333333">（V6 升級到 V7 只需要替換一下二進制重啓即可，如果是容器部署，只需要更新鏡像並重啓）</span>，今天發佈第一個 beta，此 beta 版本的優化項包括：</p><ul><li>全站暗黑主題</li><li>優化邊緣機房機器失聯告警的實現邏輯，真正做到邊緣機房告警自閉環</li><li>優化內置大盤、內置告警規則的列表頁面 UI</li><li>全局回調地址頁面展示優化，增加詳盡的文檔提示信息</li></ul><p><span style="background-color:#ffffff; color:#333333">更多優化項正在開發中，V5、V6 用户可以放心升級，V7 會是一個更好的版本。升級之前記得備份以防萬一。</span></p><h2>項目介紹</h2><p style="color:#333333; text-align:left">夜鶯監控是一款開源雲原生觀測分析工具，採用 All-in-One 的設計理念，集數據採集、可視化、監控告警、數據分析於一體，與雲原生生態緊密集成，提供開箱即用的企業級監控分析和告警能力。夜鶯於 2020 年 3 月 20 日，在 github 上發佈 v1 版本，已累計迭代 100 多個版本。</p><p style="color:#333333; text-align:left">夜鶯最初由滴滴開發和開源，並於 2022 年 5 月 11 日，捐贈予中國計算機學會開源發展委員會（CCF ODC），為 CCF ODC 成立後接受捐贈的第一個開源項目。夜鶯的核心研發團隊，也是 Open-Falcon 項目原核心研發人員，從 2014 年（Open-Falcon 是 2014 年開源）算起來，也有 10 年了，只為把監控這個事情做好。</p><h2>項目截圖</h2><p style="color:#333333; text-align:left"><img alt="20240221141801" src="https://download.flashcat.cloud/ulric/20240221141801.png" referrerpolicy="no-referrer"></p><p style="color:#333333; text-align:left"><img alt="20240221141817" src="https://download.flashcat.cloud/ulric/20240221141817.png" referrerpolicy="no-referrer"></p><h2>項目代碼</h2><ul><li>後端：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fccfos%2Fnightingale" target="_blank">💡 https://github.com/ccfos/nightingale</a></li><li>前端：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fn9e%2Ffe" target="_blank">💡 https://github.com/n9e/fe</a></li></ul><p style="color:#333333; text-align:left">夜鶯項目已收穫 8000 多 github stars，1000 多 forks，100 多 contributors 參與其中，歡迎大家在<span>&nbsp;</span><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fccfos%2Fnightingale" target="_blank"><strong>GitHub</strong></a></strong><span>&nbsp;</span>上關注夜鶯項目，及時獲取項目更新動態，有任何問題，也歡迎提交 issues，以及提交 pull requests，開源社區需要大家一起參與才能有蓬勃的生命力。</p><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 04:15:52 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279741/nightingale-7-0-0-beta-0-released</guid>
            <link>https://www.oschina.net/news/279741/nightingale-7-0-0-beta-0-released</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[開源日報：等到 Sora 開源了立刻推出屬於我們自己的大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>歡迎閲讀 OSCHINA 編輯部出品的開源日報，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.21</strong></span></h3><h2><strong><span style="color:#16a085">今日要點</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/279326/linux-kernel-is-a-cna" target="_blank">2023 年度 Rust 調查報告</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000">有 93% 的受訪者稱自己是 Rust 用户，其中 49% 的人每天（或幾乎每天）都會使用 Rust，相較上一年小幅增加 2 個百分點。在沒有使用 Rust 的用户中，31% 的人表示主要原因時使用 Rust 有難度；67% 的人表示他們還沒有機會優先學習 Rust，這也是最常見的原因。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000"><img alt="" height="429" src="https://oscimg.oschina.net/oscnet/up-335afef97d65f03d34d0f08eb8831153557.png" width="500" referrerpolicy="no-referrer"></span></p><h3><a href="https://www.oschina.net/news/279389/dart-3-3-released" target="_blank">Go 1.22 正式發佈</a></h3><p>Go 1.22 中新增的優化之一是改進了虛擬化，允許靜態調度更多的接口方法調用。啓用 PGO 後，大多數程序的性能將提高 2% 至 14%。 此外，Go 運行時中的內存優化可將 CPU 性能提高 1-3%，同時還可將大多數 Go 程序的內存開銷減少約 1%。</p><p>新的 math/rand/v2 軟件包提供了更簡潔、更一致的應用程序接口，並使用了質量更高、速度更快的偽隨機生成算法。&nbsp;</p><hr><h2><strong><span style="color:#16a085">今日觀察</span></strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-9fc6f45ad8365450e0980fe7fe1855a56c5.png" referrerpolicy="no-referrer"></p><p>- 微博 <u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F6640496217%2FO1tM7BenZ" target="_blank">-赤犬龍之介-</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-d423a4b13075c233da7226ccd5235dc0a3e.png" referrerpolicy="no-referrer"></p><p>-&nbsp;<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fishare.ifeng.com%2Fc%2Fs%2F8XMQgC7LlaK" target="_blank">21 世紀經濟報道</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推薦</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-25f6c0a5f5becd775a0ad6bd1080893da87.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>開源之聲</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-bbc6216c930e22f859d514d8becc057daae.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日項目榜</strong></span></h2><p>Gitee 榜單：</p><p><img src="https://oscimg.oschina.net/oscnet/up-5ab4f59bdf01add8c61aac9c617fa074d2c.png" referrerpolicy="no-referrer"></p><hr><p><strong>往期回顧</strong></p><ul><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">開源日報第 012 期：Sora 給中國 AI 帶來的真實變化；Dart 3.3 發佈</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">開源日報第 011 期：目前還沒有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">開源日報第 010 期：Tauri v2 支持 Android 和 iOS，跨平台開發新選擇</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">開源日報第 009 期：Vue.js 誕生 10 週年；扎克伯格解釋 Meta 為什麼要開源其 AI 技術</a></li><li><a href="https://www.oschina.net/news/277585">開源日報第 008 期：推動中國開源軟硬件發展的經驗與建議</a></li><li><a href="https://www.oschina.net/news/277415">開源日報第 007 期：「Linux 中國」 開源社區宣佈停止運營</a></li><li><a href="https://www.oschina.net/news/277214">開源日報第 006 期：選擇技術棧一定要選擇開源的</a></li><li><a href="http://www.oschina.net/news/277040">開源日報第 005 期：RISC-V 萬兆開源交換機發售；npm 存在大量武林外傳視頻</a></li><li><a href="https://www.oschina.net/news/276864">開源日報第 004 期：百度輸入法在候選詞區域植入廣告；大神用 Excel 構建 CPU</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:47:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279735</guid>
            <link>https://www.oschina.net/news/279735</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[數百位名人簽署公開信，呼籲制定反深度偽造立法]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>數百名 AI 界人士簽署了一封<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenletter.net%2Fl%2Fdisrupting-deepfakes" target="_blank">公開信</a>，呼籲嚴格監管 AI 生成的冒名頂替或深度偽造（Deepfake）內容。</p><p><img height="274" src="https://oscimg.oschina.net/oscnet/up-df56f62fff93b887bf1198aca4ccf4bcb5e.png" width="500" referrerpolicy="no-referrer"></p><p>公開信指出，"深度偽造"是指未經同意或嚴重誤導的人工智能生成的聲音、圖像或視頻，合理的人會誤以為是真實的。這不包括對圖像或聲音的輕微改動，也不包括容易識別為合成的無害娛樂或諷刺。</p><p>如今，深度偽造通常涉及性圖像、欺詐或政治虛假信息。由於人工智能發展迅速，使得深度偽造變得更加容易，因此我們需要為數字基礎設施的運行和完整性提供保障。「Deepfakes 對社會的威脅日益嚴重，政府必須在整個供應鏈中施加義務，以阻止 Deepfakes 的擴散。」</p><p>信中呼籲：</p><ul><li>將深度偽造的兒童性虐待材料（CSAM，又名兒童色情製品）完全定為刑事犯罪，無論所描繪的人物是真實的還是虛構的。</li><li>在任何情況下，如果有人制造或傳播有害的深度偽造品，都需要受到刑事處罰。</li><li>要求軟件開發商和分銷商防止其音頻和視頻產品被用於製造有害的深度偽造品，如果他們的預防措施不充分，就要承擔責任接受處罰。</li></ul><p>他們認為，如果設計得當，這些法律可以在不會造成過重負擔的同時，培育有社會責任感的企業。</p><p>這封信中較為知名的簽名者包括：</p><ul><li>Jaron Lanier</li><li>Frances Haugen</li><li>Stuart Russell</li><li>Andrew Yang</li><li>Marietje Schaake</li><li>Steven Pinker</li><li>Gary Marcus</li><li>Oren Etzioni</li><li>Genevieve smith</li><li>Yoshua Bengio</li><li>Dan Hendrycks</li><li>Tim Wu</li></ul><p>事實上，這也不是首次出現相關的呼籲。在本月早些時候正式提出之前，歐盟已就此類措施進行了多年辯論。外媒 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2024%2F02%2F21%2Fhundreds-of-ai-luminaries-sign-letter-calling-for-anti-deepfake-legislation%2F" target="_blank">TechCrunch</a> 指出，也許正是歐盟願意進行審議和落實，才激活了這些研究人員、創作者和管理人員的發言權。雖然此舉不一定能推動真正的立法，但它確實是業界專家們如何看待這一爭議問題的風向標。</p><p>更多詳情可查看此處：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenletter.net%2Fl%2Fdisrupting-deepfakes" target="_blank">https://openletter.net/l/disrupting-deepfakes</a>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:32:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279733/disrupting-deepfakes</guid>
            <link>https://www.oschina.net/news/279733/disrupting-deepfakes</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[出門問問創始人李志飛點評谷歌開源大模型 Gemma：差點意思]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌在北京時間昨晚發佈了<u><a href="https://www.oschina.net/news/279713/google-gemma-open-models">開源大模型 Gemma</a></u>，對標 Meta 旗下 Llama 2。</p><p><img src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><p>出門問問創始人李志飛發表文章點評，<strong>稱 Gemma 推出時間有點晚、開源力度不夠、未放下高貴的頭顱</strong>。</p><p>李志飛在文章中表示，相比於去年上半年就開源，現在可能要花數倍的努力進行模型的差異化以及推廣的投入，才有可能在眾多開源模型中脱穎而出。「面對 OpenAI 的強力競爭，只有殺敵一千、自損一千五。」</p><p>以下為李志飛全文：</p><blockquote><p>看到 Google 開源了小的語言模型 Gemma，直接狙擊 Llama 2，回顧去年 5 月對 Google 關於開源和競爭的看法，幾點思考如下：</p><p>1. 時間有點晚：相比於去年上半年就開源，現在可能要花數倍的努力進行模型的差異化以及推廣的投入，才有可能在眾多開源模型中脱穎而出。</p><p>2. 開源力度不夠：感覺這次開源還是被動防禦和略顯扭捏的應對之策，不是進攻。比如説，開個 7B 的模型實在是太小兒科了，一點殺傷力都沒有。應該直接開源一個超越市場上所有開源的至少 100B 的模型、1M 的超長上下文、完善的推理 infra 方案、外加送一定的 cloud credit。是的，再不歇斯底里 Google 真的就晚了。面對 OpenAI 的強力競爭，只有殺敵一千、自損一千五。</p><p>3. 未放下高貴的頭顱：有種感覺，Google 覺得自己還是 AI 王者，放不下高貴的頭顱，很多發佈都有點不痛不癢，還是沿着過去研發驅動的老路而不是產品和競爭驅動，比如不停發論文、取新名字（多模態相關模型過去半年就發了 Palme、RT-2、Gemini、VideoPoet、W.A.L.T 等）、發佈的模型又完整度不夠，感覺就沒有一個絕對能打的產品。Google 可能要意識到在公眾眼中，他在 AI 領域已經是廉頗老矣潰不成軍，經常起大早趕晚集（比如説這次 Sora 借鑑的 ViT、ViViT、NaVit、MAGVit 等核心組件技術都是它家寫的論文）。</p><p>4. 希望亡羊補牢未為晚也：Google 作為一個僵化的大公司，動作慢一點可以理解，但是如果再不努力是不是就是 PC 互聯網的 IBM、移動互聯網的 Microsoft？ 作為 Google 的鐵粉，還是希望他能打起精神一戰，AI 產業需要強力的競爭才能不停向前發展，也需要他在前沿研究和系統的開源才能幫助一大眾「貧窮」的 AI 創業公司。</p><p>5. 另外，除了對外開源外，Google 應該組成三個方陣面對大模型的競爭，詳見去年 3 月發文。</p><p>回顧科技競爭史，PC 互聯網時代的 IBM、移動互聯網時代的 Microsoft、AGI 時代的 Google，新時代來臨後，難道上一個時代科技霸主都難逃衰落的宿命？</p><p>當然，Microsoft 靠 Office SaaS、雲和 OpenAI 又翻盤了。</p><p>歷史的鐵律，有被改寫的可能嗎？</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:15:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279728</guid>
            <link>https://www.oschina.net/news/279728</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenAI 員工的一天]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Chain Of Thought (CoT) 第一作者、從谷歌跳槽到 OpenAI 的 Jason Wei 分享了自己在 OpenAI 的一天：</p><blockquote><p>[9:00am] 起牀</p><p>[9:30am] 搭乘 Waymo 前往 Mission SF，途中在 Tartine 買個牛油果吐司</p><p>[9:45am] 背誦 OpenAI 章程。向優化之神致敬，學習《The Bitter Lession》（強化學習之父 Rich Sutton 著）</p><p>[10:00am] 在 Google Meet 上開會，討論如何在更多數據上訓練更大的模型</p><p>[11:00am] 敲代碼，在更多數據上訓練更大的模型。搭檔是 Hyung Won Chung</p><p>[12:00pm] 去食堂吃午飯（純素且無麩質）</p><p>[1:00pm] 真正開始在大量數據上訓練大模型</p><p>[2:00pm] 處理基礎設施問題（我是腦子被驢踢了嗎為啥要從 master 分支拉代碼？）</p><p>[3:00pm] 監控模型訓練進度，玩 Sora</p><p>[4:00pm] 給剛才訓練的大模型上提示工程</p><p>[4:30pm] 短暫休息，坐在牛油果椅子上，思考 Gemini Ultra 的性能到底有多強</p><p>[5:00pm] 頭腦風暴模型可能的算法改進</p><p>[5:05pm] 修改算法風險太高，pass。最安全的策略還是力大磚飛（增加算力和數據規模）</p><p>[6:00pm] 晚餐時間，和 Roon 一起享用蛤蜊濃湯</p><p>[7:00pm] 回家</p><p>[8:00pm] 喝點小酒，繼續寫碼。Ballmer’s peak（酒精帶來的編碼高效階段）即將到來</p><p>[9:00pm] 分析實驗結果，我對 wandb 又愛又恨</p><p>[10:00pm] 啓動實驗，讓它自己跑一晚上，第二天查看結果</p><p>[1:00am] 實驗真正開始運行</p><p>[1:15am] 上牀睡覺。在納德拉和黃仁勳的守護下入夢。心想：壓縮才是真諦。晚安</p><p><img src="https://oscimg.oschina.net/oscnet/up-3041206e9fa5084776ebedb71c760828888.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2F_jasonwei%2Fstatus%2F1760032264120041684" target="_blank">https://twitter.com/_jasonwei/status/1760032264120041684</a></u></em></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:46:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279722</guid>
            <link>https://www.oschina.net/news/279722</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[StarkNet 撒錢，程序員在 Web3 領了 14 萬 ​​​]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>StarkNet 啓動空投活動，GitHub 排名前 5000 開源項目的貢獻者可領取價值 $200 獎勵。</p><h2>背景</h2><ul><li>StarkNet 公鏈項目為了激勵開發者參與其平台建設，啓動了空投活動。</li><li>如果曾向 GitHub 上獲得較多 Star 的項目提交過 PR ，就有資格領取 111.1 STRK 的空投獎勵。</li><li>只需要使用 OAuth 2.0 登錄，就可以直接領取。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-44f62dafa84c34781d15d46c71c7c8c862c.jpg" referrerpolicy="no-referrer"></p><p>有程序員曬出自己在這次空投活動獲得的獎勵——近 14 萬人民幣。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-96e62f68c4891cdbfd1f43b86ddfcdcbf79.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2Fstrrlthedev%2Fstatus%2F1760182038504837287" target="_blank">https://twitter.com/strrlthedev/status/1760182038504837287</a></u></em></p></blockquote><h2>領取規則</h2><ol><li>截止到 2023 年 11 月 15 日，至少對全球排名前 5000 的倉庫提交過三次代碼貢獻。</li><li>其中至少有一次貢獻是在 2018 年或之後完成的。</li></ol><h2>領取步驟</h2><p>領取地址：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fprovisions.starknet.io%2F" target="_blank">https://provisions.starknet.io/</a></u></em></p><ol><li>訪問獎勵領取頁面並連接錢包（推薦使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchromewebstore.google.com%2Fdetail%2Fargent-x-starknet-wallet%2Fdlcobpjiigpikoobohmabehhmhfoodbb" target="_blank">Argent X</a>）。</li><li>通過 GitHub 登錄，採用 OAuth 2.0 驗證方式。</li><li>直接領取獎勵。後續的治理投票和問卷調查可忽略不計。</li></ol></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:30:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279720</guid>
            <link>https://www.oschina.net/news/279720</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[華為：2 月 26 日將首發華為通信大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">華為官方微信公眾號消息<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F7dnrCXQoOqWSfKBrCJWl6Q" target="_blank">顯示</a>， 在 2 月 26 日華為產品與解決方案發佈會上，即將首發華為通信大模型。</span></p><p><img height="677" src="https://oscimg.oschina.net/oscnet/up-59659387ba8ade9e0cf71c6f84d476a728e.png" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">華為稱，2024 年，5G-A 商用元年正式開啓！萬兆時代已來，共同見證將 5G-A 帶入現實。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:23:42 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279717</guid>
            <link>https://www.oschina.net/news/279717</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[谷歌發佈輕量級開源大語言模型 Gemma]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌發佈了開源大語言模型 Gemma，這是一款輕量級、先進的開源模型，供開發者和研究人員用於 AI 構建。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><p>Gemma 模型家族包括 2B（20 億參數）和 7B（70 億參數）兩種尺寸，能夠在不同的設備類型上運行，包括筆記本電腦、桌面電腦、IoT 設備、移動設備和雲端。</p><p><strong>性能和設計</strong></p><p>Gemma 模型在技術和基礎設施組件上與 Gemini 共享，這使得 Gemma 2B 和 7B 在其大小範圍內相比其他開放模型具有最佳性能。</p><p>Gemma 模型不僅可以直接在開發者的筆記本電腦或桌面電腦上運行，而且在關鍵基準測試中的表現超過了更大的模型，同時遵循嚴格的安全和負責任輸出標準。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-50f9213a30f71b22bb88d2abb40a2f7a116.png" referrerpolicy="no-referrer"></p><p><strong>主要特點</strong></p><ol><li><strong>輕量級、高性能模型</strong>：Gemma 模型家族包括 Gemma 2B 和 Gemma 7B 兩種尺寸，提供預訓練和指令調優的變體，針對其大小範圍內相比其他開放模型具有最佳性能。</li><li><strong>跨框架工具鏈支持</strong>：支持 JAX、PyTorch 和 TensorFlow 通過原生 Keras 3.0 進行推理和監督式微調（SFT），適應多種開發需求和環境。</li><li><strong>易於入門和集成</strong>：提供準備就緒的 Colab 和 Kaggle 筆記本，以及與 Hugging Face、MaxText、NVIDIA NeMo 和 TensorRT-LLM 等流行工具的集成，方便開發者快速上手。</li><li><strong>高效的運算能力</strong>：針對多個 AI 硬件平台上進行優化，確保在 NVIDIA GPU 和 Google Cloud TPU 上的行業領先性能。通過與 NVIDIA 的合作，無論是在數據中心、雲端還是本地 RTX AI PC 上，都確保了行業領先的性能和與尖端技術的集成。</li></ol><p>Gemma 模型能夠在不同的設備類型上運行，包括筆記本電腦、桌面電腦、IoT 設備、移動設備和雲端。這種廣泛的兼容性使得模型能夠適應各種應用場景和需求。</p><ul><li><span>模型地址：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhuggingface.co%2Fmodels%3Fother%3Dgemma%26sort%3Dtrending%26search%3Dgoogle" target="_blank">https://huggingface.co/models?other=gemma&amp;sort=trending&amp;search=google…</a><span></span></li><li><span>博客：</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgemma-open-models%2F" target="_blank">https://blog.google/technology/developers/gemma-open-models/</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:17:47 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279713/google-gemma-open-models</guid>
            <link>https://www.oschina.net/news/279713/google-gemma-open-models</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[你好，iLogtail 2.0！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>作者：張浩翔（篤敏）</p><h2>概述</h2><p>隨着可觀測數據採集需求的不斷推陳出新，多樣化的數據輸入輸出選項、個性化的數據處理能力組合、以及高性能的數據處理吞吐能力已經成為頂流可觀測數據採集器的必備條件。然而，由於歷史原因，現有的 iLogtail 架構和採集配置結構已經無法繼續滿足上述需求，逐漸成為制約 iLogtail 繼續向前快速演進的瓶頸：</p><p>▶︎ iLogtail 設計之初完全面向文件日誌採集至日誌服務的場景：</p><p>1）簡單地將日誌分為多種格式，每種格式的日誌僅支持一種處理方式（如正則解析、Json 解析等）；</p><p>2）功能實現與日誌服務相關概念（如 Logstore 等）強綁定；</p><p>基於此設計思想，現有的 iLogtail 架構偏向於單體架構，導致模塊間耦合嚴重，可擴展性和普適性較差，難以提供多個處理流程級聯的能力。</p><p>▶︎ Golang 插件系統的引入極大地擴展了 iLogtail 的輸入輸出通道，且一定程度提升了 iLogtail 的處理能力。然而，囿於 C++ 部分的實現，輸入輸出與處理模塊間的組合能力仍然嚴重受限：</p><p>1）C++ 部分原生的高性能處理能力仍然僅限於採集日誌文件並投遞至日誌服務的場景使用；</p><p>2）C++ 部分的處理能力無法與插件系統的處理能力相結合，二者只能選其一，從而降低了複雜日誌處理場景的性能。</p><p>▶︎ 與 iLogtail 整體架構類似，現有的 iLogtail 採集配置結構也採用平鋪結構，缺乏處理流水線的概念，無法表達處理流程級聯的語義。</p><p>基於上述原因，在 iLogtail 誕生 10 週年之際，日誌服務啓動對 iLogtail 的升級改造，寄希望於讓 iLogtail 的易用性更佳，性能更優，可擴展性更強，從而更好地服務廣大用户。</p><p>目前，經過半年多的重構與優化，iLogtail 2.0 已經呼之欲出。接下來，就讓我們來搶先了解一下 iLogtail 2.0 的新特性吧！</p><h2>新特性</h2><h3>（一）【商業版】採集配置全面升級流水線結構</h3><p>為瞭解決舊版採集配置平鋪結構無法表達複雜採集行為的問題，iLogtail 2.0 全面擁抱新版流水線配置，即每一個配置對應一條處理流水線，包括輸入模塊、處理模塊和輸出模塊，每個模塊由若干個插件組成，各模塊的插件功能如下：</p><ul><li><strong>輸入插件：</strong> 用於從指定輸入源獲取數據（各插件具體功能詳見輸入插件 <strong>[</strong><strong>1]</strong> ）</li><li><strong>處理插件：</strong> 用於對日誌進行解析和處理（各插件具體功能詳見處理插件 <strong>[</strong><strong>2]</strong> ），可進一步分為原生處理插件和擴展處理插件</li></ul><p>&lt;!----&gt;</p><ul><li>原生處理插件：性能較優，適用於大部分業務場景，推薦優先使用</li><li>擴展處理插件：功能覆蓋更廣，但性能劣於原生處理插件，建議僅在原生處理插件無法完成全部處理需求時使用</li></ul><p>&lt;!----&gt;</p><ul><li><strong>輸出插件：</strong> 用於將處理後的數據發送至指定的存儲</li></ul><p>我們可以用一個 JSON 對象來表示一個流水線配置：</p><p><img src="https://oscimg.oschina.net/oscnet/up-f6cc95f0ae9e478bd8c674f6fae4ee9ceb9.png" alt="" referrerpolicy="no-referrer"></p><p>其中，inputs、processors 和 flushers 即代表輸入、處理和輸出模塊，列表中的每一個元素 {...} 即代表一個插件；global 代表流水線的一些配置。有關流水線配置結構的具體信息，可參見 iLogtail 流水線配置結構 <strong>[</strong><strong>3]</strong> 。</p><blockquote><p>示例：採集 /var/log 目錄下的 test.log，對日誌進行 json 解析後發送到日誌服務。以下是實現該採集需求對應的舊版和新版配置，可以看到新版配置十分精煉，執行的操作一目瞭然。</p><p><strong>舊版配置：</strong></p><pre><code>{
&nbsp;&nbsp;&nbsp;&nbsp;"configName":&nbsp;"test-config",
&nbsp;&nbsp;&nbsp;&nbsp;"inputType":&nbsp;"file",
&nbsp;&nbsp;&nbsp;&nbsp;"inputDetail":&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"topicFormat":&nbsp;"none",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"priority":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logPath":&nbsp;"/var/log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"filePattern":&nbsp;"test.log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"maxDepth":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tailExisted":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"fileEncoding":&nbsp;"utf8",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logBeginRegex":&nbsp;".*",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerFile":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerIncludeLabel":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerExcludeLabel":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerIncludeEnv":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"dockerExcludeEnv":&nbsp;{},
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"preserve":&nbsp;true,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"preserveDepth":&nbsp;1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"delaySkipBytes":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"delayAlarmBytes":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logType":&nbsp;"json_log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"timeKey":&nbsp;"",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"timeFormat":&nbsp;"",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"adjustTimezone":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logTimezone":&nbsp;"",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"filterRegex":&nbsp;[],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"filterKey":&nbsp;[],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"discardNonUtf8":&nbsp;false,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"sensitive_keys":&nbsp;[],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"mergeType":&nbsp;"topic",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"sendRateExpire":&nbsp;0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"maxSendRate":&nbsp;-1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"localStorage":&nbsp;true
&nbsp;&nbsp;&nbsp;&nbsp;},
&nbsp;&nbsp;&nbsp;&nbsp;"outputType":&nbsp;"LogService",
&nbsp;&nbsp;&nbsp;&nbsp;"outputDetail":&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"logstoreName":&nbsp;"test_logstore"
&nbsp;&nbsp;&nbsp;&nbsp;}
}
</code></pre><p><strong>新版流水線配置：</strong></p><pre><code>{
&nbsp;&nbsp;&nbsp;&nbsp;"configName":&nbsp;"test-config",
&nbsp;&nbsp;&nbsp;&nbsp;"inputs":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"file_log",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"FilePaths":&nbsp;"/var/log/test.log"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp;&nbsp;&nbsp;"processors":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_parse_json_native"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"SourceKey":&nbsp;"content"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp;&nbsp;&nbsp;"flushers":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"flusher_sls",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Logstore":&nbsp;"test_logstore"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;]
}
</code></pre><p>如果在執行 json 解析後需要進一步處理，在流水線配置中只需額外增加一個處理插件即可，但是在舊版配置中已經無法表達上述需求。</p></blockquote><p>有關新版流水線配置和舊版配置的兼容性問題，請參見文末兼容性説明板塊。</p><h4>全新 API</h4><p>為了支持流水線配置，同時區分舊版配置結構，我們提供了全新的用於管理流水線配置的 API 接口，包括：</p><ul><li>CreateLogtailPipelineConfig</li><li>UpdateCreateLogtailPipelineConfig</li><li>GetLogtailPipelineConfig</li><li>DeleteLogtailPipelineConfig</li><li>ListLogtailPipelineConfig</li></ul><p>有關這些接口的詳細信息，請參見 OpenAPI 文檔 <strong>[</strong><strong>4]</strong> 。</p><h4>全新控制枱界面</h4><p>與流水線採集配置結構相對應，前端控制枱界面也進行了全新升級，分為了全局配置、輸入配置、處理配置和輸出配置。</p><p><img src="https://oscimg.oschina.net/oscnet/up-52a9a2935a7738f4ddd6cc8123ae3551f2d.png" alt="" referrerpolicy="no-referrer"></p><p>與舊版控制枱界面相比，新版控制枱具有如下特點：</p><p><strong>參數內聚：</strong> 某一功能相關的參數集中展示，避免了舊版控制枱參數散落各處出現漏配置。</p><blockquote><p>示例：最大目錄監控深度與日誌路徑中的**密切相關，舊版界面中，二者分隔較遠，容易遺忘；在新版界面中，二者在一起，便於理解。</p><p><strong>舊版控制枱：</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-04d5bc90063cbedb86c8049d54635e3023c.png" alt="" referrerpolicy="no-referrer"></p><p><strong>新版控制枱：</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-b559b0e4ad0b5877d16aa684f9ba4d50d00.png" alt="" referrerpolicy="no-referrer"></p></blockquote><p><strong>所有參數均為有效參數：</strong> 在舊版控制枱中，啓用插件處理後，部分控制枱參數會失效，從而引起不必要的誤解。新版控制枱所有參數均為有效參數。</p><h4>全新 CRD</h4><p>同樣，與新版採集配置對應，K8s 場景中與採集配置對應的 CRD 資源也全新升級。與舊版 CRD 相比，新版 CRD 具有如下特點：</p><ul><li>支持新版流水線採集配置</li><li>CRD 類型調整為 Cluster 級別，且將 CRD 名稱直接作為採集配置名稱，避免同一集羣多個不同的 CRD 資源指向同一個採集配置引起衝突</li><li>對所有操作的結果進行定義，避免出現多次操作舊版 CRD 後出現的行為未定義情況</li></ul><pre><code>apiVersion:&nbsp;log.alibabacloud.com/v1alpha1
kind:&nbsp;ClusterAliyunLogConfig
metadata:
&nbsp;&nbsp;name:&nbsp;test-config
spec:
&nbsp;&nbsp;project:
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;test-project
&nbsp;&nbsp;logstore:
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;test-logstore
&nbsp;&nbsp;machineGroup:
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;test-machine_group
&nbsp;&nbsp;config:
&nbsp;&nbsp;&nbsp;&nbsp;inputs:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;Type:&nbsp;input_file
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FilePaths:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;/var/log/test.log
&nbsp;&nbsp;&nbsp;&nbsp;processors:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;Type:&nbsp;processor_parse_json_native
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SourceKey:&nbsp;content
</code></pre><h3>（二）處理插件組合更加靈活</h3><p>對於文本日誌採集場景，當您的日誌較為複雜需要多次解析時，您是否在為只能使用擴展處理插件而困惑？是否為因此帶來的性能損失和各種不一致問題而煩惱？</p><p>升級 iLogtail 2.0，以上問題都將成為過去！</p><p>iLogtail 2.0 的處理流水線支持全新級聯模式，和 1.x 系列相比，有以下能力升級：</p><ul><li><p><strong>原生處理插件可任意組合：</strong></p><p>原有原生處理插件間的依賴限制不復存在，您可以隨意組合原生處理插件以滿足您的處理需求。</p></li><li><p><strong>原生處理插件和擴展處理插件可同時使用：</strong></p><p>對於複雜日誌解析場景，如果僅用原生處理插件無法滿足處理需求，您可進一步添加擴展處理插件進行處理。</p></li></ul><p><strong>🔔 注意：</strong> 擴展處理插件只能出現在所有的原生處理插件之後，不能出現在任何原生處理插件之前。</p><blockquote><p>示例：假如您的文本日誌為如下內容：</p><p>{"time": "2024-01-22T14:00:00.745074", "level": "warning", "module": "box", "detail": "127.0.0.1 GET 200"}</p><p>您需要將 time、level 和 module 字段解析出來，同時還需要將 detail 字段做進一步正則解析，拆分出 ip、method 和 status 字段，最後丟棄 drop 字段，則您可以按順序使用「Json 解析原生處理插件」、「正則解析原生處理插件」和「丟棄字段擴展處理插件」完成相關需求：</p><p>【商業版】</p><p><img src="https://oscimg.oschina.net/oscnet/up-6310f6b8a781ce3e256042e7f44ed09a75d.png" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-c5dea6cb6a41a50be2f5eb48e2dbeeda960.png" alt="" referrerpolicy="no-referrer"></p><p>【開源版】</p><pre><code>{
&nbsp;&nbsp;"configName":&nbsp;"test-config"
&nbsp;&nbsp;"inputs":&nbsp;[...],
&nbsp;&nbsp;"processors":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_parse_json_native",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"SourceKey":&nbsp;"content"
&nbsp;&nbsp;&nbsp;&nbsp;},
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_parse_regex_native",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"SourceKey":&nbsp;"detail",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Regex":&nbsp;"(\S)+\s(\S)+\s(.*)",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Keys":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"ip",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"method",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"status"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Type":&nbsp;"processor_drop",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"DropKeys":&nbsp;[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"module"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;],
&nbsp;&nbsp;"flushers":&nbsp;[...]
}
</code></pre><p>採集結果如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-f272a5d2ade5c817461bf406d1a73836f89.png" alt="" referrerpolicy="no-referrer"></p></blockquote><h3>（三）新增 SPL 處理模式</h3><p>除了使用處理插件組合來處理日誌，iLogtail 2.0 還新增了 SPL（SLS Processing Language）處理模式，即使用日誌服務提供的用於統一查詢、端上處理、數據加工等的語法，來實現端上的數據處理。使用 SPL 處理模式的優勢在於：</p><ul><li>擁有豐富的工具和函數：支持多級管道操作，內置功能豐富的算子和函數</li><li>上手難度低：低代碼，簡單易學</li><li>【商業版】統一語法：一個語言玩轉日誌採集、查詢、加工和消費</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-4513da15197b25b1099a5707d39a222214d.png" alt="" referrerpolicy="no-referrer"></p><h4>SPL 語法</h4><h5>整體結構：</h5><ul><li>指令式語句，支持結構化數據和非結構化數據統一處理</li><li>管道符（|）引導的探索式語法，複雜邏輯編排簡便</li></ul><pre><code>&lt;data-source&gt;&nbsp;
|&nbsp;&lt;spl-cmd&gt;&nbsp;-option=&lt;option&gt;&nbsp;-option&nbsp;...&nbsp;&lt;expression&gt;,&nbsp;...&nbsp;as&nbsp;&lt;output&gt;,&nbsp;...
|&nbsp;&lt;spl-cmd&gt;&nbsp;...
|&nbsp;&lt;spl-cmd&gt;&nbsp;...
</code></pre><h5>結構化數據 SQL 計算指令：</h5><ul><li>where&nbsp;通過 SQL 表達式計算結果產生新字段</li><li>extend&nbsp;根據 SQL 表達式計算結果過濾數據條目</li></ul><pre><code>*
|&nbsp;extend&nbsp;latency=cast(latency&nbsp;as&nbsp;BIGINT)
|&nbsp;where&nbsp;status='200'&nbsp;AND&nbsp;latency&gt;100
</code></pre><h5>非結構化數據提取指令：</h5><ul><li>parse-regexp&nbsp;提取指定字段中的正則表達式分組匹配信息</li><li>parse-json&nbsp;提取指定字段中的第一層 JSON 信息</li><li>parse-csv&nbsp;提取指定字段中的 CSV 格式信息</li></ul><pre><code>*
|&nbsp;project-csv&nbsp;-delim='^_^'&nbsp;content&nbsp;as&nbsp;time,&nbsp;body
|&nbsp;project-regexp&nbsp;body,&nbsp;'(\S+)\s+(\w+)'&nbsp;as&nbsp;msg,&nbsp;user
</code></pre><h3>（四）日誌解析控制更加精細</h3><p>對於原生解析類插件，iLogtail 2.0 提供了更精細的解析控制，包括如下參數：</p><ul><li>KeepingSourceWhenParseFail：解析失敗時，是否保留原始字段。若不配置，默認不保留。</li><li>KeepingSourceWhenParseSucceed：解析成功時，是否保留原始字段。若不配置，默認不保留。</li><li>RenameSourceKey：當原始字段被保留時，用於存儲原始字段的字段名。若不配置，默認不改名。</li></ul><blockquote><p>示例：假設需要在日誌字段內容解析失敗時在日誌中保留該字段，並重命名為 raw，則可配置如下參數：</p><ul><li>KeepingSourceWhenParseFail：true</li><li>RenameSourceKey：raw</li></ul></blockquote><h3>（五）【商業版】日誌時間解析支持納秒級精度</h3><p>在 iLogtail 1.x 版本中，如果您需要提取日誌時間字段到納秒精度，日誌服務只能在您的日誌中額外添加「納秒時間戳」字段。在 iLogtail 2.0 版本中，納秒信息將直接附加至日誌採集時間（<strong>time</strong>）而無需額外添加字段，不僅減少了不必要的日誌存儲空間，也為您在 SLS 控制枱根據納秒時間精度對日誌進行排序提供方便。</p><p>如果需要在 iLogtail 2.0 中提取日誌時間字段到納秒精度，您需要首先配置時間解析原生處理插件，並在「源時間格式（SourceFormat）」的末尾添加「.%f」，然後在全局參數中增加"EnableTimestampNanosecond": true。</p><blockquote><p>示例：假設日誌中存在字段 time，其值為 2024-01-23T14:00:00.745074，時區為東 8 區，現在需要解析該時間至納秒精度並將 <strong>time</strong> 置為該值。</p><p><img src="https://oscimg.oschina.net/oscnet/up-443ebb6650ab6fb7334c3b3b1c5527ac60c.png" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-6237e2882b874738edda03a89819297d75b.png" alt="" referrerpolicy="no-referrer"></p><p>採集結果如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-519f2570442d18bdb7f276c108ff1112546.png" alt="" referrerpolicy="no-referrer"></p></blockquote><p><strong>🔔 注意：</strong> iLogtail 2.0 不再支持 1.x 版本中提取納秒時間戳的方式，如果您在 1.x 版本中已經使用了提取納秒時間戳功能，在升級 iLogtail 2.0 後，需要按照上述示例手動開啓新版納秒精度提取功能，詳細信息參見文末兼容性説明。</p><h3>（六）【商業版】狀態觀測更加清晰</h3><p>相比於 iLogtail 1.x 暴露的簡單指標，iLogtail 2.0 極大地完善了自身可觀測性的建設：</p><ul><li>所有采集配置都有完整指標，可以在 Project/Logstore 等維度上進行不同採集配置的統計與比較</li><li>所有插件都有自己的指標，可以構建完整流水線的拓撲圖，每個插件的狀態可以進行清楚的觀測</li><li>C++ 原生插件提供更加詳細的指標，可以用來監控與優化插件的配置參數</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-1eafd646874b01b9014f3feb5edee33dd27.png" alt="" referrerpolicy="no-referrer"></p><h3>（七）運行更快更安全</h3><p>iLogtail 2.0 支持 C++ 17 語法，C++ 編譯器升級至 gcc 9，同時更新了 C++ 依賴庫的版本，使得 iLogtail 的運行更快更安全。</p><p>表：iLogtail 2.0 單線程處理日誌的性能（以單條日誌長度 1KB 為例）</p><table><thead><tr><th align="left"><strong>場景</strong></th><th align="left"><strong>CPU（核）</strong></th><th align="left"><strong>內存（MB）</strong></th><th align="left"><strong>處理速率（MB/s）</strong></th></tr></thead><tbody><tr><td align="left">單行日誌採集</td><td align="left">1.06</td><td align="left">33</td><td align="left">400</td></tr><tr><td align="left">多行日誌採集</td><td align="left">1.04</td><td align="left">33</td><td align="left">150</td></tr></tbody></table><h2>兼容性説明</h2><h3>（一）採集配置</h3><h4>商業版</h4><ul><li>新版流水線採集配置是完全向前兼容舊版採集配置的，因此：</li></ul><p>&lt;!----&gt;</p><ul><li>在您升級 iLogtail 至 2.0 版本的過程中，日誌服務會在下發配置時自動將您的舊版配置轉換為新版流水線配置，您無需執行任何額外操作。您可以通過 GetLogtailPipelineConfig 接口直接獲取舊版配置對應的新版流水線配置</li></ul><p>&lt;!----&gt;</p><ul><li>舊版採集配置並不完全向後兼容新配流水線配置</li></ul><p>&lt;!----&gt;</p><ul><li>如果流水線配置描述的採集處理能力可用舊版配置表達，則該流水線配置依然可以被 iLogtail 0.x 和 1.x 版本使用，日誌服務會在向 iLogtail 下發配置時自動將新版流水線配置轉換為舊版配置</li><li>反之，該流水線配置會被 iLogtail 0.x 和 1.x 版本忽略</li></ul><h4>開源版</h4><p>新版採集配置與舊版採集配置存在少量不兼容情況，詳見 iLogtail 2.0 版本採集配置不兼容變更説明 <strong>[</strong><strong>5]</strong> 。</p><h3>（二）iLogtail 客户端</h3><p><strong>1. 使用擴展處理插件時的 Tag 存儲位置</strong></p><p>當您使用擴展插件處理日誌時，iLogtail 1.x 版本由於實現原因會將部分 tag 存放在日誌的普通字段中，從而為您後續在 SLS 控制枱使用查詢、搜索和消費等功能時帶來諸多不便。為瞭解決這一問題，iLogtail 2.0 將默認將所有 tag 歸位，如果您仍希望保持 1.x 版本行為，您可以在配置的全局參數中增加"UsingOldContentTag": true。</p><ul><li>對於通過舊版控制枱界面和舊版 API 創建的採集配置，在您升級 iLogtail 2.0 後，tag 的存儲位置仍然與 1.x 版本一致；</li><li>對於通過新版控制枱界面和新版 API 創建的採集配置，在您升級 iLogtail 2.0 後，tag 的存儲位置將默認歸位。</li></ul><p><strong>2. 高精度日誌時間提取</strong></p><p>2.0 版本不再支持 1.x 版本的 PreciseTimestampKey 和 PreciseTimestampUnit 參數，當您升級 iLogtail 2.0 版本後，原有納秒時間戳提取功能將失效，如果您仍需解析納秒精度時間戳，您需要參照日誌時間解析支持納秒精度板塊對配置進行手動更新。</p><p><strong>3. 飛天格式日誌微秒時間戳時區調整</strong></p><p>2.0 版本的飛天解析原生處理插件將不再支持 1.x 版本的 AdjustingMicroTimezone 參數，默認微秒時間戳也會根據配置的時區進行正確的時區調整。</p><p><strong>4. 日誌解析控制</strong></p><p>對於原生解析類插件，除了日誌解析控制更加精細板塊中提到的 3 個參數，還存在 CopyingRawLog 參數，該參數僅在 KeepingSourceWhenParseFail 和 KeepingSourceWhenParseSucceed 都為 true 時有效，它將在日誌解析失敗時，在日誌中額外增加 <strong>raw_log</strong> 字段，字段內容為解析失敗的內容。</p><p>該參數的存在是為了兼容舊版配置，當您升級 iLogtail 2.0 版本後，建議您及時刪去該參數以減少不必要的重複日誌上傳。</p><h2>總結</h2><p>為用户提供更舒適便捷的用户體驗一直是日誌服務的宗旨。相比於 iLogtail 1.x 時代，iLogtail 2.0 的變化是比較明顯的，但這些轉變只是 iLogtail 邁向現代可觀測數據採集器的序曲。我們強烈建議您在條件允許的情況下嘗試 iLogtail 2.0，也許您在轉換之初會有些許的不適應，但我們相信，您很快會被 iLogtail 2.0 更強大的功能和更出色的性能所吸引。</p><p><strong>相關鏈接：</strong></p><p>[1]&nbsp;輸入插件</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fsls%2Fuser-guide%2Foverview-19%3Fspm%3Da2c4g.11186623.0.0.2a755c0dN5uxv4" target="_blank">https://help.aliyun.com/zh/sls/user-guide/overview-19?spm=a2c4g.11186623.0.0.2a755c0dN5uxv4</a></em></p><p>[2]&nbsp;處理插件</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhelp.aliyun.com%2Fzh%2Fsls%2Fuser-guide%2Foverview-22%3Fspm%3Da2c4g.11186623.0.0.2f2d1279yGXSce" target="_blank">https://help.aliyun.com/zh/sls/user-guide/overview-22?spm=a2c4g.11186623.0.0.2f2d1279yGXSce</a></em></p><p>[3]&nbsp;iLogtail 流水線配置結構</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnext.api.aliyun.com%2Fstruct%2FSls%2F2020-12-30%2FLogtailPipelineConfig%3Fspm%3Dapi-workbench.api_explorer.0.0.65e61a47jWtoir" target="_blank">https://next.api.aliyun.com/struct/Sls/2020-12-30/LogtailPipelineConfig?spm=api-workbench.api_explorer.0.0.65e61a47jWtoir</a></em></p><p>[4]&nbsp;OpenAPI 文檔</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnext.api.aliyun.com%2Fdocument%2FSls%2F2020-12-30%2FCreateLogtailPipelineConfig%3Fspm%3Dapi-workbench.api_explorer.0.0.65e61a47jWtoir" target="_blank">https://next.api.aliyun.com/document/Sls/2020-12-30/CreateLogtailPipelineConfig?spm=api-workbench.api_explorer.0.0.65e61a47jWtoir</a></em></p><p>[5]&nbsp;iLogtail 2.0 版本採集配置不兼容變更説明</p><p><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Falibaba%2Filogtail%2Fdiscussions%2F1294" target="_blank">https://github.com/alibaba/ilogtail/discussions/1294</a></em></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:08:47 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/3874284/blog/11044307</guid>
            <link>https://my.oschina.net/u/3874284/blog/11044307</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[程序員因 bug 事故被公司強制要求歸還 4 萬年終獎]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>某程序員在 V2EX 發帖稱，因線上流量異常事故，自己被公司進行處罰。處罰的結果是被要求將去年發的 4 萬多年終獎歸還給公司，如果逾期不還，將以每天萬分之 5 的利息收取滯納金。</p><p>該程序員還稱，公司 hr 還揚言三個月內還是不還就免費開除。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-11365064309a71d744e9abe207d19996d40.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.v2ex.com%2Ft%2F1016302" target="_blank">https://www.v2ex.com/t/1016302</a></u></em></p></blockquote><p>最新後續：</p><blockquote><p><img height="4236" src="https://oscimg.oschina.net/oscnet/up-0b14b51439371608fa1837e2222c753a465.png" width="1504" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.v2ex.com%2Ft%2F1017164" target="_blank">https://www.v2ex.com/t/1017164</a></u></em></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Wed, 21 Feb 2024 10:31:56 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279644</guid>
            <link>https://www.oschina.net/news/279644</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
    </channel>
</rss>
