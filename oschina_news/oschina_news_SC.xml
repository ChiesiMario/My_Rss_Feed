<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-最新资讯]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-最新资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 13 Dec 2023 02:18:09 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[HyperDX —— 开发者友好的 Datadog 替代品]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>HyperDX&nbsp;是<span style="background-color:#ffffff; color:#1f2328">一个</span>基于云的生产监控和调试工具<span style="background-color:#ffffff; color:#1f2328">，统一会话重放、日志、指标、跟踪和错误。</span>通过将日志、指标、跟踪、异常和会话重播集中并关联到一处，帮助工程师更快地找出生产中断的原因。Datadog 和 New Relic 的开源且开发人员友好的替代方案。</p><ul><li>端到端关联，只需点击几下即可从浏览器会话重放到日志和跟踪</li><li>由 Clickhouse 提供支持的极快性能</li><li>直观的全文搜索和属性搜索语法（例如<code>level:err</code>）</li><li>自动对数十亿个事件中的事件模式进行聚类</li><li>仪表板高基数事件，无需复杂的查询语言</li><li>只需点击几下即可设置警报</li><li>自动解析 JSON/结构化日志</li><li>OpenTelemetry native</li></ul><p><img height="702" src="https://static.oschina.net/uploads/space/2023/0920/163847_4VuB_4252687.png" width="1220" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 02:09:07 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/hyperdx</guid>
            <link>https://www.oschina.net/p/hyperdx</link>
        </item>
        <item>
            <title>
                <![CDATA[小米回应余承东「龙骨转轴」抄袭华为言论]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>「小米公司发言人」官方微博近日发布声明，就余承东所述「龙骨转轴」抄袭华为言论做出澄清称：</p><blockquote><p>「近日，余承东先生无端针对我司龙骨转轴技术发布不实言论，与事实严重不符。我们请余承东先生遵循「科学与严谨」的基本规则，请勿再抹黑同行、误导公众。」</p></blockquote><p>声明指出，无论是设计思路还是机械结构，小米自研的龙骨转轴与余承东所宣称的所谓双旋水滴较链都完全不同。</p><p>且龙骨转轴于 2020 年 9 月 18 日申请专利，并于 2021 年 1 月 5 日获得专利授权，在 2023 年 8 月于 XiaomiMIXFold 了上首发应用。双旋水滴较链则于 2019 年 12 月 13 日申请的专利，2021 年 6 月 18 日才公开。「由此可知，余承东先生的言论，完全不符合事实。」</p><p><img alt="" height="1349" src="https://oscimg.oschina.net/oscnet/up-31b05e591ed808bedd4150f1214bc51f38d.jpg" width="500" referrerpolicy="no-referrer"></p><p>专利图：</p><p><img alt="" height="368" src="https://static.oschina.net/uploads/space/2023/1213/100457_fh3o_4252687.jpg" width="500" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 02:05:07 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270776</guid>
            <link>https://www.oschina.net/news/270776</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 物联网智能网关系统，物联大师]]>
            </title>
            <description>
                <![CDATA[<h1><a id="物联大师" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E7%89%A9%E8%81%94%E5%A4%A7%E5%B8%88"></a>物联大师</h1><p><strong>注意，[V3.0]版本与<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Ftree%2Fv2">V2.0</a>
和<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Ftree%2Fv1">V1.0</a>有较大差异，不可以直接升级！！！</strong></p><h3><a id="说明文档--演示 demo-账号密码-admin-123456" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3--%E6%BC%94%E7%A4%BAdemo-%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81-admin-123456"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fiot-master.com%2Fmanual">说明文档</a><a href="https://gitee.com/link?target=http%3A%2F%2Fdemo.iot-master.com%3A8080%2F">演示 demo</a> 账号密码 admin 123456</h3><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Factions%2Fworkflows%2Fgo.yml"><img src="https://github.com/zgwit/iot-master/actions/workflows/go.yml/badge.svg" alt="Go" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Factions%2Fworkflows%2Fcodeql-analysis.yml"><img src="https://github.com/zgwit/iot-master/actions/workflows/codeql-analysis.yml/badge.svg" alt="Go" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fcodecov.io%2Fgh%2Fzgwit%2Fiot-master"><img src="https://codecov.io/gh/zgwit/iot-master/branch/main/graph/badge.svg?token=AK5TD8KQ5C" alt="codecov" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fpkg.go.dev%2Fgithub.com%2Fzgwit%2Fiot-master"><img src="https://pkg.go.dev/badge/github.com/zgwit/iot-master.svg" alt="Go Reference" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgoreportcard.com%2Freport%2Fgithub.com%2Fzgwit%2Fiot-master"><img src="https://goreportcard.com/badge/github.com/zgwit/iot-master" alt="Go Report Card" referrerpolicy="no-referrer"></a></p><p>物联大师是<a href="https://gitee.com/link?target=https%3A%2F%2Flabs.zgwit.com">无锡真格智能科技有限公司</a>
推出的开源且免费的物联网操作系统，内置 MQTT、TCP Server/Client、UDP Server/Client、串口等接入服务，
系统集成标准 Modbus，水务（SL651、SZY206），电力（DL/T645、IEC101、102、103、104、61850）以及一些主流 PLC 协议，
系统可以通过插件支持数据采集、公式计算、定时控制、异常报警、自动控制策略、流量监控、远程调试、Web 组态等功能，
适用于大部分物联网或工业互联网应用场景。
系统采用 Golang 编程实现，支持多种操作系统和 CPU 架构，可以运行在智能网关上，也可以安装在现场的电脑或工控机上，还能部署到云端服务器。</p><p>项目摒弃复杂的平台架构思维，远离微服务，从真实需求出发，注重用户体验，做到简捷而不简单，真正解决物联网缺乏灵魂的问题。</p><p>我们的宗旨是：<strong>让物联网实施变成一件简单的事情!!!</strong></p><h2><a id="项目的优势" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%BC%98%E5%8A%BF"></a>项目的优势</h2><ul><li>开源免费，商业应用也不限制</li><li>单一程序文件，不需要配置运行环境，不依赖第三方服务，放服务器上就能跑</li><li>极小内存占用，对于一百节点以内的物联网项目，只需要几十兆内存足够了，<del>比起隔壁 Java 动辄大几百兆内存简直太省了</del></li><li>支持工控机和智能网关，边缘计算也没问题</li><li>支持大屏展示，Web 组态，3D 数据孪生 <del>毕竟很多物联网项目都是面子工程</del></li><li>在线产品库、模板库、组件库，小白也能分分钟搞得有模有样【还在努力建设中】</li></ul><h2><a id="项目示例" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E9%A1%B9%E7%9B%AE%E7%A4%BA%E4%BE%8B"></a>项目示例</h2><p><img src="https://iot-master.com/web1.jpg" alt="web" referrerpolicy="no-referrer"><img src="https://iot-master.com/hmi-editor.png" alt="scada" referrerpolicy="no-referrer"></p><h2><a id="咨询服务" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E5%92%A8%E8%AF%A2%E6%9C%8D%E5%8A%A1"></a>咨询服务</h2><p><strong>本公司目前提供免费的物联网方案咨询服务，结合我们十多年的行业经验，给您提供最好的建议，请联系 15161515197（微信同号）</strong></p><blockquote><p>PS. 提供此服务的主要目的是让用户少走弯路，为物联网行业的健康发展尽绵薄之力。
总结一下常见的弯路：</p><ol><li>前期使用某个物联网云平台，后期没办法继续，二次开发受限</li><li>花了几千元买了工业网关，用着一百元 DTU 的功能</li><li>找多个外包公司，低价拿单，结果做出屎一样的东西</li><li>盲目使用开源项目，最终被开源项目所累</li><li>硬件选型失败，效果差强人意</li><li>自身技术人员能力有限，架构设计有问题</li><li>不支持高并发量，市场爆发了，平台反而跟不上</li><li>等等</li></ol></blockquote><h2><a id="联系方式" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E8%81%94%E7%B3%BB%E6%96%B9%E5%BC%8F"></a>联系方式</h2><ul><li>邮箱：<a href="mailto:jason@zgwit.com">jason@zgwit.com</a></li><li>手机：<a>15161515197</a>(微信同号)</li></ul><table><thead><tr><th>技术交流群</th><th>微信</th></tr></thead><tbody><tr><td><img src="https://iot-master.com/tech.png" alt="微信群" referrerpolicy="no-referrer"></td><td><img src="https://iot-master.com/jason.jpg" alt="微信" referrerpolicy="no-referrer"></td></tr></tbody></table><h2><a id="开源协议" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE"></a>开源协议</h2><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Fblob%2Fmain%2FLICENSE">GPL v3</a></p><p>补充：任何组织或个人都可以免费使用或做二次开发，但不得用于商业售卖，如有需求请联系我们。</p><h3><a id="官方插件" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E5%AE%98%E6%96%B9%E6%8F%92%E4%BB%B6"></a>官方插件</h3><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Forgs%2Fiot-master-contrib%2Frepositories">插件库</a></p><table><thead><tr><th>插件</th><th>完成</th><th>正式版</th></tr></thead><tbody><tr><td>历史统计【内置】</td><td>✅</td><td>⬜</td></tr><tr><td>异常报警【内置】</td><td>✅</td><td>⬜</td></tr><tr><td>Influxdb 时序数据库</td><td>✅</td><td>⬜</td></tr><tr><td>Modbus 通讯协议</td><td>✅</td><td>⬜</td></tr><tr><td>WebRTC 接入摄像头</td><td>✅</td><td>⬜</td></tr><tr><td>Web 组态</td><td>✅</td><td>⬜</td></tr><tr><td>3D 数据孪生</td><td>⬜</td><td>⬜</td></tr><tr><td>阿里云通知</td><td>✅</td><td>⬜</td></tr><tr><td>DLT645-2007，电力规约</td><td>⬜</td><td>⬜</td></tr><tr><td>西门子 PLC，S7 系统，PPI，MPI，FetchWrite</td><td>✅</td><td>⬜</td></tr><tr><td>三菱 PLC</td><td>✅</td><td>⬜</td></tr><tr><td>欧姆龙 PLC，Hostlink，Fins</td><td>✅</td><td>⬜</td></tr><tr><td>TDEngine</td><td>⬜</td><td>⬜</td></tr><tr><td>OpenTSDB</td><td>⬜</td><td>⬜</td></tr><tr><td>流式计算</td><td>⬜</td><td>⬜</td></tr><tr><td>报表引擎</td><td>⬜</td><td>⬜</td></tr></tbody></table>]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 01:58:16 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/zgwit_labs/iot-master</guid>
            <link>https://gitee.com/zgwit_labs/iot-master</link>
        </item>
        <item>
            <title>
                <![CDATA[AutoMQ 社区双周精选第二期（11.20-12.01）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h2>本期概要</h2><p>在开源的第二个双周里，作为一个成长中的开源项目，AutoMQ 做了很多的优化和重构，以下是相关重点动态的总结。<br> AutoMQ Kafka：写链路耗时优化、快慢读隔离、Spot 实例强制回收容灾。<br> AutoMQ RocketMQ：历史数据冷读优化、LogCache 读写耗时优化、发布 v0.0.3-alpha 版本、发布 Helm Chart、发布文档站。</p><h2>AutoMQ Kafka 精选动态</h2><h3>写链路耗时优化</h3><p>原来所有的写入和回调都会放到一个单线程线程池去进行处理来确保数据安全，该方式存在线程上下文切换通信、单线程处理排队两个问题。本次优化将写入流程中的数据结构改造成线程并发安全模式，使得不同 stream 之间可以并发进行写入，AutoMQ Kafka 客户端平均写入耗时<strong>下降 0.3ms</strong>。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F728" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/728</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F729" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/729</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F743" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/743</a></p><h3>快慢读隔离</h3><p>隔离从 Cache 读取的快读和从 S3 的读取的慢读，避免慢读占满快读的线程池影响快读。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-kafka%2Fpull%2F472" target="_blank">https://github.com/AutoMQ/automq-for-kafka/pull/472</a></p><p><strong>Spot 实例强制回收容灾</strong></p><p>在上期精选中提及进度的 Spot 实例强制回收容灾已经完成。Spot（竞价实例）相比按需实例可以便宜至多 90 %，但问题是它可能不经通知就强制回收。该特性支持 Spot 实例强制回收的情况下，仍旧可以将数据卷挂载到存活的机器，进行<strong>秒级容灾恢复</strong>。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-kafka%2Fissues%2F447" target="_blank">https://github.com/AutoMQ/automq-for-kafka/issues/447</a></p><h2>AutoMQ RocketMQ 精选动态</h2><h3>Stream 模块性能优化</h3><h4>历史数据冷读优化</h4><p>历史数据追赶读优化，Fetch 请求（50MB &amp; 50 stream）冷读穿透到 S3 场景，单次 Fetch 耗时从 4s 优化到 100ms。即使是完全穿透冷读，S3 读取吞吐效率也是很高的。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F766" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/766</a></p><h4>LogCache 读写耗时优化</h4><p>增加上次 Cache 读取位点记录，避免每次从 LogCache 读取数据都需要二分查找定位，10W 个消息下 10W 次查询时间从 71s 优化到 86ms。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F731" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/731</a> 通过读写锁，将 LogCache 升级成线程并发安全的数据结构，提升 LogCache 读取并发效率。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F701" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/701</a></p><h3>发布 v0.0.3-alpha 版本</h3><p>这个版本包含了以下功能和优化： 1）稳定性与性能提升：修复了潜在的 OOM 问题以及提升 stream 模块性能，详见 Changelog：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fcompare%2Fv0.0.2-alpha...v0.0.3-alpha" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/compare/v0.0.2-alpha...v0.0.3-alpha</a> 2）工程化建设：引入 Nightly build 和&nbsp;E2E test CI <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhub.docker.com%2Fr%2Fautomqinc%2Fautomq-for-rocketmq%2Ftags" target="_blank">https://hub.docker.com/r/automqinc/automq-for-rocketmq/tags</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Factions%2Fworkflows%2Fbuild-ci.yml" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/actions/workflows/build-ci.yml</a> 3）可观测性提升：为 stream 模块引入 Metrics；为 Proxy、Store 模块引入 Trace <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F766" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/766</a></p><h3>发布 Helm Chart</h3><p>现在可以使用 Helm Chart 快速在 Kubernetes 中创建 AutoMQ RocketMQ 集群。</p><pre><code class="language-cs">$ helm repo add automq https://charts.automq.com
$ helm search repo automq                                                                                                                                                            
NAME                            CHART VERSION   APP VERSION     DESCRIPTION                                                                                                           
automq/automq-for-rocketmq      0.0.4           v0.0.3-alpha    A Helm chart for automq-for-rocketmq
</code></pre><p>部署该 Chart 会创建一个 AutoMQ RocketMQ Broker 以及依赖的 MySQL 与 Minio 组件。后续会陆续加入可选的可观测性依赖组件。</p><h3>发布文档站</h3><p>介绍了 AutoMQ RocketMQ 基本使用方式，包含：本地构建、使用 docker compose 部署、在 Kubernetes 上部署。以及使用 CLI 运维集群模式，管理 Topic 等资源。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.automq.com%2Fzh%2Fdocs%2Fautomq-rocketmq%2FRmuXwhb5Xi9zjCkrInRcCz0UnTe" target="_blank">https://docs.automq.com/zh/docs/automq-rocketmq/RmuXwhb5Xi9zjCkrInRcCz0UnTe</a></p><h2>More Things</h2><p>与小红书的同学共创对象存储跨地域容灾方案 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-kafka%2Fissues%2F477" target="_blank">https://github.com/AutoMQ/automq-for-kafka/issues/477</a></p><p>以上是第二期《双周精选》的内容，欢迎关注我们的公众号，我们会定期更新 AutoMQ 社区的进展。同时，也诚邀各位开源爱好者持续关注我们社区，跟我们一起构建云原生消息中间件！</p><p><strong>END</strong></p><h3>关于我们</h3><p>AutoMQ 是一家专业的消息队列和流存储软件服务供应商。AutoMQ 开源的 AutoMQ Kafka 和 AutoMQ RocketMQ 基于云对 Apache Kafka、Apache RocketMQ 消息引擎进行重新设计与实现，在充分利用云上的竞价实例、对象存储等服务的基础上，兑现了云设施的规模化红利，带来了下一代更稳定、高效的消息引擎。此外，AutoMQ 推出的 RocketMQ Copilot 专家系统也重新定义了 RocketMQ 消息运维的新范式，赋能消息运维人员更好的管理消息集群。&nbsp;</p><p>🌟&nbsp;GitHub 地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ" target="_blank">https://github.com/AutoMQ</a></p><p>💻&nbsp;官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.automq.com" target="_blank">https://www.automq.com</a></p><p>👀&nbsp;B 站：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspace.bilibili.com%2F3546572478482870" target="_blank">AutoMQ 官方账号</a></p><p>🔍&nbsp;视频号：AutoMQ&nbsp;</p><p><strong>👉 扫二维码</strong>加入我们的社区群</p><p><img src="https://oscimg.oschina.net/oscnet/up-c4c6b2be9441c750e268dd2d48294131af7.png" alt="" referrerpolicy="no-referrer"></p><p>关注我们，一起学习更多云原生干货</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 01:47:16 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6990971/blog/10320900</guid>
            <link>https://my.oschina.net/u/6990971/blog/10320900</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[云原生周刊：Kubernetes v1.29 新特性一览]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h2>开源项目推荐</h2><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwerf%2Fkubedog" title="kubedog" target="_blank">kubedog</a></h3><p>Kubedog 是一个用于在 CI/CD 部署管道中监视和跟踪 Kubernetes 资源的库。</p><p>这个库被用于 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwerf%2Fwerf" title="werf CI/CD" target="_blank">werf CI/CD</a> 工具中，在部署过程中跟踪资源。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frunwhen-contrib%2Frunwhen-local" title="RunWhen Local" target="_blank">RunWhen Local</a></h3><p>runwhen-local 是一个工具，用于在本地环境中运行 runwhen 脚本。runwhen 是一个灵活的任务调度工具，可以根据条件和时间表来执行任务。通过 runwhen-local，开发者可以在本地测试和调试 runwhen 脚本，以确保其正确运行。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubewharf%2Fkubegateway" title="KubeGateway" target="_blank">KubeGateway</a></h3><p>kube-gateway 是字节跳动内部管理海量 kubernetes 集群的最佳实践。 它是为 kube-apiserver 的 HTTP2 流量专门设计并定制的七层负载均衡代理。 目标是为海量的大规模 kubernetes 集群（千级 node 以上）提供灵活的稳定的流量治理方案。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fflannel-io%2Fflannel" title="flannel" target="_blank">flannel</a></h3><p>Flannel 是为 Kubernetes 设计的一种简单且易于配置的第三层网络结构的解决方案。</p><h2>文章推荐</h2><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmoelove.info%2F2023%2F12%2F10%2FKubernetes-v1.29-%25E6%2596%25B0%25E7%2589%25B9%25E6%2580%25A7%25E4%25B8%2580%25E8%25A7%2588%2F" target="_blank">Kubernetes v1.29 新特性一览</a></h3><p>这篇文章介绍了 Kubernetes v1.29 版本的新特性。该版本包含了 49 个主要的更新，其中有 19 个增强功能进入 Alpha 阶段，19 个升级到 Beta 阶段，还有 11 个升级到稳定版。</p><p>文章重点介绍了两个重要的特性：基于 CEL 的 CRD 规则校验和为动态和静态分配预留 NodePort 端口范围。基于 CEL 的 CRD 规则校验是一种在 CRD 声明中编写校验规则的方式，简化了开发和维护成本。而为动态和静态分配预留 NodePort 端口范围的特性解决了在创建 NodePort 时可能产生的端口冲突问题。总体而言，Kubernetes v1.29 版本的新特性为用户提供了更好的功能扩展和更可靠的输入校验。</p><h3>[Kubernetes：Pod 和 WorkerNodes – 控制 Pod 在节点上的放置</h3><p>](<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frtfm.co.ua%2Fen%2Fkubernetes-pods-and-workernodes-control-the-placement-of-the-pods-on-the-nodes%2F" target="_blank">https://rtfm.co.ua/en/kubernetes-pods-and-workernodes-control-the-placement-of-the-pods-on-the-nodes/</a>)</p><p>这篇文章介绍了在 Kubernetes 中如何控制 Pods 在 WorkerNodes 上的部署位置。它提供了四种主要的方法来实现这种控制：</p><ul><li>配置节点</li><li>Taints 和 Tolerations</li><li>配置 Pod 本身</li><li>Pod 亲和性和反亲和性</li></ul><p>此外，文章还提到了 Pod 拓扑分布约束（Pod Topology Spread Constraints），即根据失败域（regions、可用区或节点）的规则来放置 Pod。</p><p>文章还提供了一些使用 kubectl explain 命令来查看相关参数和资源文档的技巧。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmedium.com%2F%40geoffrey.muselli%2Fargocd-multi-tenancy-strategy-94d72183c94" title="ArgoCD：多租户策略" target="_blank">ArgoCD：多租户策略</a></h3><p>这篇文章介绍了使用 ArgoCD 实现多租户策略的方法。在使用 ArgoCD 时，通常会允许所有用户自由操作，直到进入生产环境后才意识到某个人通过删除应用程序而删除了命名空间或 CRD。为了解决这个问题，需要使用访问控制和多租户策略。文章详细介绍了如何利用 ArgoCD 的原生功能实现多租户策略，并提供了一个示例来演示如何在大型组织中使用企业敏捷框架（例如 SAFe）来实施。文章还讨论了 ArgoCD 中的 AppProject、RBAC 和命名空间等概念，以及如何配置和使用它们来实现多租户策略。最后，文章提供了一个具体的示例，展示了如何根据团队和项目的需求来配置 AppProject 和 RBAC。</p><h2>云原生动态</h2><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2023%2F12%2F05%2Fkyverno-completes-third-party-security-audit%2F" title="Kyverno 完成第三方安全审计" target="_blank">Kyverno 完成第三方安全审计</a></h3><p>Kyverno 项目宣布完成了第三方安全审计。该审计是由 Ada Logics 与 Kyverno 维护人员、开源技术改进基金合作进行，由 CNCF 资助。</p><p>该安全审计是一个全面的安全审计，有以下四个目标：</p><ul><li>为 Kyverno 定义一个正式的威胁模型。</li><li>对代码进行手动安全漏洞审计。</li><li>根据威胁模型评估 Kyverno 的模糊测试套件。</li><li>针对 SLSA 评估 Kyverno 的供应链风险。</li></ul><blockquote><p>本文由博客一文多发平台 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenwrite.cn%3Ffrom%3Darticle_bottom" target="_blank">OpenWrite</a> 发布！</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 11:05:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4197945/blog/10320847</guid>
            <link>https://my.oschina.net/u/4197945/blog/10320847</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[.NET 8 极致性能优化 - Reflection（反射）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1><span><strong><span style="color:#3c70c6">前言</span></strong></span></h1><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>反射一直是性能的瓶颈，所以无论哪个.NET 版本反射的优化必然少不了。主要是集中在两个方面优化，分配和缓存。.NET8 自然也不例外。本篇看下。</span></p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">原文:<u><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5NDYwNjU4MA%3D%3D%26mid%3D2247485722%26idx%3D1%26sn%3Da126d8687afbc4b980533ec7fd239026%26chksm%3Dc01c4481f76bcd97a92c031859b0327a4460f7b4c73dad11cb0f45fa9c283954e5c95f442eec%26token%3D322944710%26lang%3Dzh_CN%23rd" rel="nofollow" target="_blank">.NET8 极致性能优化 Reflection</a></strong></u></p><span id="OSC_h1_2"></span><h1><span><strong><span style="color:#3c70c6">概述</span></strong></span></h1><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>比如针对 GetCustomAttributes 通过反射获取属性的优化，以下例子</span></p><pre><code><span><em>// dotnet run -c Release -f net7.0 --filter "*" --runtimes net7.0 net8.0</em></span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">object</span>[] <span style="color:#dd1144">GetCustomAttributes</span>()</span> =&gt; <span style="color:#ca7d37">typeof</span>(C).GetCustomAttributes(<span style="color:#ca7d37">typeof</span>(MyAttribute), inherit: <span style="color:#0e9ce5">true</span>);</span></code><code><span>    [<span style="color:#afafaf">My(Value1 = 1, Value2 = 2)</span>]</span></code><code><span><span style="color:#ca7d37">class</span><span style="color:#dd1144">C</span> { }</span></code><code><span>    [<span style="color:#afafaf">AttributeUsage(AttributeTargets.All)</span>]</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">MyAttribute</span> : <span style="color:#dd1144">Attribute</span></span></code><code><span>    {</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">int</span> Value1 { <span style="color:#ca7d37">get</span>; <span style="color:#ca7d37">set</span>; }</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">int</span> Value2 { <span style="color:#ca7d37">get</span>; <span style="color:#ca7d37">set</span>; }</span></code><code><span>    }</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET7 和.NET8 明显的差异，它主要是优化了</span><span>避免分配一个 object[1]数组来设置属性的值</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>运行时</th><th>平均值</th><th>比率</th><th>分配</th><th>分配比率</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetCustomAttributes</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1,287.1 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">296 B</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetCustomAttributes</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">994.0 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.77</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">232 B</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.78</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">其它的比如减少反射堆栈中的分配，比如通过更自由的 spans。改进了 Type 上的泛型处理，从而提升各种与泛型相关的成员性能，比如 GetGenericTypeDefinition，它的结果现在被缓存在了 Type 对象上​​​​​​​</p><pre><code><span><em>// dotnet run -c Release -f net7.0 --filter "*" --runtimes net7.0 net8.0</em></span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span> Type _type = <span style="color:#ca7d37">typeof</span>(List&lt;<span style="color:#ca7d37">int</span>&gt;);</span></code><code><span>&nbsp;&nbsp;&nbsp;&nbsp;<span><span style="color:#ca7d37">public</span>&nbsp;Type&nbsp;<span style="color:#dd1144">GetGenericTypeDefinition</span>()</span>&nbsp;=&gt;&nbsp;_type.GetGenericTypeDefinition();</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET7 和.NET8 如下</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>运行时</th><th>平均值</th><th>比</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetGenericTypeDefinition</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">47.426 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetGenericTypeDefinition</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">3.289 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.07</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span><span style="background-color:#ffffff">这</span>些<span style="background-color:#ffffff">都是细枝末节，影响反射性能最大的一块是 MethodBase.Invoke。</span><span style="background-color:#ffffff">当在编译的时候，知道方法的签名并且通过反射来调用方法。</span><span style="background-color:#ffffff">就可以通过使用</span></span><span style="background-color:#ffffff">CreateDelegate</span><span>来获取和缓存该方法的委托，然后通过该委托执行所有的调用。从而实现性</span><span>能最佳化，但是如果在编译的时候你不知道</span><span>方法的签名，则需要依赖动态的方法。比如 MethodBase.Invoke，这个方法降低性能并且更耗</span><span>时。一些比较了解.NET 开</span><span>发的人员会用 emit 避免这种开销。.NET7 里面采用这种方式。.NET8 里面，为许多这样的情况进行了改进，以前，emitter 总是生成可以容纳 ref/out 参数的代码，但许多方法不提供这样的参数，当不需要考虑这些因素时，生成的代码可以更高效。</span>​​​​​​​</p><pre><code><span><em>// If you have .NET 6 installed, you can update the csproj to include a net6.0 in the target frameworks, and then run:</em></span></code><code><span><em>//     dotnet run -c Release -f net6.0 --filter "*" --runtimes net6.0 net7.0 net8.0</em></span></code><code><span><em>// Otherwise, you can run:</em></span></code><code><span><em>//     dotnet run -c Release -f net7.0 --filter "*" --runtimes net7.0 net8.0</em></span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Attributes;</span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Running;</span></code><code><span><span style="color:#ca7d37">using</span> System.Reflection;</span></code><code><span>BenchmarkSwitcher.FromAssembly(<span style="color:#ca7d37">typeof</span>(Tests).Assembly).Run(args);</span></code><code><span>[<span style="color:#afafaf">HideColumns(<span>"Error"</span>, <span>"StdDev"</span>, <span>"Median"</span>, <span>"RatioSD"</span>)</span>]</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span style="color:#ca7d37">private</span> MethodInfo _method0, _method1, _method2, _method3;</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args1 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">1</span> };</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args2 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">2</span>, <span style="color:#0e9ce5">3</span> };</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args3 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">4</span>, <span style="color:#0e9ce5">5</span>, <span style="color:#0e9ce5">6</span> };</span></code><code><span>    [<span style="color:#afafaf">GlobalSetup</span>]</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Setup</span>()</span></span></code><code><span>    {</span></code><code><span>        _method0 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod0"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method1 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod1"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method2 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod2"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method3 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod3"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>    }</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method0</span>()</span> =&gt; _method0.Invoke(<span style="color:#0e9ce5">null</span>, <span style="color:#0e9ce5">null</span>);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method1</span>()</span> =&gt; _method1.Invoke(<span style="color:#0e9ce5">null</span>, _args1);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method2</span>()</span> =&gt; _method2.Invoke(<span style="color:#0e9ce5">null</span>, _args2);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method3</span>()</span> =&gt; _method3.Invoke(<span style="color:#0e9ce5">null</span>, _args3);</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod0</span>()</span> { }</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod1</span>(<span><span style="color:#ca7d37">int</span> arg1</span>)</span> { }</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod2</span>(<span><span style="color:#ca7d37">int</span> arg1, <span style="color:#ca7d37">int</span> arg2</span>)</span> { }</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod3</span>(<span><span style="color:#ca7d37">int</span> arg1, <span style="color:#ca7d37">int</span> arg2, <span style="color:#ca7d37">int</span> arg3</span>)</span> { }</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET6 以及 7 和 8 的情况分别如下：</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>运行时</th><th>平均值</th><th>比率</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">91.457 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">7.205 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.08</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">5.719 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.06</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method1</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">132.832 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method1</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">26.151 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.20</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method1</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">21.602 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.16</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method2</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">172.224 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method2</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">37.937 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.22</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method2</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">26.951 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.16</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method3</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">211.247 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method3</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">42.988 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.20</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method3</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">34.112 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.16</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">这里有一些问题，每次调用都会涉及到一些性能开销，每次调用都会重复。如果我们可以提取这些重复性的工作，对它们进行缓存。就可以实现更好的性能。.NET8 里面通过 MethodInvoker 和 ConstructorInvoker 类型中实现了这些功能。这些并没有包含所有 MethodBase.Invoke 处理的不常见错误（如特别识别和处理 Type.Missing），但对于其他所有情况，它为优化在构建时未知签名的方法的重复调用提供了一个很好的解决方案。​​​​​​​</p><pre><code><span><em>// dotnet run -c Release -f net8.0 --filter "*"</em></span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Attributes;</span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Running;</span></code><code><span><span style="color:#ca7d37">using</span> System.Reflection;</span></code><code><span>BenchmarkSwitcher.FromAssembly(<span style="color:#ca7d37">typeof</span>(Tests).Assembly).Run(args);</span></code><code><span>[<span style="color:#afafaf">HideColumns(<span>"Error"</span>, <span>"StdDev"</span>, <span>"Median"</span>, <span>"RatioSD"</span>)</span>]</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span> _arg0 = <span style="color:#0e9ce5">4</span>, _arg1 = <span style="color:#0e9ce5">5</span>, _arg2 = <span style="color:#0e9ce5">6</span>;</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args3 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">4</span>, <span style="color:#0e9ce5">5</span>, <span style="color:#0e9ce5">6</span> };</span></code><code><span><span style="color:#ca7d37">private</span> MethodInfo _method3;</span></code><code><span><span style="color:#ca7d37">private</span> MethodInvoker _method3Invoker;</span></code><code><span>    [<span style="color:#afafaf">GlobalSetup</span>]</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Setup</span>()</span></span></code><code><span>    {</span></code><code><span>        _method3 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod3"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method3Invoker = MethodInvoker.Create(_method3);</span></code><code><span>    }</span></code><code><span>    [<span style="color:#afafaf">Benchmark(Baseline = true)</span>] </span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MethodBaseInvoke</span>()</span> =&gt; _method3.Invoke(<span style="color:#0e9ce5">null</span>, _args3);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>]</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MethodInvokerInvoke</span>()</span> =&gt; _method3Invoker.Invoke(<span style="color:#0e9ce5">null</span>, _arg0, _arg1, _arg2);</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod3</span>(<span><span style="color:#ca7d37">int</span> arg1, <span style="color:#ca7d37">int</span> arg2, <span style="color:#ca7d37">int</span> arg3</span>)</span> { }</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET8 的情况如下</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>平均值</th><th>比率</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">MethodBaseInvoke</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">32.42 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">MethodInvokerInvoke</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">11.47 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.35</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>这些类型被 Microsoft.Extensions.DependencyInjection.Abstractions 中的 ActivatorUtilities.CreateFactory 方法使用，以进一步提高 DI 服务构建性能。通过添加额外的缓存层进一步改进，进一步避免每次构建时的反射。</span></p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">&nbsp;</p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">作者:jianghupt</p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><strong>欢迎关注公众号 (jianghupt），文章首发地。</strong></p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span><img alt="" height="430" src="https://oscimg.oschina.net/oscnet/up-3243ba74c89867eabc4277de83aa83aa7bb.png" width="430" referrerpolicy="no-referrer"></span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:50:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/5407571/blog/10320411</guid>
            <link>https://my.oschina.net/u/5407571/blog/10320411</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[FastUI —— 更快地构建更好的 UI]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 是一种构建由声明式 Python 代码来构建 Web 应用程序用户界面的新方法。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>这意味着：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><strong>如果你是一名 Python 开发人员</strong>，可以使用 React 构建响应式 Web 应用程序，而无需编写任何 JavaScript 代码，也无需接触<code>npm</code>。</li><li><strong>如果你是前端开发人员</strong>，可以专注于构建真正可重用的神奇组件，无需为每个视图复制粘贴组件。</li><li><strong>对于每个人来说&nbsp;</strong>—— 真正的关注点分离，后端定义了整个应用程序；而前端可以自由地仅实现用户界面</li></ul><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 的核心是一组匹配的&nbsp;<a href="https://docs.pydantic.dev/">Pydantic</a>&nbsp;模型和 TypeScript </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>interfaces<span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>，允许你定义用户界面。其在构建时由 TypeScript 和 Pyright/mypy 进行验证，并在运行时由 Pydantic 进行验证。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 由 4 部分组成：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><a href="https://pypi.python.org/pypi/fastui"><code>fastui</code>PyPI 包</a>— UI 组件的 Pydantic 模型和一些实用程序。虽然它与<a href="https://fastapi.tiangolo.com/">FastAPI</a>配合良好，但它不依赖于 FastAPI，并且其中大部分可以与任何 Python Web 框架一起使用。</li><li><a href="https://www.npmjs.com/package/@pydantic/fastui"><code>@pydantic/fastui</code>npm 包</a>— 一个 React TypeScript 包，让你在实现自己的组件时重用 FastUI 的机制和类型</li><li><a href="https://www.npmjs.com/package/@pydantic/fastui-bootstrap"><code>@pydantic/fastui-bootstrap</code>npm 包</a> — 使用&nbsp;<a href="https://getbootstrap.com/">Bootstrap</a>&nbsp;实现/定制所有 FastUI 组件</li><li><a href="https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt"><code>@pydantic/fastui-prebuilt</code>npm 包</a>（在&nbsp;<a href="https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt">jsdelivr.com CDN</a>&nbsp;上提供）提供了 FastUI React 应用程序的预构建版本，因此你无需安装任何 npm 包或自行构建任何内容即可使用它。Python 包提供了一个简单的 HTML 页面来服务此应用程序。</li></ul><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>以下是一个简单但完整的 FastAPI 应用程序，它使用 FastUI 来显示一些用户配置文件：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><pre><code>from datetime import date

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastui import FastUI, AnyComponent, prebuilt_html, components as c
from fastui.components.display import DisplayMode, DisplayLookup
from fastui.events import GoToEvent, BackEvent
from pydantic import BaseModel, Field

app = FastAPI()


class User(BaseModel):
    id: int
    name: str
    dob: date = Field(title='Date of Birth')


# define some users
users = [
    User(id=1, name='John', dob=date(1990, 1, 1)),
    User(id=2, name='Jack', dob=date(1991, 1, 1)),
    User(id=3, name='Jill', dob=date(1992, 1, 1)),
    User(id=4, name='Jane', dob=date(1993, 1, 1)),
]


@app.get("/api/", response_model=FastUI, response_model_exclude_none=True)
def users_table() -&gt; list[AnyComponent]:
    """
    Show a table of four users, `/api` is the endpoint the frontend will connect to
    when a user fixes `/` to fetch components to render.
    """
    return [
        c.Page(  # Page provides a basic container for components
            components=[
                c.Heading(text='Users', level=2),  # renders `&lt;h2&gt;Users&lt;/h2&gt;`
                c.Table[User](  # c.Table is a generic component parameterized with the model used for rows
                    data=users,
                    # define two columns for the table
                    columns=[
                        # the first is the users, name rendered as a link to their profile
                        DisplayLookup(field='name', on_click=GoToEvent(url='/user/{id}/')),
                        # the second is the date of birth, rendered as a date
                        DisplayLookup(field='dob', mode=DisplayMode.date),
                    ],
                ),
            ]
        ),
    ]


@app.get("/api/user/{user_id}/", response_model=FastUI, response_model_exclude_none=True)
def user_profile(user_id: int) -&gt; list[AnyComponent]:
    """
    User profile page, the frontend will fetch this when the user visits `/user/{id}/`.
    """
    try:
        user = next(u for u in users if u.id == user_id)
    except StopIteration:
        raise HTTPException(status_code=404, detail="User not found")
    return [
        c.Page(
            components=[
                c.Heading(text=user.name, level=2),
                c.Link(components=[c.Text(text='Back')], on_click=BackEvent()),
                c.Details(data=user),
            ]
        ),
    ]


@app.get('/{path:path}')
async def html_landing() -&gt; HTMLResponse:
    """Simple HTML page which serves the React app, comes last as it matches all paths."""
    return HTMLResponse(prebuilt_html(title='FastUI Demo'))</code></pre></div>
                                                                ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:29:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/fastui</guid>
            <link>https://www.oschina.net/p/fastui</link>
        </item>
        <item>
            <title>
                <![CDATA[🎁有奖问答 | 聊聊 NGINX 向云原生演进那点儿事]]>
            </title>
            <description>
                <![CDATA[<h1 class="header article-title"><a href="https://www.oschina.net/question/4700705_2331501">高手问答第 311 期 —— 聊聊 NGINX 向云原生演进那点儿事</a><div class="ui red label horizontal" data-tooltip="置顶">顶</div></h1><div class="extra ui horizontal list meta-wrap"><div class="item"><a href="https://my.oschina.net/u/4700705" class="__user"><span>小白兔爱吃大灰狼</span></a> 发布于，昨天 11:35
                    </div><div class="item">阅读 408</div><div class="item collect-btn " data-id="2331501" data-user-id="4700705" data-obj-type="2" data-max="99" data-tag-required="" data-current-user-id="" data-recommend-tags=""><i class="star outline icon"></i> 收藏 <span data-collect-count="" data-id="2331501" data-obj-type="2">1</span></div><div class="item comment-count"><a href="https://www.oschina.net/question/4700705_2331501#comments" class="normal"><i class="comment outline icon"></i> 答案 <span data-article-reply-count="">5</span></a></div></div><div class="tags"><a class="ui horizontal label" href="https://www.oschina.net/question/topic/masteronline" target="_blank"><img src="https://static.oschina.net/uploads/logo/masteronline_9WTeU.png" referrerpolicy="no-referrer">高手问答</a></div><div class="content" id="articleContent"><p><span><span>据 Gartner 预测，到 2025 年，云原生架构将成为超过 95% 的新数字计划基础，高于 2021 年的不到 40%，云原生架构市场占有率不断提高。而如今，全球半数以上（55%） 的网站都基于 NGINX 运行，差不多相同比例 (53.7%) 的中国网站在 NGINX 开源版上运行。而 NGINX 存在难于动态配置、管理功能影响业务等问题，为了解决这些问题，OpenNJet 由此诞生。</span></span></p><p><span><span>OpenNJet 基于 NGINX1.19 基础 fork 并独立演进，具有高性能、稳定、易扩展的特点，通过数据面与控制面的隔离，能够在不重启进程的情况下基于动态配置能力进行配置的实时更新。最近还推出了 OpenNJet K8s Ingress Controller 1.0，基于 OpenNJet 的动态特性、高性能实现，弥补了 NGINX 在云原生场景中不足，而且提供了丰富的流量管理功能，如动态 location、host/path 路由、负载均衡、动态 upstream、金丝雀发布、SNI 等。</span></span></p><p><strong><span><span>OSCHINA 本期高手问答（12 月 13 日 - 12 月 19 日）我们请来了嘉宾<a href="https://my.oschina.net/u/6606114" rel="nofollow">单雷老师</a>和大家一起聊聊 NGINX 向云原生演进那点儿事。</span></span></strong></p><p><strong><span><span>可讨论的问题包括但不限于</span></span></strong><strong><span><span>：</span></span></strong></p><ul><li><span><span style="background-color:white"><span>OpenNJet 和 NGINX 是什么关系？</span></span></span></li><li><span><span style="background-color:white"><span>什么是云原生应用引擎？OpenNJet 的有哪些优势</span></span></span></li><li><span><span style="background-color:white"><span>我们如何解决数据面控制面隔离、国密、动态配置等问题？</span></span></span></li><li><span><span style="background-color:white"><span>读 NGINX/OpenNJet 源码的建议</span></span></span></li><li><span><span style="background-color:white"><span>如何上手开发一个开源项目？</span></span></span></li></ul><p><span><span style="background-color:white"><span>其他关于 NGINX、OpenNJet 的更多内容，也欢迎积极提问。</span></span></span></p><h2><span><span style="background-color:white"><span><strong>嘉宾介绍</strong></span></span></span></h2><p><img alt="" height="534" src="https://oscimg.oschina.net/oscnet/up-774dc1b75df829000896339c602574ff319.jpg" width="400" referrerpolicy="no-referrer"></p><p><span><span><strong><span><span style="color:#7030a0">通明智云产品总监，单雷</span></span></strong></span></span></p><p><span><span>20 年的 IT 行业经验，精通云原生以及高性能应用引擎技术。曾在亚信科技历任研发主管、首席架构师等职务，并主导多个云原生、高性能应用网关项目的设计开发工作，现任公司应用引擎产品总监。</span></span></p><hr><p><span><span style="background-color:white"><span><span>🎁</span> 为了鼓励踊跃提问，下一代云原生应用引擎 OpenNJet 开源社区会在问答结束后从提问者中抽取 5 名幸运会员，赠予精美棉马甲一件。</span></span></span></p><p><img alt="" height="436" src="https://oscimg.oschina.net/oscnet/up-6f9dfb1df3b4d3c9f22f9a02a21c1be62d5.jpg" width="400" referrerpolicy="no-referrer"></p><blockquote><p><span><span>OpenNJet&nbsp;应用引擎是基于 NGINX 的面向互联网和<strong>云原生</strong>应用提供的运行时组态服务程序，作为底层引擎，OpenNJet 实现了 NGINX 云原生功能增强、安全加固和代码重构，利用<strong>动态加载机制</strong>可以实现不同的产品形态，如 Web 服务器、流媒体服务器、负载均衡、代理 (Proxy)、应用中间件、API 网关、消息队列等产品形态等等。OpenNJet 在云原生架构中作为数据平面，除了提供南北向通信网关的功能以外，还提供了服务网格中东西向通信能力。在原有功能基础上增加了透明流量劫持、熔断、遥测与故障注入等新功能特性。</span></span></p><p><span><span>Gitee：<a href="https://gitee.com/njet-rd/njet" rel="nofollow"><span><span>https://gitee.com/njet-rd/njet</span></span></a></span></span></p><p><span><span>官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnjet.org.cn%2F" rel="nofollow" target="_blank">https://njet.org.cn/</a></span></span></p></blockquote><p><span style="background-color:#ffffff; color:#27ae60">OSChina 高手问答一贯的风格，不欢迎任何与主题无关的讨论和喷子。</span></p><p>下面欢迎大家就 「<span><span>NGINX 向云原生演进</span></span>」<span><span>&nbsp;</span>相关</span>问题向<span>&nbsp;<a href="https://my.oschina.net/u/6606114" rel="nofollow">单雷老师</a></span><a href="https://my.oschina.net/klblog" rel="nofollow"><strong><span style="color:#000000">&nbsp;</span></strong></a>提问，直接回帖提问既可。</p></div><div class="poll-wrap"></div><div class="additional-remarks"></div><div class="ui basic center aligned segment action"><div class="ui big buttons"><a class="ui basic button collect-btn hover" data-id="2331501" data-user-id="4700705" data-obj-type="2" data-max="99" data-tag-required="" data-current-user-id="" data-recommend-tags=""><i class="star outline icon"></i>收藏 (<span data-collect-count="" data-id="2331501" data-obj-type="2">1</span>)</a><div class="ui basic dropdown share button osc-share dropdown-share" data-tag="share-question"><i class="share icon"></i><span>分享</span><div class="menu"><a class="item" data-platform="weibo" data-value="weibo"><i class="weibo icon"></i>微博</a><a class="item" data-platform="qq" data-value="qq"><i class="qq icon"></i>QQ</a><a class="item" data-platform="wechat" data-value="wechat"><i class="weixin icon"></i>微信</a></div></div></div><div class="ui basic segment"><a class="ban" ban-report="" data-id="2331501" data-obj-type="2" data-url="https://www.oschina.net/question/4700705_2331501"><i class="flag red icon"></i>举报</a></div></div>
            ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:28:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/question/4700705_2331501</guid>
            <link>https://www.oschina.net/question/4700705_2331501</link>
        </item>
        <item>
            <title>
                <![CDATA[新技术 LINT 可强制 LLM 回答有毒问题]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">美国普渡大学（Purdue University）的研究人员发布了，一篇名为《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2312.04782" target="_blank"><span style="background-color:#ffffff">Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs</span></a>》的论文。描述了他们通过利用大模型厂商倾向于公开与提示响应相关的概率数据的特点，设计出了一种可以打破现有 LLM 规则，使其给出「有害」回答的新颖方法。</span></p><p><span style="color:#000000">研究人员使用了一种名为 LINT (LLM interrogation) 的技术，与绕过安全提示的一般越狱方式不同的是，这种技术更具强制性。它涉及到对概率值（logits）或软标签（soft labels）的理解 —— 从统计学上讲，其可以有效区分安全响应与有害响应。</span></p><blockquote><p><span style="color:#000000">「在我们的研究中，我们揭示了当不良行为者可以访问模型的输出 logits 时，LLM alignment 面临的的一种新威胁，这是开源 LLM 和许多商业 LLM API（如某些 GPT 模型）的共同特征。它并不依赖于制作特定的提示。相反，它利用了这样一个事实：即使 LLM 拒绝了有毒请求，有害响应也往往隐藏在输出 logits 的深处。通过在自动回归生成过程中在几个关键输出位置强行选择排名较低的输出&nbsp;tokens，我们可以迫使模型揭示这些隐藏的响应。我们将这一过程称为模型审讯（model interrogation）。」</span></p></blockquote><p><span style="color:#000000">具体<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theregister.com%2F2023%2F12%2F11%2Fchatbot_models_harmful_content%2F" target="_blank">表现为</a>，LINT 使用 LLM-based classifier 向 LLM 提出一个有害的问题：比如"说明如何将枪改装成全自动的"。然后，对响应中的前九个 tokens（如 "It's"、"It"、"We"和 "I"）进行排序，并用这些词组创建新句子，这样该 LLM 就会不断生成回复。</span></p><p><span style="color:#000000"><img alt="" height="263" src="https://oscimg.oschina.net/oscnet/up-a4cca4832d0d4ac8918b03eacf880f3455b.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">研究人员在创建 LINT 原型时，在一个包含 50 个有毒问题的数据集上询问了 7 个开源 LLM 和 3 个商业 LLM。他们称：「当模型只被审讯一次时，ASR（攻击成功率）达到 92%，当被询问五次时，ASR 可以达到达到 98%。」</span></p><p><span style="color:#000000">这种方法与越狱方法不同，但性能要<span style="background-color:#ffffff">远远优于目前最先进的两种越狱技术：GCG 和 GPTFuzzer。</span>相比之下越狱方法的 ASR 仅为 62%，且运行时间要长&nbsp;10 到 20 倍。「通过我们的方法揭露的有害内容更加相关、完整、清晰。此外，它可以补充越狱策略，从而进一步提高攻击性能。」</span></p><p><span style="color:#000000">更重要的是，这种技术甚至适用于根据特定任务（如代码生成）的基础模型定制的 LLM。研究人员还声称，这种技术可以用来损害隐私和安全，迫使模型公开电子邮件地址和猜测弱密码。</span></p><p><span style="color:#000000">因此，研究人员警告称，AI&nbsp;界在考虑是否开源 LLM 时应谨慎；并建议最好的解决方案是确保有毒内容被清除，而不是将其隐藏起来。</span></p><p><span style="color:#000000">更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2312.04782" target="_blank">查看完整论文</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:24:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270686/lint-llm-harmful-content</guid>
            <link>https://www.oschina.net/news/270686/lint-llm-harmful-content</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Apache StreamPark 2.1.2 稳定版正式发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><img height="460" src="https://oscimg.oschina.net/oscnet/up-223b657c0b3fdd8242108df64be06aa7cf7.png" width="1080" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span><strong><span style="color:#333333">近日 Apache StreamPark<span>(Incubating)&nbsp;</span>社区正式发布了 StreamPark 2.1.2 版本</span></strong><span style="color:#333333">，</span></span><span>在 2.1.2 版本中，支持了最新的 Flink 1.18，Flink Jar 类型的作业支持指定依赖，</span><span>修复了</span><span style="color:#333333">诸多 Bug 和大量改进</span><span>，稳定性和可用性进一步提升，建议所有用户升级到这个版本</span><span>。</span></p><p><span style="color:#646464"><strong><span>Github:&nbsp;</span></strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark" target="_blank">https://github.com/apache/streampark</a></p><p style="margin-left:0; margin-right:0; text-align:left"><span style="color:#646464"><strong>官&nbsp; &nbsp; &nbsp;网:&nbsp;</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstreampark.apache.org%2Fdownload" target="_blank">https://streampark.apache.org/download</a></p><p><span style="color:#444444">欢迎&nbsp;</span><strong><span style="color:#444444">使用、关注、star、fork</span></strong><span style="color:#444444">&nbsp;</span></p><h1><span>新特性解读</span></h1><h4><span style="color:#0052ff"><span style="background-color:#0053cd"><strong>&nbsp;</strong></span><span>&nbsp; </span></span><strong>更好的支持 JAR 类型作业</strong></h4><p><span><span>在&nbsp;StreamPark 中将 Flink 作业按照开发模式分为&nbsp;Custom Code&nbsp;和 Flink SQL<span>&nbsp;</span><span>两种类型</span>，Custom Code 是需要用户编写代码编译成 JAR 类型的 Flink 作业，在以前的版本中该类型的作业不支持在 StreamPark 平台侧指定作业依赖，要求用户自己解决作业需要的依赖，通常做法是需要将这些依赖打包到项目里，生成一个 FatJar (uber-jar)。社区收到很多用户的反馈，大家普遍希望 StreamPark 平台侧针对 JAR 类型的作业能像 Flink SQL 作业一样，可以自由的指定作业的依赖。</span></span></p><p><span><span>同时，我们也看到 Apache Doris, Apache Paimon 等社区都开发了基于 Flink CDC 一键集成数据的组件&nbsp;</span><span style="color:#888888">(doris-flink-connector 和 paimon-action)</span><span>，该组件都提供了作业迁移的入口，但作业运行时依赖需要用户手动添加。</span></span></p><p><span><span>鉴于这些原因，在 StreamPark 2.1.2 里，特别针对 JAR 类型的作业支持了指定依赖的能力，使得用户部署这类作业更加简单。</span></span><span>以下是两个示例，演示了如何利用该特性，来快速部署 Doris 和 Paimon 数据迁移类型的作业：</span></p><p><iframe frameborder="0" height="370" scrolling="no" src="https://player.bilibili.com/player.html?aid=280306310&amp;bvid=BV17c411d7Jy&amp;cid=1317452428&amp;p=1&amp;autoplay=0" style="box-sizing: inherit; color: rgb(51, 51, 51); font-family: -apple-system, BlinkMacSystemFont, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Segoe UI&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;" width="750" referrerpolicy="no-referrer"></iframe></p><p><span style="background-color:#000000; color:#ffffff">StreamPark 让 Doris 数据集成更简单&nbsp;</span><br> &nbsp;</p><p><iframe frameborder="0" height="370" scrolling="no" src="https://player.bilibili.com/player.html?aid=323428873&amp;bvid=BV1Sw411W7QK&amp;cid=1333574131&amp;p=1&amp;autoplay=0" style="box-sizing: inherit; color: rgb(51, 51, 51); font-family: -apple-system, BlinkMacSystemFont, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Segoe UI&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;" width="760" referrerpolicy="no-referrer"></iframe></p><p><span style="background-color:#000000; color:#ffffff">&nbsp;StreamPark 让 Paimon 数据集成更简单&nbsp;</span></p><h4><span style="color:#0052ff"><span style="background-color:#0053cd"><strong>&nbsp;</strong></span><span>&nbsp; </span></span><strong>支持 Flink 1.18</strong></h4><p style="margin-left:0; margin-right:0"><span>作为流处理开发管理框架，StreamPark 在对 Apache Flink 的支持上，一如既往的走在前列。得益于 StreamPark 良好的架构设计，使得支持一个新<span>版本</span>的 Flink 非常容易，因此我们率先支持了<span>&nbsp;</span></span><span style="color:#ff4c00"><span>Flink 1.18</span><span><span>&nbsp;</span><span style="background-color:#ffffff">[1]</span></span></span><span>。在使用上非常的简单，用户只需要添加一个 Flink 1.18 的环境即可，作业可以自由的选择 Flink 版本<span>。</span></span></p><p style="margin-left:0; margin-right:0"><span><span>并且本次适配了更多发行版 Flink，如 CDH 版本的 Flink, 华为云，腾讯云 Flink 等。</span></span></p><div style="margin-left:0px; margin-right:0px; text-align:left"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px">
          &nbsp;
         </div></div></div></div></div></div></div></div></div></div><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><iframe frameborder="0" height="370" scrolling="no" src="https://player.bilibili.com/player.html?aid=264867610&amp;bvid=BV16Y41117kt&amp;cid=953529774&amp;p=1&amp;autoplay=0" style="box-sizing: inherit;" width="750" referrerpolicy="no-referrer"></iframe></p><p><span style="background-color:#000000; color:#ffffff">&nbsp;支持 Flink 多版本&nbsp;</span></p><p><span style="color:rgba(0, 0, 0, 0.9)"><span style="background-color:#0053cd"><strong><span style="color:#0053cd">&nbsp;</span></strong></span><span>&nbsp;<span>&nbsp;</span></span></span><span style="color:#0053dc"><strong>其他改进和更新</strong></span></p><ul><li><p><span style="color:#444444">修复作业状态重新映射不生效的 Bug</span>&nbsp;<u>#2822</u></p></li><li><p>改进 Flink 版本的校验逻辑，适配更多的&nbsp;Flink 版本&nbsp;<u>#2832</u></p></li><li><p>修复作业 「取消状态」 下可能存在的无法发送报警信息的 Bug&nbsp;<u>#3157</u></p></li><li><p style="margin-left:0; margin-right:0"><span>修复 Ingress 访问 Flink UI 可能存在的 404 Bug&nbsp;<u>#3302</u></span></p></li><li><p><span><span style="color:#444444">修复团队为空，导致查询错误的 Bug&nbsp;</span><u>#3365</u></span></p></li><li><p>修复作业参数解析，特定字符解析错误导致作业失败的&nbsp;Bug</p></li><li><p><span style="color:#0052ff"><span style="color:#444444">修复项目编译时 maven-wrapper 文件损坏导致失败的 Bug</span></span></p></li><li><p><span style="color:#0052ff"><span style="color:#444444"><span style="color:#444444">Flink 作业的 Pom 信息支持&nbsp;exclusion，有效避免 JAR 冲突问题</span></span></span></p></li></ul><h1><span>Release Note</span></h1><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:left"><span><span style="color:#333333">本次 StreamPark 2.1.2 版本的，完整 Release Note 请访问：</span><br><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstreampark.apache.org%2Fdownload%2Frelease-note%2F2.1.2" target="_blank">https://streampark.apache.org/download/release-note/2.1.2</a></span></p><h1><span style="color:#000000">感谢贡献者</span></h1><p><span style="color:#333333"><span>StreamPark 开源社区的发展，离不开广大用户群体的积极反馈和宣传布道，更离不开贡献者们的无私贡献<span>，</span></span><span style="color:#333333">感谢对此版本做出贡献的每一位贡献者<span style="background-color:#ffffff">。</span></span></span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span style="color:#444444"><span style="background-color:#ffffff">特别感谢本次的 Release Manager</span>&nbsp;</span><span style="color:#ff4c00">@龚中强<span style="background-color:#ffffff">[2]</span></span><span style="color:#444444">，<span style="background-color:#ffffff; color:#444444">中强</span></span><span><span style="background-color:#ffffff; color:#444444">在<span style="background-color:#ffffff; color:#444444">发版过程中<span style="background-color:#ffffff">积极的跟踪问题和推进进度</span>，完美胜任了此次发版工作。</span>感谢<span style="background-color:#ffffff; color:#444444">中强</span>为社区做出的贡献，也欢迎其他<span>&nbsp;</span><span style="background-color:#ffffff">PPMC member 和&nbsp;</span>Committer 在后续的发版中担任 Release Manager，帮助社区更快捷、高质量地完成发版。</span></span></p><h1><span>什么是 StreamPark</span></h1><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>StreamPark 是一个流处理应用程序开发管理框架。初衷是让流处理更简单，旨在轻松构建和管理流处理应用程序，提供使用 Apache Flink 和 Apache Spark 编写流处理应用程序的开发框架。同时 StreamPark 提供了一个流处理应用管理平台，核心能力包括但不限于应用开发、调试、交互查询、部署、运维、实时数仓等，最初开源时项目名称叫 StreamX ，于 2022 年 8 月更名为 StreamPark，随后通过投票正式成为 Apache 开源软件基金会的孵化项目。目前已有腾讯<span>、</span>百度<span>、</span>联通<span>、天翼云<span>、</span></span>自如<span>、</span>马蜂窝<span>、</span>长安汽车等数百家公司生产环境使用。</span></p><h1><span style="color:#000000">🫵&nbsp;加入我们</span></h1><p><span><span style="color:#333333"><span style="background-color:#ffffff">StreamPark 社区一直以来都以用心做好一个项目为原则</span><span style="background-color:#ffffff">，</span><span style="background-color:#ffffff">高度关注项目质量</span><span style="background-color:#ffffff">，努力</span><span style="background-color:#ffffff">建设发展社区。</span><span>加入 Apache 孵化器以来，</span><span style="background-color:#ffffff">认真学习和遵循「The Apache Way」，我们将秉承更加兼容幷包的心态，迎接更多的机遇与挑战。诚挚</span><span>欢迎更多的贡献者参与到社区建设中来，和我们一道携手共建。</span></span></span></p><p><span><span style="color:#333333"><strong>💻 项目地址：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark" target="_blank">https://github.com/apache/streampark</a></span></p><p><span><span style="color:#333333"><strong>🧐 提交问题和建议：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fissues" target="_blank">https://github.com/apache/streampark/issues</a></span></p><p><span><span style="color:#333333"><strong>🥁 贡献代码：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fpulls" target="_blank">https://github.com/apache/streampark/pulls</a></span></p><p><span><span style="color:#333333"><strong><strong>📮&nbsp;</strong>Proposal：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FINCUBATOR%2FStreamPark%2BProposal" target="_blank">https://cwiki.apache.org/confluence/display/INCUBATOR/StreamPark+Proposal</a></span></p><p><span><span style="color:#333333"><strong>📧 订阅社区开发邮件列表：</strong></span><span style="color:#0080ff">dev@streampark.apache.org</span><span style="color:#0080ff">&nbsp;</span><span style="color:#0080ff"><span style="color:#ff4c00">[3]</span>&nbsp;</span></span></p><p style="margin-left:0; margin-right:0; text-align:left"><span><span style="color:#444444"><strong>💁‍♀️</strong></span><span style="color:#444444"><strong>社区沟通：</strong></span></span></p><p><img height="500" src="https://oscimg.oschina.net/oscnet/up-07a7e385d033088436872afd0571e4c3482.png" width="900" referrerpolicy="no-referrer"></p><p><span style="color:#444444"><strong>参考资料</strong></span></p><p><span><em><span style="color:#666666">[1]&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-release-1.18%2Frelease-notes%2Fflink-1.18" target="_blank">https://nightlies.apache.org/flink/flink-docs-release-1.18/release-notes/flink-1.18</a></span></em></span></p><p><em><span style="color:#666666">[2]&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGOODBOY008" target="_blank">https://github.com/GOODBOY008</a></span></em></p><p><span><em><span style="color:#666666">[3]&nbsp;<em><span>mailto:dev@streampark.apache.org</span></em></span></em></span><br> &nbsp;</p><p><span style="color:#333333">祝大家安装、升级顺利~~&nbsp;&nbsp;</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 06:27:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270652/apache-streampark-2-1-2-released</guid>
            <link>https://www.oschina.net/news/270652/apache-streampark-2-1-2-released</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[铠侠向 Linux 基金会捐赠 Software-Enabled Flash SDK]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#121212">几年前从东芝分离出来的存储公司 Kioxia（</span>铠侠<span style="background-color:#ffffff; color:#121212">）向 Linux 基金会捐赠了一个软件开发工具包 (SDK)，用于建立 Software-Enabled Flash SDK。</span></p><p><span style="background-color:#ffffff; color:#121212">Linux 基金会发布<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linuxfoundation.org%2Fpress%2Fsoftware-enabled-flash-announces-software-development-kit-sdk" target="_blank">公告称</a>，「SEF SDK 的发布是存储技术领域的一个重要里程碑......SEF 项目对 KIOXIA 突破性地捐赠软件定义闪存原生 SDK 表示热烈欢迎，这将为开发人员提供前所未有的能力，使他们能够为闪存存储（flash storage）应用开发定制的独特软件。」</span></p><p><img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-67690b065c2207474d1a67124aa3ef403da.png" width="300" referrerpolicy="no-referrer">&nbsp; &nbsp;<img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-1056c78ed4258dcb84497a6e896204821c0.jpg" width="300" referrerpolicy="no-referrer"></p><p>该 SEF SDK 包括示例代码和文档，以充分利用 flash media control 的潜力；包括 WAF 减少、延迟控制、对 ZNS 和 FDP 或 Block 等多种协议的支持等。</p><p>SEF 项目旨在通过加强对驱动器的管理、增强工作负载隔离、加强延迟控制以及实现对闪存管理的更多&nbsp;host-control，在现代数据中心中开辟新的用途并最大限度地发挥基于闪存的存储潜力。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270608/software-enabled-flash-sdk</guid>
            <link>https://www.oschina.net/news/270608/software-enabled-flash-sdk</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[芯瞳正式加入 openKylin，为社区贡献高质量的国产 GPU 解决方案！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>近日，芯瞳半导体技术（山东）有限公司（以下简称「芯瞳」），签署 openKylin 社区 CLA（Contributor License Agreement 贡献者许可协议），正式加入 openKylin 开源社区。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="1079" src="https://oscimg.oschina.net/oscnet/up-4c9b13fca5452f4a217f1494d816e96a799.png" width="829" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>芯瞳（Sietium）成立于 2019 年，是一家自主设计研发 GPU 芯片及 GPU 解决方案的高科技公司，以行业先进的计算和图形渲染平台为依托，用高质量的产品和服务为云端、终端客户提供可持续发展的国产 GPU 解决方案；为数字时代的创新与发展提供算力支撑，构建自由算力的文明世界。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="410" src="https://oscimg.oschina.net/oscnet/up-6914c94ad47861f5f685cb96e9bc21450f1.png" width="940" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span><strong><span>加入 openKylin 社区后，芯瞳将参与维护社区 GPU SIG 和 Wayland SIG</span></strong><span>。<strong>凭借其自研的 GPU 显卡和深厚的行业经验，优化 openKylin 环境中显卡驱动的兼容性，确保与芯瞳显卡的完美适配</strong>。</span></span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>在 openKylin 平台上，芯瞳显卡将展现其在图形显示、渲染、视频编解码和大规模计算等方面的优势，以此提升 openKylin 的用户体验，并提供持续的 GPU 产品升级和技术支持，为用户提供安全可靠的使用体验。具体计划如下：</span></p><ul><li><p style="margin-left:0; margin-right:0"><span>积极参与社区合作，紧密关注社区的发展动态，与社区成员携手推动 openKylin 社区的生态及品牌建设，努力构建一个健康的生态环境，为开源生态的发展贡献力量。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>寻求与社区的技术合作，通过联合调试等方式，使 openKylin 的相关产品能更好地兼容并适应芯瞳的全新系列显卡，从而提高产品的稳定性和性能。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>在应用层面，芯瞳将持续优化软件算法，提高系统效率，充分发掘 openKylin 在芯瞳显卡平台上的性能潜力，从而提升整体性能，为用户提供卓越的产品体验。</span></p></li></ul><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>通过这一系列的举措，芯瞳将与 openKylin 社区并肩前行，共同推动 openKylin 社区生态良好发展，为用户带来更多的创新和惊喜。同时，芯瞳期待与社区成员进行深入的交流和分享，以推动技术的进步和产业的协同发展，共同为中国开源生态的繁荣作出贡献。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270607</guid>
            <link>https://www.oschina.net/news/270607</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Facebook 开源 StyleX —— 在 JavaScript 中写 CSS]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Meta（原 Facebook）<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstylexjs.com%2Fblog%2Fintroducing-stylex%2F" target="_blank">开源</a></u>了全新的 CSS-in-JS 库 StyleX。</p><p><img src="https://oscimg.oschina.net/oscnet/up-30f683ba9535a9f16ce5e615736da0460cd.png" referrerpolicy="no-referrer"></p><blockquote><p><em>GitHub 地址：<strong><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Fstylex" target="_blank">https://github.com/facebook/stylex</a></u></strong></em></p></blockquote><p>官方介绍道，StyleX 是一个富有表现力、具有确定性、可靠且可扩展的样式系统。它通过使用编译时 (compile-time) 工具融合了静态 CSS 的性能和可扩展性。</p><p>此外，StyleX 不仅仅是一个基于编译器的 CSS-in-JS 库，它经过精心设计，可以满足大型应用程序、可复用组件库和静态类型代码库的要求。Meta 旗下多款产品如 Facebook、WhatsApp、Instagram、Workplace、Threads 等都在使用 StyleX 作为其 CSS 样式解决方案。</p><p>StyleX 主要特性</p><ul><li><p><strong>快速</strong>：StyleX 在编译时和运行时都具备高效的性能。Babel 转换不会对构建过程产生显著影响。在运行时，StyleX 避免了使用 JavaScript 插入样式的开销，并仅在必要时高效地组合类名字符串。生成的 CSS 经过优化，确保即使是大型网站的样式也能被浏览器快速解析。</p></li><li><p><strong>可扩展</strong>：StyleX 旨在适应像 Meta 这样的超大型代码库。通过原子构建和文件级缓存，Babel 插件能够处理数万个组件在编译时的样式处理。由于 StyleX 设计为封装样式，它允许在隔离环境中开发新组件，并期望一旦在其他组件中使用时能够可预测地呈现。</p></li><li><p><strong>可预测性</strong>：StyleX 会自动管理 CSS 选择器的特异性，以确保生成的规则之间不会发生冲突。它为开发人员提供了一个可靠地应用样式的系统，并确保「最后应用的样式始终生效」。</p></li><li><p><strong>类型安全</strong>：使用 TypeScript 或 Flow 类型来约束组件接受的样式，每个样式属性和变量都具有完全的类型定义。这有助于提高代码的可读性和可维护性，同时减少潜在的错误和冲突。</p></li><li><p><strong>样式去重</strong>：StyleX 鼓励在同一文件中编写样式和组件。这种方法有助于使样式在长期内更具可读性和可维护性。StyleX 能够利用静态分析和构建时工具来跨组件去重样式，并删除未使用的样式。</p></li><li><p><strong>可测试性</strong>：StyleX 可以配置为输出调试类名，而不是功能性的原子类名。这可以用于生成快照，以便在对设计进行轻微更改时不会经常变化。通过这种方式，开发人员可以更轻松地测试和验证样式的正确性，从而提高开发效率和产品质量。</p></li></ul><p><strong>示例代码</strong></p><pre><code class="language-javascript">import stylex from '@stylexjs/stylex';

const styles = stylex.create({
  root: {
    padding: 10,
  },
  element: {
    backgroundColor: 'red',
  },
});

const styleProps = stylex.apply(styles.root, styles.element);</code></pre><p><strong>下面是一个按钮组件的示例代码</strong></p><pre><code class="language-javascript">import * as stylex from "@stylexjs/stylex";

const styles = stylex.create({
  base: {
    appearance: "none",
    borderWidth: 0,
    borderStyle: "none",
    backgroundColor: "blue",
    color: "white",
    borderRadius: 4,
    paddingBlock: 4,
    paddingInline: 8,
  },
});

export default function Button({
  onClick,
  children,
}: Readonly&lt;{
  onClick: () =&gt; void;
  children: React.ReactNode;
}&gt;) {
  return (
    &lt;button {...stylex.props(styles.base)} onClick={onClick}&gt;
      {children}
    &lt;/button&gt;
  );
}</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:39:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270597/facebook-stylex-css-in-js</guid>
            <link>https://www.oschina.net/news/270597/facebook-stylex-css-in-js</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Arc 浏览器开始 Windows 版 Beta 测试]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>12 月 11 日，Arc 浏览器开始 Windows 版 Beta 测试，第一批邀请已在加入等待队列的用户中筛选并发送完毕。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e387f48e7934b2c6d34f92595dbaea17a39.png" referrerpolicy="no-referrer"></p><p>感兴趣的用户可以在上线的&nbsp;<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.isarconwindowsyet.com%2F" target="_blank">IsArcOnWindowsYet</a></u>&nbsp;页面中，填写表单加入等待队列。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-688afd52e0b014510739c8706073d792afb.png" referrerpolicy="no-referrer"></p><p><strong>Arc 基于 Chromium 并用 Swift 语言编写</strong>。它支持 Chrome 浏览器扩充功能，同时默认使用 Google 搜索。7 月份，Arc 正式发布了 1.0。</p><blockquote><p><u><strong><em><a href="https://www.oschina.net/news/251034/arc-browser-1-0-mac-released">Arc 浏览器正式发布 1.0，声称是 Chrome 的替代品</a></em></strong></u></p></blockquote><p>Arc 旨在成为一个 「万维网的操作系统」，并试图将网页浏览与内置应用程序和功能整合在一起。其内置的功能包括虚拟记事本、拼贴风格的 「easel」 和 「boosts」，该功能允许用户美化和重新设计网站界面。Arc 的选项卡垂直排列在侧边栏中，侧边栏包含除浏览窗口之外的所有浏览器功能。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-34b7b76a856863e0f84e96557bd15c058e6.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270593/arc-for-windows-is-in-beta</guid>
            <link>https://www.oschina.net/news/270593/arc-for-windows-is-in-beta</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Google Colab 现已支持直接使用 🤗 transformers 库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 编辑器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;"><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Google Colab，全称 Colaboratory，是 Google Research 团队开发的一款产品。在 Colab 中，任何人都可以通过浏览器编写和执行任意 Python 代码。它尤其适合机器学习、数据分析和教育目的。从技术上来说，Colab 是一种托管式 Jupyter 笔记本服务。用户无需设置，就可以直接使用，同时还能获得 GPU 等计算资源的免费使用权限。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005864" data-ratio="0.6592592592592592" src="https://oscimg.oschina.net/oscnet/6aca6440-d2d5-4972-8624-54894772e85a.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">通过与 Colab 团队的共同努力，Colab 托管的运行时镜像现已默认集成了 Hugging Face transformers 库，只需简单执行 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">import transformers</code> 即可轻松接入！对于使用 Colab 进行机器学习和深度学习研究的开发者来说，这是一个非常重要的更新。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你想使用最新版本的 transformers，Colab 团队也提供了一个简单的命令 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">!pip install transformers --upgrade</code>，以便于随时更新至最新版本。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">除了提升用户体验，这一更新还开启了一些有趣的新功能。例如，用户现在可以直接从 Pandas 读取 Hugging Face 数据集，这将大大简化数据处理和模型训练的工作流程。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005865" data-ratio="0.4203703703703704" src="https://oscimg.oschina.net/oscnet/8ecbb7d1-9659-48de-9e0f-64e60f62d9ef.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">本合作和更新还开启了一些有趣的新功能。例如，用户现在可以直接从 Pandas 读取 Hugging Face 数据集，这将大大简化数据处理和模型训练的工作流程。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">你可以通过 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">hf://datasets/</code> 的方式在 Pandas 中直接读取 Hugging Face Hub 上的数据集。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">感谢 Colab 团队的朋友们，也希望社区的成员们喜欢本次的合作和功能更新！</p></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - Hugging Face（gh_504339124f0f）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:07:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10316003</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10316003</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 开源调试工具 ixGDB]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-readme-for-ixgdb-release" class="anchor" href="https://gitee.com/deep-spark/ixgdb#readme-for-ixgdb-release"></a>README for ixGDB release</h1><h2><a id="user-content-introduction" class="anchor" href="https://gitee.com/deep-spark/ixgdb#introduction"></a>INTRODUCTION</h2><p>ixGDB is Iluvatar CUDA source-level debugger for Linux OS, based on NVIDIA <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fcuda-gdb">CUDA-GDB</a> 10.2.</p><p>ixGDB provides the following capabilities:</p><ul><li>Provides a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application.</li><li>Supports debugging C/C++ applications and all CUDA applications, which might use CUDA driver APIs or CUDA runtime APIs.</li><li>Supports setting breakpoints.</li></ul><h2><a id="user-content-build-instructions-example-only-adjust-as-needed" class="anchor" href="https://gitee.com/deep-spark/ixgdb#build-instructions-example-only-adjust-as-needed"></a>BUILD INSTRUCTIONS (example only, adjust as needed)</h2><p>First, make sure that libtermcap and other required dependent packages are
installed (try "sudo yum install ncurses-devel"). The "configure" command will
report an error if some packages are missing.</p><p>Please note that the libexpat development headers must be present if ixGDB is to be used for cross-platform debugging.</p><p>Issue the following commands to build ixGDB:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">./configure --program-prefix=cuda- \</span><span id="LC2" class="line">    --enable-cuda \</span><span id="LC3" class="line">    --enable-targets="x86_64-apple-darwin,x86_64-unknown-linux-gnu,\</span><span id="LC4" class="line">    arm-elf-linux-gnu,m68k-unknown-linux-gnu" \</span><span id="LC5" class="line">    CFLAGS='-I/usr/local/cuda/include' \</span><span id="LC6" class="line">    LDFLAGS='-lpthread'</span><span id="LC7" class="line">make</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-using-ixgdb" class="anchor" href="https://gitee.com/deep-spark/ixgdb#using-ixgdb"></a>USING ixGDB</h2><p>All standard GDB commands could be used for both CPU and GPU code debugging. In addition to that, ixGDB provides CUDA-specific command families like "info cuda ..." to query GPU states, "cuda .." to control debugger focus on GPU and "[get|set] cuda .." to alter/query CUDA debugger configuration. If you want to know more about how to use ixGDB, please go to Iluvatar CoreX support <a href="https://gitee.com/link?target=https%3A%2F%2Fsupport.iluvatar.com%2F%23%2FDocumentCentre%3Fid%3D1%26nameCenter%3D1%26productId%3D">official site</a> and use "ixgdb" as the keyword to find document "SDK Tools User Guide", which includes detailed usage of ixGDB.</p><h2><a id="user-content-communication" class="anchor" href="https://gitee.com/deep-spark/ixgdb#communication"></a>COMMUNICATION</h2><p><a href="https://gitee.com/deep-spark/ixgdb/issues">Gitee Issues</a>: bug reports, feature requests, install issues, usage issues, etc.</p><h2><a id="user-content-license" class="anchor" href="https://gitee.com/deep-spark/ixgdb#license"></a>LICENSE</h2><p>Licensee's use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">This product includes copyrighted third-party software licensed under the terms of the GNU General Public License v3 ("GPL v3"). All third-party software packages are copyright by their respective authors.</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses.</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Component    License</span><span id="LC2" class="line">ixGDB        GPL v3</span></pre><div class="markdown-code-block-copy-btn"></div></div></div>]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/deep-spark/ixgdb</guid>
            <link>https://gitee.com/deep-spark/ixgdb</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 语言大模型的推理技巧]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p><img src="https://oscimg.oschina.net/oscnet/7d0eafbb-7a1e-416d-83e4-ac07a8583a4b.jpg" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px"><span>本文探讨了一系列<span style="background-color:#efefef">语言大模型的</span>推理优化技巧，涵盖 KV 缓存、量化和稀疏性等方法，并分享了如何有效实施这些技术。对于想要优化 Transformer 模型，以期提升推理速度或效</span><span>率的人来说</span><span>值得一读。</span></p><p>&nbsp;</p><p><span>本文作者为机器学习研究员 Finbarr Timbers，他曾是 DeepMind 的工程师。</span><span>（本文由 OneFlow 编译发布，转载请联系授权。原文：</span><span>https://www.artfintel.com/p/transformer-inference-tricks）</span></p><p>&nbsp;</p><p><strong><span style="color:#3f3f3f">作者 |&nbsp;</span></strong><strong><span>Finbarr Timbers</span></strong></p><p><strong><span style="color:#3f3f3f">OneFlow 编译</span></strong></p><p><strong><span style="color:#3f3f3f">翻译｜杨婷、宛子琳</span></strong></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">1</span></strong></span></p><span id="OSC_h2_1"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">键值（KV）缓存</span></strong></span></h2><p>&nbsp;</p><p><span>目前，键值（KV）缓存是最常见（也是最重要）的解码器优化方法。在解码器模型中，对于每次解码迭代，提示的键和值将是相同的。此外，一旦你运行了一个词元，该词元的键和值将在后续的每个迭代中保持不变。因此，你可以缓存提示，并在解码时逐渐将每个词元的 KV 张量添加到缓存中，这样可以减少大量计算。在注意力机制中，我们能够将形状为（batch, context_length, feature_dim）的两个张量相乘，变为将形状为（batch, 1, feature_dim）的查询张量与形状为（batch, context_length, feature_dim）的 KV 张量相乘。因此，采样的复杂度不再是二次方，这使我们能够获得更长上下文长度的良好解码（采样）性能。</span></p><p>&nbsp;</p><p><span>实际上，这会在你的实现中增加复杂性，因为现在你不仅仅是运行纯函数，而且有了状态（state），所以即便一个序列已经完成了推理，你仍需要持续运行推理（<span style="color:#888888"><em>参见 Google MaxText 的实现，https://github.com/google/maxtext</em></span>）。</span></p><p>&nbsp;</p><p><span>KV 缓存需要 2 * n_layers * n_heads * d_head 个参数。对于 GPT-3，其中 n_layers = 96，n_heads = 96，d_head = 128，这意味着每个上下文中的词元需要 2.4m 个参数。使用典型的 16 位精度，每个词元需要 5MB；如果上下文窗口有 2048 个词元，那就需要将 10GB 的 HBM 用于 KV 缓存。这虽然昂贵，但每 GB 的消耗都物有所值。</span></p><p>&nbsp;</p><p><span>这些内存需求是在消费级 GPU 上训练语言大模型如此困难的重要原因之一。目前最强大的消费级显卡是 4090，只有 24GB 的 HBM。虽然其每秒浮点运算次数（FLOPS）可与企业级芯片相媲美，但其内存限制要低得多，这使得难以将权重和 KV 缓存置入内存。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">2</span></strong></span></p><span id="OSC_h2_2"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">推测性解码</span></strong></span></h2><p>&nbsp;</p><p><span>推测性解码是一种在计算能力充裕时使用的技术，通常用于本地推理设置。它利用了现代加速器的特性，即在批次数据上运行推理所需的时间与在单个数据点上运行推理的时间相同。以 A100 为例，你可以在相同的时间内对多达 160 个数据点进行推理，所需推理时间与单个数据点相同。因此，现在已经出现了许多利用这一特性的技术，如束搜索（beam search）、MCTS（蒙特卡洛树搜索）或推测性解码。</span></p><p>&nbsp;</p><p><span>推测性解码包括两个模型：一个小而快的模型以及大而慢的模型。由于现代解码器的推理速度与参数数量成正比，使用较小的模型可以在大型模型运行一次推理所需的时间内运行多次推理。</span></p><p>&nbsp;</p><p><span>现代解码器模型（如 GPT 系列）使用了自回归采样技术，即要对 N 个词元的序列进行采样，模型会进行 N 次推理，每次推理都要使用前一次推理的结果。</span></p><p>&nbsp;</p><p><span>在推测性解码中，你会并行运行这两个模型。快速模型会运行一批推理并猜测大模型将预测哪些词元，然后将这些猜测相叠加。与此同时，大模型在后台运行，检查较小模型是否记录了相同结果。较小模型能够在大模型进行一次推理的时间内进行多次猜测。然而，鉴于我们有多余的计算能力，大模型能够并行评估所有猜测。因此，我们支付顺序生成序列成本的唯一地方是在较小的模型上。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/59e12167-b54f-4b1c-bcb5-4479facca980.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>推测性解码的主要缺点是它需要一个「草稿（draft）」模型，该模型能够预测较大模型的输出，而且你必须让两个模型同时存在于同一台机器的内存中（或者在多 GPU 设置下的同一节点上）。这增加了复杂性，需要额外的工作，因为你必须训练两个模型（原始模型和「草稿」模型）。此外，任何性能提升都受限于小模型能够在多大程度上精确地预测大模型。如果小模型始终能够准确预测大模型的行为，那么我们就可以直接使用它！因此，推测性解码能够发挥作用的程度存在根本差距。HuggingFace 声称它通常可以将解码速率提高一倍，这与原始论文（<span style="color:#888888"><em>https://arxiv.org/abs/2211.17192</em></span>）中声称的 2 至 3 倍的提升一致。</span></p><p>&nbsp;</p><p><span>最近出现了一种试图改进推测性解码的前向解码（Lookahead Decoding）技术（<span style="color:#888888"><em>https://lmsys.org/blog/2023-11-21-lookahead-decoding/</em></span>），该技术让模型生成 n-gram，然后在无需草稿模型的情况下递归匹配这些 n-gram。这种技术被称为 Jacobi 解码（来自他们的博客截图），可能是对贪婪解码的潜在改进。Jacobi 解码的工作原理是在生成词元的每一点上生成 n 个词元，对整个序列进行「猜测」。然后，将其与先前的猜测相验证，如果两者匹配，就接受该猜测。这可以在没有副作用的情况下减少时延，因为在最坏的情况下，它会变成贪婪解码。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/d545a2df-0729-4b32-a7dc-390784598002.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>前向解码通过保留解码过程中生成的 n-gram，并尝试将它们用作猜测，进一步改进了这一技术。鉴于已生成的文本与将要生成的文本之间存在很高的相关性，这也有可能以极低的成本，显著改进时延。这一技巧非常巧妙。考虑到这项技术才发布不久，我非常好奇它在实际场景中的性能表现。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/cdf7da2f-7ac8-475c-90ff-a2baacf4b4c3.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">3</span></strong></span></p><span id="OSC_h2_3"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">有效稀疏性</span></strong></span></h2><p>&nbsp;</p><p><span>在仅解码器 Transformer 中，模型核心是注意力机制，可总结为如下的注意力方程：</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/51f1f3da-d1e8-4cff-b3c2-b1f2f9a7af30.png" referrerpolicy="no-referrer"></p><p><span>softmax 操作会使非最大值变得很小。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/a66d9144-d636-4c8e-a891-8a79045d5e40.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p style="text-align:left"><span>因此，我们将数值张量（在注意力方程中用 V 表示）与一个主要由零（zero）组成的张量相乘。结果，注意力机制的输出中包含了大量的零，最高可达 97%（<span style="color:#888888"><em>https://x.com/YesThisIsLion/status/1647747069086666752?s=20）</em></span>。类似地，在多层感知器网络（MLP）中的每个 ReLU 之后，我们也得到了大量稀疏性。</span></p><p>&nbsp;</p><p><span>不幸的是，现在要实际利用这一点比较困难。如果权重中存在稀疏性，那么可通过结构化稀疏性（例如</span><span>tor<span>‍</span>ch.sparse</span><span>）做大量工作，但目前还不清楚系统能够多大程度地利用激活的稀疏性。</span></p><p>&nbsp;</p><p><span>可以进行的一个优化是：如果某个激活为零，那么可以跳过加载与该激活对应的权重，并避免相应计算。据我所知，这并未很好地得到主流张量计算程序的支持，但对于</span><span>Llama.cpp</span><span>等自定义推理实现来说，这一优化比较容易实现。</span></p><p>&nbsp;</p><p><span>这是因为激活是每个词元的函数，因此有效稀疏性也是随机分布在词元上。因此，这种优化的效果会随着批大小的增加呈指数级衰减。假设我们的有效稀疏性为 X%，批大小为 N，那么对于一个给定激活的所有条目在整个批次中都为零的概率可以表示为 X^N。我制作了一张表格，列出了不同 X 和 N 值的情况。这种衰减效应非常显著。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/a82589c0-f80d-4a43-aab4-348ae7b1f293.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>因此，除批大小为 1 的情况，利用这一方法十分困难，即使在这种情况下，使用推测性解码通常更为有效。但如果你想要在本地运行推理，并且确实需要降低时延，这可能是一个很棒的技巧。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">4</span></strong></span></p><span id="OSC_h2_4"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">量化</span></strong></span></h2><p>&nbsp;</p><p><span>量化是人们更为熟悉的技巧之一。我之前已经写过量化的相关内容 (<span style="color:#888888"><em>https://finbarrtimbers.substack.com/p/efficient-llm-inference</em></span>），所以不打算在具体方法上花费太多时间。我们很难精确度量量化的效果。GPTQ 论文等文献所使用的模型与 SOTA 模型差距较大，因为大型实验室并未公开其所使用的模型，并且学术界无法与大型实验室所拥有的资源相匹敌。</span></p><p>&nbsp;</p><p><img src="https://oscimg.oschina.net/oscnet/1bcc4196-002b-475d-a149-812f9b6e70b7.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>例如，GPTQ 报告了 OPT 和 BLOOM 模型的量化结果，这些结果远不如当前的一系列开源模型，更不用说 GPT-4 了。</span></p><p>&nbsp;</p><p><span>当然，大型实验室并未公开其研究进展，而我看到的大部分个案报告都来自那些试图在消费级硬件上运行较小模型的人，这种硬件的内存非常有限。我认为，很多业余爱好者（即非大型实验室研究人员）都被在本地运行庞大模型的吸引力所诱惑，因此他们对量化产生了浓厚兴趣。但实际上，量化并不具备固有优势！从第一性原则出发，如果你有两个位数相同的模型，它们应该具有相同数量的词元/秒，并且应该具有类似的性能水平。只有在使用更高精度格式的位数时做得很糟，才会有较大差异。</span></p><p>&nbsp;</p><p><span>但文献中的观点与我的直觉不一致。上述 GPTQ 论文发现，将模型量化为低至 4 倍的精度时，性能的下降微乎其微。我认为，这是因为性能更差的模型更容易在量化过程中保持其性能不受损。如果假设两个相同的 LLM，一个经过 2 万亿词元的训练，另一个经过 5000 亿词元的训练（分别称为 LLM-2T、LLM-500B），在进行量化时，我认为经过更多词元训练的模型在性能上受到的影响更大，因为它应该更充分地利用这些词元。我们仍然预计经过量化的 LLM-2T 会优于 LLM-500B，但我认为从 LLM-2T 到经过量化的 LLM-2T 的性能下降，会比从 LLM-500B 到经过量化的 LLM-500B 的下降更显著。</span></p><p>&nbsp;</p><p><span>注：虽然上述论点很有说服力，但实际上并没有相关的文献支持。量化似乎确实非常接近于「免费的午餐」。</span></p><p>&nbsp;</p><p style="text-align:left"><span>近期的研究，如关于 k-bit 推理规模定律的论文（<span style="color:#888888"><em>https://arxiv.org/abs/2212.09720</em></span>），在一系列 LLM 架构上进行了大量实验，得出了不同的位数分配对模型性能的影响。他们研究了在给定精度水平下使用 N 个参数的模型与使用 2N 个参数和一半精度的模型之间的权衡。其结果非常引人注目，与未进行量化的性能几乎没有差别（至少对于 4 位或更多位而言）。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/e695f055-912b-4be6-99bf-e046a7191224.png" referrerpolicy="no-referrer"></p><p><span>基本上，他们发现可以将精度降至 4 位而不损失任何性能，量化几乎不会导致任何权衡。你可以运行一个小 4 倍的模型而不会显著降低性能。由于在现代加速器上推理性能等于处理的位数（即使用较少精度时每秒可以获得更多的运算次数），这点很有帮助。</span></p><p>&nbsp;</p><p><span>因此，我的结论是：推荐采纳「k-bit 推理论文」的建议。然而，对于生产负载，我对使用低于 8 位的精度还有些犹豫。fp8 是目前现代加速器本地支持的最低精度浮点格式，即使如此，支持也是有限的。我建议在 fp8 精度下进行训练和推理，并观察进一步量化可能带来的精度损失对你的用例来说是否可以接受。当生产环境中缺乏来自这些平台（例如 Nvidia 和 Torch/JAX 团队）的本地支持时，我很难推荐在生产环境中使用更低级别的精度。</span></p><p>&nbsp;</p><p><span>根据我从文献中了解到的（这与我的直觉相符），fp8 严格来说优于 int8，但在硬件上的支持有限。如果你在一个 GPU 资源充沛的组织，并且能够将 H100 用于所有任务，那么请使用 fp8。否则，也可以使用 int8，而且相比起来要容易得多（PyTorch 使其变得相当容易，尽管 API 不太稳定）。</span></p><p>&nbsp;</p><p><span>关于实际进行模型量化，PyTorch 团队已经撰写了一篇关于如何具体操作的文章（<span style="color:#888888"><em>https://pytorch.org/blog/accelerating-generative-ai/</em></span>），并提供了一系列 API 用于简化操作，尽管它们不太稳定。此外，bitsandbytes 是另一个出色的量化库，不过我个人还未使用过。</span></p><p>&nbsp;</p><p><span>（特别感谢@cis_female 与我讨论稀疏性的复杂性，以及@nostalgebraist 纠正量化部分中的错误。我现在认为，证据表明，至少量化到 4 位或更多位，在性能方面的权衡非常小。）</span></p><p>&nbsp;</p><p>&nbsp;</p><span id="OSC_h2_5"></span><h2 style="margin-left:8px; margin-right:8px; text-align:left">&nbsp;</h2><p><span style="background-color:#ffffff; color:#888888">其他人都在看</span></p><span id="OSC_h3_6"></span><h3 style="text-align:left">&nbsp;</h3><ul><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247491309%26idx%3D1%26sn%3D407fefb7ec76a2c9fdfe1ae960f7de4d%26chksm%3Dfe4190dbc93619cd0b9bfecb979e142a125fd8548d323dd9bdc2cbeb619400202e6e1affced0%26scene%3D21%23wechat_redirect" target="_blank">大型语言模型的推理演算</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493088%26idx%3D1%26sn%3Dff319e3b8cc19f165232c3226779588c%26chksm%3Dfe426bd6c935e2c0946fdaa96378d04123f31eb797e39c8ab0d619bbb00db8b665354a167583%26scene%3D21%23wechat_redirect" target="_blank">LoRA 微调语言大模型的实用技巧</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492976%26idx%3D1%26sn%3Dd919a508ce238048ae44e58b9cc06b71%26chksm%3Dfe426b46c935e2500178c2d2c8845fcd3e47fdeb5ec51f55b80cbca5f7b78382cdb3e6fe6a32%26scene%3D21%23wechat_redirect" target="_blank">可复现的语言大模型推理性能指标</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492990%26idx%3D1%26sn%3D50844c8911834baf44863a9e3754175f%26chksm%3Dfe426b48c935e25ede3f772624ba262011b1b48f8ee78ac6d3b1daa5aaf71e7583828740b5cd%26scene%3D21%23wechat_redirect" target="_blank">ChatGPT 规模化服务的经验与教训</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493053%26idx%3D1%26sn%3Dfe7a51fbda920626b55d8919dd780e05%26chksm%3Dfe426b8bc935e29d3a806dfd6682619ee46effa39b09777907d5bccfd80d8cd639580c6f6e23%26scene%3D21%23wechat_redirect" target="_blank">机器学习硬件十年：性能变迁与趋势</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492951%26idx%3D1%26sn%3D873b7c63ea18d638a9570bb582cddbb5%26chksm%3Dfe426b61c935e277e17fd2d4b06fa3ec998479ae87d84312f064dbae0e65875a7cb45829807d%26scene%3D21%23wechat_redirect" target="_blank">微调语言大模型选 LoRA 还是全参数？</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492811%26idx%3D1%26sn%3D916e330a2a4152dab3192635c3e475fa%26chksm%3Dfe426afdc935e3eb2f371ff5f56247c95800ce91a950a89bea871c26ddc4c3d13371acf03978%26scene%3D21%23wechat_redirect" target="_blank">语言大模型推理性能工程：最佳实践</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493030%26idx%3D1%26sn%3D58a43ed078977019c997a110526d7c02%26chksm%3Dfe426b90c935e28688b6e317a991bedaaa164471a275d64e60851a09b00f7f6b718e27d7b411%26scene%3D21%23wechat_redirect" target="_blank">语言大模型的分布式训练与高效微调指南</a></p></li></ul><span id="OSC_h3_7"></span><h3 style="text-align:left">&nbsp;</h3><p><strong><span>试用 OneFlow: <a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgithub.com%2FOneflow-Inc%2Foneflow%2F" target="_blank">github.com/Oneflow-Inc/oneflow/</a></span></strong></p><p style="color:#3f3f3f; margin-left:8px; margin-right:8px; text-align:left"><img src="https://oscimg.oschina.net/oscnet/f2b38f8c-5887-4315-9787-03816b68ada4.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p></div><p style="color:#858585">本文分享自微信公众号 - OneFlow（OneFlowTechnology）。<br> 如有侵权，请联系 support@oschina.cn 删除。<br> 本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/oneflow/blog/10320747</guid>
            <link>https://my.oschina.net/oneflow/blog/10320747</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源 MoE 模型 Mixtral 8x7B 性能超过 GPT-3.5]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>大模型创业公司 Mistral AI 终于<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">介绍了</a></u>前两天「开源」的&nbsp;MoE 模型 <strong>Mixtral 8x7B</strong>。</p><blockquote><p><strong><em><u><a href="https://www.oschina.net/news/270317/mixtral-8x7b-32kseqlen">Mistral AI 用「磁链链接」开源了 87 GB 的 8x7B MoE 模型</a></u></em></strong></p></blockquote><p>官方称，Mixtral 8x7B 是开放权重的高质量<strong>稀疏混合专家模型 (SMoE)</strong>，采用 Apache 2.0 License 开源。在大多数基准测试中，Mixtral 的成绩都优于 Llama 2-70B，且推理速度提升了 6 倍。而且在大多数标准基准测试中超过 GPT-3.5。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7a689c4f538b591b9744038a052717945e6.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-84fefd9ee6c091c07c894031a1af2faf2e3.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-9f9aaad324856028fb1e796beb2d7685020.png" referrerpolicy="no-referrer"></p><p>因此，Mistral AI 称 Mixtral 是最强大的开放权重模型，也是成本/性能权衡方面的最佳模型。</p><p><strong>Mixtral 主要特性</strong></p><p>• 32k 上下文<br> • 支持英语、法语、意大利语、德语和西班牙语<br> • 性能超过 Llama 2 系列和 GPT-3.5<br> • 在代码生成方面具有强劲性能<br> • 在 MT-Bench 上获得 8.3 分</p><p>Mixtral 作为稀疏混合专家网络，是一个纯解码器模型，其中前馈块从 8 组不同的参数组中选择。在每一层，对于每个 token，路由网络选择两组「专家」来处理 token 并相加地结合它们的输出。</p><p>Mixtral 总共有 45B 个参数，但每个 token 只使用 12B 个参数。因此，它以与 12B 模型相同的速度和成本处理输入和生成输出。</p><p>更多细节查看：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">https://mistral.ai/news/mixtral-of-experts/</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 10:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270511/mixtral-of-experts</guid>
            <link>https://www.oschina.net/news/270511/mixtral-of-experts</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[荣耀申请魔方大模型商标]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#222222">天眼查信息显示，荣耀终端有限公司近日申请注册「荣耀魔方大模型」商标，国际分类为网站服务，当前商标状态为等待实质审查。</span></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-7baf34d7d00360b976559630121d67b0da4.png" width="700" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">此前，该公司曾申请两枚「MAGIC&nbsp;大模型」商标。荣耀 CEO 赵明曾发文称，荣耀即将推出自研端侧 AI 大模型和全新云服务。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 08:25:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270492</guid>
            <link>https://www.oschina.net/news/270492</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[双十一弹性能力支撑 - ECI 稳定性建设]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h3_1"></span><h3>一、关于 ECI</h3><p style="text-align:justify">背景从 2018 年正式发布，<strong>ECI 已经打磨了整整 4 个年头</strong>，如今也已经快速成长为了阿里云 serverless 容器的基础设施，服务着阿里内外众多的公有云客户与云产品，每天承接着数百万的弹性容器创建。</p><p style="text-align:justify">然而，ECI 这些年却未参与到集团双十一大促，双十一可以说是阿里技术人的阅兵，能不能承接住双十一的流量成为了检验一个产品是否稳定可靠的重要标准。但一切都是水到渠成，就在今年，ASI 开始与 ECI 对接，尝试让 ECI 承接双十一大促的弹性的 30W 核算力，我们都知道双十一大促对于整个阿里集团的意义，使命将至，我们必将全身心地投入到对接、压测、护航的工作中。经过长达两个多月的业务适配、压测、备战，最终完成了双十一大促的弹性容器的圆满交付。这背后，离不开 ASI、ECI 以及参与到其中的每一位脚踏实地、用心钻研、保驾护航的同学的努力。ECI 今年首次作为集团大促弹性基础设施，根据线上数据统计，大促期间 ECI 弹性资源使用共计约 400W 核，从资源的瞬时弹性、保有规模、系统稳定性等多方面对云原生系统都是一次巨大的考验。作为底层的计算单元，ECI 此次也成功顶住了双十一弹性流量洪峰的考验，在感叹 serverless、容器这些技术发展迅猛的同时，对于全新的系统架构稳定性的考验也不小。</p><p style="text-align:justify">如今再回过头来看 ECI 的第一次双十一，我们有必要做一次全面的总结，我们为集团弹性保障做了哪些工作，哪些是将来可以复用的工作，哪些是可以给其他的团队作为借鉴的技术和经验，以及哪些地方还可以做的更好，为下一次大促做准备。</p><p style="text-align:justify">本文我们将为大家介绍，<strong>ECI 这些年在稳定性方面做了哪些工作，以及是如何来为集团双十一保驾护航的。</strong></p><span id="OSC_h3_2"></span><h3>二、遇到的挑战</h3><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-484afd33bc5b8c5f4774ab7fddf53a98_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">大规模并发带来的稳定性挑战遇到的最大挑战首先是大规模并发带来的。容器保有量增多之后，从容器实例生产方面来看对于云管控系统是不小的考验，尤其是对于弹性场景来讲，需要在极短的时间进行实例的生产，镜像的大规模拉取，进而保障容器的成功启动。</p><p style="text-align:justify"><strong>如何能保障实例的大规模成功生产</strong>，如何先于线上发现问题，以及即使出现了问题如何第一时间止血并进行故障恢复，这对于集团双十一期间的业务重保都是尤为重要的。除此之外，对于公有云环境来讲，不能影响到其他的公有云客户也是需要重点关注的，因此需要具备一套完整的稳定性保障体系以及故障应对方案以确保双十一期间的业务能够顺利进行。实例生产系统稳定性 ECI 和 ECS 共用一套资源调度系统，相对于 ECS 容忍度为分钟级别的应用来讲，ECI 实例频繁的创建删除对调度系统的要求更为苛刻，对系统容量以及稳定性保障方面提了更高的要求。服务可用性保障 ECI 安全沙箱由于某种原因异常（OOM/物理机宕机/kernel panic），导致不健康情况。这种情况下，k8s 层面如果不从 endpoint 上摘除这个 ECI Pod，会导致请求通过负载均衡依然可以路由到这台不健康的 ECI 上，会导致业务请求成功率下降，因此对于集团业务服务可用性保障也是尤为重要的。</p><span id="OSC_h3_3"></span><h3>三、ECI 稳定性技术建设</h3><p style="text-align:justify">稳定性保障从需求收集准备阶段开始，双十一大促持续两个月之久，为了配合集团全链路验收，ECI 自身的稳定性保障工作也随之紧锣密鼓地进行。</p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-353460c616617e11f187c76359272f10_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">稳定性的保障贯穿了整个大促过程，大促前慎重/减少系统变更以排除人为因素的干扰，敬畏发布，多次压测演预案确保系统稳定性，不断提升系统抵抗力稳定性和系统恢复力稳定性，以保障大促的顺利进行，最后通过问题覆盘沉淀出可复制的大客户重保策略，这对于未经过双十一实战演练具有积极的意义。</p><p style="text-align:justify">因此我们梳理出了整个大促期间围绕稳定性方面做的主要工作，主要包括风险控制、关键业务依赖梳理、技术保障、压测预案、运行时保障、故障运维能力、以及最后的覆盘优化，希望以此能对今后的大促工作作为指导，并沉淀出稳定性治理的经验。接下来我们对此次大促涉及到的主要稳定性保障方法以及如何应用进行介绍。</p><p style="text-align:justify">实例生产保障 VM 复用技术实例生产行为的保障是集团弹性使用 ECI 的重中之重。一个典型的实例生产过程如图所示，<strong>ECS 和 ECI 在控制面共用一套管控系统，</strong>ECI 管控侧调用资源调度系统之后会分配计算资源之后会调用 pync（阿里云单机管控组件），进而调用 avs(阿里云单机网络组件) 和 tdc(阿里云单机存储组件) 分别生产网卡与磁盘。在此过程中，对于调用 ECS 依赖的 open api 接口较重，在大规模创建删除场景很快成为系统瓶颈，此前我们专门针对容器实例高频创建删除场景开发了 VM 复用功能，对于高频场景删除容器实例的场景，延迟 vm 的回收，并复用容器实例的网卡、镜像、计算资源，降低对管控系统整体的冲击，以此来保障实例生产系统的稳定性，从此次双十一的实战演练效果来看，vm 复用取得了很好的效果，管控系统容量整体处于正常水位，保障了集团双十一实例稳定的弹性能力。</p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-dcd697c3f23312c91e09c0272f34fbd8_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">重调度机制对于库存不足或者远程服务调用超时等情况，为了保障实例生产的最终一致性，对于 ECI 实例生产我们设计了相应的故障处理策略策略，取值如下：fail-back：失败自动恢复。即 Pod 创建失败后自动尝试重新创建 fail-over：失败转移。效果等同于 fail-backfail-fast：快速失败。即 Pod 创建失败后直接报错故障处理策略本质上是一种重调度的策略。原生的 k8s 调度支持重调度，即调度失败后会将 pod 重新放入调度队列等待下次调度，类比 k8s 的重调度行为，当 eci 管控系统收到创建请求的时候，首先会进入一个队列，然后有个异步定时任务会将创建从队列中捞起，提交到异步工作流进行实际的资源生产、以及容器的启动等。即便是结合了多可用区和多规格的优化，异步工作流依然有可能失败的，比如资源的争抢、内网 ip 不足、启动失败等，这时候就需要将创建请求再次重回队列，等待被重新调度生产。</p><p style="text-align:justify">我们目前对于<strong>故障处理策略</strong>：</p><p style="text-align:justify">1、失败的任务会一直重试，但是我们会计算每个任务的执行周期，重试次数越多，执行周期越长，以达到退避效果。</p><p style="text-align:justify">2、优先级策略会考虑用户级别、任务类型、任务上次失败的原因等因素，优先级高的任务优先提交执行。</p><p style="text-align:justify">3、每次调度失败的原因都会以标准事件的方式通知到 k8s 集群。队列里的任务的整个执行流程的状态机如下：</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-e09200aea1a82cf515617f061a7bb54a_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">所有执行失败的任务都会重新进入队列，等待被再次调度。由于任务会在任何一步失败，所以所有生产出来的资源都会回滚，回滚结束后，进入初始状态。初始状态的任务会被拉起执行，然后提交到异步生产。如果生产失败，就会再次回到等待调度的状态。如果生产成功，任务就结束，到达终态。基于我们的重调度机制，可以极大的减少由于生产系统抖动造成实例生产失败的情况，对于容器启动成功率要求高的场景可以保障实例生产的最终一致性，对于容器启动成功率要求不那么严格的场景可以快速失败，由上层业务进行处理。</p><p style="text-align:justify">服务容错降级对于故障场景，系统依赖服务的降级也是十分重要的。大多数进行限流降级的方案主要关注点在服务的稳定性，当调用链路中某个资源依赖出现异常，例如，表现为 timeout，异常比例升高的时候，则对这个资源的调用进行限制，并让请求快速失败或返回预设静态值，避免影响到其它的资源，最终产生雪崩的效果。ECI 目前实现了基于历史日志自学习进行无损降级、本地 cache 降级、流控降级 3 级降级机制框架，ECS/ECI openapi 全面接入，内部依赖 200+接口接入，根据每个接口的调用频率、RT 分布、超时时间设置来单独分析，选择合适的降级策略，设置合理的阀值，能让系统出问题时，智能降级从而进行系统保护。<strong>一个典型的降级机制实现过程如图：</strong></p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-1239ddaa0f4eab60e7996e6fec9fe364_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify"><strong>当有非资源类核心 API 新请求进入</strong>，如果历史缓存数据未过期则直接返回缓存数据，结束业务逻辑反之则请求远程接口。如果请求成功，返回数据，对数据进行缓存，同时将缓存数据以日志方式存入 sls cache log 日志用于未来降级，结束业务逻辑当远程请求失败时触发降级策略：如果失败指标（例如指定时间内异常比例）在预设时间窗口内未达到配置的降级策略阈值，则直接抛出相应业务异常，结束业务逻辑如达到降级策略阈值则按以下顺序实行降级策略：从 sls 缓存日志查找历史日志数据作为降级返回值，同时将返回值重新写入缓存，结束业务逻辑如果 sls 缓存日志没有相应日志则返回：预设静态值或空值，结束业务逻辑对于一些跟用户资源无关，更新少，属于全局参数的服务/接口，以上通用降级策略和方案可能因为降级规则阈值难以界定而无法有效执行。</p><p style="text-align:justify">针对这些接口采用 dubbo 异常直接降级的策略涉及到降级或熔断的条件：自动降级 (可选利用 Sentinal 进行自动降级)： 超时，异常，限流手动开关支持核心非资源 api 直接进行 openapi 本地降级 cache 对于严重的系统故障，可以将核心几个 describe api 进行 openapi 本地 cache，发生故障，或有雪崩出现时，全部切到 openapi 本地 cache，在降级影响面的同时，也能减轻对下层服务的调用压力来赢取恢复时间。</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-0ce94b0355dad7e7cbc7463c70e8ad87_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">依赖服务非创建链路 dubbo 或 http 请求进行本地 moc 对于几乎不会频繁变化的依赖服务，通过每日 sls 分析进行 kv 的存储，当故障发生时，降级为备用，让降级影响面趋向于 0。其他服务降级机制大分页流控 cache 创建类 api 进行依赖 dubbo 或 http 服务降级，<strong>异步补偿操作类 api 进行链路降级，</strong>取消非必需依赖数据库降级 ro 库流量降级隔离，用户级别流量切到灰度 api 级别流量切到灰度/独立线程池日志 debug 及调用链路跟踪使用 apicontext 实现详细日志 debug 及调用全链路跟踪能力核心 api debug 日志建设，支持按用户开启 debug 日志打印 requestId 贯穿到 dao，支持随时采样，及时发现 dao 异常调用服务依赖降级容错机制可以在保障服务稳定性的前提上，利用相关接口的历史缓存数据，基于 SLS 日志无损降级，当 SLS 无数据的时候也可以采用本地静态数据兜底，构建有效返回值，在服务触发流控降级熔断后，大部分用户不会感知到服务异常。</p><p style="text-align:justify">在内部的多次故障演练中，服务降级机制可以有效保护系统由于发生故障带来的系统瘫痪。服务可用性保障在传统的 Kubernetes 集群中，如果 Node 变得不可用且达到时间阈值，那么会将 Node 上的 Pod 进行驱逐，重新在其他 Node 上拉起新的 Pod。而在 Serverless 场景下，ECI 管控会通过异步检测机制检测不健康 ECI，修改状态为不可用，同时增加导致不可用的事件，告知 ECI 用户，之后 ECI 会通过主动运维的手段治愈不健康 ECI，之后触发控制面将 ECI 恢复为 Ready 状态，<strong>主要过程如图所示：</strong></p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-790b2ccc925edb4964dede90b43256af_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify"><strong>处理不健康 ECI 的流程：</strong></p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-4e6eaa3bddc8488957271e480923d34a_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify"><strong>恢复 Ready ECI 流程预案&amp;压测除了技术方面的保障，故障注入、应急预案、压测演练在稳定性建设中也尤为重要。</strong>在双十一活动期间我们内部进行了多次压测演练，对系统中常见的性能瓶颈进行故障注入，用以模拟故障的发生，同时制定应急预案，以此应对故障已经发生时的场景。通过多次的压测摸高，一方面可以评估系统容量的承载上限，另一方面可以借此机会进行大规模压测演练，验证系统降级方案并对系统稳定性进行评估。预警&amp;监控大促进行时，预警和监控是保证系统运行时稳定性的重要措施。通过监控和预警可以及时发现系统故障，进而快速进行恢复。</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-bfb30f596763e9976d35e307e15d74a2_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-c27bfedd2fd094067dc495bbdab8340a_720w.webp" referrerpolicy="no-referrer"></p><span id="OSC_h3_4"></span><h3>四、系统的健壮性</h3><p style="text-align:justify">思考沉淀一个健壮的系统不仅需要减少问题的发生，同时要具备故障发现以及故障快速恢复的能力。除了预警和监控，运维能力建设也十分重要。</p><p style="text-align:justify"><strong>一个系统的健壮性体现在系统的容量，</strong>系统的容错能力以及系统依赖的各个资源的 sla，尤其是在云上覆杂的资源环境下，由于「木桶效应」，某一项依赖资源的很可能造成整个系统的直接不可用。因此，随着系统不断完善，我们需要通过混沌工程等方法来找出当前系统的「弱点」进而对其进行专项优化，进而提升整个系统的健壮性；其二对于系统的故障恢复以及降级能力也很重要，历史上 ECS/ECI 管控多次由於单用户或系统某个环节变慢，导致系统全链路雪崩，最终导致 P1P2 故障，ECS/ECI 管控是阿里云最复杂的管控系统，复杂的业务逻辑，内部系统依赖，非常多的环节出问题都有可能导致全链路某个应用雪崩进而全局不可用，因此，对于故障已经来临时，依赖降级能力能非常有效的保护我们的系统，这也是稳定性建设的一个十分重要的方向。</p><span id="OSC_h3_5"></span><h3>五、总结</h3><p style="text-align:justify">未来展望随着双十一最后一波流量高峰结束，ECI 顺利通过了对阿里人最严苛的技术考验--双十一，本文围绕此次参与双十一活动的经历做出总结，希望可以为今后 ECI 稳定性方面的建设积累经验，当然，这对 ECI 来说也仅仅是一步试金石，作为云原生时代的基础设施，ECI 任重而道远，共勉！</p><p style="text-align:justify">本文出品及鸣谢： 柳密、羽云、景奇、存诚、 煜枫、景止、皓瑜、月悬、佐井、尚哲、涌泉、十刀、 木名、秉辰、易观、冬岛、不物、潇洛、 怀欢、 尝君、寒亭、伯琰。</p><p style="text-align:justify"><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1389034%3Futm_content%3Dg_1000385342" target="_blank">原文链接</a></strong></p><p style="text-align:justify"><strong>本文为阿里云原创内容，未经允许不得转载。</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 06 Dec 2023 08:48:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/yunqi/blog/10319647</guid>
            <link>https://my.oschina.net/yunqi/blog/10319647</link>
            <author>
                <![CDATA[阿里云云栖号]]>
            </author>
        </item>
    </channel>
</rss>
