<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-最新资讯]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-最新资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Sun, 08 Oct 2023 01:12:14 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[苹果 App Store 免费榜第一是黄色软件]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>澎湃新闻今日报道苹果 App Store 出现伪装成学习软件的黄色软件，并且冲上了「免费 App」排行榜第一名。</p><p>据悉，该软件的年龄分级为 4 岁以上，但是会引导用户进入赌博和其他黄色网站。网友小同表示，他下载了这款软件，想要学习英语字母，结果发现是一个色情视频软件。他认为这种伪装成学习软件的行为很危险，很容易对孩子造成不良影响。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-c7a0ce5e4272f1b06c5119529647215fb11.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-69f6205b4eceb10672c0a3cff67e4f52d48.png" referrerpolicy="no-referrer"></p><p>事件被曝光后，苹果客服虽然进行了回应，但直到下午仍未下架软件。甚至排行榜更新后，App Store 免费榜第一、二名再次出现黄色软件，名为「骑 XX」、「牡丹 XXX」，年龄分级为 4 岁以上。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-8b7f27342ea504c47ae6724514736b106a3.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-db2cf9f1307034d7e3a68e35ea067a0fe4f.png" referrerpolicy="no-referrer"></p><p><strong style="color:#424242">截至发稿，这些软件已被下架</strong><span style="background-color:#ffffff; color:#424242">。</span></p><p><span style="background-color:#ffffff; color:#424242">众所周知，苹果应用商店的审核规则极为严格。</span>上面提到的 App 其实就是浏览器套壳，前端显示的内容可以通过后台随意修改。但问题在于，苹果 App 的审核团队为何让这些「套壳」 App 上架到了应用商店？</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 14:03:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260711</guid>
            <link>https://www.oschina.net/news/260711</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Rails 7.1 正式发布：可生成 Dockerfiles、更强大的自行构建身份验证系统]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Rails 7.1 已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frubyonrails.org%2F2023%2F10%2F5%2FRails-7-1-0-has-been-released" target="_blank">发布</a>。公告写道，自 Rails 7.0 以来，此版本由 800 多名贡献者提交了 5000 多次 commit，包含许多新功能和改进。</p><p><img src="https://static.oschina.net/uploads/space/2023/1007/180625_fnVn_2720166.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">主要变化如下：</p><ul><li><strong>为新的应用生成 Dockerfile</strong></li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">当运行<span>&nbsp;</span><code>rails new</code><span>&nbsp;</span>时，Rails 将生成需要使用 Kamal 或任何其他基于 Docker 的部署设置来部署应用程序的所有 Dockerfile。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">这些 Dockerfile 经过调整，适合用于生产环境，具有合适的缓存层、多阶段构建以最小化镜像，以及无论是否使用 JavaScript 构建环境所需的所有依赖项。</p><ul><li><strong>支持 Bun</strong></li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Bun 是 Zig 编写的 JavaScript 运行时，近日正式发布&nbsp;<a href="https://www.oschina.net/news/257450/bun-v1-0">1.0 正式版本</a>。此版本支持使用 Bun 作为 JavaScript 运行时来生成新应用程序。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">因此，开发者可以将<span>&nbsp;</span><code>--javascript=bun</code><span>&nbsp;</span>选项传递给<span>&nbsp;</span><code>rails new</code>。</p><ul><li><strong>更强大的自行构建身份验证系统</strong></li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">为了补充<span>&nbsp;</span><code>has_secure_password</code><span>&nbsp;</span>功能，Rails 7.1 带来了新特性来帮助开发者自行构建身份验证系统。</p><hr><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">其他变化：</p><ul><li>Active Record 支持更多异步查询</li><li>对 Trilogy MySQL 适配器的内置支持</li><li>在&nbsp;Active Record 中支持复合主键</li><li>使用&nbsp;<code>perform_all_later</code>&nbsp;对大规模任务进行排队</li><li>引入用于增强自动加载 (Enhanced Autoloading) 的&nbsp;<code>config.autoload_lib</code>&nbsp;和&nbsp;<code>config.autoload_lib_once</code></li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frubyonrails.org%2F2023%2F10%2F5%2FRails-7-1-0-has-been-released" target="_blank">详情查看 Release Notes</a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 10:07:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260696/rails-7-1-0-released</guid>
            <link>https://www.oschina.net/news/260696/rails-7-1-0-released</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Docker 与 Neo4j 等合作推出 GenAI Stack]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">Docker 在其年度 DockerCon 开发者大会主题演讲中<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.docker.com%2Fblog%2Fintroducing-a-new-genai-stack%2F" target="_blank">宣布</a>与 Neo4j、LangChain 和 Ollama 合作推出新的 GenAI Stack。该 GenAI Stack <span style="background-color:#ffffff">简化了 AI/ML 集成，</span>旨在帮助开发人员快速轻松地构建生成式 AI 应用程序，而无需搜索和配置各种技术。</span></p><p><span style="color:#000000"><img alt="" height="263" src="https://oscimg.oschina.net/oscnet/up-0852df0e6f3480e6f6d1ddd240cf679021f.webp" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">根据介绍，GenAI Stack 中包含的内容包括有：</span></p><ul><li style="text-align:start"><span style="color:#000000"><span style="background-color:#ffffff"><strong>预配置的 LLM</strong>：提供预配置的大语言模型 (LLM)，例如 Llama2、GPT-3.5 和 GPT-4，以快速启动 AI 项目。</span></span></li><li style="text-align:start"><span style="color:#000000"><span style="background-color:#ffffff"><strong>Ollama&nbsp;管理</strong>：Ollama 简化了开源 LLM 的本地管理，让你的 AI 开发过程更加顺畅。</span></span></li><li style="text-align:start"><span style="color:#000000"><span style="background-color:#ffffff"><strong>Neo4j 作为默认数据库</strong>：Neo4j 作为默认数据库，提供图形和原生向量搜索功能。这有助于揭示数据模式和关系，最终提高 AI/ML 模型的速度和准确性。Neo4j 还充当这些模型的长期存储器。</span></span></li><li style="text-align:start"><span style="color:#000000"><span style="background-color:#ffffff"><strong>Neo4j 知识图谱</strong>：Neo4j 知识图谱为 LLM 提供更精确的 GenAI 预测和结果。</span></span></li><li style="text-align:start"><span style="color:#000000"><span style="background-color:#ffffff"><strong>LangChain 编排</strong>：LangChain 促进了 LLM、应用程序和数据库之间的通信，并提供了一个强大的向量索引。LangChain 是一个用于开发由 LLM 支持的应用程序的框架。其中包括 LangSmith，一种调试、测试、评估和监控 LLM 应用程序的新方法。</span></span></li><li style="text-align:start"><span style="color:#000000"><span style="background-color:#ffffff"><strong>全面支持</strong>：提供了一系列有用的工具、代码模板、操作指南和 GenAI 最佳实践。</span></span></li></ul><p><img alt="" height="263" src="https://oscimg.oschina.net/oscnet/up-346a6330b9b20f9dbd5753904b2051aeda1.webp" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">此外，该公司还通过</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.docker.com%2Fai-early-access-program%2F" target="_blank">抢先体验计划</a><span style="color:#000000">推出了一款新的生成式 AI 助手，名为 Docker AI。&nbsp;Docker 首席执行官 Scott Johnston 表示，与 Copilot 或&nbsp;Amazon&nbsp;CodeWhisperer 等其他代码生成助手相比，Docker AI 助手可以帮助开发人员定义应用程序的各个方面并排除故障。</span></p><p><span style="color:#000000">"当开发人员编辑 Dockerfile 或 Docker Compose 文件、调试本地 docker build 或在本地运行测试时，Docker AI 会根据具体情况提供自动指导。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 09:05:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260683/docker-genai-stack</guid>
            <link>https://www.oschina.net/news/260683/docker-genai-stack</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[elementary OS 7.1]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>elementary OS 7.1 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.elementary.io%2Fos-7-1-available-now%2F" target="_blank">已正式 GA</a>，距离<a href="https://www.oschina.net/news/226770/elementary-os-7-released" target="_blank"> 7.0 </a>发布已过去 8 个月。</p><p><img src="https://blog.elementary.io/images/os-7-1-available-now/desktop-onboarding.png" referrerpolicy="no-referrer"></p><p><strong>新版本重要变化</strong></p><ul><li><strong>提供个性化选项和功能</strong>，提升操作系统包容性和易用性</li><li><strong>保护隐私</strong>，确保应用程序始终在用户明确同意的情况下运行</li><li>优化触控手势和键盘导航</li><li>优化登录和锁屏界面</li><li>更新固件以支持更多硬件</li><li>优化自带 Web 浏览器，提升性能，速度更快，以及改进兼容性</li><li>改进「文件」应用 UI</li><li>优化通知指示器</li><li>包含 200 多个错误修复、设计更改和新功能</li></ul><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.elementary.io%2Fos-7-1-available-now%2F" target="_blank">详情查看发布公告</a>。</p><hr><p>推荐阅读：<a href="https://www.oschina.net/news/226770/elementary-os-7-released" target="_blank">历经风波，elementary OS 7 正式发布</a></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 06:22:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260628/elementary-os-7-1-released</guid>
            <link>https://www.oschina.net/news/260628/elementary-os-7-1-released</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Citus 12.1 发布，支持 PG16]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CitusDB 是一个基于最新 PostgreSQL 构建的分布式数据库。CitusDB 可对 PostgreSQL 数据库进行伸缩以适合大数据的处理。可在集群中进行自动分片和碎片复制，运行在云端或者混合系统中。数据库的查询可在集群中进行分布式处理，充分利用集群中每个节点的计算能力。CitusDB 可提升 PostgreSQL 的高并发性和 JSON 支持，可用作事务以及分析数据库场景。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Citus 12.1 现已发布，此版本增加了对 Postgres 16 的支持。主要更新内容包括：</p><ul><li>从任意节点查询时的负载均衡</li><li>分布式 Citus 集群中的 pg_stat_io</li><li>支持 JSON_ARRAYAGG() 和 JSON_OBJECTAGG() 聚合</li><li>支持 COPY FROM 的 DEFAULT 选项</li><li>传播自定义 ICU 整理规则</li><li>支持 Citus 外部表上的 TRUNCATE 触发器</li><li>将新的 CREATE TABLE、VACUUM 和 ANALYZE 选项传播到 Citus 节点</li></ul><p style="text-align:left"><span><span><span><span style="color:#000000"><span><span><span><span><span><span><span><span><span><span><span><span><span><span>除了支持 Postgres 16 之外，Citus 12.1 还包括：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li>授予数据库创建权限</li><li>使用 citus_schema_move () 进行分布式模式移动</li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.postgresql.org%2Fabout%2Fnews%2Fcitus-121-released-2727%2F" target="_blank">查看官方公告</a>。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">下载地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.citusdata.com%2Fdownload%2F" target="_blank">https://www.citusdata.com/download/</a></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 06:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260625/citus-121-released</guid>
            <link>https://www.oschina.net/news/260625/citus-121-released</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[ChatGPT 遭「卡脖子」，OpenAI 计划自研 AI 芯片]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#000000; text-align:start">根据&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2023%2F10%2F06%2Fopenai-said-to-be-considering-developing-its-own-ai-chips%2F%3Fguccounter%3D1%26guce_referrer%3DaHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS5oay8%26guce_referrer_sig%3DAQAAAL3fVeW5BS1z3Z9olNE6c2iybavGH0APpfPZxySiJi7WUXe83N7739IvRls5vIwuXKyA2eYoWcTiKlUTh7jVhzMkKKxJTSaY_n4awPm8XvK2tXu2OjLfdsRALDvUWwB1idflbNBNoRwu_fzD-uhZrxP90RGZfxjBWi5mEUiKzpMc" target="_blank">TechCrunch</a>&nbsp;的报道，随着 AI 芯片短缺的问题日益严重，OpenAI 现已开始考虑自研 AI 芯片。</p><p style="color:#000000; text-align:start">据悉，从去年开始 OpenAI 内部就已经开始讨论 AI 芯片战略，以解决其 AI 芯片短缺的问题。这些方案包括自研 AI 芯片、与英伟达等芯片制造商展开更紧密的合作、实现供应商多元化等。</p><p>OpenAI 首席执行官 Sam Altman 去年就公开抱怨英伟达 GPU 芯片稀缺，称公司受到 GPU 的严重限制。</p><p>由于英伟达主导了全球 95% 的 Al 训练领域市场，随着英伟达 GPU 显卡稀缺，加上 AI 算力成本持续攀升，即便强如 OpenAI 也在寻找新方案，从而避免长期被「卡脖子」。</p><p style="color:#000000; text-align:start">报道称，该公司尚未决定继续推进。Sam Altman 此前表示已将收购更多 AI 芯片作为公司的首要任务。</p><p style="color:#000000; text-align:start"><img alt="" src="https://static.oschina.net/uploads/space/2023/1007/112609_aSEr_2720166.jpeg" referrerpolicy="no-referrer"></p><p style="color:#000000; text-align:start">OpenAI 和大多数竞争对手一样，依赖基于 GPU 的硬件来开发 ChatGPT、GPT-4 和 DALL-E 3 等模型。GPU 能够并行执行许多计算，因此非常适合训练当今最强大的人工智能。</p><p style="color:#000000; text-align:start">不过 GPU 等芯片目前面临严重短缺的问题，据报道，英伟达性能最好的人工智能芯片在 2024 年之前都已售罄。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 03:26:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260598</guid>
            <link>https://www.oschina.net/news/260598</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CoDeF —— 强时序一致性视频处理算法]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CoDeF 是能够高度保持视频时序一致性的的视频处理算法，可以轻松完成视频风格迁移、视频关键点追踪（包括流体）、用户自定义的视频内容编辑等任务。</p><p>CoDeF 支持将图像风格化算法升级为视频风格化算法，将图像关键点检测算法升级为视频关键点跟踪算法（甚至包括水和烟雾等非刚性物体的追踪），将图像语义分割算法升级为视频物体跟踪算法，将图像超分算法升级为视频超分算法，同时支持用户可交互的视频内容编辑。</p><p><img src="https://oscimg.oschina.net/oscnet/up-86a32563b77d2c6eaf06e2b3c03c320f292.gif" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 02:57:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/codef</guid>
            <link>https://www.oschina.net/p/codef</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 字符串插值变量处理工具库 FlexVars]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-flexvars" class="anchor" href="https://gitee.com/zhangfisher/flexvars#flexvars"></a>FlexVars</h1><p>Powerful string interpolation tool library</p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fzhangfisher.github.io%2Fflexvars%2F%23%2Fcn%2Freadme">中文</a><a href="https://gitee.com/link?target=https%3A%2F%2Fzhangfisher.github.io%2Fflexvars%2F%23%2Fen%2Freadme">English</a></p><h2><a id="user-content-features" class="anchor" href="https://gitee.com/zhangfisher/flexvars#features"></a>Features</h2><p>-Supports positional and dictionary interpolation
-Supports multiple error handling mechanisms
-Support for null value processing mechanism
-Support filter chain processing of input variable values
-Support variable prefix suffix
-98%+unit test coverage</p><h2><a id="user-content-getting-started" class="anchor" href="https://gitee.com/zhangfisher/flexvars#getting-started"></a>Getting Started</h2><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="k">import</span><span class="p">{</span><span class="nx">FlexVars</span><span class="p">}</span><span class="k">from</span><span class="dl">"</span><span class="s2">flexvars</span><span class="dl">"</span></span><span id="LC2" class="line"></span><span id="LC3" class="line"><span class="kd">const</span><span class="nx">flexvars</span><span class="o">=</span><span class="k">new</span><span class="nx">FlexVars</span><span class="p">({</span></span><span id="LC4" class="line"><span class="na">filters</span><span class="p">:{</span></span><span id="LC5" class="line"><span class="na">currency</span><span class="p">:{</span></span><span id="LC6" class="line"><span class="na">args</span><span class="p">:[</span><span class="dl">"</span><span class="s2">prefix</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">suffix</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">sign</span><span class="dl">"</span><span class="p">],</span></span><span id="LC7" class="line"><span class="na">default</span><span class="p">:{</span><span class="na">prefix</span><span class="p">:</span><span class="dl">"</span><span class="s2">USD </span><span class="dl">"</span><span class="p">,</span><span class="na">suffix</span><span class="p">:</span><span class="dl">""</span><span class="p">,</span><span class="na">sign</span><span class="p">:</span><span class="dl">"</span><span class="s2">$</span><span class="dl">"</span><span class="p">}</span></span><span id="LC8" class="line"><span class="nl">next</span><span class="p">:(</span><span class="na">value</span><span class="p">:</span><span class="kr">any</span><span class="p">,</span><span class="na">args</span><span class="p">:</span><span class="nb">Record</span><span class="o">&lt;</span><span class="kr">string</span><span class="p">,</span><span class="kr">any</span><span class="o">&gt;</span><span class="p">,</span><span class="na">context</span><span class="p">:</span><span class="nx">FlexFilterContext</span><span class="p">)</span><span class="o">=&gt;</span><span class="p">{</span></span><span id="LC9" class="line"><span class="k">return</span><span class="s2">`</span><span class="p">${</span><span class="nx">args</span><span class="p">.</span><span class="nx">prefix</span><span class="p">}${</span><span class="nx">args</span><span class="p">.</span><span class="nx">sign</span><span class="p">}${</span><span class="nx">value</span><span class="p">}${</span><span class="nx">args</span><span class="p">.</span><span class="nx">suffix</span><span class="p">}</span><span class="s2">`</span></span><span id="LC10" class="line"><span class="p">}</span></span><span id="LC11" class="line"><span class="p">}</span></span><span id="LC12" class="line"><span class="p">}</span></span><span id="LC13" class="line"><span class="p">})</span></span><span id="LC14" class="line"></span><span id="LC15" class="line"><span class="kd">const</span><span class="nx">_</span><span class="o">=</span><span class="nx">flexvars</span><span class="p">.</span><span class="nx">replace</span></span><span id="LC16" class="line"></span><span id="LC17" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello {}</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">flexvars</span><span class="dl">"</span><span class="p">))</span></span><span id="LC18" class="line"><span class="c1">// =&gt; hello flexvars</span></span><span id="LC19" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">I am {}</span><span class="dl">"</span><span class="p">,</span><span class="dl">"</span><span class="s2">tom</span><span class="dl">"</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">I am tom</span><span class="dl">"</span><span class="p">)</span></span><span id="LC20" class="line"><span class="c1">// =&gt; I am tom</span></span><span id="LC21" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">{ value | currency}</span><span class="dl">"</span><span class="p">,</span><span class="mi">100</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">USD $100</span><span class="dl">"</span><span class="p">))</span></span><span id="LC22" class="line"><span class="c1">// =&gt; USD $100</span></span><span id="LC23" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">{ value | currency('RMB','￥','元')}</span><span class="dl">"</span><span class="p">,</span><span class="mi">100</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">RMB ￥100 元</span><span class="dl">"</span><span class="p">))</span><span class="c1">// </span></span><span id="LC24" class="line"><span class="c1">// =&gt; RMB ￥100 元</span></span><span id="LC25" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">{ value | currency({prefix:'EUR '',suffix:''',sign:'€'})}</span><span class="dl">"</span><span class="p">,</span><span class="mi">100</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">RMB €100</span><span class="dl">"</span><span class="p">))</span></span><span id="LC26" class="line"><span class="c1">// =&gt; EUR €100</span></span><span id="LC27" class="line"></span><span id="LC28" class="line"><span class="nx">flexvars</span><span class="p">.</span><span class="nx">addFilter</span><span class="p">({</span></span><span id="LC29" class="line"><span class="na">name</span><span class="p">:</span><span class="dl">"</span><span class="s2">add</span><span class="dl">"</span><span class="p">,</span></span><span id="LC30" class="line"><span class="na">args</span><span class="p">:[</span><span class="dl">"</span><span class="s2">step</span><span class="dl">"</span><span class="p">],</span></span><span id="LC31" class="line"><span class="na">default</span><span class="p">:{</span><span class="na">step</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span></span><span id="LC32" class="line"><span class="nx">next</span><span class="p">(</span><span class="na">value</span><span class="p">:</span><span class="kr">any</span><span class="p">,</span><span class="na">args</span><span class="p">:</span><span class="nb">Record</span><span class="o">&lt;</span><span class="kr">string</span><span class="p">,</span><span class="kr">any</span><span class="o">&gt;</span><span class="p">,</span><span class="na">context</span><span class="p">:</span><span class="nx">FlexFilterContext</span><span class="p">){</span></span><span id="LC33" class="line"><span class="k">return</span><span class="nb">parseInt</span><span class="p">(</span><span class="nx">value</span><span class="p">)</span><span class="o">+</span><span class="nx">args</span><span class="p">.</span><span class="nx">step</span></span><span id="LC34" class="line"><span class="p">}</span></span><span id="LC35" class="line"><span class="p">})</span></span><span id="LC36" class="line"><span class="c1">// call chaining</span></span><span id="LC37" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">{ value | add}</span><span class="dl">"</span><span class="p">,</span><span class="mi">100</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">101</span><span class="dl">"</span><span class="p">)</span></span><span id="LC38" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">{ value | add|add }</span><span class="dl">"</span><span class="p">,</span><span class="mi">100</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">102</span><span class="dl">"</span><span class="p">)</span></span><span id="LC39" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">{ value | add(2)|add(3) }</span><span class="dl">"</span><span class="p">,</span><span class="mi">100</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">105</span><span class="dl">"</span><span class="p">)</span></span><span id="LC40" class="line"><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">_</span><span class="p">(</span><span class="dl">"</span><span class="s2">{ value | add(2)|add(3)|add(4) }</span><span class="dl">"</span><span class="p">,</span><span class="mi">100</span><span class="p">)).</span><span class="nx">toBe</span><span class="p">(</span><span class="dl">"</span><span class="s2">109</span><span class="dl">"</span><span class="p">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div>]]>
            </description>
            <pubDate>Sat, 07 Oct 2023 02:45:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/zhangfisher/flexvars</guid>
            <link>https://gitee.com/zhangfisher/flexvars</link>
        </item>
        <item>
            <title>
                <![CDATA[VS Code 的 C# 开发套件 (C# Dev Kit) 正式 GA]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>今年 6 月，微软在 Visual Studio Code 的插件市场<a href="https://www.oschina.net/news/244148/c-sharp-dev-kit-for-visual-studio-code" target="_blank">上架</a>了官方打造的<strong> C# 开发套件 —— C# Dev Kit</strong>，让开发者在 VS Code 中方便地进行 C# 开发。</p><p>据介绍，C# Dev Kit 提高了开发者在使用 VS Code 过程中开发 C# 语言产品的效率。该套件兼容 C# 扩展，由语言服务器协议&nbsp; (LSP) 主机提供支持，从而创建一个高性能、可扩展且灵活的工具环境，可轻松将新体验集成到 C# for VS Code 中。</p><p>经过 4 个多月的测试和打磨，微软近日宣布&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fdotnet%2Fcsharp-dev-kit-now-generally-available%2F" target="_blank"><strong>C# Dev Kit 正式 GA</strong></a>。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-abd88ca70f16b7de5318e2944c0f5c847dd.png" referrerpolicy="no-referrer"></p><p>微软表示在预览版期间，累计为 C# Dev Kit 修复了 350 多个问题，其中大部分由社区报告，并对该产品进行了 300 多项有针对性的改进。</p><p>微软称用户的反馈加速推进了 C# Dev Kit 的正式发布，开发团队会继续提升性能和可靠性，并将每月添加新功能。</p><p>根据微软的介绍，C# Dev Kit 从 Visual Studio 中借用了一些开发者们熟悉的概念，并能够与现有的 C# 扩展一起使用，以及通过增加一套强大的工具和实用程序来增强 C# 开发环境，这些工具和实用程序与 VS Code 原生集成，以帮助 C# 开发者更快地编写、调试和维护他们的代码，并减少错误。</p><p>C# Dev Kit 由以下部分组成：</p><ul><li><strong>C# 扩展</strong>：它提供基本的语言服务支持，并继续独立于这项工作进行维护；</li><li><strong>C# Dev Kit 扩展</strong>：它建立在 Visual Studio 的基础上，提供解决方案管理、模板、测试、调试；</li><li><strong>IntelliCode for C# Dev Kit 扩展</strong>：它将 AI 驱动的开发带到了编辑器中；</li></ul><p><img alt="" src="https://static.oschina.net/uploads/space/2023/0607/112538_up8O_4937141.png" referrerpolicy="no-referrer"></p><p><strong><a href="https://www.oschina.net/news/244148/c-sharp-dev-kit-for-visual-studio-code" target="_blank">点此查看详细介绍</a></strong>。</p><p>C# Dev Kit 下载地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmarketplace.visualstudio.com%2Fitems%3FitemName%3Dms-dotnettools.csdevkit%26ssr%3Dfalse" target="_blank">https://marketplace.visualstudio.com/</a></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 09:22:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260688/vs-code-csharp-dev-kit-ga</guid>
            <link>https://www.oschina.net/news/260688/vs-code-csharp-dev-kit-ga</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Ubuntu 23.10 将正式支持 Raspberry Pi 5]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>根据 omgubuntu 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2023%2F10%2Fubuntu-23-10-will-support-raspberry-pi-5" target="_blank">报道</a>，即将发布的 Ubuntu 23.10 会正式支持<a href="https://www.oschina.net/news/259858/introducing-raspberry-pi-5" target="_blank">树莓派 5</a>。</p><blockquote><p style="margin-left:0px; margin-right:0px; text-align:start"><strong>延伸阅读：<a href="https://www.oschina.net/news/259858/introducing-raspberry-pi-5" target="_blank">Raspberry Pi 5 将于 10 月底发布，60 美元起售</a></strong></p></blockquote><p>报道指出，由于 Canonical 开发者可以提前使用树莓派 5，因此他们能够在设备上测试即将发布的 Ubuntu 版本，确定需要支持新硬件的领域，并将所需的新（和已升级）软件包放入 file_Feature Freeze Exceptions_to （文件_功能冻结异常_队列）中。</p><p>部分针对树莓派 5 的改进包括：引入新的&nbsp;<code>pisp</code>&nbsp;包来处理树莓派 5 大大增强的相机功能；并对&nbsp;<code>python3-gpiozero</code>&nbsp;进行重大更新，以符合新型号对其 GPIO 操作所做的更改。</p><p>另外要注意的是，更新的 rpiboot 软件包将无法在 Ubuntu 23.10 发布时及时提供，但由于这不是严格要求的，所以问题不大。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 07:05:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260645/ubuntu-23-10-will-support-raspberry-pi-5</guid>
            <link>https://www.oschina.net/news/260645/ubuntu-23-10-will-support-raspberry-pi-5</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[阿里云 PAI - 灵骏大模型训练工具 Pai-Megatron-Patch 正式开源]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h3_1"></span><h3>Pai-Megatron-Patch 是什么</h3><p style="text-align:justify">Pai-Megatron-Patch 工具是阿里云机器学习平台 PAI 算法团队研发，基于阿里云智算服务 PAI-灵骏平台的大模型最佳实践解决方案配套工具，旨在帮助大模型开发者快速上手灵骏产品，完成大语言模型（LLM）的高效分布式训练，有监督指令微调，模型离线推理验证等完整大模型开发链路。该项目提供了业界主流开源大模型基于 Megatron-LM 的训练&amp;离线推理验证流程，方便用户快速上手大模型训练。</p><span id="OSC_h3_2"></span><h3>主要特性</h3><ul><li>多款热门大模型支持：llama，llama-2，codellama, 百川，通义千问，Falcon，GLM，Starcoder，Bloom，chatglm 等</li><li>支持模型权重互转转换：在 Huggingface，Megatron 和 Transformer Engine 之间进行算子命名空间映射</li><li>支持 Flash Attention 2.0 和 Transformer Engine 模式下的 FP8 训练加速且确保收敛</li><li>丰富且简单易用的使用示例，支持大模型预训练，微调，评估和推理，强化学习全流程最佳实践</li></ul><span id="OSC_h3_3"></span><h3>开源地址</h3><p style="text-align:justify"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Falibaba%2FPai-Megatron-Patch%253Fspm%253Da2c6h.13046898.publish-article.3.5d586ffa9uOzwk" target="_blank">https://github.com/alibaba/Pai-Megatron-Patch</a></p><span id="OSC_h3_4"></span><h3>技术架构</h3><p style="text-align:justify">Pai-Megatron-Patch 的设计理念是不对 Megatron-LM 的源码进行侵入式修改，即不在 Megatron-LM 里面添加新的功能特性，将需要扩充完善的部分以 patch 补丁的方式呈现。在 patch 中构建 LLM 训练链路通过依赖 Megatron-LM 核心库的方法实现和 Megatron-LM 的解耦合。这样解耦合的好处就是 Megatron-LM 的升级不会影响用户的 LLM 最佳实践体验。</p><p style="text-align:justify">Pai-Megatron-Patch 中包含模型库，分词器，模型转换，强化学习，离线文本生成以及使用示例和工具集等用于构建 LLM 训练的关键要素。在模型库中包含热门大模型的 Megatron 版本实现，例如 baichuan，bloom，chatglm，falcon，galactica，glm，llama，qwen 和 starcoder，后续还会根据需要及时添加新的 Megatron 版大模型实现。同时 patch 还提供了 huggingface 模型权重和 Megatron 模型权重之间的双向转换。一方面是方便用户加载 huggingface 的权重在 Megatron 中继续预训练或者微调，另一方面是方便用户对训练好的 Megatron 模型使用 huggingface 的评估/推理流程对模型质量进行客观评估。在强化学习部分，patch 提供了 PPO 训练流程等，方便用户使用 SFT 模型和 RM 模型进行强化学习。最后 patch 提供了大量的使用示例帮助用户快速开始大模型训练&amp;离线推理。具体请参考阿里云灵骏产品的使用流程:&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2505831.html%253Fspm%253Da2c6h.13046898.publish-article.4.5d586ffa9uOzwk%2526tab%253Donestop" target="_blank">智算服务 PAI 灵骏大模型分布式训练方案</a></p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-2459be9f4c59a8fd9ac6472cd888c176_720w.webp" referrerpolicy="no-referrer"></p><span id="OSC_h3_5"></span><h3>关键技术</h3><span id="OSC_h4_6"></span><h4>模型权重转换</h4><p style="text-align:justify">研发 Megatron-Patch 的初衷之一就是能将世界各地研发机构在 Huggingface 上放出的热门大模型使用 Megatron 引擎进行继续预训练或者继续微调。这就需要首先将 Huggingface 模型格式的 ckpt 转换成 Megatron 模型格式，才能正确加载进来，否则会出现 pytorch 加载模型失败。Megatron-Patch 的一个核心可靠性保障特征就是在采用算子拆分，流水并行，序列并行，Zero 显存优化，BF16 混合精度，梯度检查点等训练加速技术确保模型训练吞吐速度平均提升 1.5 倍以上的同时，在评估任务模式下的单一样本前向 loss 值，预训练/微调任务模式下的 loss 曲线，离线文本生成任务模式下的生成效果这三个方面和 Huggingface 是对齐的，从而确保 Megatron 版模型的可靠性。</p><p style="text-align:justify">另一方面，Megatron 版的 transformer 实现方式提供了一种让用户仅仅通过设置开关就能实现不同种类 GPT 模式的能力。比如 llama 模型打开如下开关即可</p><pre><code>--swiglu \
  --use-rotary-position-embeddings \
  --no-position-embedding \
  --untie-embeddings-and-output-weights \
  --disable-bias-linear</code></pre><p style="text-align:justify">如果想将 llama 模式变成 baichuan 模型，那么仅仅需要添加采用--use-alibi-mask 开关，同时关闭 Rotary Embeeding 开关即可，具体配置如下所示：</p><pre><code>--swiglu \
  --use-alibi-mask \
  --position-embedding-type none \
  --untie-embeddings-and-output-weights \
  --disable-bias-linear</code></pre><p style="text-align:justify">下面我们以 llama-2 为例，详解从 huggingface 到 megatron 的模型权重转换技术。下表总结了两者在不同 module 上的命名对应关系。在 patch 实现过程中，我们首先将 HF 格式的 ckpt 转换到一种内部格式，然后再把这种内部格式转换成对应的外部格式。这样做可以最大程度复用已有的转换逻辑来处理新模型。在转换为内部格式的过程中，</p><p style="text-align:justify">q_proj, k_proj, v_proj 需要沿着第 0 维拼接在一起后赋值给内部变量 query_key_value。</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-e3637f10e008e7548046df938ee8bac6_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">当用户在资源受限情况下需要按照 TP&gt;1 来拆分权重的时候，这里需要注意的是针对 MLP 层的 gate_proj 和 up_proj 的操作。不能像 qkv 那样在转换成内部格式的时候进行 merge 再执行算子拆分。需要在拆分前加入如下针对 MLP 层的权重合并的代码逻辑才能确保正确收敛。</p><pre><code>for i in range(tp_size):
    params_dict = get_element_from_dict_by_path(output_state_dict[i],
                                                "model.language_model.encoder")
    dense_h_to_4h_1_name = 'mlp.dense_h_to_4h_1.weight'
    dense_h_to_4h_1_layer_name = f"layers.{layer}.{dense_h_to_4h_1_name}"
    dense_h_to_4h_1_weight = params_dict[dense_h_to_4h_1_layer_name]
    dense_h_to_4h_2_name = 'mlp.dense_h_to_4h_2.weight'
    dense_h_to_4h_2_layer_name = f"layers.{layer}.{dense_h_to_4h_2_name}"
    dense_h_to_4h_2_weight = params_dict[dense_h_to_4h_2_layer_name]
    dense_h_to_4h_name = 'mlp.dense_h_to_4h.weight'
    dense_h_to_4h_layer_name = f"layers.{layer}.{dense_h_to_4h_name}"
    params_dict[dense_h_to_4h_layer_name] = torch.cat(
    [dense_h_to_4h_1_weight, dense_h_to_4h_2_weight], dim=0)</code></pre><span id="OSC_h4_7"></span><h4>基于 TE 的 FP8 训练收敛</h4><p style="text-align:justify">Transformer Engine(TE) 是一个在英伟达 GPUS 上运行的针对 Transformer 模型的加速库，其中包括针对 Hopper GPU 的 FP8 混合精度，该精度可以在较低的显存利用率下提供更好的训练&amp;推理速度。在 TE 内部封装了 Flash Attention 实现，同时 TE 还提供了一组高度优化后的算子用来构建 Transformer 模型。比如 LayerNormLinear 就是将 LayerNorm 和 QKV-Proojection 进行算子融合，LayerNormMLP 就是将 layernorm 和 mlp 进行算子融合。如下图所示：</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-214b47fb7b967d3f92dd7dd58092446b_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">从 Huggingface 到 TE 模型的权重转换技术和之前是类似的，也需要事先找到两者之间的映射关系。从下表可以看出，TE 中多了_extra_state 是用来存 fp8 训练的 scale 和 history 的，这些在加载的时候会出现冲突，这时只要将 load_state_dict 函数的 strict 设置成 False 就可以了，比如 load_state_dict(state_dict_, strict=False)。</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-125038fa0f82beec327ee0234b3b79c2_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">在 Megatron-Patch 中使用示例中打开 FP8 混合精度训练开关也很容易，如下所示：</p><pre><code>if [ $PR = fp16 ]; then
    pr_options=" \
        --fp16"
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16
        --fp8-hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024 \
        --transformer-impl transformer_engine"
fi</code></pre><p style="text-align:justify">我们可以使用如下训练脚本<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Falibaba%2FPai-Megatron-Patch%2Fblob%2Fmain%2Fexamples%2Fgpt3_llama%2Frun_pretrain_megatron_llama_enwiki.sh%253Fspm%253Da2c6h.13046898.publish-article.5.5d586ffa9uOzwk%2526file%253Drun_pretrain_megatron_llama_enwiki.sh" target="_blank">run_pretrain_megatron_llama_enwiki.sh</a>来测试打开 FP8 开关后的预训练收敛性。下图展示了 llama-7B 和 llama-2-70B 模型在打开和关闭 FP8 时的 loss 曲线对比，可以看出基本是重合的。</p><p style="text-align:justify">LLama-7B</p><p style="text-align:center"><img src="https://pic2.zhimg.com/80/v2-6b4c07368bdeb4e3c80251d6972511f1_720w.webp" referrerpolicy="no-referrer"></p><p>LLama2-70B</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-6ed82c26ed8ee7661a913687e7905a6b_720w.webp" referrerpolicy="no-referrer"></p><span id="OSC_h4_8"></span><h4>大模型训练&amp;推理</h4><p style="text-align:justify">从 github 上获取 Megatron 模型训练工具 PAI-Megatron-Patch（<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Falibaba%2FPai-Megatron-Patch%253Fspm%253Da2c6h.13046898.publish-article.6.5d586ffa9uOzwk" target="_blank">https://github.com/alibaba/Pai-Megatron-Patch</a>）源代码并拷贝到工作目录/mnt/workspace/下。</p><p style="text-align:justify"><strong>模型格式转换</strong></p><p style="text-align:justify">使用我们提供的模型转换脚本，将 huggingface 格式的模型文件转换为 megatron 格式：</p><pre><code>cd /mnt/workspace/
mkdir llama2-ckpts
cd llama2-ckpts
wget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/llama2-ckpts/Llama-2-7b-hf.tgz
tar -zxf Llama-2-7b-hf.tgz
mv Llama-2-7b-hf llama2-7b-hf
cd /mnt/workspace/PAI-Megatron-Patch/toolkits/model_checkpoints_convertor/llama
sh model_convertor.sh \
/root/Megatron-LM-23.04        \
/mnt/workspace/llama2-ckpts/llama2-7b-hf         \
/mnt/workspace/llama2-ckpts/llama2-7b-hf-to-megatron-tp1-pp1  \
1  \
1  \
llama-7b \
0 \
false</code></pre><p style="text-align:justify"><strong>继续预训练</strong></p><p style="text-align:justify">中文继续预训练汉化指引</p><p style="text-align:justify">Step1: 获取需要扩充词表的模型（如 llama-13b-hf）</p><p style="text-align:justify">Step2: 获取需要扩充的词表</p><ul><li>使用 sentence-piece 代码库从自有文本语料中学习词表，得到 randeng-sp.model 文件</li></ul><p style="text-align:justify">Step3: 词表扩充</p><ul><li>扩充模型 tokenizer：将 randeng-sp.model 中的词表添加到 llama-13b-hf 文件夹下 tokenizer.model 中</li><li>扩充模型词表对应的参数矩阵 
  <ul><li>word_embedding、lm_head</li><li>新词向量可以使用原词向量均值作为初始化，比如「天气」=mean([「天」，「气」])</li></ul></li><li>修改与词表大小相关的文件并保存，如 config.json</li></ul><p style="text-align:justify">运行继续预训练脚本&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Falibaba%2FPai-Megatron-Patch%2Fblob%2Fmain%2Fexamples%2Fllama2%2Frun_pretrain_megatron_llama.sh%253Fspm%253Da2c6h.13046898.publish-article.7.5d586ffa9uOzwk%2526file%253Drun_pretrain_megatron_llama.sh" target="_blank">run_pretrain_megatron_llama.sh</a>，需要传入的参数列表如下</p><pre><code>ENV=$1                          # 运行环境: dlc, dsw
MEGATRON_PATH=$2                # 设置开源 Megatron 的代码路径
MEGATRON_PATCH_PATH=$3          # 设置 Megatron Patch 的代码路径
MODEL_SIZE=$4                   # 模型结构参数量级：7B, 13B
BATCH_SIZE=$5                   # 每卡训练一次迭代样本数: 4, 8
GLOBAL_BATCH_SIZE=$6            # 全局 batch size
LR=$7                           # 学习率: 1e-5, 5e-5
MIN_LR=$8                       # 最小学习率: 1e-6, 5e-6
SEQ_LEN=$9                      # 序列长度
PAD_LEN=${10}                   # Padding 长度：100
EXTRA_VOCAB_SIZE=${11}          # 词表扩充大小
PR=${12}                        # 训练精度: fp16, bf16
TP=${13}                        # 模型并行度
PP=${14}                        # 流水并行度
AC=${15}                        # 激活检查点模式: sel, full
DO=${16}                        # 是否使用 Megatron 版 Zero-1 降显存优化器: true, false
FL=${17}                        # 是否使用 Flash Attention: true, false
SP=${18}                        # 是否使用序列并行: true, false
SAVE_INTERVAL=${19}             # 保存 ckpt 的间隔
DATASET_PATH=${20}              # 训练数据集路径
PRETRAIN_CHECKPOINT_PATH=${21}  # 预训练模型路径
TRAIN_TOKENS=${22}              # 训练 token 数
WARMUP_TOKENS=${23}             # 预热 token 数
OUTPUT_BASEPATH=${24}           # 训练输出文件路径</code></pre><p style="text-align:justify">注意设置正确的数据集<strong>挂载路径 WORK_DIR</strong>以及<strong>运行环境 ENV</strong>，运行示例如下所示：</p><pre><code>export WORK_DIR=/mnt/workspace
cd ${WORK_DIR}/PAI-Megatron-Patch/examples/llama2
bash run_pretrain_megatron_llama.sh \
dlc \
/root/Megatron-LM-23.04   \
${WORK_DIR}/PAI-Megatron-Patch  \
7B   \
1    \
16 \
1e-5   \
1e-6   \
2048  \
80  \
0   \
fp16  \
1   \
1  \
sel  \
true   \
false  \
false   \
100000  \
${WORK_DIR}/llama2-datasets/wudao/wudao_llamabpe_text_document   \
${WORK_DIR}/llama2-ckpts/llama2-7b-hf-to-megatron-tp1-pp1   \
100000000   \
10000   \
${WORK_DIR}/output_megatron_llama2/</code></pre><p style="text-align:justify"><strong>有监督微调</strong></p><p style="text-align:justify">在微调开始之前，请先进入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Falibaba%2FPai-Megatron-Patch%2Fblob%2Fmain%2Ftoolkits%2Fpretrain_data_preprocessing%2FREADME.md%253Fspm%253Da2c6h.13046898.publish-article.8.5d586ffa9uOzwk%2526file%253DREADME.md" target="_blank">https://github.com/alibaba/Pai-Megatron-Patch/blob/main/toolkits/pretrain_data_preprocessing/README.md</a>&nbsp;获取 json 文件。运行<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Falibaba%2FPai-Megatron-Patch%2Fblob%2Fmain%2Fexamples%2Fllama2%2Frun_finetune_megatron_llama.sh%253Fspm%253Da2c6h.13046898.publish-article.9.5d586ffa9uOzwk%2526file%253Drun_finetune_megatron_llama.sh" target="_blank">run_finetune_megatron_llama.sh</a>脚本，需要传入的参数列表如下</p><pre><code>ENV=$1                          # 运行环境: dlc, dsw
MEGATRON_PATH=$2                # 设置开源 Megatron 的代码路径
MEGATRON_PATCH_PATH=$3          # 设置 Megatron Patch 的代码路径
MODEL_SIZE=$4                   # 模型结构参数量级: 7B, 13B
BATCH_SIZE=$5                   # 每卡训练一次迭代样本数: 4, 8
LR=$6                           # 学习率: 1e-5, 5e-5
MIN_LR=$7                       # 最小学习率: 1e-6, 5e-6
SEQ_LEN=$8                      # 序列长度
PAD_LEN=$9                      # Padding 长度：100
EXTRA_VOCAB_SIZE=${10}          # 词表扩充大小
PR=${11}                        # 训练精度: fp16, bf16
TP=${12}                        # 模型并行度
PP=${13}                        # 流水并行度
AC=${14}                        # 激活检查点模式: sel, full
DO=${15}                        # 是否使用 Megatron 版 Zero-1 降显存优化器: true, false
FL=${16}                        # 是否使用 Flash Attention: true, false
SP=${17}                        # 是否使用序列并行: true, false
TRAIN_DATASET_PATH=${18}        # 训练数据集路径
VALID_DATASET_PATH=${19}        # 验证数据集路径
PRETRAIN_CHECKPOINT_PATH=${20}  # 预训练模型路径
EPOCH=${21}                     # 训练迭代轮次
OUTPUT_BASEPATH=${22}           # 训练输出文件路径</code></pre><p style="text-align:justify">多节点运行示例如下所示：</p><pre><code>export WORK_DIR=/mnt/workspace
cd ${WORK_DIR}/PAI-Megatron-Patch/examples/llama2
sh run_finetune_megatron_llama.sh  \
dlc    \
/root/Megatron-LM-23.04   \
${WORK_DIR}/PAI-Megatron-Patch  \
7B     \
1      \
1e-5   \
1e-6   \
2048   \
80     \
0      \
fp16   \
1      \
1      \
sel    \
true   \
false  \
false  \
${WORK_DIR}/llama2-datasets/wudao_train.json   \
${WORK_DIR}/llama2-datasets/wudao_valid.json   \
${WORK_DIR}/llama2-ckpts/llama2-7b-hf-to-megatron-tp1-pp1   \
2   \
${WORK_DIR}/output_megatron_llama2/
</code></pre><p style="text-align:justify"><strong>离线推理</strong></p><p style="text-align:justify">模型训练完成后，可以进行离线推理，评估模型效果。根据上面的训练流程不同，我们提供了 Megatron 格式的推理链路。对于 Megatron 训练的模型，可以直接用 Megatron 框架进行推理。</p><pre><code>ENV=$1                          # 运行环境: dlc, dsw
MEGATRON_PATH=$2                # 设置开源 Megatron 的代码路径
MEGATRON_PATCH_PATH=$3          # 设置 Megatron Patch 的代码路径
CHECKPOINT_PATH=$4              # 模型微调阶段的模型保存路径
MODEL_SIZE=$5                   # 模型结构参数量级: 1.1B, 1.7B, 7.1B
TP=$6                           # 模型并行度
BS=$7                           # 每卡推理一次迭代样本数: 1, 4, 8
SEQ_LEN=$8                      # 序列长度: 256, 512, 1024
PAD_LEN=$9                      # PAD 长度：需要将文本拼接到的长度
EXTRA_VOCAB_SIZE=${10}          # 模型转换时增加的 token 数量
PR=${11}                        # 推理采用的精度: fp16, bf16
TOP_K=${12}                     # 采样策略中选择排在前面的候选词数量 (0-n): 0, 5, 10, 20
INPUT_SEQ_LEN=${13}             # 输入序列长度: 512
OUTPUT_SEQ_LEN=${14}            # 输出序列长度: 256
INPUT_FILE=${15}                # 需要推理的文本文件: input.txt, 每行为一个样本
OUTPUT_FILE=${16}               # 推理输出的文件: output.txt
# TOP_K 和 TOP_P 必须有一个为 0
TOP_P=${17}                     # 采样策略中选择排在前面的候选词百分比 (0-1): 0, 0.85, 0.95
TEMPERATURE=${18}               # 采样策略中温度惩罚: 1-n
REPETITION_PENALTY=${19}        # 避免生成是产生大量重复，可以设置为 (1-2) 默认为 1.2</code></pre><ul><li>此处提供一个离线推理输出的文件，推理的数据组织形式需要与微调时的保持一致。 
  <ul><li>测试样本：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fatp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com%2Frelease%2Fmodels%2Fpai-megatron-patch%2Fllama2-datasets%2Fpred_input.jsonl%253Fspm%253Da2c6h.13046898.publish-article.10.5d586ffa9uOzwk%2526file%253Dpred_input.jsonl" target="_blank">https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/llama2-datasets/pred_input.jsonl</a></li></ul></li><li>注意： 
  <ul><li>模型保存的路径下缺少 tokenizer 依赖的文件，需要将微调前模型路径下所有 json 文件及 tokenizer.model 拷贝至保存模型的路径下（位于{OUTPUT_BASEPATH }/checkpoint），与 latest_checkpointed_iteration.txt 同级。</li></ul></li></ul><p style="text-align:justify">以下有监督微调过程保存模型的推理代码，需要将 run_text_generation_megatron_llama.sh 脚本中 CUDA_VISIBLE_DEVICES 参数设置为 0；GPUS_PER_NODE 参数设置为 1；同时使用下列代码进行推理。此时使用单卡进行推理。<strong>注意：此处模型 tp 为 1，可使用单卡推理；如果 tp&gt;1，则需使用相应卡数进行推理。</strong></p><pre><code>export WORK_DIR=/mnt/workspace
cd ${WORK_DIR}/PAI-Megatron-Patch/examples/llama2
bash run_text_generation_megatron_llama.sh \
dsw \
/root/Megatron-LM-23.04 \
${WORK_DIR}/PAI-Megatron-Patch \
../../../llama2-train \
7B \
1 \
1 \
1024 \
1024 \
0 \
fp16 \
10 \
512 \
512 \
${WORK_DIR}/pred_input.jsonl \
${WORK_DIR}/llama2_pred.txt \
0 \
1.0 \
1.2</code></pre><span id="OSC_h4_9"></span><h4>大模型强化学习</h4><p style="text-align:justify">一般来说，SFT 微调过的模型在对话场景已经会有不错的表现了。如果想进一步提升模型效果，可以再加上 RLHF 训练。包括奖励模型（Reward Model）的训练和强化学习（PPO）的训练。这里展示了如何使用当前最常用的 RLHF 开源代码框架，DeepSpeed-Chat 和 trlx，来进行奖励函数训练（RM），以及强化学习优化（PPO）。</p><p style="text-align:justify"><strong>模型格式转换</strong></p><p style="text-align:justify">如果基于 huggingface 格式的模型直接进行奖励模型训练（RM）和强化学习优化（PPO），可以跳过此步骤。</p><p style="text-align:justify">如果基于 Megatron 格式的模型，如 PAI-Megatron-Patch 训练好的 SFT 模型，进行 RM 和 PPO 训练，需要使用我们提供的模型转换脚本，先将 Megatron 格式的模型文件转换为 huggingface 格式。</p><p style="text-align:justify">LLaMA2 模型转换：</p><pre><code>cd PAI-Megatron-Patch/toolkits/model_checkpoints_convertor/gpt3_llama
bash model_convertor.sh \
/path/to/Megatron-LM \
/path/to/megatron_llama2_ckpt \
/path/to/hf_llama2_ckpt \
1 \
1 \
llama-7b \
0 \
true</code></pre><p style="text-align:justify">BLOOM 模型转换：</p><pre><code>cd PAI-Megatron-Patch/toolkits/model_checkpoints_convertor/bloom
bash model_convertor_huggingface_megatron.sh \
/path/to/Megatron-LM \
/path/to/megatron_bloom_ckpt \
/path/to/hf_bloom_ckpt \
1 \
1 \
true</code></pre><p style="text-align:justify"><strong>DeepSpeed-Chat</strong></p><p style="text-align:justify">下载安装开源社区 DeepSpeed-Chat 源代码：</p><pre><code>cd PAI-Megatron-Patch/rlhf/deepspeed-chat
git clone https://github.com/microsoft/DeepSpeedExamples.git
cp -f rm_main.py DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/main.py
cp -f utils.py DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/utils.py
cd DeepSpeedExamples/applications/DeepSpeed-Chat/
pip install -r requirements.txt</code></pre><p style="text-align:justify">基于 LLaMA2 模型训练奖励模型（RM）：</p><pre><code>cd training/step2_reward_model_finetuning/ &amp;&amp; bash training_scripts/llama2/run_llama2_7b.sh</code></pre><p style="text-align:justify">基于 LLaMA2 进行强化学习优化训练（PPO）：</p><pre><code>cd training/step3_rlhf_finetuning/ &amp;&amp; bash training_scripts/llama2/run_llama2_7b_lora.sh</code></pre><p style="text-align:justify"><strong>trlx</strong></p><p style="text-align:justify">下载安装开源社区 trlx 源代码：</p><pre><code>cd PAI-Megatron-Patch/rlhf/trlx
git clone https://github.com/CarperAI/trlx.git
cp trlx_bloom_rlhf.py trlx_bloom_rlhf_test.py trlx/examples/summarize_rlhf/
cp train_reward_model_bloom.py reward_model_bloom.py ds_config_bloom.json trlx/examples/summarize_rlhf/reward_model/
cp -f ds_config_trlx_gptj_summarize.json trlx/examples/summarize_rlhf/configs/
cd trlx
pip install -e .</code></pre><p style="text-align:justify">基于 BLOOM 模型训练奖励模型（RM）：</p><pre><code>cd examples/summarize_rlhf/reward_model/ &amp;&amp; deepspeed train_reward_model_bloom.py</code></pre><p style="text-align:justify">基于 GPT-J 模型训练奖励模型（RM）：</p><pre><code>cd examples/summarize_rlhf/reward_model/ &amp;&amp; deepspeed train_reward_model_gptj.py</code></pre><p style="text-align:justify">基于 BLOOM 模型进行强化学习优化训练（PPO）：</p><pre><code>cd examples/summarize_rlhf/ &amp;&amp; accelerate launch --config_file configs/default_accelerate_config.yaml trlx_bloom_rlhf.py</code></pre><p style="text-align:justify">基于 GPT-J 模型进行强化学习优化训练（PPO）：</p><pre><code>cd examples/summarize_rlhf/ &amp;&amp; accelerate launch --config_file configs/default_accelerate_config.yaml trlx_gptj_text_summarization.py</code></pre><p style="text-align:justify">PPO 单测</p><p style="text-align:justify">如果您想跳过，有监督微调（SFT）与，奖励模型训练（RM）两个步骤，只单独测试 PPO 模块的性能，可以运行如下指令单测 PPO：</p><pre><code>cd examples/summarize_rlhf/ &amp;&amp; accelerate launch --config_file configs/default_accelerate_config.yaml trlx_bloom_rlhf_test.py</code></pre><span id="OSC_h3_10"></span><h3>开源生态——构想和未来</h3><p style="text-align:justify">在 PAI-Megatron-Patch 的开发过程中，我们围绕中文大模型训练加速落地沉淀了以下几个方面的内容：</p><ul><li>Huggingface 的模型权重无损转换成 Megatron 或者 Transformer Engine 可读的模型权重。</li><li>H800 集群开启 FP8 混合精度训练确保收敛。</li><li>LLM 大模型在 PAI 灵骏智算平台上的最佳实践。</li><li>强化学习技术在 PAI 灵骏智算平台上的最佳实践。</li></ul><p style="text-align:justify">后续在 PAI-Megatron-Patch 中还会陆续放出更多高质量的大模型和最佳实践。</p><span id="OSC_h3_11"></span><h3>参考文献</h3><p style="text-align:justify">[1]. Attention Is All You Need</p><p style="text-align:justify">[2]. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</p><p style="text-align:justify">[3]. Reducing Activation Recomputation in Large Transformer Models</p><p style="text-align:justify">[4]. FP8 Formats for Deep Learning</p><p style="text-align:justify">[5]. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</p><p style="text-align:justify">[6]. LLaMA: Open and Efficient Foundation Language Models</p><p style="text-align:justify">[7]. Llama 2: Open Foundation and Fine-Tuned Chat Models</p><p style="text-align:justify">[8]. Benchmarking Large Language Models on NVIDIA H100 GPUs with CoreWeave</p><blockquote><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fclick.aliyun.com%2Fm%2F1000373503%2F" target="_blank"><span style="color:#e67e22">点击立即免费试用云产品，开启云上实践之旅！</span></a></strong></blockquote><p style="text-align:justify"><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1337652%3Futm_content%3Dg_1000381155" target="_blank">原文链接</a></strong></p><p style="text-align:justify"><strong>本文为阿里云原创内容，未经允许不得转载。</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 03:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/yunqi/blog/10115767</guid>
            <link>https://my.oschina.net/yunqi/blog/10115767</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenJDK 合并英特尔 x86-simd-sort，将数据排序速度提高 7-15 倍]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">今年早些时候，英特尔发布了<span style="background-color:#ffffff">一个利用了 AVX-512 的 x86-simd-sort 快速排序库</span>；当 Numpy 将 <span style="background-color:#ffffff">x86-simd-sort 代码进行合并后发现</span>，对于 16 位到 64 位的数据类型，排序速度提高了 10~17 倍。如今，英特尔软件工程师又发布了 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fintel%2Fx86-simd-sort%2Freleases%2Ftag%2Fv3.0" target="_blank">x86-simd-sort 3.0</a>，OpenJDK 也已经将这一修改版进行了合并。</span></p><p><img height="254" src="https://oscimg.oschina.net/oscnet/up-e6ce9bf7b77cccf7d2121e07398176707ac.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">x86-simd-sort 3.0 添加了一个新的「avx512_argselect」方法</span><span style="background-color:#ffffff; color:#000000">，用于</span><span style="color:#000000">计算 arg nth_element，该</span><span style="background-color:#ffffff; color:#000000">方法</span><span style="color:#000000">返回一个对数据数组进行分区的索引数组。x86-simd-sort 3.0 版本还对其 benchmarks 进行了改进，现在使用 __builtin_cpu_supports 而不是 querying cpuinfo，</span><span style="background-color:#ffffff; color:#000000">并进行了各种其他更改。</span><br><br><span style="color:#000000">目前，x86-simd-sort 3.0 已合并至&nbsp;Numpy 主分支中，它提供了 np.partition 和 np.argpartition 的 AVX-512 矢量化版本。将 np.partition 的 16 位速度提高了 25 倍，将 32 位 dtypes 的速度提高了 17 倍，将 64 位 dtypes 的速度提高了约 8 倍。与此同时，<span style="background-color:#ffffff">新的 avx512_argselect 方法还使&nbsp;</span>np.argpartition 的速度提高了 6.5 倍。</span></p><p><span style="color:#000000">并入 OpenJDK 的 x86-simd-sort 是一个略有修改的版本，该版本将 <span style="background-color:#ffffff">32 位数据排序速度提高了 15 倍，64 位数据排序速度提高了约 7 倍。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">更多详情<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopenjdk%2Fjdk%2Fpull%2F14227" target="_blank">可查看此处</a>。</span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 03:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260605/intel-x86-simd-sort-3-0-openjdk</guid>
            <link>https://www.oschina.net/news/260605/intel-x86-simd-sort-3-0-openjdk</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[AgentVerse —— 多 LLM 环境模拟框架]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>AgentVerse 提供了一个多功能框架，可简化为大型语言模型（LLM）创建定制多代理环境的过程。框架旨在以最小的投入促进快速开发和定制，从而使研究人员能够专注于他们的研究，而不是被实施细节所困扰。</p><p><img alt="" height="333" src="https://static.oschina.net/uploads/space/2023/0915/161848_BpYH_4252687.png" width="500" referrerpolicy="no-referrer"></p><h4 style="text-align:start"><strong><span><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>特点</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></strong></h4><ul><li><p><span><span><strong>高效的环境构建：</strong>我们的框架提供了一系列基本构建块，可以轻松创建多代理环境。只需要配置文件中的几行，就可以轻松构建 LLM 聊天室等基本环境。此过程需要为 LLM 士定义环境设置和提示，使研究人员能够专注于实验和分析。</span></span></p></li><li><p><span><span><strong>可定制组件</strong>：AgentVerse 通过将多代理环境划分为五个功能模块并定义各自的接口来简化多代理环境。对于使用 AgentVerse 提供的基础模块无法直接构建的复杂环境，你可以自定义这五个功能模块中的一个或多个接口，根据你的需求高效地创建你自己的多 Agent 环境。</span></span></p></li><li><p><span><span><strong>工具（插件）利用</strong>：AgentVerse 通过工具支持多代理环境。目前，AgentVerse 支持<a href="https://github.com/OpenBMB/BMTools">BMTools</a>中提供的工具。</span></span></p></li></ul><p><img height="287" src="https://static.oschina.net/uploads/space/2023/0915/161814_aa6e_4252687.png" width="500" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 03:40:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/agentverse</guid>
            <link>https://www.oschina.net/p/agentverse</link>
        </item>
        <item>
            <title>
                <![CDATA[Android 14 正式发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Android 14 现已正式<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fandroid-developers.googleblog.com%2F2023%2F10%2Fandroid-14-is-live-in-aosp.html" target="_blank">发布</a>，该版本旨在提高开发人员的工作效率，同时增强性能、隐私、安全和用户定制功能。</p><p>目前 Android 14 已经面向部分 Pixel 设备推出，今年晚些时候将在三星、iQOO、Nothing、一加、Oppo、Realme、夏普、索尼、传音、vivo 和小米等设备上更新。</p><p><img alt="" height="124" src="https://oscimg.oschina.net/oscnet/up-cbfa279b16f4fbb420ecf378ff77fddbdf8.png" width="700" referrerpolicy="no-referrer"></p><p>该版本的一些重点更新内容包括：</p><h4><strong>性能和效率</strong></h4><p>Android 14 的一大重点是提高平台的性能和效率。</p><ul><li><strong>冻结缓存应用。</strong>在 Android 14 之前，缓存应用程序可以不受任何限制地运行。在 Android 14 中，会在短时间内冻结缓存应用程序，使其 CPU 时间为零。</li><li><strong style="color:#202124">优化广播。</strong>为了延长冻结应用程序的冻结时间（即不占用 CPU 时间），此版本调整了应用程序进入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Fguide%2Fcomponents%2Factivities%2Fprocess-lifecycle" target="_blank">缓存状态后接收</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Fguide%2Fcomponents%2Fbroadcasts%23context-registered-receivers" target="_blank">上下文注册</a><span style="color:#202124">广播的方式；它们可能会排队，并且重复的（例如&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Freference%2Fandroid%2Fcontent%2FIntent%23ACTION_BATTERY_CHANGED" target="_blank">BATTERY_CHANGED</a><span style="color:#202124">）可能会合并到一个广播中。</span></li><li><p style="margin-left:0; margin-right:0; text-align:start"><strong style="color:#202124">更快的应用程序启动</strong>。<span><span><span><span style="color:#202124"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>在 8GB 设备上，测试组发现应用程序冷启动减少了 20%，在 12GB 设备上减少了 30% 以上。与热启动相比，冷启动速度较慢，而且耗电量大。这项工作有效地改善了电量使用和整体应用启动时间。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p></li><li><p style="margin-left:0; margin-right:0; text-align:start"><strong style="color:#202124">减少内存占用。</strong><span><span><span><span style="color:#202124"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>在 Android 14 中，Android Runtime (ART) 在不影响性能的情况下优化了 code size，平均减少了 9.3%。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p></li></ul><h4><strong><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="color:#202124"><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>定制化</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>&nbsp;</strong></h4><ul><li style="text-align:left"><strong>非线性字体放大。</strong><span style="color:#000000">从 Android 14 开始，系统默认支持字体放大至 200%。</span></li></ul><p style="color:#333333; margin-left:0px; margin-right:0px; text-align:left"><span style="color:#000000"><img alt="" height="342" src="https://oscimg.oschina.net/oscnet/up-330d4f6e4e9bf64f78b10cacb9b63690486.png" width="500" referrerpolicy="no-referrer"></span></p><ul><li style="text-align:left"><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span><span style="color:#202124"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><strong>每个应用程序的语言偏好。</strong>可以使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fd.android.com%2Freference%2Fandroid%2Fapp%2FLocaleManager%3Fhl%3Den%23setOverrideLocaleConfig%28android.app.LocaleConfig%29" target="_blank"><span><span><span>LocaleManager.setOverrideLocaleConfig</span></span></span></a>动态更新应用程序的 localeConfig&nbsp;，以自定义 Android 设置中每个应用程序语言列表中显示的语言集。IME 现在可以使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Freference%2Fandroid%2Fapp%2FLocaleManager%23getApplicationLocales%2528%2529" target="_blank"><span><span><span>LocaleManager.getApplicationLocales</span></span></span></a>来了解当前应用程序的 UI 语言，以更新键盘语言。从<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Fguide%2Ftopics%2Fresources%2Fapp-languages%23auto-localeconfig" target="_blank">Android Studio Giraffe</a>和 AGP 8.1 开始，可以将应用配置为自动支持 Android 13 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Fguide%2Ftopics%2Fresources%2Fapp-languages" target="_blank">每个应用语言偏好</a>。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p></li><li style="text-align:left"><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span><span style="color:#202124"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><strong>区域偏好</strong>。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Freference%2Fandroidx%2Fcore%2Ftext%2Futil%2FLocalePreferences" target="_blank">区域偏好</a>使用户能够个性化温度单位、一周的第一天和编号系统。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p></li><li style="text-align:left"><p style="margin-left:0; margin-right:0; text-align:start"><span><span><span><span style="color:#202124"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><strong>语法变形</strong>。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Freference%2Fandroidx%2Fcore%2Ftext%2Futil%2FLocalePreferences" target="_blank">语法变形 API</a>&nbsp;允许用户更轻松地为使用具有语法性别的语言的用户添加支持。要显示个性化翻译，你只需为受影响的语言添加针对每种语法性别的变形翻译并集成 API。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p></li></ul><h4 style="text-align:start"><strong><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="color:#202124"><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>新媒体能力</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></strong></h4><ul><li><strong style="color:#202124">图像的 Ultra HDR</strong>。Android 14 增加了对 10 位高动态范围 (HDR) 图像的支持，并支持&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Fguide%2Ftopics%2Fmedia%2Fhdr-image-format" target="_blank">Ultra HDR 图像格式</a>。该格式完全向后兼容 JPEG，允许应用程序与 HDR 图像无缝互操作。</li></ul><p><img alt="" height="251" src="https://oscimg.oschina.net/oscnet/up-c3de6e626e20d082b57a43109bcfbb12949.png" width="200" referrerpolicy="no-referrer"></p><ul><li style="text-align:start"><span><span><span><span style="color:#202124"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><strong>相机扩展中的缩放、聚焦、后视图等</strong>。Android 14 升级并改进了<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Ftraining%2Fcamera%2Fcamera-extensions" target="_blank">相机扩展</a>，允许应用处理更长的处理时间，并在支持的设备上使用低光摄影等计算密集型算法来改进图像。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></li><li style="text-align:start"><span><span><span><span style="color:#202124"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><strong>无损 USB 音频</strong>。Android 14 设备可以支持无损<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.android.com%2Freference%2Fandroid%2Fmedia%2FAudioMixerAttributes" target="_blank">音频格式</a>，通过 USB 有线耳机提供发烧级体验。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p>此外，<span style="background-color:#ffffff; color:#121212">Android 14 还带来了各种图形驱动程序增强、OpenJDK 17 支持以及各种隐私和保护安全更新。</span>&nbsp;</p><p>更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fandroid-developers.googleblog.com%2F2023%2F10%2Fandroid-14-is-live-in-aosp.html" target="_blank">查看官方博客</a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 03:24:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260444/android-14</guid>
            <link>https://www.oschina.net/news/260444/android-14</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 买彩票能中大奖？用 Java 盘点常见的概率悖论]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1>引言</h1><p>《双色球头奖概率与被雷劈中的概率哪个高？》</p><p>《3 人轮流射击，枪法最差的反而更容易活下来？》</p><p>让我们用 Java 来探索 ta 们！</p><span id="OSC_h1_2"></span><h1>悖论 1：著名的三门问题</h1><p><strong>规则描述</strong>：你正在参加一个游戏节目，你被要求在三扇门中选择一扇：其中一扇后面有一辆车；其余两扇后面则是山羊。你选择了一道门，假设是一号门，然后知道门后面有什么的主持人，开启了另一扇后面有山羊的门，假设是三号门。他然后问你：「你想选择二号门吗？请问若想获得车，参赛者应该换二号门吗？</p><p><img alt="" src="https://mp.toutiao.com/mp/agw/article_material/open_image/get?code=ZjdjNjc4YjA2ZDYwMTE0MjNiYmViZTFjN2ZkMDQxZWIsMTY5NDA3OTk1MDA0Ng==" referrerpolicy="no-referrer"></p><p><strong>论证</strong>：分析需求，拆解为如下代码</p><pre><code>/**
 * &lt;p&gt; 三门问题解决方案 &lt;/p&gt;
 * @author yuanfeng.wang
 * @since 2023/8/29
 */
import java.util.Random;

public class ThreeDoorSolution {

    public static void main(String[] args) {
        // 模拟执行 1 万次，打印获胜的概率
        threeDoor(10000);
    }

    /**
     * 三门问题逻辑拆解
     * @param numSimulations 总共执行多少轮游戏
     */
    private static void threeDoor(int numSimulations) {
        int switchWins = 0;
        int stayWins = 0;

        Random random = new Random();
        for (int i = 0; i &lt; numSimulations; i++) {
            // 随机确定车所在的门
            int carDoor = random.nextInt(3);

            // 玩家随机选择一扇门
            int playerChoice = random.nextInt(3);

            // 主持人随机打开一扇门：要求该门不是玩家选择的，且必须是羊
            int openedDoor;
            do {
                openedDoor = random.nextInt(3);
            } while (openedDoor == carDoor || openedDoor == playerChoice);

            // 换门后的选择：不能是打开的门，不能是玩家选择的门，则是交换之后的门
            int finalChoice;
            do {
                finalChoice = random.nextInt(3);
            } while (finalChoice == playerChoice || finalChoice == openedDoor);

            // 计算是否换门获胜
            if (finalChoice == carDoor) {
                switchWins++;
            }

            // 计算不换门获胜
            if (playerChoice == carDoor) {
                stayWins++;
            }
        }

        // 输出结果
        System.out.println("在 " + numSimulations + " 次模拟中：");
        System.out.println("换门获胜的概率：" + (double) switchWins / numSimulations);
        System.out.println("不换门获胜的概率：" + (double) stayWins / numSimulations);
    }
}
// 模拟运行，打印结果如下
// 在 10000 次模拟中：
// 换门获胜的概率：0.6679
// 不换门获胜的概率：0.3321

</code></pre><p><strong>结论</strong>：三门问题看似一道简单的概率题，几十年来却一直引发巨大争议，持两种不同观点的人基本是五五开；事实上始终选择换门的玩家，获胜的概率 2/3，而保持原方案的胜率只有 1/3</p><span id="OSC_h1_3"></span><h1>悖论 2：双色球我能中大奖</h1><p><strong>规则描述</strong>：从 1-33 个红色球中随机选出 6 个，再从 1-16 个蓝色球中随机选择 1 个，最终开奖出一注 6+1 组合球，无顺序要求；</p><ul><li>一等奖：中 6 红 + 1 蓝</li><li>二等奖：中 6 红</li><li>三等奖：中 5 红 + 1 蓝</li><li>四等奖：中 4 红 + 1 蓝，或只中 5 个红</li><li>五等奖：中 3 红 + 1 蓝，或只中 4 个红</li><li>六等奖：中 1 蓝</li></ul><p><strong>论证</strong>：分析玩法，计算一等奖中奖率，从 33 个红球样本中选择 6 个，计算总共的组合数，即数学公式 C(n, m) = n!/((n-m)! * m!)，代入计算 C(33, 6) = 33!/((33-6)! * 6!) = 1107568，再乘以 16，最终得出一等奖获奖概率 1/17721088。</p><p>分析规则，以下代码展示了开奖一次，购买 N 注时，打印中奖信息的程序，当代入 N=500 万时，多次执行，可以很轻松打印出一等奖</p><pre><code>
import java.util.*;

/**
 * &lt;p&gt;双色球随机模拟&lt;/p&gt;
 * @author yuanfeng.wang
 * @since 2023/8/29
 */
public class SsqSolution {

    private static Random random = new Random();

    /**
     * 开奖的红球
     */
    private static Set&lt;Integer&gt; winningRedBalls;

    /**
     * 开奖的蓝球
     */
    private static int winningBlueBall;

    // 静态块初始化一组开奖号码
    static {
        // 篮球 01-16
        winningBlueBall = random.nextInt(16) + 1;

        // 红球 01-33 生成 6 个
        winningRedBalls = new HashSet&lt;&gt;();
        while (winningRedBalls.size() &lt; 6) {
            int num = random.nextInt(33) + 1;
            winningRedBalls.add(num);
        }
    }

    public static void main(String[] args) {
        play(500_0000);
    }

    /**
     *
     * @param num 运行一次程序只开一次奖，此参数表示总共购买多少注
     */
    public static void play(int num) {
        System.out.println("\n 本期开奖号码：");
        System.out.println("红球：" + winningRedBalls + " 篮球：" + winningBlueBall);
        for (int i = 0; i &lt; num; i++) {
            playOnce();
        }
    }

    private static void playOnce() {
        Set&lt;Integer&gt; userRedBalls = getUserSelectedRedBalls();
        int userBlueBall = getUserSelectedBlueBall();

        int redBallMatch = countMatchingBalls(userRedBalls, winningRedBalls);
        boolean blueBallMatch = (userBlueBall == winningBlueBall);

        if (redBallMatch == 6 &amp;&amp; blueBallMatch) {
            System.out.println("\n 恭喜你中了一等奖！");
            System.out.println("玩家购买的号码：");
            System.out.println("红球：" + userRedBalls + " 蓝球：" + userBlueBall);
        } else if (redBallMatch == 6) {
            System.out.println("\n 恭喜你中了二等奖！");
        } else if (redBallMatch == 5 &amp;&amp; blueBallMatch) {
//            System.out.println("\n 恭喜你中了三等奖！");
        } else if (redBallMatch == 5 || (redBallMatch == 4 &amp;&amp; blueBallMatch)) {
//            System.out.println("\n 恭喜你中了四等奖！");
        } else if (redBallMatch == 4 || (redBallMatch == 3 &amp;&amp; blueBallMatch)) {
//            System.out.println("\n 恭喜你中了五等奖！");
        } else if (blueBallMatch) {
//            System.out.println("\n 恭喜你中了最小奖！");
        } else {
            //没中奖，不打印记录
        }
    }

    /**
     * 返回玩家选择的 6 个红球,范围 1-33，不重复
     */
    private static Set&lt;Integer&gt; getUserSelectedRedBalls() {
        Set&lt;Integer&gt; userRedBalls = new HashSet&lt;&gt;();
        while (userRedBalls.size() &lt; 6) {
            int num = random.nextInt(33) + 1;
            userRedBalls.add(num);
        }
        return userRedBalls;
    }

    /**
     * 玩家选择的 1 个蓝球,范围 1-16
     */
    private static int getUserSelectedBlueBall() {
        return random.nextInt(16) + 1;
    }

    /**
     * 匹配中了几个红球
     * @return 中红球个数
     */
    private static int countMatchingBalls(Set&lt;Integer&gt; userBalls, Set&lt;Integer&gt; winningBalls) {
        int count = 0;
        for (int ball : userBalls) {
            if (winningBalls.contains(ball)) {
                count++;
            }
        }
        return count;
    }

}

</code></pre><p><strong>结论</strong>：排除其它因素，头奖概率约 1700 万分之 1，这个结论并不直观，例举如下几个进行对比</p><p>1.一家祖孙三代人的生日都在同一天的概率约为 27 万分之一</p><p>2.小行星撞击地球的概率保守推测是 200 万分之一</p><p>3.生出全男或全女四胞胎的概率约为 352 万分之一</p><span id="OSC_h1_4"></span><h1>悖论 3：三个枪手</h1><p><strong>描述</strong>：三个小伙子同时爱上了一个姑娘，为了决定他们谁能娶这个姑娘，他们决定用枪进行一次决斗。A 的命中率是 30％，B 比他好些，命中率是 50％，最出色的枪手是 C，他从不失误，命中率是 100％。由于这个显而易见的事实，为公平起见，他们决定按这样的顺序：A 先开枪，B 第二，C 最后。然后这样循环，直到他们只剩下一个人。那么 A 第一枪应该怎么打？谁活下来的概率最大？</p><p><strong>论证</strong>：每个人的目标都是活下来，为了目标寻找最好的策略。以下开始分人讨论</p><p><strong>A：</strong></p><ul><li>若 A 开枪射杀了 B，则下个开枪是 C，C 会 100% 射杀 A，这不是一个好策略</li><li>若 A 开枪射杀了 C，则下一轮 B 会有 50% 的几率杀掉自己</li><li>若 A 开枪未打中，则下一轮可以坐山观虎斗，所以 A 最好的策略看似是故意打空枪更好一些</li></ul><p><strong>B：</strong></p><ul><li>若 A 已经将 C 射杀，此时 B 与 A 互相射击，B 的生存率高于 A</li><li>B 只能选择射杀 C，因为只要 C 活着，都会优先射杀 B</li></ul><p><strong>C：</strong></p><ul><li>先消除威胁大的 B，然后再杀掉 A，只要自己有开 2 枪的机会，直接获胜</li></ul><p><strong>结论</strong>：需求太复杂，暂未实现生存概率计算😭，欢迎补充悖论 3 的代码论证过程</p><blockquote><p>作者：京东保险&nbsp;王苑沣</p><p>来源：京东云开发者社区，转载请注明来源</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 02:40:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/10109696</guid>
            <link>https://my.oschina.net/u/4090830/blog/10109696</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[KubeSphere 社区双周报 | OpenFunction v1.2.0 发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>KubeSphere 社区双周报主要整理展示新增的贡献者名单和证书、新增的讲师证书以及两周内提交过 commit 的贡献者，并对近期重要的 PR 进行解析，同时还包含了线上/线下活动和布道推广等一系列社区动态。</p><p>本次双周报涵盖时间为：2023.09.15-2023.09.28。</p><h2>贡献者名单</h2><p><img src="https://oscimg.oschina.net/oscnet/up-cce05b68864a3f222cd17e6df1aa1c3655b.gif" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-7e6818d775388b1291392b3250789734a38.png" alt="" referrerpolicy="no-referrer"></p><h2>新晋 KubeSphere Contributor</h2><p>两周内共有 2 位新晋 KubeSphere Contributor，感谢各位对 KubeSphere 社区的贡献！</p><p><img src="https://oscimg.oschina.net/oscnet/up-ca0b0945c103e0492a91de92a5132784bea.png" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-e3ad1819572baf498ffd15433cf938b034d.png" alt="" referrerpolicy="no-referrer"></p><h2>近期更新</h2><h3>KubeSphere</h3><h4>1. 支持通过 IP 搜索 pod</h4><p>相关 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fpull%2F5921" target="_blank">https://github.com/kubesphere/kubesphere/pull/5921</a></p><p>贡献者：zhou1203</p><h3>OpenFunction</h3><h4>1. 发布了 OpenFunction v1.2.0，支持使用 keda-addons-http 作为同步函数运行时</h4><p>相关 Release: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenFunction%2FOpenFunction%2Freleases%2Ftag%2Fv1.2.0" target="_blank">https://github.com/OpenFunction/OpenFunction/releases/tag/v1.2.0</a></p><p>贡献者：wrongerror</p><h4>2. 升级 OpenFunction Chart 依赖组件 Dapr, Keda 以及 contour 的版本</h4><p>相关 PR: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpenFunction%2Fcharts%2Fpull%2F51" target="_blank">https://github.com/OpenFunction/charts/pull/51</a></p><p>贡献者：wrongerror</p><h3>KubeKey</h3><h4>1. 支持部署 Kubernetes v1.27+</h4><p>相关 PR: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubekey%2Fpull%2F2000" target="_blank">https://github.com/kubesphere/kubekey/pull/2000</a></p><p>贡献者：pixiake</p><h4>2. 支持部署高可用 Harbor</h4><p>相关 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubekey%2Fpull%2F1992" target="_blank">https://github.com/kubesphere/kubekey/pull/1992</a></p><p>贡献者：wenwenxiong</p><blockquote><p>本文由博客一文多发平台 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenwrite.cn%3Ffrom%3Darticle_bottom" target="_blank">OpenWrite</a> 发布！</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 06 Oct 2023 02:16:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4197945/blog/10115731</guid>
            <link>https://my.oschina.net/u/4197945/blog/10115731</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[越来越多开源项目停更，Java 生态受影响最大]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#333333">Sonatype&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.sonatype.com%2Fintroducing-our-9th-annual-state-of-the-software-supply-chain-report" target="_blank">发布</a>了最新的一份</span>《软件供应链状况》报告，深入探讨了如何在充满选择的世界中定义更好的软件，并探讨人工智能 (AI) 对软件开发的深远影响；还研究了开源供应、需求和安全之间错综复杂的相互作用。</p><p>报告跟踪了 Java (Maven)、JavaScript (npm)、Python (PyPI)、.NET (NuGet Gallery) 四大开源生态系统的开源应用增长情况。2022 年至 2023 年间，可用开源项目的数量平均增长了 29%。2023 年，开源项目平均发布了 15 个可供使用的版本，不同开源注册中心的特定生态系统平均有 10 到 22 个版本。这意味着每个月都会发布 1-2 个新版本，在观察到的生态系统中总共发布了 6000 万个新版本。</p><p><img height="293" src="https://oscimg.oschina.net/oscnet/up-a3af85a689f0adbcfb8236b4dac77b7f235.png" width="500" referrerpolicy="no-referrer"></p><p>每个受检测的生态系统都表现出一致的项目增长率，平均同比增长率高达 29%。</p><p><img height="295" src="https://oscimg.oschina.net/oscnet/up-9ab7acc5786b753b241922eeaa519953a3d.png" width="500" referrerpolicy="no-referrer"></p><p>但随着开源组件供应量的持续增长，其需求却未能与之同步。在过去两年中，下载量的增长率逐渐下降。2023 年的平均增长率为 33%，与 2021 年 73% 的增长率相比大幅下降。</p><p><span style="color:#000000">与此同时，开源软件安全问题没有放缓的迹象。截至 2023 年 9 月，研究团队共发现了&nbsp;245,032 个恶意软件包，是往年总和的 2 倍。八分之一的开源下载存在已知风险，且仍有 23% 的 Log4j 下载存在严重漏洞。</span></p><p><img height="304" src="https://oscimg.oschina.net/oscnet/up-b8b946760f70163d67ebbf53b91a14930fd.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">开源项目的主动维护也变得越来越少。研究表明，去年有近五分之一（18.6%）的项目停止维护，影响了 Java 和 JavaScript 生态系统。<span style="background-color:#ffffff">只有 11% 的开源项目实际上得到了积极维护。</span>尽管存在这些缺陷，但 Sonatype 仍然表示，近 96% 存在已知漏洞的组件下载可以通过选择无漏洞版本来避免。</span></p><p><span style="color:#000000">就软件开发中的人工智能而言，97% 的受访 DevOps 和 SecOps 领导者表示，他们目前在工作流程中某种程度上使用了人工智能，大多数人每天使用两个或更多工具。</span><span><span><span><span style="color:#000000"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>去年，企业环境中 AI 和 ML 组件的采用率增加了 135%。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">研究还发现，企业自认为的安全程度与实际情况之间存在脱节。67% 的公司表示，他们确信自己的系统中没有来自漏洞库的代码，但今年有 10% 的公司因漏洞组件而遭遇安全漏洞。39% 的公司可以在</span><span style="background-color:#ffffff">&nbsp;1 到 7 天的时间内发现漏洞，29% 的公司需要一周以上的时间，28% 的公司只需要不到一天的时间。</span></span></p><p><span style="color:#000000"><span style="background-color:#ffffff">更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sonatype.com%2Fstate-of-the-software-supply-chain%2Fintroduction" target="_blank">查看完整报告</a>。</span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 05 Oct 2023 04:46:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260454/9th-annual-state-of-the-software-supply-chain-report</guid>
            <link>https://www.oschina.net/news/260454/9th-annual-state-of-the-software-supply-chain-report</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[GCC 安全策略文档已合并到仓库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#000000; text-align:start">最近几周在 GCC 邮件列表进行讨论后，开发团队为 GCC 代码库添加了 GCC 安全策略，以概述编译器项目的安全流程。</p><p style="color:#000000; text-align:start"><img alt="" src="https://static.oschina.net/uploads/space/2023/1006/113615_PVqj_2720166.jpeg" referrerpolicy="no-referrer"></p><p style="color:#000000; text-align:start">该文档概述了 GCC 安全漏洞处理建议、GCC 语言运行库的安全注意事项、在 GCC 中实现的安全功能，以及私下报告安全漏洞的最佳方式。</p><p style="color:#000000; text-align:start">GCC 安全政策文档于周三<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgcc.gnu.org%2Fgit%2F%3Fp%3Dgcc.git%3Ba%3Dcommit%3Bh%3D4cac1d2eec5549927fe0caee179f80007e8d729b" target="_blank">提交</a>到代码库。如果希望了解更多关于 GCC 安全策略的内容，可以在&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgcc.gnu.org%2Fgit%2F%3Fp%3Dgcc.git%3Ba%3Dblob%3Bf%3DSECURITY.txt" target="_blank">SECURITY.txt</a>&nbsp;中阅读。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 05 Oct 2023 03:37:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/260447/gcc-security-policy</guid>
            <link>https://www.oschina.net/news/260447/gcc-security-policy</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | Rocky Linux 系统安全加固工具 narsil]]>
            </title>
            <description>
                <![CDATA[<p><a href="https://gitee.com/seatonjiang/narsil/blob/main/README.md">English</a> | 简体中文</p><p align="center"><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%23gh-light-mode-only"><img src="https://gitee.com/seatonjiang/narsil/raw/main/.github/narsil-light.png#gh-light-mode-only" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%23gh-dark-mode-only"><img src="https://gitee.com/seatonjiang/narsil/raw/main/.github/narsil-dark.png#gh-dark-mode-only" referrerpolicy="no-referrer"></a></p><p align="center"><img src="https://img.shields.io/static/v1?style=flat-square&amp;message=Rocky%20Linux&amp;color=15B076&amp;logo=rockylinux&amp;logoColor=FFFFFF&amp;label=" referrerpolicy="no-referrer"><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%2Fissues"><img src="https://img.shields.io/github/issues/seatonjiang/narsil?style=flat-square&amp;color=blue" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%2Fpulls"><img src="https://img.shields.io/github/issues-pr/seatonjiang/narsil?style=flat-square&amp;color=brightgreen" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%2Fblob%2Fmain%2FLICENSE"><img src="https://img.shields.io/github/license/seatonjiang/narsil?&amp;style=flat-square" referrerpolicy="no-referrer"></a></p><p align="center"><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%2Fissues">报告问题</a>
    ·
    <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%2Fissues">功能需求</a></p><p align="center">Rocky Linux 的系统安全加固工具</p><h2><a id="user-content--工具截图" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E5%B7%A5%E5%85%B7%E6%88%AA%E5%9B%BE"></a>💻 工具截图</h2><h3><a id="user-content-脚本执行" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C"></a>脚本执行</h3><p align="center"><img src="https://gitee.com/seatonjiang/narsil/raw/main/.github/script-execution.png" referrerpolicy="no-referrer"></p><h3><a id="user-content-登录信息" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E7%99%BB%E5%BD%95%E4%BF%A1%E6%81%AF"></a>登录信息</h3><p align="center"><img src="https://gitee.com/seatonjiang/narsil/raw/main/.github/login-information.png" referrerpolicy="no-referrer"></p><h3><a id="user-content-挂载硬盘" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E6%8C%82%E8%BD%BD%E7%A1%AC%E7%9B%98"></a>挂载硬盘</h3><p align="center"><img src="https://gitee.com/seatonjiang/narsil/raw/main/.github/mount-disk.png" referrerpolicy="no-referrer"></p><h2><a id="user-content--工具特性" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E5%B7%A5%E5%85%B7%E7%89%B9%E6%80%A7"></a>✨ 工具特性</h2><ul><li>限制密码使用期限为 30 天</li><li>密码过期 30 天后，该账户将被禁用</li><li>设置两次修改密码的时间间隔为 1 天</li><li>在密码过期前 7 天将发出警告</li><li>将系统默认加密算法设置为 SHA512</li><li>将会话超时策略设置为 180 秒</li><li>为新建的用户创建并加入一个同名的组</li><li>将新建用户的 home 目录权限设置为 0750</li><li>将存量用户的 home 目录权限设置为 0750</li><li>强化 OpenSSH 配置（有些配置需要手动配置）</li><li>禁止没有 home 目录的用户登录</li><li>禁止新建的用户使用 SHELL 登录</li><li>禁止上传和用户信息的功能</li><li>禁止删除用户时同步删除该用户的组</li></ul><p>还有很多特性没有被列举出来，可以参考 <code>scripts</code> 目录下的文件了解更多信息。</p><h2><a id="user-content--使用说明" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"></a>🚀 使用说明</h2><h3><a id="user-content-第一步克隆仓库" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E7%AC%AC%E4%B8%80%E6%AD%A5%E5%85%8B%E9%9A%86%E4%BB%93%E5%BA%93"></a>第一步：克隆仓库</h3><p>确保服务器安装了 Git，否则需要先用 <code>sudo dnf install -y git</code> 命令安装软件：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone https://github.com/seatonjiang/narsil.git</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果因为网络问题无法连接，可以使用 Gitee 镜像仓库，但是镜像仓库会有 <code>30</code> 分钟的延迟：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone https://gitee.com/seatonjiang/narsil.git</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-第二步编辑配置" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E7%BC%96%E8%BE%91%E9%85%8D%E7%BD%AE"></a>第二步：编辑配置</h3><p>进入项目文件夹：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">cd </span>narsil</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>核对配置文件中的配置信息（配置文件说明在下文）：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">vi narsil.conf</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-第三步运行脚本" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E7%AC%AC%E4%B8%89%E6%AD%A5%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%AC"></a>第三步：运行脚本</h3><p>如果是 root 账号，可以直接运行，如果是普通账号，需要使用 <code>sudo</code> 运行，而且必须用 <code>bash</code> 运行该脚本：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content--配置文件" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"></a>📝 配置文件</h2><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 每一项操作完成后进行验证</span></span><span id="LC2" class="line"><span class="py">VERIFY</span><span class="p">=</span><span class="s">'Y'</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 云服务器使用 Metadata 覆盖默认配置</span></span><span id="LC5" class="line"><span class="py">METADATA</span><span class="p">=</span><span class="s">'Y'</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="c"># 在 banner 中添加生产环境的提示</span></span><span id="LC8" class="line"><span class="py">PROD_TIPS</span><span class="p">=</span><span class="s">'Y'</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="c"># SSH 端口配置</span></span><span id="LC11" class="line"><span class="py">SSH_PORT</span><span class="p">=</span><span class="s">'22'</span></span><span id="LC12" class="line"></span><span id="LC13" class="line"><span class="c"># 时区配置</span></span><span id="LC14" class="line"><span class="py">TIME_ZONE</span><span class="p">=</span><span class="s">'Asia/Shanghai'</span></span><span id="LC15" class="line"></span><span id="LC16" class="line"><span class="c"># 主机名称配置（当 METADATA 为 Y 时会自动拉取元数据）</span></span><span id="LC17" class="line"><span class="py">HOSTNAME</span><span class="p">=</span><span class="s">'rockylinux'</span></span><span id="LC18" class="line"></span><span id="LC19" class="line"><span class="c"># DNS 服务器配置（当 METADATA 为 Y 时会自动拉取元数据）</span></span><span id="LC20" class="line"><span class="py">DNS_SERVER</span><span class="p">=</span><span class="s">'119.29.29.29 119.28.28.28'</span></span><span id="LC21" class="line"></span><span id="LC22" class="line"><span class="c"># NTP 服务器配置（当 METADATA 为 Y 时会自动拉取元数据）</span></span><span id="LC23" class="line"><span class="py">NTP_SERVER</span><span class="p">=</span><span class="s">'ntp.tencent.com'</span></span><span id="LC24" class="line"></span><span id="LC25" class="line"><span class="c"># Docker 配置</span></span><span id="LC26" class="line"><span class="py">DOCKER_CE_REPO</span><span class="p">=</span><span class="s">'http://mirrors.tencent.com/docker-ce/linux/centos/docker-ce.repo'</span></span><span id="LC27" class="line"><span class="py">DOCKER_CE_MIRROR</span><span class="p">=</span><span class="s">'mirrors.tencent.com'</span></span><span id="LC28" class="line"><span class="py">DOCKER_HUB_MIRRORS</span><span class="p">=</span><span class="s">'https://hub-mirror.c.163.com'</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content--独立功能" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E7%8B%AC%E7%AB%8B%E5%8A%9F%E8%83%BD"></a>🔨 独立功能</h2><p>Narsil 中包含了一些独立的功能，这些功能并不在自动执行的脚本中，需要使用参数单独使用，可以使用 <code>sudo bash narsil.sh -h</code> 命令查看所有独立功能。</p><h3><a id="user-content-清理垃圾" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E6%B8%85%E7%90%86%E5%9E%83%E5%9C%BE"></a>清理垃圾</h3><p>清理所有的系统日志文件。</p><blockquote><p>某些云服务商提供的镜像由于制作的过程不规范，导致打包了一些垃圾文件到镜像中，建议在初始化系统之前先进行清理。</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh <span class="nt">-c</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-安装-docker" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E5%AE%89%E8%A3%85-docker"></a>安装 Docker</h3><p>安装 Docker 服务并设置镜像加速（腾讯云会自动使用内网加速地址）。</p><blockquote><p>安装完成后，可以使用 <code>docker run hello-world</code> 测试 Docker 的相关功能是否正常。</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh <span class="nt">-d</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-挂载硬盘-1" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E6%8C%82%E8%BD%BD%E7%A1%AC%E7%9B%98-1"></a>挂载硬盘</h3><p>交互式挂载数据盘（腾讯云会使用弹性云硬盘的软链接方式挂载），数据无价，操作过程切记小心！</p><blockquote><p>如果所选的硬盘已经被挂载，会提示解除挂载及格式化操作。</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh <span class="nt">-f</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-修改主机名称" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D%E7%A7%B0"></a>修改主机名称</h3><p>配置文件的参数如果没有变化，那么将优先获取元数据。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh <span class="nt">-h</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-修改端口" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E4%BF%AE%E6%94%B9%E7%AB%AF%E5%8F%A3"></a>修改端口</h3><p>交互式修改 SSH 端口。</p><blockquote><p>端口范围需要在 10000 到 65535 之间。</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh <span class="nt">-p</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-卸载监控" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E5%8D%B8%E8%BD%BD%E7%9B%91%E6%8E%A7"></a>卸载监控</h3><p>卸载云服务商安装到服务器中的各种监控组件。</p><blockquote><p>目前已经支持腾讯云监控组件的卸载。</p></blockquote><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh <span class="nt">-r</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-添加交换空间" class="anchor" href="https://gitee.com/seatonjiang/narsil#%E6%B7%BB%E5%8A%A0%E4%BA%A4%E6%8D%A2%E7%A9%BA%E9%97%B4"></a>添加交换空间</h3><p>如果物理内存太小，建议添加交换空间。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nb">sudo </span>bash narsil.sh <span class="nt">-s</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content--目录结构" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"></a>📂 目录结构</h2><p>下面是整个项目的文件夹结构，<code>config</code> 及 <code>scripts</code> 文件夹中的文件省略显示。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">narsil</span><span id="LC2" class="line">├── narsil.sh</span><span id="LC3" class="line">├── narsil.conf</span><span id="LC4" class="line">├── config</span><span id="LC5" class="line">│   └── <span class="o">(</span>some config files<span class="o">)</span></span><span id="LC6" class="line">└── scripts</span><span id="LC7" class="line">    └── <span class="o">(</span>some script files<span class="o">)</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content--参与共建" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E5%8F%82%E4%B8%8E%E5%85%B1%E5%BB%BA"></a>🤝 参与共建</h2><p>我们欢迎所有的贡献，你可以将任何想法作为 Pull Requests 或 Issues 提交，顺颂商祺 :)</p><h2><a id="user-content--开源许可" class="anchor" href="https://gitee.com/seatonjiang/narsil#-%E5%BC%80%E6%BA%90%E8%AE%B8%E5%8F%AF"></a>📃 开源许可</h2><p>项目基于 GNU 通用公共许可证 v3.0 发布，详细说明请参阅 <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fseatonjiang%2Fnarsil%2Fblob%2Fmain%2FLICENSE">LICENCE</a> 文件。</p>]]>
            </description>
            <pubDate>Thu, 05 Oct 2023 03:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/seatonjiang/narsil</guid>
            <link>https://gitee.com/seatonjiang/narsil</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 使用 FHE 实现加密大语言模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 编辑器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;" data-mpa-powered-by="yiban.io"><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">近来，大语言模型 (LLM) 已被证明是提高编程、内容生成、文本分析、网络搜索及远程学习等诸多领域生产力的可靠工具。</p><span id="OSC_h2_1"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">大语言模型对用户隐私的影响</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">尽管 LLM 很有吸引力，但如何保护好 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">输入给这些模型的用户查询中的隐私</code> 这一问题仍然存在。一方面，我们想充分利用 LLM 的力量，但另一方面，存在向 LLM 服务提供商泄露敏感信息的风险。在某些领域，例如医疗保健、金融或法律，这种隐私风险甚至有一票否决权。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">一种备选解决方案是本地化部署，LLM 所有者将其模型部署在客户的计算机上。然而，这不是最佳解决方案，因为构建 LLM 可能需要花费数百万美元 (GPT3 为 460 万美元)，而本地部署有泄露模型知识产权 (intellectual property, IP) 的风险。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Zama 相信有两全其美之法: 我们的目标是同时保护用户的隐私和模型的 IP。通过本文，你将了解如何利用 Hugging Face transformers 库并让这些模型的某些部分在加密数据上运行。完整代码见，此处。</p><span id="OSC_h2_2"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">全同态加密 (Fully Homomorphic Encryption，FHE) 可以解决 LLM 隐私挑战</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">针对 LLM 部署的隐私挑战，Zama 的解决方案是使用全同态加密 (FHE)，在加密数据上执行函数。这种做法可以实现两难自解，既可以保护模型所有者知识产权，同时又能维护用户的数据隐私。我们的演示表明，在 FHE 中实现的 LLM 模型保持了原始模型的预测质量。为此，我们需要调整 Hugging Face transformers 库，中的 GPT2 实现，使用 Concrete-Python 对推理部分进行改造，这样就可以将 Python 函数转换为其 FHE 等效函数。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-ratio="1.2027809965237544" data-type="png" data-w="863" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/338741ab-aa57-4681-b21f-eeeabcd24fce.png" referrerpolicy="no-referrer"><figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     图 1. GPT2 架构; 图源: https://en.wikipedia.org/wiki/GPT-2 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">图 1 展示了由多个 transformer block 堆叠而成的 GPT2 架构: 其中最主要的是多头注意力 (multi-head attention，MHA) 层。每个 MHA 层使用模型权重来对输入进行投影，然后各自计算注意力，并将注意力的输出重新投影到新的张量中。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在 TFHE 中，模型权重和激活均用整数表示。非线性函数必须通过可编程自举 (Programmable Bootstrapping，PBS) 操作来实现。PBS 对加密数据实施查表 (table lookup，TLU) 操作，同时刷新密文以支持，任意计算。不好的一面是，此时 PBS 的计算时间在线性运算中占主导地位。利用这两种类型的运算，你可以在 FHE 中表达任何子模型的计算，甚至完整的 LLM 计算。</p><span id="OSC_h2_3"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">使用 FHE 实现 LLM 的一层</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">接下来，你将了解如何加密多头注意力 (MHA) 中的一个注意力头。你可以在，此处，找到完整的 MHA 实现代码。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-ratio="1.0334538878842676" data-type="svg" data-w="1106" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/72fc90eb-872e-4a86-9f18-c949824b4c7d.svg" referrerpolicy="no-referrer"><figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     图 2. 在 FHE 中运行 LLM 模型的某些部分 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">图 2 概述了一个简化的底层实现。在这个方案中，模型权重会被分成两个部分，分别存储在客户端和服务端。首先，客户端在本地开始推理，直至遇到已第一个不在本地的层。用户将中间结果加密并发送给服务端。服务端对其执行相应的注意力机制计算，然后将结果返回给客户端，客户端对结果进行解密并继续在本地推理。</p><span id="OSC_h3_4"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">量化</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">首先，为了对加密值进行模型推理，模型的权重和激活必须被量化并转换为整数。理想情况是使用，训练后量化，这样就不需要重新训练模型了。这里，我们使用整数和 PBS 来实现 FHE 兼容的注意力机制，并检查其对 LLM 准确率的影响。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">要评估量化的影响，我们运行完整的 GPT2 模型，并让其中的一个 LLM 头进行密态计算。然后我们基于此评估权重和激活的量化比特数对准确率的影响。</p><figure data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-ratio="0.8024691358024691" data-type="png" data-w="567" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;height: auto !important;" src="https://oscimg.oschina.net/oscnet/e0260d01-fa3c-414c-a6d7-a0c0025b9e4b.png" referrerpolicy="no-referrer"><figcaption style="margin-top: 5px;text-align: center;color: #dda52d;font-size: 14px;">
     单注意力头量化的平均 top-k 准确率 
   </figcaption></figure><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">上图表明 4 比特量化保持了原始精度的 96%。该实验基于含有约 80 个句子的数据集，并通过将原始模型的 logits 预测与带有量化注意力头的模型的 logits 预测进行比较来计算最终指标。</p><span id="OSC_h3_5"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">在 Hugging Face GPT2 模型中使用 FHE</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">我们需要在 Hugging Face 的 transformers 库的基础上重写加密模块的前向传播，以使其包含量化算子。首先通过加载 GPT2LMHeadModel 构建一个 SingleHeadQGPT2Model 实例，然后手动使用 QGPT2SingleHeadAttention 替换第一个多头注意力模块，代码如下。你可以在，这里，找到模型的完整实现。</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">self.transformer.h[<span style="color: #008080;line-height: 26px;">0</span>].attn&nbsp;=&nbsp;QGPT2SingleHeadAttention(config,&nbsp;n_bits=n_bits)<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">至此，前向传播已被重载成用 FHE 算子去执行多头注意力的第一个头，包括构建查询、键和值矩阵的投影。以下代码中的 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">QGPT2</code> 模块的代码见，此处。</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;"><span style="line-height: 26px;"><span style="font-weight: bold;line-height: 26px;">class</span>&nbsp;<span style="color: #458;font-weight: bold;line-height: 26px;">SingleHeadAttention</span><span style="line-height: 26px;">(QGPT2)</span>:</span><br>&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #d14;line-height: 26px;">"""Class&nbsp;representing&nbsp;a&nbsp;single&nbsp;attention&nbsp;head&nbsp;implemented&nbsp;with&nbsp;quantization&nbsp;methods."""</span><br><br><br>&nbsp;&nbsp;&nbsp;&nbsp;<span style="line-height: 26px;"><span style="font-weight: bold;line-height: 26px;">def</span>&nbsp;<span style="color: #900;font-weight: bold;line-height: 26px;">run_numpy</span><span style="line-height: 26px;">(self,&nbsp;q_hidden_states:&nbsp;np.ndarray)</span>:</span><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #998;font-style: italic;line-height: 26px;">#&nbsp;Convert&nbsp;the&nbsp;input&nbsp;to&nbsp;a&nbsp;DualArray&nbsp;instance</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q_x&nbsp;=&nbsp;DualArray(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float_array=self.x_calib,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int_array=q_hidden_states,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;quantizer=self.quantizer<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #998;font-style: italic;line-height: 26px;">#&nbsp;Extract&nbsp;the&nbsp;attention&nbsp;base&nbsp;module&nbsp;name</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mha_weights_name&nbsp;=&nbsp;<span style="color: #d14;line-height: 26px;">f"transformer.h.<span style="color: rgb(51, 51, 51);line-height: 26px;">{self.layer}</span>.attn."</span><br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #998;font-style: italic;line-height: 26px;">#&nbsp;Extract&nbsp;the&nbsp;query,&nbsp;key&nbsp;and&nbsp;value&nbsp;weight&nbsp;and&nbsp;bias&nbsp;values&nbsp;using&nbsp;the&nbsp;proper&nbsp;indices</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;head_0_indices&nbsp;=&nbsp;[<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;list(range(i&nbsp;*&nbsp;self.n_embd,&nbsp;i&nbsp;*&nbsp;self.n_embd&nbsp;+&nbsp;self.head_dim))<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="font-weight: bold;line-height: 26px;">for</span>&nbsp;i&nbsp;<span style="font-weight: bold;line-height: 26px;">in</span>&nbsp;range(<span style="color: #008080;line-height: 26px;">3</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q_qkv_weights&nbsp;=&nbsp;...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q_qkv_bias&nbsp;=&nbsp;...<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #998;font-style: italic;line-height: 26px;">#&nbsp;Apply&nbsp;the&nbsp;first&nbsp;projection&nbsp;in&nbsp;order&nbsp;to&nbsp;extract&nbsp;Q,&nbsp;K&nbsp;and&nbsp;V&nbsp;as&nbsp;a&nbsp;single&nbsp;array</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q_qkv&nbsp;=&nbsp;q_x.linear(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weight=q_qkv_weights,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bias=q_qkv_bias,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key=<span style="color: #d14;line-height: 26px;">f"attention_qkv_proj_layer_<span style="color: rgb(51, 51, 51);line-height: 26px;">{self.layer}</span>"</span>,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #998;font-style: italic;line-height: 26px;">#&nbsp;Extract&nbsp;the&nbsp;queries,&nbsp;keys&nbsp;and&nbsp;vales</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q_qkv&nbsp;=&nbsp;q_qkv.expand_dims(axis=<span style="color: #008080;line-height: 26px;">1</span>,&nbsp;key=<span style="color: #d14;line-height: 26px;">f"unsqueeze_<span style="color: rgb(51, 51, 51);line-height: 26px;">{self.layer}</span>"</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q_q,&nbsp;q_k,&nbsp;q_v&nbsp;=&nbsp;q_qkv.enc_split(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #008080;line-height: 26px;">3</span>,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;axis=<span style="color: #008080;line-height: 26px;">-1</span>,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key=<span style="color: #d14;line-height: 26px;">f"qkv_split_layer_<span style="color: rgb(51, 51, 51);line-height: 26px;">{self.layer}</span>"</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #998;font-style: italic;line-height: 26px;">#&nbsp;Compute&nbsp;attention&nbsp;mechanism</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q_y&nbsp;=&nbsp;self.attention(q_q,&nbsp;q_k,&nbsp;q_v)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="font-weight: bold;line-height: 26px;">return</span>&nbsp;self.finalize(q_y)<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">模型中的其他计算仍以浮点形式进行，未加密，并由客户端在本地执行。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">将预训练的权重加载到修改后的 GPT2 模型中，然后调用 <em style="color: black;">generate</em> 方法:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">qgpt2_model&nbsp;=&nbsp;SingleHeadQGPT2Model.from_pretrained(<br>&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #d14;line-height: 26px;">"gpt2_model"</span>,&nbsp;n_bits=<span style="color: #008080;line-height: 26px;">4</span>,&nbsp;use_cache=<span style="color: #008080;line-height: 26px;">False</span><br>)<br><br>output_ids&nbsp;=&nbsp;qgpt2_model.generate(input_ids)<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">举个例子，你可以要求量化模型补全短语 「Cryptography is a」 。在 FHE 中运行模型时，如果量化精度足够，生成的输出为:</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">「Cryptography is a very important part of the security of your computer」</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">当量化精度太低时，您会得到:</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">「Cryptography is a great way to learn about the world around you」</p><span id="OSC_h3_6"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">编译为 FHE</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">现在，你可以使用以下 Concrete-ML 代码编译注意力头:</p><pre data-tool="mdnice 编辑器" style="margin-top: 10px;margin-bottom: 10px;"><code style="overflow-x: auto;padding: 16px;color: #333;background: #f8f8f8;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;border-radius: 0px;font-size: 12px;-webkit-overflow-scrolling: touch;">circuit_head&nbsp;=&nbsp;qgpt2_model.compile(input_ids)<br></code></pre><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">运行此代码，你将看到以下打印输出: 「Circuit compiled with 8 bit-width」。该配置与 FHE 兼容，显示了在 FHE 中执行的操作所需的最大位宽。</p><span id="OSC_h3_7"></span><h3 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 20px;line-height: 1.4;padding-top: 10px;margin-top: 10px;margin-bottom: 5px;"><span style="display: none;"></span><span style="color: rgb(81, 81, 81);font-size: 17px;padding-left: 1em;border-left: 3px solid rgb(249, 191, 69);">复杂度</span><span style="display: none;"></span></h3><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">在 transformer 模型中，计算量最大的操作是注意力机制，它将查询、键和值相乘。在 FHE 中，加密域中乘法的特殊性加剧了成本。此外，随着序列长度的增加，这些乘法的数量还会呈二次方增长。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">而就加密注意力头而言，长度为 6 的序列需要 11622 次 PBS 操作。我们目前的实验还很初步，尚未对性能进行优化。虽然可以在几秒钟内运行，但不可否认它需要相当多的计算能力。幸运的是，我们预期，几年后，硬件会将延迟提高 1000 倍到 10000 倍，使原来在 CPU 上需要几分钟的操作缩短到 ASIC 上的低于 100 毫秒。有关这些估算的更多信息，请参阅，此博文。</p><span id="OSC_h2_8"></span><h2 data-tool="mdnice 编辑器" style="font-weight: bold;font-size: 22px;line-height: 1.2em;margin-top: 2em;margin-bottom: 35px;color: rgb(255, 157, 0);"><span style="font-size: 18px;color: rgb(255, 157, 11);padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;display: none;"></span><span style="color: rgb(255, 157, 11);visibility: visible;display: inline-block;border-left: 5px solid rgb(255, 157, 0);padding: 2px 13px;margin-right: 3px;height: 50%;font-size: 18px;">总结</span><span style="font-size: 18px;color: rgb(255, 157, 11);display: inline-block;padding-left: 10px;border-left: 5px solid rgb(255, 157, 11);visibility: visible;"></span></h2><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">大语言模型有望使能大量应用场景，但其实现引发了用户隐私的重大关切。在本文中，我们朝着密态 LLM 迈出了第一步，我们的最终愿景是让整个模型完全在云上运行，同时用户的隐私还能得到充分尊重。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">当前的做法包括将 GPT2 等模型中的特定部分转换至 FHE 域。我们的实现利用了 transformers 库，用户还能评估模型的一部分在加密数据上运行时对准确率的影响。除了保护用户隐私之外，这种方法还允许模型所有者对其模型的主要部分保密。你可在，此处，找到完整代码。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Zama 库 Concrete 和 Concrete-ML (别忘了给我们的 github 代码库点个星星 ⭐️💛) 允许直接构建 ML 模型并将其转换至等价的 FHE 域，从而使之能够对加密数据进行计算和预测。</p><p data-tool="mdnice 编辑器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">希望你喜欢这篇文章。请随时分享你的想法/反馈！</p><hr data-tool="mdnice 编辑器" style="height: 1px;border-right: none;border-bottom: none;border-left: none;border-top-style: solid;border-top-color: rgb(249, 191, 69);margin-top: 20px;margin-bottom: 20px;"><blockquote data-tool="mdnice 编辑器" style="border-top: none;border-right: none;border-bottom: none;color: rgb(91, 91, 91);background: rgba(158, 158, 158, 0.1);padding-top: 1px;padding-bottom: 1px;padding-left: 5px;margin-top: 0px;margin-bottom: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><blockquote style="border-width: initial;border-style: none;border-color: initial;margin-top: 0px;margin-bottom: 0em;padding-top: 0px;padding-left: 0px;"><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">英文原文:&nbsp;<span style="color: rgb(136, 136, 136);letter-spacing: 0px;">https://hf.co/blog/encrypted-llm</span></p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">原文作者: Roman Bredehoft，Jordan Frery</p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">译者: Matrix Yao (姚伟峰)，英特尔深度学习工程师，工作方向为 transformer-family 模型在各模态数据上的应用及大规模模型的训练推理。</p><p style="color: rgb(63, 63, 63);line-height: 1.5;font-size: 14px;margin: 10px;">审校/排版: zhongdongy (阿东)</p></blockquote></blockquote></blockquote></blockquote></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - Hugging Face（gh_504339124f0f）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 05 Oct 2023 03:29:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10112832</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10112832</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
    </channel>
</rss>
