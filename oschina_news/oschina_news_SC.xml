<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-最新资讯]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-最新资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Tue, 23 Jan 2024 03:12:25 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[惊讶！史上最年轻的 Apache Committer 诞生！！！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-b74c559a04078fb6182d6af9815c351d338.png" referrerpolicy="no-referrer"></p><p>在 Apache StreamPark 被数以千计的星星和诸多荣誉的眷顾下，用户越来越多，遍布各个行业。这一切的背后都是因为有一帮热爱开源，愿意投身开源项目建设的贡献者。这其中有一批 「00 后」 的新星正悄然在社区崭露头角，他们正用自己的实际行动书写着自己的开源故事。Apache StreamPark 社区近日迎来了两位 「00」 后 Committer，下面让我们一睹两位 Committer 的风采。</p><span id="OSC_h1_1"></span><h1><strong>新晋 Committer&nbsp;之张超篇</strong></h1><hr><p><img height="648" src="https://oscimg.oschina.net/oscnet/up-63b75bc7d4a38c12ff48b29d44c7cf3bbe6.png" width="1216" referrerpolicy="no-referrer"></p><p><br><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">很开心能够被提名为 Apache StreamPark 的 Committer！非常感谢支持社区的每一位小伙伴！也感谢参与项目时社区的各位技术大拿们给予的指导和帮助！</span></p><p><iframe frameborder="no" height="400px" scrolling="no" src="https://player.bilibili.com/player.html?aid=666424225&amp;bvid=BV1Ra4y1C7EE&amp;cid=1411725695&amp;p=1" width="720px" referrerpolicy="no-referrer"></iframe></p><span id="OSC_h4_2"></span><h4>&nbsp;</h4><span id="OSC_h4_3"></span><h4><strong><strong><strong><strong><strong>主要贡献</strong></strong></strong></strong></strong></h4><ol><li><p>Apache StreamPark 进入 Apache 孵化器期间参与从 MySQL 到 H2 数据库的适配</p></li><li><p>变更 Mybatis-Plus 配置方式，简化配置文件配置项</p></li><li><p>完善用户模块删除功能，部分前后端功能开发与适配，并追踪后续讨论、改进</p></li><li><p>参与整理、完成代码风格和质量指南文档，并参与相关代码规范的推进工作</p></li><li><p>参与官网文档的改进，完成中英文文档编写各 2 篇、翻译 15&nbsp;篇以及其他文档完善</p></li></ol><span id="OSC_h4_4"></span><h4><strong><strong><strong><strong><strong>结缘社区</strong></strong></strong></strong></strong></h4><p>对我来说，Apache 不仅仅是一个技术社区，它更像是一种信仰。在我还是个初学者的时候，提起 Apache Tomcat、Apache Maven 这样的顶级项目，它们在我的心中就好像是编程世界的神话传说。对于热爱编程的我而言，Apache 代表着技术的极致、创新的精神，激励着我不断前行。想到有一天可能成为这个信仰的一份子，和全球的技术大牛们肩并肩，这种感觉，就像是梦想变成现实。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7e1ecdc00c6b91bb4bab730f471cf204ac8.png" referrerpolicy="no-referrer"></p><p>我参与 Apache StreamPark 提交的第一个 PR 是处理客户端发起 GET 请求时携带了 「    」问题，是在 2022 年 8 月 28 日提交的，只用了一行箭头函数的代码就解决！持续贡献到现在也已经一年多了。</p><p>在我印象中，那时 Apache&nbsp;StreamPark 还叫 StreamX，整个团队都在为加入 Apache 孵化器做准备，这时项目急需对 H2 数据库进行适配，而我正好是 Mybatis-Plus 的开发者。PMC 成员华杰哥[1]邀请我一同参与，我立马答应了下来，然后开始从零了解 Apache StreamPark。即刻怀揣着兴奋的心情，连夜肝代码。第一个挑战是 Windows 下的开发环境配置，消耗九牛二虎之力，总算终于跑通了代码！等我抬头一看发现天都亮了——时间真会玩，就这么偷偷溜走了: )。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-270fbabed9d20e29eabf0f3261a698241f2.jpg" referrerpolicy="no-referrer"></p><p>后面我在陆陆续续参与开发中也遇到很多问题，记得印象很深的一次是有一天晚上在进行 Apache StreamPark 前端开发时，遇到一个前端组件的 BUG，处理了半天无果，最后求助社区的 PPMC 成员思柱哥[2]。思柱哥不仅帮我解决了问题，还手把手教我原理，真的很感谢思柱哥！这也是我切身感受到参与开源的乐趣之一，有很多技术大拿愿意带着你成长。</p><p>今年八月，我有幸去北京参加了 Apache Community Over Code 2023[3]。跟 Apache 社区的大佬们近距离接触，真是开眼界了。Apache Way 最让我印象深刻的就是 「社区大于代码」 这个理念。特别是听了 Apache Pulsar PMC 成员小狐狸小姐姐的演讲后，我就豁然开朗：开源项目不仅仅是代码，也需要一个文档来告诉你它怎么运作。因此我主动请缨参与文档的完善工作，帮助社区文档更友好！</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-02f03dc61579e20ce426bef16ae9accec92.png" referrerpolicy="no-referrer"></p><p>我每天都会坚持写一篇博客，哪怕没什么可写的技术题材，我也会坚持随便积累一点。顺带一提我在任何地方都可以打开电脑写代码，譬如咖啡厅、飞机上、地铁上等，甚至我去韩国旅游时也在随便找的一家咖啡厅处理 issue。</p><span id="OSC_h4_5"></span><h4><strong>社区印象</strong></h4><p>社区的大佬们真的各显神通，在 Apache&nbsp;StreamPark 社区里经常有热心技术大拿愿意提供帮助。并且也进一步地学习到不少开发规范和实用技巧，就比如让代码 「一尘不染」 的工具 Spotless、以及 Vue3 的知识点掌握等！在 Apache&nbsp;StreamPark 有一个使用登记收集的 Issue，其中有一条特别引起了我的注意：来自 「袁隆平农业高科技股份有限公司」 的使用登记。这家公司的名字源于杂交水稻之父袁隆平老先生，我为身处的社区能服务于这样一家公司感到超级自豪。参与开源真的是一件很酷的事情！</p><span id="OSC_h4_6"></span><h4><strong>寄，语</strong></h4><p>从 Apache&nbsp;StreamPark 进入 Apache 孵化器，到现在一周年已经过去啦，期间让人感动的瞬间数不胜数！期待更多小伙伴的加入和陪伴，咱们一同见证其从孵化器毕业，进一步为实时领域发展作出贡献！</p><span id="OSC_h1_7"></span><h1><strong>新晋 Committer&nbsp;之蔡灿材篇</strong></h1><hr><p><img height="648" src="https://oscimg.oschina.net/oscnet/up-8120852d62a802a07553d710655b8046808.png" width="1216" referrerpolicy="no-referrer"></p><p>非常荣幸被提名为 Apache&nbsp;StreamPark 的 Committer！感谢社区的信任和肯定！感谢历次讨论、设计、协作和 review 过程中社区开发者给予的指导和帮助！</p><span id="OSC_h4_8"></span><h4><strong><strong><strong><strong><strong>主要贡献</strong></strong></strong></strong></strong></h4><ol><li><p>参与 Kubernetes V2 Operator 重构 streampark-flink-kubernetes 模块</p></li><li><p>测试、修复 streamp-flink-kubernetes 模块并补充单元测试</p></li><li><p>积极参与方案讨论、规范制定和代码 Review</p></li><li><p>修复并完善官网文档</p></li></ol><span id="OSC_h4_9"></span><h4><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>结缘社区</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></h4><p>Apache&nbsp;StreamPark 算是我第一个正式参与的开源项目,与社区结缘主要是因为 StremaPark PPMC 成员余林颖哥[4](Al-assad)。那时候我想加入开源社区，但又觉得自己实力太菜，所以一直没付诸行动。后面 Al-assad 就问我要不要参与 Apache&nbsp;StreamPark 社区，说有很多工作可以做，那时候我还有点怕自己会拖后腿，但大佬说有他兜底。我也就抱着试一试态度去参加了。</p><p>刚进社区，我就接下了三个 feature（真是初生牛犊不怕虎)，真实情况是因为那三个 feature 是很类似的需求，我就想一个是做，两个也是做，那就一起接过来算了。一开始环境搭建在社区的各位大佬的帮助下，也是花了一星期时间成功搭建好了，后面就开始漫长的 debug 之旅。</p><p>在 debug 过程中，我也发现 Apache&nbsp;StreamPark 的一些问题，提了一些 PR 进行修复。令我比较惊讶的是，我的每个 PR 社区都回复的很快，并提了很多改进的意见、让我学到了很多，也让我更有动力。印象比较深的是：在完成第一个 feature 时，我写的代码其实很丑陋，几个 PMC 成员们非凡不嫌弃还提了很多修改意见、鼓励我慢慢改进，最后还是林颖哥出手才让我第一个 feature 成功完成，那时候感觉自己帮了倒忙，感觉很不好意思。但 Al-assad 鼓励我说是新人是这样的啦，多进行尝试就好了。在对第一个 feature 进行认真 review 代码后，我后面两个 feature 也顺利完成了（虽然也是磕磕绊绊）。还记得最后一个 feature 合并的时候，我的心情都激动得快哭了，原来我也能做到自己曾经大学可望而可不及的事情（开源之夏）。记得之前有人看某个开源项目的时候，跟我说，你看这个人和你和你差不多大已经完成了三个 feature 了，我那时候在想什么时候我也能这么牛叉就好了，没想到过了没多久我也成为了 「这样的人」，哈哈。</p><p>一开始，我打算搞完这三个 feature 就不参与社区了，后面认识了社区很多优秀的开发者让我改变了这个想法，Apache&nbsp;StreamPark 社区开发者们非常的团结友善，核心贡献者们更是牺牲休息时间投入到项目中，知道这一点的时候我十分吃惊。所以在这里你能认识一群真正热爱代码的人，我十分感谢在 Apache&nbsp;StreamPark 社区遇到每一位开发者，他们教会了我很多。在这个社区，我仿佛意识到开源精神的某种含义。</p><span id="OSC_h4_10"></span><h4><strong>寄，语</strong></h4><p>当然是希望 Apache&nbsp;StreamPark 社区越来越好啦，毕竟是我参加开源以来第一个加入的社区，期待新的小伙伴能加入进来，一起建设社区。如果是新手也不用害怕，我也是新手不也成功加入进来了吗。祝 Apache StreamPark 能够早日孵化成功, 祝各位 Apache&nbsp;StreamPark 社区的小伙伴都玩得开心，have fun！</p><span id="OSC_h1_11"></span><h1>**&nbsp;加，入 我，们&nbsp;**</h1><p>Apache&nbsp;StreamPark 是一个流处理应用程序开发管理框架。初衷是让流处理更简单，旨在轻松构建和管理流处理应用程序，提供使用 Apache Flink 和 Apache Spark 编写流处理应用程序的开发框架，未来将支持更多其他引擎。同时，Apache&nbsp;StreamPark 提供了一个流处理应用管理平台，核心能力包括但不限于应用开发、调试、交互查询、部署、运维、实时数仓等，于 2022 年 9 月通过投票正式成为 Apache 开源软件基金会的孵化项目。</p><p>Apache&nbsp;StreamPark 社区一直以来都以用心做好一个项目为原则，高度关注项目质量，努力建设发展社区。我们时刻保持开发者谦逊朴素的本质，认真学习和遵循「The Apache Way」，秉承更加兼容幷包的心态，迎接更多的机遇与挑战。诚挚欢迎更多的贡献者参与到社区建设中来，和我们一道携手共建。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fb1f818f6981c4a0f1366a2ec6f60a7252e.png" referrerpolicy="no-referrer"></p><p>💻 项目地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark" target="_blank">https://github.com/apache/streampark</a><br> 🧐 提交问题和建议：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fissues" target="_blank">https://github.com/apache/streampark/issues</a><br> 🥁 贡献代码：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fpulls" target="_blank">https://github.com/apache/streampark/pulls</a><br> 📮 Proposal：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FINCUBATOR%2FStreamPark%2BProposal" target="_blank">https://cwiki.apache.org/confluence/display/INCUBATOR/StreamPark+Proposal</a><br> 📧 订阅社区开发邮件列表：**dev@streampark.apache.org&nbsp;<br> 💁‍♀️&nbsp;<strong>社区沟通：</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-d2fb1e3ea10be8a185704b27e79d83d2ad4.png" referrerpolicy="no-referrer"></p><p><strong>参考资料</strong><br> [1]&nbsp;<em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwolfboys" target="_blank">https://github.com/wolfboys</a></em><br> [2]&nbsp;<em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwangsizhu0504" target="_blank">https://github.com/wangsizhu0504</a></em><br> [3]&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunityovercode.org" target="_blank">https://communityovercode.org</a><br> [4]&nbsp;<em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAl-assad" target="_blank">https://github.com/Al-assad</a></em></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:30:22 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/streampark/blog/10890500</guid>
            <link>https://my.oschina.net/streampark/blog/10890500</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[SQLite 3.45 发布，添加对 JSONB 格式支持]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>SQLite 3.45 <u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sqlite.org%2Fchanges.html%23version_3_45_0" target="_blank">已发布</a></u>。</p><blockquote><p>SQLite 是一个 C 语言库，实现了一个小型、快速、独立、高可靠性、全功能的 SQL 数据库引擎。SQLite 是世界上使用最多的数据库引擎。SQLite 的源代码属于公共领域，每个人都可以免费使用，用于任何目的。</p></blockquote><p>新版本重写了所有 JSON 函数，改为新的内部解析树格式 JSONB。JSONB 格式可以序列化并存储在数据库中，避免在使用 JSON 值时，还需要进行不必要的重复解析。因此通过支持 JSONB，SQLite 在处理 JSON 数据时能够更高效灵活。</p><p>此外，JSONB 保留所有旧功能，并且所有 JSON 函数也可以接受 JSONB 二进制内容。</p><p>SQLite 承诺其生成的 JSONB 格式良好，但是由于 JSONB 本质上就是一个 BLOB，因此可能存在格式不正确的 JSONB，而 SQLite 在处理格式不正确的 JSONB，就可能出现不同行为，从而发生错误。不过 SQLite 也保证，格式错误的 JSONB 永远不会导致内存错误或是类似问题，也不会出现系统漏洞。</p><p>&nbsp;</p><p>详情查看&nbsp;<em><u><a href="https://www.oschina.net/news/269878/sqlite-lands-jsonb" target="_blank">SQLite 重写了 JSON 函数 —— 推出 JSONB</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:27:22 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276404/sqlite-3-45-released</guid>
            <link>https://www.oschina.net/news/276404/sqlite-3-45-released</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2024-1 月 | WHLUG（武汉 Linux 用户组）活动回顾，这是没有过的全新版本]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><div><div><p style="margin-left:0; margin-right:0">内容来源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Findex%2Fzh" target="_blank">deepin 社区</a></p><hr><p style="margin-left:0; margin-right:0">2024 年 1 月 20 日下午，由 deepin（深度）社区联合 Linux 中国、龙蜥社区（OpenAnolis）、华中科技大学网络空间安全学院开源俱乐部一起举办的 Linux 爱好者线下沙龙活动（WHLUG）在武汉未来科技城成功举办。</p></div></div><p style="margin-left:0; margin-right:0">本次活动聚集了近 40 名来自武汉地区的 Linux 爱好者线下参与，大家共同探讨技术话题，分享技术经验和见解，现场气氛热烈。现在就让我们一起来回顾本次活动的精彩瞬间吧！</p></div><p style="margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="611" src="https://storage.deepin.org/thread/202401220819117663_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059112961557.png" width="825" referrerpolicy="no-referrer"></p><div><div>
  来自 deepin 团队的研发工程师 Black desk 在会议上与参会者分享了一些 Linux 内核提供的容器相关特性的用户态接口使用方法、非 root 用户使用 Linux 容器的限制，和其他有趣的 Linux 容器技术细节，同时也深入介绍了玲珑方案产生的背景、希望去解决的问题，以及玲珑的优势和目前存在的问题。
 </div></div><div>
 &nbsp;
</div><div><img alt="" height="603" src="https://storage.deepin.org/thread/202401220820237378_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113054411.png" width="827" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div>
   开源已经迈入了「新生代」，我们需要资深的开发者，也需要更张扬的年轻人。在本次分享中，来自华中科技大学开放原子开源俱乐部的慕冬亮老师，便从「领路人」的视角，为大家分享了他是如何带领学生走进 Linux 内核的世界，一同挖掘、修复 Linux 内核漏洞，并通过内部审核机制保障内核补丁正确性的故事。
  </div></div></div><div>
 &nbsp;
</div><div>
 &nbsp;
</div><div><img alt="" height="617" src="https://storage.deepin.org/thread/202401220820504191_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113182099.png" width="825" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div>
   那从学生的视角来看，开源参与又会有哪些不一样呢？来自华中科技大学的朱俊星同学结合自身从 0-1 参与开源的经历，为大家介绍了他是如何从「发布项目无人问津、参与项目无从下手」的小白，逐步深入探索、找到组织，并进阶成为开源项目的核心贡献者的。他强调，「开源不仅仅是开放源代码，更是一种分享合作、共同进步的精神」，希望大家能够抛开顾虑，勇敢尝试。
  </div></div></div><div>
 &nbsp;
</div><div><img alt="" height="461" src="https://storage.deepin.org/thread/202401220821331561_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113277879.png" width="826" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div>
   虚拟网卡是云计算重要组成部分，目前正面临着性能和功能的双重挑战。在本次活动中，来自龙蜥社区高性能网络 SIG 的成员衡琪，向大家分享了 Anolis OS 是如何通过优化 Virtio 协议来增强网络性能，从而解决这一挑战。衡琪在分享中提到了一些具体的优化措施，如制定新的 Virtio 标准，包括 Inner Hash、Virtio Checksum 修复、NetDIM、Device Stats 等技术。
  </div></div><div>
  &nbsp;
 </div></div><div><img alt="" height="615" src="https://storage.deepin.org/thread/202401220822105140_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113375653.png" width="827" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div><p style="margin-left:0; margin-right:0">除了主题演讲之外，来自现场爱好者的闪电演讲的分享，也赢得了满堂喝彩。</p><p style="margin-left:0; margin-right:0">钟同学为大家分享了自己在 Treeland 及窗管调优方面的经验与探索，向大家展示了他是如何基于 Treeland 进行窗管深度客制化，以及在三个月内打造的台前调度。这个分享引发了现场观众的兴趣和探讨，大家对于钟同学的经验和成果表示赞赏，并积极参与了与之相关的交流和讨论。</p></div></div></div><div><img alt="" height="463" src="https://storage.deepin.org/thread/202401220822396312_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113493557.png" width="828" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div>
 &nbsp;
</div><div><div><div><p style="margin-left:0; margin-right:0">桑同学结合自己的工作和大家分享了有关路由器和无线网卡的相关知识，并讲述了在工作中遇到的一些有趣的事情。其中，当提到客户经常将需求误认为 BUG 并直接反馈给技术同学时，更是引发了现场从事 B 端业务的研发同学们的共鸣，并引发了一系列讨论和交流。</p><p style="margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="608" src="https://storage.deepin.org/thread/202401220823104278_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113564231.png" width="822" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0">&nbsp;</p></div></div></div><div><div><div><h1><strong>网上相会，何如线下相见</strong></h1><p style="margin-left:0; margin-right:0">WHLUG 的创办希望帮助大家在武汉遇到同为 Linux 爱好者的小伙伴，一起在某个周末，交流技术，学习技术。基本的形式如下：</p></div></div></div><div><img alt="" height="217" src="https://storage.deepin.org/thread/202401220823526444_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059114451088.png" width="807" referrerpolicy="no-referrer"></div><div><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">我们热切的希望能够有更多武汉当地的学校、企业、开源爱好者加入我们，共同参与到 WHLUG 活动的策划和组织中。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>所以，我们会面向核心成员招募组织人，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwj.qq.com%2Fs2%2F13816139%2F8b53%2F" target="_blank">如果您感兴趣，请报名。</a>期待与您一起共建！</strong></p><p style="text-align:center"><img height="271" src="https://oscimg.oschina.net/oscnet/up-07d49751e8486d624c7c08f7166f237fa1c.png" width="276" referrerpolicy="no-referrer"></p><hr><p><strong>内容来源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2F2024-01-whlug%2F" target="_blank">deepin 社区</a></strong></p><p><strong>了解 WHLUG（武汉 Linux 用户组）：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fwelcome-to-whlug%2F" target="_blank">https://www.deepin.org/zh/welcome-to-whlug/</a></strong></p><p><strong>往期 WHLUG 活动回顾：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fcategory%2Fwhlug-news%2F" target="_blank">https://www.deepin.org/zh/category/whlug-news/</a></strong></p></div></div>
                                    ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:12:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276400</guid>
            <link>https://www.oschina.net/news/276400</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Agents-Flex —— 大语言模型应用开发框架]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="margin-left:0px; margin-right:0px; text-align:left">Agents-Flex 是一个优雅的 LLM（大语言模型） 应用开发框架。其使用 Java 开发，对标 LangChain。</p><h2 style="margin-left:0; margin-right:0; text-align:left">基础能力</h2><ul><li>LLM 的访问能力</li><li>Prompt、Prompt Template 定义加载的能力</li><li>Function Calling 定义、调用和执行等能力</li><li>Embedding</li><li>Vector Storage</li><li>丰富的内容加载器</li><li>丰富的文本分割器</li><li>LLM Chain</li><li>Agents Chain</li></ul><h2 style="margin-left:0; margin-right:0; text-align:left">简单对话</h2><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">使用 OpenAi 大语言模型:</p><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">OpenAiConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">OpenAiConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"sk-rts5NF6n*******"</span><span>);</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">OpenAiLlm</strong><span>(</span><span>config</span><span>);</span></span><span><strong style="color:#445588">Prompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SimplePrompt</strong><span>(</span><span style="color:#dd2200">"请写一个关于小兔子战胜大灰狼的故事。"</span><span>);</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>llmInstance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"---&gt;"</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><strong style="color:#445588">Thread</strong><span>.</span><span style="color:#008080">sleep</span><span>(</span><span style="color:#009999">10000</span><span>);</span></span><span><span>}</span></span></pre><div style="text-align:center">&nbsp;</div></div></div><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">使用 「通义千问」 大语言模型:</p><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">QwenLlmConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">QwenLlmConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"sk-28a6be3236****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setModel</span><span>(</span><span style="color:#dd2200">"qwen-turbo"</span><span>);</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">QwenLlm</strong><span>(</span><span>config</span><span>);</span></span><span><strong style="color:#445588">Prompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SimplePrompt</strong><span>(</span><span style="color:#dd2200">"请写一个关于小兔子战胜大灰狼的故事。"</span><span>);</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>llmInstance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"---&gt;"</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><strong style="color:#445588">Thread</strong><span>.</span><span style="color:#008080">sleep</span><span>(</span><span style="color:#009999">10000</span><span>);</span></span><span><span>}</span></span></pre><div style="text-align:center">&nbsp;</div></div></div><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">使用 「讯飞星火」 大语言模型:</p><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">SparkLlmConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlmConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setAppId</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiSecret</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlm</strong><span>(</span><span>config</span><span>);</span></span><span><strong style="color:#445588">Prompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SimplePrompt</strong><span>(</span><span style="color:#dd2200">"请写一个关于小兔子战胜大灰狼的故事。"</span><span>);</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>llmInstance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"---&gt;"</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><strong style="color:#445588">Thread</strong><span>.</span><span style="color:#008080">sleep</span><span>(</span><span style="color:#009999">10000</span><span>);</span></span><span><span>}</span></span></pre><div style="text-align:center">&nbsp;</div></div></div><h2 style="margin-left:0; margin-right:0; text-align:left">历史对话示例</h2><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">SparkLlmConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlmConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setAppId</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiSecret</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span style="color:#888888">// 创建一个大模型</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlm</strong><span>(</span><span>config</span><span>);</span></span><span><span style="color:#888888">//创建一个历史对话的 prompt</span></span><span><strong style="color:#445588">HistoriesPrompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">HistoriesPrompt</strong><span>();</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"您想问什么？"</span><span>);</span></span><span><strong style="color:#445588">Scanner</strong><span>scanner</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">Scanner</strong><span>(</span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">in</span><span>);</span></span><span><span style="color:#888888">//等待用户从控制枱输入问题</span></span><span><strong style="color:#445588">String</strong><span>userInput</span><span>=</span><span>scanner</span><span>.</span><span style="color:#008080">nextLine</span><span>();</span></span><span><strong style="color:#000000">while</strong><span>(</span><span>userInput</span><span>!=</span><strong>null</strong><span>){</span></span><span><span>prompt</span><span>.</span><span style="color:#008080">addMessage</span><span>(</span><strong style="color:#000000">new</strong><strong style="color:#445588">HumanMessage</strong><span>(</span><span>userInput</span><span>));</span></span><span><span style="color:#888888">//向大模型提问</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>instance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"&gt;&gt;&gt;&gt; "</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><span style="color:#888888">//继续等待用户从控制枱输入内容</span></span><span><span>userInput</span><span>=</span><span>scanner</span><span>.</span><span style="color:#008080">nextLine</span><span>();</span></span><span><span>}</span></span><span><span>}</span></span></pre></div></div></div>
                                                                ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:10:30 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/agents-flex</guid>
            <link>https://www.oschina.net/p/agents-flex</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 云原生量子计算平台，量易伏]]>
            </title>
            <description>
                <![CDATA[<p>简体中文 | <a href="https://gitee.com/baidu/qcompute/blob/master/README.md">English</a></p><h1><a id="user-content-量易伏---qcomputesdk" class="anchor" href="https://gitee.com/baidu/qcompute#%E9%87%8F%E6%98%93%E4%BC%8F---qcomputesdk"></a>量易伏 - QComputeSDK</h1><ul><li><a href="https://gitee.com/baidu/qcompute#%E7%89%B9%E8%89%B2">特色</a></li><li><a href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4">安装步骤</a><ul><li><a href="https://gitee.com/baidu/qcompute#%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE">环境设置</a></li><li><a href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85-QComputeSDK">安装 QComputeSDK</a></li><li><a href="https://gitee.com/baidu/qcompute#%E8%BF%90%E8%A1%8C">运行</a></li><li><a href="https://gitee.com/baidu/qcompute#%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0">重大更新</a></li></ul></li><li><a href="https://gitee.com/baidu/qcompute#%E5%85%A5%E9%97%A8%E4%B8%8E%E5%BC%80%E5%8F%91">入门与开发</a><ul><li><a href="https://gitee.com/baidu/qcompute#%E6%A1%88%E4%BE%8B%E5%85%A5%E9%97%A8">案例入门</a></li><li><a href="https://gitee.com/baidu/qcompute#API-%E6%96%87%E6%A1%A3">API 文档</a></li><li><a href="https://gitee.com/baidu/qcompute#%E5%BC%80%E5%8F%91">开发</a></li></ul></li><li><a href="https://gitee.com/baidu/qcompute#%E4%BA%A4%E6%B5%81%E4%B8%8E%E5%8F%8D%E9%A6%88">交流与反馈</a></li><li><a href="https://gitee.com/baidu/qcompute#%E4%BD%BF%E7%94%A8-qcomputesdk-%E7%9A%84%E5%B7%A5%E4%BD%9C">使用 QComputeSDK 的工作</a></li><li><a href="https://gitee.com/baidu/qcompute#faq">FAQ</a></li><li><a href="https://gitee.com/baidu/qcompute#copyright-and-license">Copyright and License</a></li></ul><p><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2F">Quantum Leaf (量易伏)</a> 是<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum.baidu.com%2F">百度量子计算研究所</a>旗下全球首个云原生量子计算平台。用户可以使用量易伏进行量子编程，量子模拟和运行真实量子计算机。量易伏旨在为量子基础设施即服务 (Quantum infrastructure as a Service, QaaS) 提供量子基础开发环境。</p><p><img src="https://release-data.cdn.bcebos.com/github-qleaf%2F%E9%87%8F%E6%98%93%E4%BC%8F%E5%9B%BE%E6%A0%87.png" alt="" referrerpolicy="no-referrer"></p><p><a href="https://gitee.com/baidu/qcompute/blob/master/LICENSE"><img src="https://img.shields.io/badge/license-Apache%202.0-green" alt="" referrerpolicy="no-referrer"></a><img src="https://img.shields.io/badge/build-passing-green" alt="" referrerpolicy="no-referrer"><img src="https://img.shields.io/badge/Python-3.9--3.11-blue" alt="" referrerpolicy="no-referrer"><img src="https://img.shields.io/badge/release-v3.3.0-blue" alt="" referrerpolicy="no-referrer"></p><p><img src="https://img.shields.io/badge/OS-MacOS%20%7C%20Windows%20%7C%20Linux-lightgrey.svg?style=flat-square" alt="" referrerpolicy="no-referrer"></p><p>本安装包是 QComputeSDK 的 Python 语言实现的全量量子开源计算框架。它采用经典量子混合的编程模式并预制多种先进模块，用户不仅可以在量子环境对象 (QEnv) 下快速搭建电路，也可以将它用于各类复杂量子算法的研发。QComputeSDK 内置多类本地高性能模拟器和云端模拟器/真机调用接口，用户可以将电路在本地模拟器快速模拟验证也可以将电路任务进一步提交至云端真实量子硬件（超导、离子阱）以及高性能模拟器执行。</p><h2><a id="user-content-特色" class="anchor" href="https://gitee.com/baidu/qcompute#%E7%89%B9%E8%89%B2"></a>特色</h2><ul><li>轻松上手
<ul><li>近 50 篇教程案例，还在不断地增加</li><li>量子电路本地可视化</li><li>全自动调用相关计算模块，完成预订流程</li></ul></li><li>功能丰富
<ul><li>支持电路嵌套的量子子程序功能</li><li>本地高性能模拟器支持 32 量子比特的模拟运算</li><li>云端高性能异构模拟器支持更大规模量子模拟</li><li>支持多种噪声模型的模拟</li><li>基于英伟达 cuQuantum 的本地 GPU 模拟器</li><li>基于 Gaussian/Fork 态的本地光量子模拟器</li></ul></li><li>真实量子算力
<ul><li>接入百度自研超导量子计算机 QPUQian</li><li>接入中科院精密测量院离子阱量子计算机 IonAPM</li><li>接入中科院物理所超导量子计算机 IoPCAS</li></ul></li></ul><h2><a id="user-content-安装步骤" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4"></a>安装步骤</h2><h3><a id="user-content-环境设置" class="anchor" href="https://gitee.com/baidu/qcompute#%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE"></a>环境设置</h3><p>推荐使用 Anaconda 创建虚拟环境，</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">conda create <span class="nt">-n</span> qcompute_env <span class="nv">python</span><span class="o">=</span>3.10</span><span id="LC2" class="line">conda activate qcompute_env</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><blockquote><p>Anaconda 请从<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.anaconda.com%2Fdownload%23downloads">官网下载</a></p></blockquote><blockquote><p>注意： 无论使用 Anaconda 还是原生 Python ，Python 版本都应 &gt;= 3.9</p></blockquote><h3><a id="user-content-安装-qcomputesdk" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85-qcomputesdk"></a>安装 QComputeSDK</h3><p>通过 <code>pip</code> 完成安装，</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pip <span class="nb">install </span>qcompute</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>用户也可以选择下载全部文件后进行本地安装。我们推荐此种方式安装以及二次 SDK 开发，可以方便的形成本地开发闭环，更方便调试等动作。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone https://github.com/baidu/QCompute.git</span><span id="LC2" class="line"><span class="nb">cd </span>QCompute</span><span id="LC3" class="line">pip <span class="nb">install</span><span class="nt">-e</span><span class="nb">.</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-运行" class="anchor" href="https://gitee.com/baidu/qcompute#%E8%BF%90%E8%A1%8C"></a>运行</h3><p>如果用户选择下载全部文件，现在可以试着运行一段程序来验证是否安装成功。这里我们运行 QComputeSDK 提供的测试脚本，</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python <span class="nt">-m</span> Test.PostInstall.PostInstall_test</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>该脚本中包括执行本地与云端任务测试，云端测试前需要在命令行输入用户 Token ，Token 可登陆<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Ftoken">量易伏官网</a>查看。如不需要做云端测试可运行 <code>Ctrl+c</code> 结束。</p><blockquote><p>注意：通过 pip 安装请跳过此步。</p></blockquote><h3><a id="user-content-重大更新" class="anchor" href="https://gitee.com/baidu/qcompute#%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0"></a>重大更新</h3><p>从 QComputeSDK 3.0.0 版本开始，开发者可以通过 QComputeSDK 运行百度自研超导量子计算机干始。设备定期对外提供服务，可以从<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fservices">量易伏真机详情页</a>查看真机开放时间。该设备在 QComputeSDK 中的后端名为<code>CloudBaiduQPUQian</code>.</p><h2><a id="user-content-入门与开发" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%85%A5%E9%97%A8%E4%B8%8E%E5%BC%80%E5%8F%91"></a>入门与开发</h2><h3><a id="user-content-案例入门" class="anchor" href="https://gitee.com/baidu/qcompute#%E6%A1%88%E4%BE%8B%E5%85%A5%E9%97%A8"></a>案例入门</h3><p>QComputeSDK 是一个实现后台接入真实量子硬件的量子计算开发框架。建立起了量子计算与量子硬件的桥梁，为量子算法和应用的研发落地提供强有力的支撑，也提供了丰富的案例供开发者学习。</p><p>在这里，我们提供了初级、中级、高级案例供大家学习。初级案例中展示了使用 QComputeSDK 可以快速上手的简单示例，包括量子态制备、经典量子混合编程、以及将电路任务提交到量子计算机上执行等。中级案例中是 QComputeSDK 的进阶用法，包括模块的使用、内置转换器的使用等。高级案例中则是进阶量子算法在 QComputeSDK 上的实现示例，我们为这些算法都配套了详细的教程文档。建议用户下载 QComputeSDK 全部文件安装，本地运行进行实践。</p><ul><li><p><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1">初级案例</a></p><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/GHZ_Local.py">GHZ 态制备（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/GHZ_Cloud.py">GHZ 态制备（云端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/HCNOT_Local.py">贝尔态制备（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/HCNOT_Cloud.py">贝尔态制备（云端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Hybrid_Local.py">经典量子混合语言示例（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Hybrid_Cloud.py">经典量子混合语言示例（云端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Interact_Local.py">经典量子信息交互示例（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Interact_Cloud.py">经典量子信息交互示例（云端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/QPUCase_BaiduQPUQian.py">百度量子自研超导真机运行示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/QPUCase_IonAPM.py">中科院精密测量院离子阱运行示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/QPUCase_IoPCAS.py">中科院物理所超导真机运行示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/PhotonicFookCase_local.py">基于 Fock 态的光量子线路模拟</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/PhotonicGaussianCase_local.py">基于 Gaussian 态的光量子线路模拟</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/SimulatorCase_CuQuantum_Local.py">基于 cuQuantum 的 GPU 模拟器</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Ubqc.py">盲量子计算示例</a></li></ol><ul><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise">量子噪声模拟</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/AddNoise.py">对电路添加噪声示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/CompressNoiseTest.py">量子噪声压缩模块示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/OneQubitNoiseTest.py">一量子位电路含噪模拟</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/ParallelNoiseSimulationTest.py">多进程并行的噪声模拟</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/TwoQubitNoiseTest.py">两量子位电路含噪模拟</a></li></ol></li></ul></li><li><p><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2">中级案例</a></p><ul><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/0_OutputFormatControl">输出信息设置</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/0_OutputFormatControl/Turorials/OutputFormatControl_ZH.md">教程</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/tutorials/machine_learning/QClassifier_CN.ipynb">结果打印信息设置示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/tutorials/machine_learning/VSQL_CN.ipynb">输出文件自动清理示例</a></li></ol></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules">通用模块</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/Tutorials/OpenModules_ZH.md">教程</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/0_OpenModules.py">模块使用示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/1_InverseCircuitModule.py">量子电路逆操作模块</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/2_ReverseCircuitModule.py">量子电路反操作模块</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/3_UnrollProcedureModule.py">子程序展开模块</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/4_UnrollCircuitModule.py">量子门分解模块</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/5_CompressGateModule.py">量子门压缩模块</a></li></ol></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors">转换器</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/Tutorials/OpenConvertors_ZH.md">教程</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/0_Circuit.py">电路序列化示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/1_DrawConsole.py">终端绘图示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/2_InternalStruct.py">电路序列和反序列示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/3_JSON.py">电路转 JSON 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/4_QASM.py">电路转 QASM 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/5_Qobj.py">电路转 QOBJ 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/6_IonQ.py">电路转 IonQ 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/7_XanaduSF.py">电路转 Xanadu 示例</a></li></ol></li></ul></li><li><p><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3">高级案例</a></p><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/0_SuperdenseCoding/Tutorial-Superdense/Superdense_CN.md">量子超密编码</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/1_Deutsch-Jozsa/Tutorial-DJ/Deutsch-Jozsa_CN.md">Deutsch-Jozsa 算法</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/2_PhaseEstimation/Tutorial-phase/Phase_CN.md">量子相位估计 (QPE)</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/3_Grover/Tutorial-Grover/Grover_CN.md">格罗弗搜索算法</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/4_ShorAlgorithm/tutorial/Shor_CN.md">Shor 算法</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/5_VQE/Tutorial-VQE/VQE_CN.md">变分量子基态求解器 (VQE)</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/6_VQSD/Tutorial-VQSD/VQSD_CN.md">变分量子态对角化 (VQSD)</a></li></ol></li></ul><p>在最近的更新中，QComputeSDK 加入了本地光量子计算模拟器 (LocalBaiduSimPhotonic) 。与传统的量子电路模型不同，光量子计算具有其独特的运行方式。QComputeSDK 在架构上支撑起光学体系，也成为了首个集成通用量子计算与光量子计算双体系的量子开发套件。感兴趣的读者请参见<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fpqs%2Ftutorial-introduction">光量子计算模拟器教程</a>。</p><h3><a id="user-content-api-文档" class="anchor" href="https://gitee.com/baidu/qcompute#api-%E6%96%87%E6%A1%A3"></a>API 文档</h3><p>了解更多 QComputeSDK 使用方法，请参考 <a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fdocs%2Fqcompute%2Flatest%2F">API 文档</a>，包含了供用户使用的所有函数和类的详细说明与用法。</p><h3><a id="user-content-开发" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%BC%80%E5%8F%91"></a>开发</h3><p>QComputeSDK 中包括量子计算架构、量子模拟器、量子案例以及扩展功能等。对于需要涉及架构或模拟器源码的开发者，建议下载全部文件并本地安装调试。对于使用 QComputeSDK 研发算法应用的开发者或科研工作者，建议以 <a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/GHZ_Cloud.py">GHZ_Cloud.py</a> 作为代码框架，修改和使用这个文件可以有效帮助熟悉本量子开发套件的语法。建议开发者熟悉 QComputeSDK 的电路模型构造，注意量子位输出顺序为高位。</p><h2><a id="user-content-交流与反馈" class="anchor" href="https://gitee.com/baidu/qcompute#%E4%BA%A4%E6%B5%81%E4%B8%8E%E5%8F%8D%E9%A6%88"></a>交流与反馈</h2><ul><li>我们非常欢迎您提交问题、报告与建议，您可以通过以下渠道反馈
<ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbaidu%2FQCompute%2Fissues">GitHub Issues</a> / <a href="https://gitee.com/baidu/qcompute/issues">Gitee Issues</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Ffeedback">量易伏官网-意见反馈</a></li><li>量易伏官方邮箱 <a href="mailto:quantum@baidu.com">quantum@baidu.com</a></li></ul></li><li>技术交流 QQ 群：1147781135，欢迎扫码进群</li></ul><p><img src="https://release-data.cdn.bcebos.com/github-qleaf%2Fqrcode.png" alt="" referrerpolicy="no-referrer"></p><h2><a id="user-content-使用-qcomputesdk-的工作" class="anchor" href="https://gitee.com/baidu/qcompute#%E4%BD%BF%E7%94%A8-qcomputesdk-%E7%9A%84%E5%B7%A5%E4%BD%9C"></a>使用 QComputeSDK 的工作</h2><p>我们非常欢迎开发者使用 QComputeSDK 进行量子应用研发，如果您的工作有使用 QComputeSDK，也非常欢迎联系我们。以下为基于 QComputeSDK 开发的量子应用：</p><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqep%2Ftutorial-overview">量噪 (QEP, Quantum Error Processing)</a>，百度量子计算研究所研发的量子噪声处理工具集，主要功能包括量子性能评估、量子噪声刻画、量子噪声缓释和量子纠错。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fbqc%2Ftutorial-bqc">盲量子计算 (UBQC, Universal Blind Quantum Computation)</a>，百度量子计算研究所研发的基于 UBQC 协议的盲计算代理服务。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqapp%2Ftutorial-overview">QAPP</a> 是基于 QComputeSDK 开发的量子计算解决方案工具集，提供包括量子化学、组合优化、机器学习在内的诸多领域问题的量子计算求解服务。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqsvt%2Ftutorial-overview">量子奇异值变换 (QSVT, Quantum Singular Value Transformation)</a>，百度量子计算研究所研发的量子奇异值变换工具集，主要功能包括量子奇异值变换，对称量子信号处理，以及哈密顿量模拟。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqfinance%2Ftutorial-option-pricing">量子金融 QFinance</a> ，百度量子计算研究所研发的量子金融库，提供用于期权定价的量子蒙特卡罗方法。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fpqs%2Ftutorial-introduction">光量子计算模拟器 (PQS, Photonic Quantum Simulator)</a>，百度研究院量子计算研究所研发的光量子计算模拟器，支持基于 Gaussian 态和 Fock 态的光量子线路模拟。</li></ul><h2><a id="user-content-faq" class="anchor" href="https://gitee.com/baidu/qcompute#faq"></a>FAQ</h2><ol><li><p>问：<strong>使用 QComputeSDK 可以做什么？它有哪些应用场景？</strong></p><p>答：QComputeSDK 是一个基于 Python 的量子计算开发框架，可以用于构建、运行和优化量子算法。我们在 QComputeSDK 建设了全面且完善的基础设施用于支持各类量子算法的实现，因此在量子应用的研发上它具有广泛的应用场景。具体工作可以参考但不限于 QComputeSDK 中的<a href="https://gitee.com/baidu/qcompute/blob/master/Extensions">扩展功能</a>。</p></li><li><p>问：<strong>想用 QComputeSDK 做量子编程，但对量子计算不是很了解，该如何入门？</strong></p><p>答：Nielsen 和 Chuang 所著的《量子计算与量子信息》是量子计算领域公认的经典入门教材。建议读者首先学习这本书的第一、二、四章，介绍了量子计算中的基本概念、数学和物理基础、以及量子电路模型。读者也可以在<a href="https://gitee.com/link?target=https%3A%2F%2Fqulearn.baidu.com%2F">量易简</a>上学习，这是一个在线量子学习知识库，不仅包含量子计算教程，还有丰富的视频课程。读者还可以下载<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqmobile">量易伏 APP</a>，APP 上的量子小调包含丰富有趣的量子样例，帮助读者随时随地的学习。</p></li><li><p>问：<strong>QComputeSDK 是否免费？</strong></p><p>答：QComputeSDK 是免费的。QComputeSDK 是开源 SDK 并携带多类本地模拟器，用户执行本地模拟任务是免费的。当用户通过 QComputeSDK 将任务提交给云端模拟器或真机运行时，会扣除一定点数。详细的扣点规则可以参考<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2FquickGuide%2Faccount">用户指南</a>。用户在创建账户时我们会赠送点数，点数余额可以在<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fprofile">个人中心</a>查看。</p></li><li><p>问：<strong>点数不足怎么办？</strong></p><p>答：点数目前仅用于资源控制。点数不足时可以从量易伏官网的<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Ffeedback">意见反馈</a>或<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqmobile">量易伏 APP</a>的用户反馈提交点数申请。我们会在三个工作日内处理您的请求。</p></li></ol><h2><a id="user-content-copyright-and-license" class="anchor" href="https://gitee.com/baidu/qcompute#copyright-and-license"></a>Copyright and License</h2><p>QComputeSDK 使用 <a href="https://gitee.com/baidu/qcompute/blob/master/LICENSE">Apache-2.0 license</a> 许可证。</p><h2><a id="user-content-作者" class="anchor" href="https://gitee.com/baidu/qcompute#%E4%BD%9C%E8%80%85"></a>作者</h2><ul><li>刘树森</li><li>贺旸</li><li>江云帆</li><li>张文学</li><li>孙文赟</li><li>付永凡</li><li>陈建萧</li><li>沈豪杰</li><li>吕申进</li><li>王友琪</li></ul>]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:04:30 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/baidu/qcompute</guid>
            <link>https://gitee.com/baidu/qcompute</link>
        </item>
        <item>
            <title>
                <![CDATA[德国程序员因报告漏洞被判罚 2.4 万元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">德国于利希地方法院近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.heise.de%2Fnews%2FWarum-ein-Sicherheitsforscher-im-Fall-Modern-Solution-verurteilt-wurde-9601392.html" target="_blank">宣布</a>了一项最新判决结果，认定一名程序员因未经授权访问第三方计算机系统和刺探数据，违反《德国刑法典》（StGB）中所谓的黑客条款 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gesetze-im-internet.de%2Fstgb%2F__202a.html" target="_blank">202a</a> 而处以 3000 欧元的罚款（约 2.35 万元），同时承担所有的诉讼费用。</span></p><p><img height="203" src="https://oscimg.oschina.net/oscnet/up-1685fc08f7024fc47d741af2e54fc872cf5.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#000000">2021 年 6 月，这位名为 Hendrik H. 的研究人员在为 IT 服务公司 Modern Solution GmbH 的一位客户排除软件故障时发现，Modern Solution 的代码通过 MySQL 连接至一台 MariaDB 数据库服务器。而访问远程服务器的密码则以纯文本形式存储在程序文件 MSConnect.exe 中，任何人使用简单的文本编辑器就能打开该文件查看内容，并找到未加密的硬编码密码。</span></p><p><span style="color:#000000">也正是因为这个唾手可得的密码，导致任何人都可以登录远程服务器访问 Modern Solution 的客户的数据，同时还可以访问存储在该数据库服务器上的供应商所有客户的数据。总的来说，这个数据库漏洞暴露了近 70 万条客户记录，包括姓名、电子邮件地址、电话号码、银行信息、密码以及对话和通话记录等。</span></p><p><span style="color:#000000">在发现这一漏洞后，该程序员在一名技术博客作者 Mark Steier 的帮助下联系了相关公司，后者随后修复了安全漏洞，并报警追究这名程序员的责任。2021 年 9 月，德国警方扣押了 Hendrik H. 的电脑，因为 Modern Solution 指控他是通过内部信息获得的密码，并声称他是竞争对手。</span></p><p><span style="color:#000000">2023 年 6 月，德国于利希地方法院以 Modern Solution 软件保护不力为由，支持了 Hendrik H 的诉讼请求。但亚琛地区法院指令于利希地方法院再次审理此案，原先的裁定被推翻。2024 年 1 月 17 日，于利希地方法院最终宣判对 Hendrik H. 处以罚款，并责令其支付诉讼费用。</span></p><p><span style="color:#000000">这一判决不可避免的在广大网络安全专家和研究人员当中引起了争议。Steier 发帖<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwortfilter.de%2Fentdecker-des-datenlecks-modern-solution-heute-vor-gericht%2F" target="_blank">表示</a>，这一判决从根本上就是错误的。「几乎以纯文本形式保存的密码并不构成第 202 条所要求的'special security'。法官无法对此作出判断是可以理解的，但这样一来就必须就这个问题听取专家的意见。遗憾的是，这并没有发生。」</span></p><p><span style="color:#000000">不过，该判决尚未具有法律约束力。</span><span style="background-color:#ffffff; color:#323232">被告的辩护律师辩称，即使法院判定他有罪，他的当事人的行为也是为了公众利益。</span><span style="color:#000000">被指控的程序员已于 1 月 19 日宣布，正在对判决提出上诉。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 09:41:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276369/germany-programmer-fined-security</guid>
            <link>https://www.oschina.net/news/276369/germany-programmer-fined-security</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Visual Studio 2022 17.9 Preview 3 发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Visual Studio 2022 17.9 Preview 3 现已发布。此版本增强了 Code Search（也称为 All-In-One Search）体验，现在用户可以在整个解决方案中搜索任何单词或字符串，对代码库中的文件和符号结果进行补充。并且可以搜索代码库中的局部变量名称、注释中的单词、参数名称或任何其他字符串。</p><p>新增的文本支持将有助于完善使用 Code Search 的体验。更专业的文本搜索体验，可在 <span style="background-color:#ffffff; color:#333333">Find in Files (Ctrl+Shift+F)<span>&nbsp;</span></span>&nbsp;和 <span style="background-color:#ffffff; color:#333333">Quick Find (Ctrl+F)<span>&nbsp;</span></span>中可获取更多选项和功能。</p><p><img alt="" height="324" src="https://oscimg.oschina.net/oscnet/up-820f75d056a71a0f336d9f2b66556367c6c.png" width="500" referrerpolicy="no-referrer"></p><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>默认的 &nbsp;Code Search 体验（Ctrl+T 或 Ctrl+,）将在适用时包含精确的文本匹配，包括以前未包括的注释和局部变量等项目。与 files、types 和 members 相比，文本结果的优先级较低，因此不会影响当前的工作流程。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>对于 </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="background-color:#ffffff; color:#333333">text-only&nbsp;</span><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>视图，可以通过执行以下操作之一筛选出纯文本结果：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li>单击 search bow 下方的 「text (x:)」 按钮，</li><li>在搜索查询前加上「x:」前缀，</li><li>使用键盘快捷键「Shift+Alt+F」，或者</li><li>转到菜单选项「Edit &gt; Go To &gt; Go To Text」。</li></ul><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>在 text-only&nbsp;体验中，还可以通过搜索栏最右侧的按钮切换「Match case」、「Match whole word」和「Use regular expressions」。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><img alt="" height="324" src="https://oscimg.oschina.net/oscnet/up-959a0534f74574e5f1b4d52a4608423dda3.png" width="500" referrerpolicy="no-referrer"></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:left"><span style="background-color:#ffffff; color:#333333">详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fvisualstudio%2F17-9-preview-3-brings-exciting-changes-to-code-search%2F" target="_blank">查看</a></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fvisualstudio%2F17-9-preview-3-brings-exciting-changes-to-code-search%2F" target="_blank">更新公告</a><span style="background-color:#ffffff; color:#333333">。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 06:23:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276349/visual-studio-2022-17-9-preview3-released</guid>
            <link>https://www.oschina.net/news/276349/visual-studio-2022-17-9-preview3-released</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[北京获准向公众开放的生成式 AI 大模型产品占全国近半]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>北京市第十六届人民代表大会第二次会议于日前召开，会上透露，2023,年，北京获准向公众开放的生成式人工智能大模型产品占全国近一半。今年，北京将推动人工智能模型对标国际先进水平，加快在政务、医疗、工业、生活服务等领域应用。</p><p>北京市市长殷勇作政府工作报告时指出，2023 年，北京加快建设国际科技创新中心，加强科技领军人才尤其是青年人才培养引进，实施基础研究领先行动和关键核心技术攻坚战行动，推动在京国家实验室高质量运行，支持新型研发机构开展有组织科研，加快构建以企业为主导的产学研深度融合新范式。</p><p>北京巩固提升高精尖产业发展优势，出台通用人工智能、人形机器人等 30 余项细分产业支持政策，新设 4 支政府高精尖产业基金，一批创新药品、医疗器械获批上市，小米智能手机工厂、理想汽车旗舰工厂提前投产。</p><p>北京精心打造全球数字经济标杆城市，率先建成全球性能领先的区块链基础设施，新增 5G 基站 3 万个，获准向公众开放的生成式人工智能大模型产品占全国近一半，「京通」「京办」「京智」三个智慧城市应用终端快速升级拓展，高级别自动驾驶示范区实现 160 平方公里连片运行，全国首个数据基础制度先行区启动建设，数字经济增加值占地区生产总值比重达 42.9%。</p><p>殷勇说，今年，北京将加快发展新质生产力。实施制造业重点产业链高质量发展行动，提升产业链供应链韧性和安全水平。加强原创新药和高端医疗器械研发，培育生物制造等医药健康产业新增长点。推动新能源汽车产业高质量发展，积极布局电机、电池、电控等关键零部件产业链。推进超高清视频全产业链优化升级。促进新能源、新材料、商业航天、低空经济等战略性新兴产业发展，开辟量子、生命科学等未来产业新赛道。优化专精特新企业梯队培育体系，助力更多企业发展壮大。</p><p>殷勇指出，今年，北京将促进平台经济有序竞争、创新发展，推动先进数字技术向中小企业深度普及，构建开放共享、充满活力的创新生态。提升人工智能底层技术和基础底座自主可控能力，推动人工智能模型对标国际先进水平，加快在政务、医疗、工业、生活服务等领域应用，保持人工智能研发应用领先水平。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 05:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276347</guid>
            <link>https://www.oschina.net/news/276347</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[RustDesk 新增 2FA 双重认证功能，增强远程桌面访问安全性]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>RustDesk <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frustdesk%2Frustdesk%2Freleases%2Ftag%2Fnightly" target="_blank">nightly</a>&nbsp;新增 2FA 双重认证功能，增强远程桌面访问安全性，欢迎大家试用反馈。</p><p><img height="1246" src="https://oscimg.oschina.net/oscnet/up-0de6626da796bb7195b23fc861ee98e2f12.jpg" width="1708" referrerpolicy="no-referrer"></p><p><img height="623" src="https://oscimg.oschina.net/oscnet/up-eef5d3bce35b2b05039bb7678d9ebbea95b.jpg" width="854" referrerpolicy="no-referrer"></p><p><img height="1270" src="https://oscimg.oschina.net/oscnet/up-d4017aeb3ba844b73de27c16d258a40944d.jpg" width="1780" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:20:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276342/rustdesk-2fa</guid>
            <link>https://www.oschina.net/news/276342/rustdesk-2fa</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[到底什么样的 Java 项目用 Solon 好？？？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">就像华为讲的，不要因为爱国而特意买华为手机。Solon 也是，<strong>有需要就用不需要就跳过</strong>（按正常的需求选择）：</p><ul><li>信创需要国产化，应该用 Solon 或者 Solon Cloud（有案例）</li><li>军工项目要国产化，应该用 Solon 或者 Solon Cloud（有案例）</li><li>嵌入式设备，内存有限，算力差，可以用 Solon 或者 Solon Native（有案例）</li><li>客户的希望你内存更少，可以用 Solon （有案例）</li><li>别的框架用腻了，可以用 Solon （有案例）</li><li>有新系统开发想尝新的框架，可以用 Solon （有案例）</li><li>老系统要轻量化改造，可以用 Solon（有案例）</li></ul><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">作为后来者，大家的疑或是会多一些。有问题，可以去交流群里多交流。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276341</guid>
            <link>https://www.oschina.net/news/276341</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[如何写好大模型提示词？来自大赛冠军的经验分享（进阶篇）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><blockquote><p><strong>编者按</strong>：近期，如何通过 Prompt Engineering 最大程度发挥大模型的潜力已成为一个热点话题。人们越来越关注如何通过 Prompt Engineering 技术低成本地用好大模型。</p><p>今天我们推荐的这篇文章，作者认为 Prompt Engineering 需要结合艺术与科学，需要在理解技术背景的同时，发挥创造力和战略思维。</p><p>本系列文章详细介绍了作者在新加坡首届 GPT-4 Prompt Engineering 大赛中使用的策略技巧，包括：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">使用 CO-STAR 框架构建提示语、使用分隔符明确语义单元</a>、利用 system prompts 添加行为约束、仅依靠 GPT-4 对数据集进行分析等。这些技巧都得到了实例验证，证明了 Prompt Engineering 的重要作用。</p><p>本文属于该系列文章的第二部分，详细介绍适合进阶使用的 Prompt Engineering 高级策略。</p></blockquote><p><strong>作者 |&nbsp;Sheila Teo</strong></p><p><strong>编译&nbsp;|&nbsp;岳扬</strong></p><p><strong>🚢🚢🚢欢迎小伙伴们加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技术软件及技术交流群</a>，追踪前沿热点，共探技术难题~</strong></p><p>上个月，我有幸获得新加坡首届 GPT-4 提示工程（Prompt Engineering）大赛相关奖项，该比赛由新加坡政府科技署（GovTech）组织，汇聚了超过 400&nbsp;位优秀的参与者。</p><p><strong>提示工程（Prompt Engineering）是一门融合了艺术和科学的学科——这门学科不仅需要理解技术，还需要一定的创造力和战略思维。</strong> 以下是我在学习过程中学到的提示工程（Prompt Engineering）策略汇编，这些策略可以驱动任何大语言模型（LLM）精准执行需求，甚至超常发挥！</p><blockquote><p>作者注：</p><p>在撰写本文时，我力图摒弃已在网上广泛讨论和记录的传统提示工程（Prompt Engineering）技术。相反，撰写本文的目的是给大家介绍我在实验中学到的新见解，以及对某些技术的不同理解。希望你会喜欢阅读这篇文章！</p></blockquote><p>本系列文章包括以下内容，其中🔵指的是适合初学者的提示语（prompt）技巧（<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">见基础篇</a>），而🔴指的是高级策略（本文的重点）：</p><p>1.[🔵] 使用&nbsp;CO-STAR&nbsp;框架构建提示语</p><p>2.[🔵]&nbsp;使用分隔符（delimiters）将提示语分段</p><p><strong>3.[🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;创建&nbsp;system prompts</strong>（译者注："guardrails"&nbsp;指的是一种保护机制或限制，用于确保大语言模型生成的内容符合特定标准或要求，防止产生不准确、不合适或有害的信息。）</p><p><strong>4.[🔴]&nbsp;仅使用&nbsp;LLM（无需插件或代码）分析数据集——将介绍一个使用&nbsp;GPT-4&nbsp;分析真实&nbsp;Kaggle&nbsp;数据集的实践示例</strong></p><h1><strong>01 [🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;创建&nbsp;system prompts</strong></h1><p>在进入正题之前，需要注意的是本节只适用于具有 System Prompt 功能的 LLM，而不像基础篇和本文的其他章节那样适用于任何 LLM。最著名的 LLM 当然是&nbsp;ChatGPT ，因此在本节中我们将以 ChatGPT 作为示例。</p><h2><strong>1.1 围绕&nbsp;System Prompt&nbsp;的术语</strong></h2><p>首先，让我们来理清术语，特别是关于 ChatGPT 的三种术语的使用：这三种术语在 ChatGPT 几乎可以互换使用：&nbsp;"System Prompts"、"System Messages "和&nbsp;"Custom Instructions"。这让很多人（包括我在内！）感到困惑，以至于&nbsp;OpenAI&nbsp;特意发布了一篇文章来解释这些术语。以下是其摘要：</p><ul><li><strong>"System Prompts"和"System Messages"是通过 Chat Completions API 以编程方式与 ChatGPT 进行交互时使用的术语。</strong></li><li><strong>另一方面，"Custom Instructions"是通过 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchat.openai.com%2F" target="_blank">https://chat.openai.com/</a>&nbsp; 用户界面与 ChatGPT 交互时使用的术语。</strong></li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-121921e25f98c551f3af8e6f70943254843.png" alt="" referrerpolicy="no-referrer"></p><p>Image from Enterprise DNA Blog</p><p>不过总的来说，这三个术语指的是同一件事，所以不要被这些术语混淆了！后续部分，本文将使用&nbsp;"System Prompts"一词。现在，让我们进入正题！</p><h2><strong>1.2 什么是 System Prompts ？</strong></h2><p>System Prompts 是一种额外的提示语（prompt），我们可以在其中提供有关 LLM 行为方式的 instructions。它被认为是额外的提示语（prompt），因为它不属于您给 LLM 的&nbsp;"正常"&nbsp;提示语（即 User Prompts）。</p><p>在聊天中，每当您给 LLM 发送新的提示语（prompt）时，System Prompts 都会像过滤器一样，LLM 会在回答您的新提示语（prompt）之前自动应用这些提示语（prompt）。这意味着 System Prompts 在 LLM 做出回答时都会被考虑进去。</p><h2><strong>1.3 何时使用 System Prompt ？</strong></h2><p>您心中可能会想到的第一个问题是：为什么我应该在 System Prompts 中提供 instruction，而不是在我向与 LLM 的新对话的第一个提示语（prompt）中提供 instruction，然后再与 LLM 进行更多的对话呢？</p><p>答案是，因为 LLM 的对话记忆是有限的。在后一种情况下，随着对话的继续，LLM 很可能会"忘记"您在聊天中提供的第一条提示语（prompt），从而使这些 instruction （指令）过时。</p><p>另一方面，如果在 System Prompts 中提供了 instruction （指令），那么这些 System Prompts &nbsp;会与聊天中提供的每个新提示语一起发送。这可以确保 LLM 在聊天过程中继续接收这些 instruction，无论聊天过程变得多长。</p><p>总结：</p><p>在整个聊天过程中，使用 System Prompts 提供您希望 LLM 在回答时记住的 instruction 。</p><h2><strong>1.4 System Prompt 应包括哪些内容？</strong></h2><p>System Prompt 通常应包括以下类别的 instruction ：</p><ul><li><strong>目标任务的定义（Task definition）</strong> ，这样 LLM 在整个对话过程中都会记住它必须做什么。</li><li><strong>输出格式（Output format）</strong> ，这样 LLM 在整个对话过程中都会记住它应该如何做出回答。</li><li><strong>防范措施（Guardrails）</strong> ，这样 LLM 在整个对话过程中都会记住它不应该如何做出回答。Guardrails 是 LLM governance 中的新兴领域，指的是 LLM 被允许操作的行为边界。</li></ul><p>例如，System Prompt 可能是这样的：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p><p>If the text does not contain sufficient information to answer the question,&nbsp;do not make up information and give the answer as&nbsp;「NA」.</p><p>You are only allowed to answer questions related to&nbsp;[insert scope].&nbsp;Never answer any questions related to demographic information such as age,&nbsp;gender,&nbsp;and religion.</p></blockquote><p>各部分内容涉及的类别如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-cab48977b545bba61bf371683a3bdc105af.png" alt="" referrerpolicy="no-referrer"></p><p>Breaking down a System Prompt&nbsp;—&nbsp;Image by author</p><h2><strong>1.5 但是，"正常"的聊天提示语又是什么呢？</strong></h2><p>现在你可能会想：听起来 System Prompt 中已经提供了很多信息。那我应该在聊天的"正常"提示语（即 User Prompts）中写些什么呢？</p><p>System Prompt 概述了当前的一般任务。在上面的 System Prompt 示例中，任务已被定义为只使用一段特定的文本来回答问题，并且 LLM 被指示以{"Question":&nbsp;"Answer"}的格式进行回答。</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p></blockquote><p>在这种情况下，聊天过程中的每个 User Prompt 都将简化为你希望 LLM 用文本回答的问题。例如，某个用户的提问可能是「这段文本是关于什么的？（What is the text about?）」然后 LLM 会回答说&nbsp;{"这段文本是关于什么的？（What is the text about?）":&nbsp;"这段文本是关于……（The text is about..）"}。</p><p>但是，让我们进一步概括这个任务示例。在这种情况下，我们可以将上述 System Prompt &nbsp;的第一行从：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p></blockquote><p>编辑为：</p><blockquote><p>You will answer questions using the provided text.</p></blockquote><p>现在，每个用户在聊天时的提示语（prompt）将包括进行问题回答的文本和要回答的问题，例如：</p><blockquote><p>&lt;text&gt;</p><p>[insert text]</p><p>&lt;/text&gt;</p><p>&lt;question&gt;</p><p>[insert question]</p><p>&lt;/question&gt;</p></blockquote><p>在这里，还将使用 XML 标签作为分隔符，以便以结构化的方式向 LLM 提供所需的两个信息片段。<strong>XML 标签中使用的名词「text」和「question」与 System Prompt 中使用的名词相对应，这样 LLM 就能理解标签与 System Prompt instructions 之间的关系。</strong></p><p>总之， System Prompt 应给出总体任务 instructions，而每个 User Prompt 应提供任务执行的具体细节。例如，在本例中，这些具体的细节是 text 和 question。</p><h2><strong>1.6 LLM guardrails&nbsp;动态化</strong></h2><p>上面通过 System Prompt 中的几句话添加了 guardrails 。这些 guardrails 会被固定下来，在整个聊天过程中都不会改变。但是如果您希望在对话的不同阶段设置不同的 guardrails ，该怎么办？</p><p>对于使用 ChatGPT Web 界面的用户来说，目前还没有直接的方法来做到这一点。不过，如果您正在通过编程方式与 ChatGPT 进行交互，那你就走运了！随着人们对构建有效的 LLM guardrail 的关注度越来越高，一些开源软件包也应运而生，它们可以让你以编程方式设置更详细、更动态的 guardrail。</p><p>其中值得注意的是英伟达团队开发的&nbsp;NeMo Guardrails[1]，它允许您配置用户和 LLM 之间预期的对话流程，从而在聊天的不同时间点设置不同的 guardrail ，实现随着聊天进展而不断演变的动态 guardrails 。我强烈推荐您去了解一下！</p><h1><strong>02 [🔴]&nbsp;仅使用&nbsp;LLM（无需插件或代码）分析数据集</strong></h1><p>您可能已经听说过 OpenAI 在 ChatGPT 的 GPT-4 中推出的高级数据分析插件，该插件仅高级（付费）账户可以使用。它允许用户将数据集上传到 ChatGPT，并直接在数据集上运行代码，从而进行精确的数据分析。</p><p>但你知道吗，使用 LLM 分析数据集并不一定需要这样的插件？让我们先来了解一下单纯使用 LLMs 分析数据集的优势和局限性。</p><h2><strong>2.1 大语言模型不擅长的数据集分析类型</strong></h2><p>正如您可能已经知道的那样，LLM 在进行精确数学计算方面的能力有限，因此它们不适合完成需要对数据集进行精确定量分析的任务，比如：</p><ul><li><strong>描述性统计（Descriptive Statistics）</strong> ：通过诸如均值或方差的测量来定量总结数值列。</li><li><strong>相关性分析（Correlation Analysis）</strong> ：获取列之间精确的相关系数。</li><li><strong>统计分析（Statistical Analysis）</strong> ：比如假设检验（hypothesis testing），以确定各组数据点之间是否存在统计意义上的显著差异。</li><li><strong>机器学习（Machine Learning）</strong> ：在数据集上执行预测建模，比如使用线性回归（linear regressions）、梯度提升树（gradient boosted trees）或神经网络（neural networks）。</li></ul><p>方便在数据集上执行这些定量任务正是 OpenAI 推出高级数据分析插件的原因，这样编程语言就可以在数据集上运行代码来执行此类任务。</p><p>那么，为什么有人只想使用 LLM 而不使用此类插件来分析数据集呢？</p><h2><strong>2.2 大语言模型擅长的数据集分析类型</strong></h2><p>LLM 非常擅长识别模式和趋势（patterns and trends）。这种能力源自它们在多样化和海量数据上的广泛训练，使它们能够识别那些可能无法立即察觉的复杂模式。</p><p>这使它们非常适合执行基于数据集进行模式识别的任务，例如：</p><ul><li><strong>异常检测（Anomaly detection）</strong> ：基于一列或多列的数值，识别偏离常规的异常数据点。</li><li><strong>聚类（Clustering）</strong> ：将具有相似特征的数据点分组。</li><li><strong>跨列关系（Cross-Column Relationships）</strong> ：通过分析不同列之间的关系，可以揭示数据中的复杂模式和趋势。</li><li><strong>文本分析（Textual Analysis）（针对基于文本的列）</strong> ：基于主题或情感进行分类。</li><li><strong>趋势分析（Trend Analysis）（针对具有时间特征的数据集）</strong> ：识别列中跨时间的模式、季节性变化或趋势。</li></ul><p>对于这类基于模式的任务，单独使用 LLM 可能会比使用代码在更短的时间内获得更好的结果！让我们用一个例子来充分说明这一点。</p><h2><strong>2.3 仅使用&nbsp;LLM&nbsp;分析&nbsp;Kaggle&nbsp;数据集</strong></h2><p>我们将使用一个广受欢迎的真实&nbsp;Kaggle&nbsp;数据集[2]，该数据集专为进行客户人格分析而准备，其中一家公司试图对其客户群体进行细分，以便更好地了解其客户。</p><p>为了便于之后验证 LLM 的分析结果，我们取该数据集的 50&nbsp;行为一个子集，并只保留最相关的列。之后，用于分析的数据集将如下所示，其中每一行代表一位客户，每一列描述客户信息：</p><p><img src="https://oscimg.oschina.net/oscnet/up-7c8caf80c5954ae73d0746053b0109b7737.png" alt="" referrerpolicy="no-referrer"></p><p>First 3 rows of dataset&nbsp;—&nbsp;Image by author</p><p>假设你在公司的营销团队工作。你的任务是利用这些客户信息数据集来指导营销工作。这是一项分两步走的任务：首先，利用数据集划分多个具有实际意义的客户细分群体。接下来，提出如何最好地针对每个客户群体进行创意营销。现在，这是一个实际的商业问题，LLM 的模式发现（pattern-finding，对于步骤 1 ）能力在这个问题上确实可以大显身手。</p><p>让我们按照以下方式为这个任务制定提示语（prompt），将使用&nbsp;4&nbsp;种&nbsp;prompt engineering&nbsp;技术（后续会详细介绍[3]）：</p><p><strong>1. 将复杂的任务分解为简单的步骤</strong></p><p><strong>2. 参考每个步骤的中间输出</strong></p><p><strong>3. 格式化 LLM 的回答</strong></p><p><strong>4. 将&nbsp;instructions&nbsp;与数据集分开</strong></p><blockquote><p>System Prompt:</p><p>I want you to act as a data scientist to analyze datasets.&nbsp;Do not make up information that is not in the dataset.&nbsp;For each analysis I ask for,&nbsp;provide me with the exact and definitive answer and do not provide me with code or instructions to do the analysis on other platforms.</p><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>GPT-4 的回答如下，接下来我们将数据集以 CSV 字符串的形式传递给它。</p><p><img src="https://oscimg.oschina.net/oscnet/up-e81a5f6ea27da5b058438541aa821009931.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p>随后，GPT-4 将按照我们要求的 markdown 格式回复分析结果：</p><p><img src="https://oscimg.oschina.net/oscnet/up-72040eca14932d7eb9289119eb13af4d0a8.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-03f08b1a6bfadee91b96716822f50f0c4a4.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-f81c09e878806d7f71c9773c8291d3611d5.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><h2><strong>2.4 验证&nbsp;LLM&nbsp;的分析结果</strong></h2><p>为了简洁起见，我们将挑选 LLM 生成的 2 个客户群体进行验证，比如 Young Families 和 Discerning Enthusiasts。</p><h3><strong>2.4.1 Young Families</strong></h3><ul><li>LLM 总结的该人群特征：1980 年后出生，已婚或同居，收入中等偏低，有孩子，经常进行小额购买。</li><li>LLM 将数据集中的这些行聚类到了 Young Families 这个群体中：3、4、7、10、16、20</li></ul><p>深入数据集，这些行的完整数据如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-01030fc251ada675444cc2b84f87f861ce5.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Young Families&nbsp;—&nbsp;Image by author</p><p>LLM 识别出的这部分客户资料，确实对应于所识别的客户群体。甚至能够在我们事先未经过预处理的情况下对具有空值的资料进行聚类！</p><h3><strong>2.4.2 Discerning Enthusiasts</strong></h3><ul><li>LLM 总结的该人群特征：年龄跨度广，可能是任何婚姻状况，高收入，子女状况各异，购买支出高。</li><li>LLM 认为该人群对应的数据行：2、5、18、29、34、36</li></ul><p>深入数据集，这些行的完整数据如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-2844275d166caf555082adb378ad4e1e7b6.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Discerning Enthusiasts&nbsp;—&nbsp;Image by author</p><p>再次与 LLM 识别出的人群资料非常吻合！</p><p>这个例子展示了 LLM 在发现模式、解释和提炼多维数据集，并将其提炼为有意义的见解方面的能力，同时确保其分析深深扎根于数据集的事实。</p><h2><strong>2.5 如果我们使用 ChatGPT 的高级数据分析插件会怎样呢？</strong></h2><p>为了保证分析的完整性，我尝试使用相同的提示语（prompt），并请求 ChatGPT 使用代码执行相同的分析，这就激活了它的高级数据分析插件。这个想法是让插件直接在数据集上运行 K-Means 等聚类算法的代码，以获得各个用户群体的特征，然后综合每个群体的数据来提供营销策略。</p><p>然而，尽管数据集只有 50&nbsp;行，进行了多次尝试都导致出现以下错误信息而没有任何输出：</p><p><img src="https://oscimg.oschina.net/oscnet/up-10c7526d7a4b7d296cef8b48699d37762a9.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 1&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-fa3318a26ba4ab2800121fb5ab4295345f5.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 2&nbsp;—&nbsp;Image by author</p><p>现在使用高级数据分析插件，在数据集上执行较简单的任务（如计算描述性统计数据或创建图表）似乎很容易实现，但需要某些计算算法的较高级任务有时可能会由于计算限制或其他原因导致错误或无输出。</p><h2><strong>2.6 那么......何时使用 LLM 分析数据集？</strong></h2><p>答案是取决于分析的数据类型。</p><p>对于需要精确数学计算或复杂的、基于规则处理的任务，传统的编程方法仍然更胜一筹。</p><p>对于基于模式识别（pattern-recognition）的任务，使用传统的编程或算法方式可能比较具有挑战性或更耗时。然而，LLM 擅长此类任务，甚至可以提供额外的内容输出，比如用来支持其分析的附件和 Markdown 格式的完整分析报告。</p><blockquote><p><strong>归根结底，是否使用 LLM 取决于手头任务的性质，要在 LLM 在模式识别方面的优势与传统编程技术提供的精确性和特异性之间取得平衡。</strong></p></blockquote><h2><strong>2.7 现在回到提示工程（prompt engineering）！</strong></h2><p><strong>在本节结束之前，让我们回顾一下用于生成此数据集分析的提示语（prompt），并分解所使用的 prompt engineering 关键技术。</strong></p><blockquote><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p><strong>技巧 1：将复杂任务分解为简单步骤</strong></p><p>LLM 擅长执行简单任务，但在复杂任务上表现一般。因此，<strong>对于像这样的复杂任务，重要的是要把任务分解成简单的步骤说明，让 LLM 遵循。</strong> 这样做的目的是向 LLM 提供你自己执行任务时会采取的步骤。</p><p>在本例中，步骤如下：</p><blockquote><p><strong>Use this step-by-step process and do not use code:</strong></p><p><strong>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</strong></p><p><strong>For each cluster found,</strong></p><p><strong>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</strong></p><p><strong>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</strong></p><p><strong>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</strong></p><p><strong>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</strong></p></blockquote><p>与简单地将整体任务交给 LLM 相比，例如「将客户分组，然后提出针对每个群体的营销策略」。通过逐步说明，LLM 更有可能提供正确的结果。</p><p><strong>技巧 2：引用每个步骤的中间输出</strong></p><p>在向 LLM 提供逐步说明时，可将每个步骤的中间输出命名为大写的变量名，例如 CLUSTERS、CLUSTER_INFORMATION、CLUSTER_NAME、MARKETING_IDEAS 和 RATIONALE。</p><p><strong>使用大写字母是为了将这些变量名与给出的 instructions 内容区分开。稍后可以使用方括号引用这些中间输出，如[VARIABLE_NAME]。</strong></p><p><strong>技巧 3：规范大模型回答的格式</strong></p><p>在这里，要求使用 Markdown 报告格式，以美化 LLM 的回答。在这里，中间输出中的变量名又派上了用场，可以决定报告的结构。</p><blockquote><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;<br><strong>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</strong></p></blockquote><p>事实上，您甚至可以随后要求 ChatGPT 将报告提供为可下载文件，这样您就可以根据其回答来撰写最终的报告文档。</p><p><img src="https://oscimg.oschina.net/oscnet/up-792771629e0046fa10609d608023fce4df5.png" alt="" referrerpolicy="no-referrer"></p><p>Saving GPT-4's response as a file&nbsp;—&nbsp;Image by author</p><p><strong>技巧 4：将任务说明与数据集分开</strong></p><p>您会注意到我们在第一个提示语中并没有将数据集提供给 LLM。相反，提示语只包含了数据集分析的任务说明，并在最后加上了以下内容：</p><blockquote><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>ChatGPT 随后回复说它理解了，我们将在下一个提示语中将数据集作为 CSV 字符串传递给它：</p><p><img src="https://oscimg.oschina.net/oscnet/up-3b139d01a6cf26df1f930d6e67dc5b8be24.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><strong>但为什么要将 instructions 与数据集分开呢？</strong></p><p>简单明了的答案是，LLM 的上下文窗口存在限制，即在一句提示语中可以输入的 tokens 数量存在限制。同时包含 instructions 和数据的长提示语（long prompt）可能会超过这个限制，从而导致截断（truncation）和信息丢失（loss of information）。</p><p>更复杂的答案是，<strong>将 instructions 和数据集分开可以帮助 LLM 保持清晰的理解，降低遗漏信息的可能性。</strong> 你可能遇到过这样的情况，即 LLM "不小心忘记了"&nbsp;你发送的较长提示语给出的某个 instruction ——例如，如果你要求给出 100&nbsp;字的回答，而 LLM 却给了您一个较长的段落。通过先接收 instructions ，再接收 instructions 所针对的数据集，LLM 可以先消化它应该执行的任务，然后再对接下来提供的数据集执行 instructions 。</p><p>不过请注意，只有聊天型&nbsp;LLM&nbsp;才能实现&nbsp;instruction&nbsp;和数据集的分离，因为它们会保留对话记忆，而&nbsp;completion LLM&nbsp;则不会（译者注：completion LLM 指的是一种能够根据给定的提示语来生成完整文本或完成特定任务的语言模型。这种模型通常不具备对话记忆，而是专注于根据提示语生成连贯的文本）。</p><p><strong>Thanks for reading!</strong></p><p><strong>END</strong></p><p><strong>🚢🚢🚢欢迎小伙伴们加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技术软件及技术交流群</a>，追踪前沿热点，共探技术难题~</strong></p><p><strong>参考资料</strong></p><p>[1]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FNVIDIA%2FNeMo-Guardrails" target="_blank">https://github.com/NVIDIA/NeMo-Guardrails</a></p><p>[2]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fimakash3011%2Fcustomer-personality-analysis" target="_blank">https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis</a></p><p>[3]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41%23544b" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41#544b</a></p><p><strong>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</strong></p><p><strong>原文链接：</strong></p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/10920438</guid>
            <link>https://my.oschina.net/IDP/blog/10920438</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[🔥 周热点 | 纯血 HarmonyOS NEXT 亮相；云风宣布开源自研游戏引擎；ReiserFS 作者在狱中回应被 Linux 内核弃用.....]]>
            </title>
            <description>
                <![CDATA[回顾一周热门资讯。2024.01.15-2024.01.21]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</guid>
            <link>https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</link>
        </item>
        <item>
            <title>
                <![CDATA[助力 AI 技术共享，蚂蚁开源又一核心技术 「因果学习系统 OpenASCE」]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">当地时间 2023 年 12 月 10 日，机器学习和人工智能领域的顶级国际会议 NeurIPS (Neural Information Processing Systems) 在美国路易斯安那州新奥尔良市开幕，来自全球产业界和学术领域的人工智能专家齐聚一堂。</p><h1>首个分布式全链路因果学习系统 OpenASCE</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">大会第一天，蚂蚁集团在主题为 「知识增强 AI 在垂直行业的应用探索」 的研讨会上正式开源了业界首个分布式全链路因果学习系统 OpenASCE (Open All-Scale Causal Engine) 。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">项目 GitHub：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpen-All-Scale-Causal-Engine%2FOpenASCE" target="_blank">https://github.com/Open-All-Scale-Causal-Engine/OpenASCE</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-8d931ba2b03a853fcf7d06c1a780d171b96.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">因果推断主要研究如何从数据中推断因果关系，是数据科学领域的重要分支，而传统的机器学习则主要依赖数据中的相关关系。融合因果推断和机器学习可以同时发挥两者的强项，我们称之为因果学习。因果学习作为一种深入理解数据和决策背后关系的技术，在数据驱动的运营和决策中扮演着重要的角色。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 根植于蚂蚁集团多年积累的实践经验和技术突破，相较于业界已有的一些开源框架，支持全链路大规模因果学习，包含因果发现、因果效应估计和归因，覆盖了因果各个领域的相应实现。在因果发现上，OpenASCE 支持分布式贝叶斯网络结构搜索，能够处理百节点百万样本数据；同时支持基于连续优化的因果发现，支持万级节点亿级样本数据。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 实现的大规模分布式因果纠偏树可以在 4 小时内完成 1 亿样本的训练任务，是业界唯一的分布式因果提升树实现。此外，OpenASCE 还沉淀了 20 多个工业级因果学习算法，包括 15 个以上因果技术和深度学习结合的因果表征学习方法，有效降低了因果技术的工业应用门槛，在蚂蚁集团内部多个场景实现了规模化应用。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-da7d981d440607195474f083b7a1cbb7862.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">在信贷风控领域，通过 OpenASCE 的因果学习方法，可以更准确地识别出风险因素和客户行为之间的因果关系，大幅提高了风险控制的精度和效率。在营销优化方面，OpenASCE 能够帮助营销人员有效寻找 「营销敏感人群」，提升业务指标。在推荐场景中，因果推断可以帮助机器学习纠正数据中的偏置，去除伪相关，学习更稳定的因果关系。</p><h1>开源开放，共建社区</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">作为一家科技公司，蚂蚁集团将 OpenASCE 开源，为业界提供一套大规模、高性能的因果学习技术，并通过开源吸引全球开发者共同参与项目的建设和完善，促进全链路因果学习系统领域的发展和创新。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">技术开源是蚂蚁集团的重要技术战略，我们希望通过开源建立起开放、包容的技术生态，让更多人共享技术红利。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">截至目前，蚂蚁集团已在数据库、云原生、中间件等基础软件领域开源了 1700 多个仓库、积累了 100 多个社区头部开源项目。《COPU2022 中国开源发展蓝皮书》显示，蚂蚁开源影响力排名国内前三，其中重点开源的 9 大技术均为支撑支付宝的核心技术。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:47:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276338</guid>
            <link>https://www.oschina.net/news/276338</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[因政治滥用，OpenAI 将一家 AI 初创公司拉黑]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>OpenAI 于日前<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yahoo.com%2Fnews%2Fopenai-suspends-developer-over-chatgpt-bot-that-impersonated-a-presidential-candidate-214854456.html" target="_blank">封禁</a>了一家开发 Chatbot 的 AI 初创公司 Delphi。因为该公司基于 GPT-4，出于政治目的设计了一个模仿美国民主党总统候选人 Dean Phillips 的机器人 Dean.Bot；以与潜在支持者互动并传播候选人的信息，帮助其竞选。</p><p><img height="220" src="https://oscimg.oschina.net/oscnet/up-5e13ec80f847c98829315c5bf8ebf46f766.png" width="500" referrerpolicy="no-referrer"></p><p>事实上，美国、英国、印度、巴基斯坦和南非等国都计划在 2024 年进行大选。为了防止其技术被滥用，OpenAI 在本月早些时候曾发表了一篇<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Fblog%2Fhow-openai-is-approaching-2024-worldwide-elections" target="_blank">长文</a>，介绍其计划采取的一些措施；其中明确表示不允许人们开发用于政治活动和游说的应用，并且还特别提到了禁止"冒充候选人的聊天机器人 "。</p><p>虽然 Dean.Bot 网站有提供免责声明，告知访问者所有的交互都将由聊天机器人生成，而不是 Phillips 本人。但这种使用方式还是直接违反了 OpenAI 的政策。公司发言人在给《华盛顿邮报》的一份声明中也证实了被 OpenAI 封禁的消息。Delphi 的 OpenAI 帐户据称于上周五被封禁，随后该公司就停止了对 Dean.Bot 的访问权限。</p><p>现在访问该网站的用户仍然会看到免责声明，但会显示聊天机器人本身已因"技术故障"而宕机："Apologies, DeanBot is away campaigning right now!"&nbsp;</p><p>这也是 OpenAI 首次因开发者违反其 AI 滥用准则而采取审查措施。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:34:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</guid>
            <link>https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[ElasticSearch 集群灾难：别放弃，也许能再抢救一下 | 京东云技术团队]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1>1 前言</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">Elasticsearch 作为一个分布式搜索引擎，自身是高可用的；但也架不住一些特殊情况的发生，如：</p><ul><li><p style="color:#222222; margin-left:0; margin-right:0">集群超过半数的 master 节点丢失，ES 的节点无法形成一个集群，进而导致集群不可用；</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">索引 shard 的文件损坏，分片无法被正常恢复，进而导致索引无法正常提供服务</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">本地盘节点，多数据节点故障，旧节点无法再次加入集群，数据丢失</p></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">针对上述的情况，今天来聊一聊相关的解决方案。</p><span id="OSC_h1_2"></span><h1>2 基础知识</h1><span id="OSC_h2_3"></span><h2>2.1 集群经典架构</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">在聊解决方案之前，首先来看一看 ES 集群层面的基本知识，es 的集群组成通常如图 1-1 所示<img alt="es 集群角色.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-06-15-270pb9YSxO96kLhga.png" referrerpolicy="no-referrer"></p><div><p>图 1-1 es 常用集群架构</p> 如图 1-1 所示，为生产环境 es 集群的经典架构，主要由专有主节点、专有协调节点和数据节点组成： 
</div><ul><li><p style="color:#222222; margin-left:0; margin-right:0"><strong>专有主节点 (<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23master-node" rel="nofollow" target="_blank">Master-eligible node</a>)</strong>: 具有 master 角色的节点，这使其有资格被选为主节点，只存储集群元信息包含 cluster、index、shard 级别的元数据；该种角色节点被选举为 master 之后，将作为整个 ES 集群的大脑，负责维护集群层面的元信息，创建删除索引等工作。该种节点的个数必须为奇数，通常我们固定为 3 个，如果该类节点丢失半数，es 集群将无法维持 es 节点形成一个集群。</p></li><li><p style="color:#222222; margin-left:0; margin-right:0"><strong>专有协调节点 (网关节点)</strong>: 该种节点不具有任何角色，仅仅用来处理 es 请求；比如（1）将写请求的数据归类转发到数据所属的节点（2）查询请求的二次聚合计算。通常我们也会给该类节点保留<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23node-ingest-node" rel="nofollow" target="_blank">ingest 角色</a><span>&nbsp;</span>，ingest 的主要作用是对数据进行预处理；比如：字段重命名、给数据文档打上指纹和清洗数据等功能主要通过<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fpipeline.html" rel="nofollow" target="_blank">pipeline 能力</a>进行处理</p></li><li><p style="color:#222222; margin-left:0; margin-right:0"><strong>数据节点 (<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23data-node" rel="nofollow" target="_blank">Data node</a>)</strong>: 存储数据和集群元信息，执行与数据相关的操作，如 CRUD、搜索和聚合。在数据节点上打上不同的属性，可以使其成为 hot、warm、cold 数据节点，在 es7.9 版本之后配置略有不同，但是原理基本不变。</p></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果没有显示设置节点角色，es 的每个节点都会含有以上三种角色。除此之后还有<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23remote-node" rel="nofollow" target="_blank">Remote-eligible node</a><span>&nbsp;</span>、<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23ml-node" rel="nofollow" target="_blank">ml-node</a>和<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23transform-node" rel="nofollow" target="_blank">Transform nodes</a>等角色需要显示的配置，节点才会有该角色。</p><span id="OSC_h2_4"></span><h2>2.2 集群元信息</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">集群完全启动主要包含选举主节点、元信息、主分片、数据恢复等重要阶段；如图 2-1 所示[1]。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="5211704611687_.pic.jpg" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-07-15-22jRx48QjPfQUT8WnF.jpg" referrerpolicy="no-referrer"></p><div><p>图 2-1 es 集群启动流程</p></div><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">主节点选举的过程，不是本文的重点，而是集群元信息的选举。被选举出的 master 和集群元信息新旧程度没有关系；master 节点被选举出来之后，它所要完成的第一个任务，即是选举集群元信息。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（1）Master 选举成功之后，判断其持有的集群状态中是否存在 STATE_NOT_RECOVERED_BLOCK，如果不存在，则说明元数据已<br> 经恢复，跳过 gateway 恢复过程，否则等待。<code>org.elasticsearch.gateway.GatewayService#clusterChanged</code></p><pre><code>//跳过元数据恢复
if (state.blocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK) == false) {
            // already recovered
            return;
 }
 //此处省略部分代码。
 //进入 gateway 恢复过程   
performStateRecovery(enforceRecoverAfterTime, reason); 
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（2）Master 从各个节点主动获取元数据信息。<code>org.elasticsearch.gateway.Gateway#performStateRecovery</code></p><pre><code># 获取元信息核心代码
 final String[] nodesIds = clusterService.state().nodes().getMasterNodes().keys().toArray(String.class);
        logger.trace("performing state recovery from {}", Arrays.toString(nodesIds));
        final TransportNodesListGatewayMetaState.NodesGatewayMetaState nodesState = listGatewayMetaState.list(nodesIds, null).actionGet();
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（3）从获取的元数据信息中选择版本号最大的作为最新元数据;元信息包括集群级、索引级。</p><pre><code>## org.elasticsearch.gateway.Gateway#performStateRecovery

    public void performStateRecovery(final GatewayStateRecoveredListener listener) throws GatewayException {
# 省略若干行代码

## 进入 allocation 阶段；
## final Gateway.GatewayStateRecoveredListener recoveryListener = new GatewayRecoveryListener();
## listener 为 GatewayStateRecoveredListener   
 listener.onSuccess(builder.build());    
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（4）两者确定之后，调用 allocation 模块的 reroute，对未分配，的分片执行分配，主分片分配过程中会异步获取各个 shard 级别元数据。</p><pre><code>#主要实现方法为如下方法   
#org.elasticsearch.gateway.GatewayService.GatewayRecoveryListener#onSuccess
## 主要工作是构建集群状态（ClusterState），其中的内容路由表，依赖 allocation 模块协助完成，调用 allocationService.reroute 进，入下一阶段：异步执行分片层元数据的恢复，以及分片分配。updateTask 线程结束.
   
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><strong>ES 中存储的数据</strong>：（1）state 元数据信息；（2）index Lucene 生成的索引文件；（3）translog 事务日志。<br><strong>元数据信息</strong>：</p><ol><li><p style="color:#222222; margin-left:0; margin-right:0">nodes/0/_state/*.st，集群层面元信息 MetaData（clusterUUID 、 settings 、templates 等）；</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">nodes/0/indices/{index_uuid}/_state/*.st，索引层面元信息 IndexMetaData（ numberOfShards 、mappings 等）；</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">nodes/0/indices/{index_uuid}/0/_state/*.st，分片层面元信息 ShardStateMetaData（version 、indexUUID、primary 等）。</p></li></ol><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">上述信息被持久化到磁盘：持久化的 state 不包括某个分片存在于哪个节点这种内容路由信息，集群完全重启时，依靠 gateway 的 recovery 过程重建 RoutingTable 和 RoutingNode。当读取某个文档时， 根据路由算法确定目的分片后，再从 RoutingTable 中查找分片位于哪个节点，然后将请求转发到目的节点[1]。</p><blockquote><p style="color:#999999; margin-left:0; margin-right:0">⚠️ 注意：在 es7.0.0 之后 es 的元信息存储方式发生变化；<br> es7.0.0 之后元信息存储改使用 lucene 的方式存储，见<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F50928" rel="nofollow" target="_blank">pr50928 Move metadata storage to Lucene</a>)</p></blockquote><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">7.10.2 专有主节点，集群元数据</p><pre><code>./
|-- _state
|   |-- _39h.cfe
|   |-- _39h.cfs
|   |-- _39h.si
|   |-- node-0.st
|   |-- segments_50d
|   `-- write.lock
`-- node.lock
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">6.8.13 专有主节点，集群元数据</p><pre><code>./
|-- _state
|   |-- global-230.st
|   `-- node-2.st
|-- indices
|   |-- -hiy4JnoRfqUJHTJoNUt4Q
|   |   `-- _state
|   |       `-- state-4.st
|   `-- ylJKVlqISGOi8EkpxHE_2A
|       `-- _state
|           `-- state-6.st
`-- node.lock
</code></pre><span id="OSC_h1_5"></span><h1>3 灾难场景与处理方法</h1><span id="OSC_h2_6"></span><h2>3.1 master 节点丢失</h2><blockquote><p style="color:#999999; margin-left:0; margin-right:0">⚠️ 注意本文所述的 master 节点个数，假设前提均为 3 个</p></blockquote><span id="OSC_h3_7"></span><h3>场景 1 master 节点丢失过半</h3><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">master 节点是控制整个集群；当该种节点角色丢失过半，由于集群中投票节点永远不可能达到 quorum 无法选主，将无法维持 es 节点形成一个集群；虽然集群无法形成一个集群，但所仍幸 master-eligible 节点存活,我们可以使用如下手段进行处理。</p><span id="OSC_h4_8"></span><h4>es7.0.0 版本之前</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">1 修改剩余节点的 elasticsearch.yaml 配置如下,修改 quorum 的个数，然后启动剩余的节点，形成一个新的集群；</p><pre><code>discovery.zen.minimum_master_nodes: 1
discovery.zen.ping.unicast.hosts:
- masters-0
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">2 重建补充之前丢失的 master-eligible 节点，加入集群之后.<br> 3 将集群配置修改为旧的配置，再逐一重启下集群中的节点，先从 master-eligible 开始.</p><span id="OSC_h4_9"></span><h4>es7.0.0（包含）版本之后.</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">在 es7.0.0 版本之后,由于 es 修改集群的启动配置,新增配置<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.0%2Fdiscovery-settings.html%23unicast.hosts" rel="nofollow" target="_blank">discovery.seed_hosts</a><span>&nbsp;</span>和<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.0%2Fdiscovery-settings.html%23initial_master_nodes" rel="nofollow" target="_blank">cluster.initial_master_nodes</a>；es 集群第一次启动时称为 bootstrap，该过程将配置文件中的 cluster.initial_master_node 作为初始的投票节点<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fmodules-discovery-voting.html" rel="nofollow" target="_blank">Voting configurations</a>，投票节点具有选举 master 和 commit cluster state 的权利，超过半数以上同意即投票成功。如果在集群健康的场景下，我们需要下线超过半数的 master-eligible；则必须首先使用投票配置排除 API 从投票配置中排除受影响的节点。</p><pre><code>POST _cluster/voting_config_exclusions?node_names={node_names}
POST _cluster/voting_config_exclusions?node_ids={node_ids}
DELETE _cluster/voting_config_exclusions
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">但是如果丢失的 master 节点超过半数，则可以使用新的集群处理工具 elasticsearch-node unsafe-bootstrap<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F37696" rel="nofollow" target="_blank">pr37696</a><span>&nbsp;</span>和 elasticsearch-node detach-cluster<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F37979" rel="nofollow" target="_blank">pr37979</a></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">面对丢失半数 master-eligible，es7.0.0（包含）版本之后的处理步骤如下：<br> 1 使用<code>bin/elasticsearch-node unsafe-bootstrap</code>命令让唯一主节点以不安全的方式改写投票节点，就像重新进行 bootstrap 一样，自己使用持久化的 cluster state 形成一个新集群<br> 2 其他数据节点无法加入新集群因为 UUID 不同 (es 使用 UUID 作为节点和集群的唯一表示，每个节点都会持久化当前集群的 UUID)，使用<code>bin/elasticsearch-node detach-cluster</code>命令让节点离开之前的集群<br> 3 启动数据节点和新的 master-eligible 节点 (如下补充两个新的 master-eligible)，他会加入新集群中</p><pre><code>cluster.initial_master_nodes:
- {master-0}
- {new-master-1}
- {new-master-2}
discovery.seed_hosts:
- {master-ip-0}
- {new-master-ip-1}
- {new-master-ip-2}
</code></pre><span id="OSC_h3_10"></span><h3>场景 2 master 节点全部丢失</h3><span id="OSC_h4_11"></span><h4>es7.0.0 版本之前</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">1 关闭 security 功能 (如果开启了, 最好先关闭 security 插件功能)：<br> 1.1 因为新启动的 master 节点, 没有数据节点 (如果只配置了一个 master 的角色), security 插件的初始化无法完成, 各类接口不好调用<br> 1.2 如果给新启动的 master 节点, 配置了 master and data 角色, 则 security 插件会初始化成功. 会插入 index, 但是这个 index 会和原来的 data 节点上保存的冲突. 不知道怎么解.<br> elastic 官方<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fconfiguring-security.html" rel="nofollow" target="_blank">xpack-security</a>;关闭鉴权：<code>xpack.security.enabled:false</code><br> 2 启动足够的新 master-eligible 节点形成一个新集群.</p><pre><code>discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.unicast.hosts:
- {new-masters-1}
- {new-masters-2}
- {new-masters-3}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">3 修改数据节点的为新 master 的地址,并且删除掉节点上的_state(因为新集群的 cluster UUID 不一致)，同上<br> 4 启动数据节点，数据被恢复加入到集群</p><span id="OSC_h4_12"></span><h4>es7.0.0（包含）版本之后</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">已经没有 cluster state 了，唯一的希望是数据节点上的 index 数据；恢复方式借助<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fnode-tool.html" rel="nofollow" target="_blank">elasticsearch-node</a><span>&nbsp;</span>工具</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">1 关闭 security 功能 (如果开启了, 最好先关闭 security 插件功能),原因同上<br> 2 启动足够的新 master-eligible 节点形成一个新集群</p><pre><code>cluster.initial_master_nodes:
- {new-master-0}
- {new-master-1}
- {new-master-2}
discovery.seed_hosts:
- {new-master-ip-0}
- {new-master-ip-1}
- {new-master-ip-2}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">3<span>&nbsp;</span><code>bin/elasticsearch-node detach-cluster</code>命令让数据节点离开之前的集群</p><pre><code>./bin/elasticsearch-node detach-cluster
------------------------------------------------------------------------

    WARNING: Elasticsearch MUST be stopped before running this tool.

------------------------------------------------------------------------

You should only run this tool if you have permanently lost all of the
master-eligible nodes in this cluster and you cannot restore the cluster
from a snapshot, or you have already unsafely bootstrapped a new cluster
by running `elasticsearch-node unsafe-bootstrap` on a master-eligible
node that belonged to the same cluster as this node. This tool can cause
arbitrary data loss and its use should be your last resort.

Do you want to proceed?

Confirm [y/N] y
Node was successfully detached from the cluster
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">4<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fdangling-indices-list.html" rel="nofollow" target="_blank">查询 dangling 索引</a>,<code>GET /_dangling</code>, 改 api 引入 es7.9 版本于<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F58176" rel="nofollow" target="_blank">pr58176</a><br> 5 启动数据节点并使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fdangling-index-import.html" rel="nofollow" target="_blank">Import dangling indexAPI</a>将 index 数据 import 到 cluster state 中 (官方推荐，es7.9 版本之后). 或者，配置<code>gateway.auto_import_dangling_indices: true</code>引入于 es7.6 版本<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F49174" rel="nofollow" target="_blank">pr49174</a>（es7.6.0-7.9.0 可用该配置，在 7.6 版本之前不需要配置默认加载 dangling 索引）并启动数据节点</p><pre><code>POST /_dangling/{index-uuid}?accept_data_loss=true
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">6 导入完成之后，索引 recovery 之后即可进行读写</p><blockquote><blockquote><p style="color:#999999; margin-left:0; margin-right:0">注意</p></blockquote><p style="color:#999999; margin-left:0; margin-right:0"><strong>Q1</strong>: 为什么 7.6.0 之后需要配置,才能处理悬空索引（dangling index）才能让数据加入新集群，7.6.0 之后没有悬空索引吗？<br><strong>A1</strong>: 其实也是有的，只不过在 es2 版本将配置移除（对应<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F10016" rel="nofollow" target="_blank">pr10016</a>）,默认自动加载 dangling index（es2.0-es7.6）; 具体实现于<code>org.elasticsearch.gateway.DanglingIndicesState#processDanglingIndices</code><span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F49174" rel="nofollow" target="_blank">es7.6 再次引入 dangling 配置</a>，es7.9 引入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fdangling-index-import.html" rel="nofollow" target="_blank">dangling index rest api</a></p></blockquote><blockquote><p style="color:#999999; margin-left:0; margin-right:0"><strong>Q2</strong>: 什么是 dangling 索引？<br><strong>A2</strong>: 当一个节点加入集群时，如果发现存储在其本地数据目录中的任何分片（shard）不存在于集群中，将认为这些分片属于「悬空」索引。悬空索引产生的场景（1）在 Elasticsearch 节点离线时删除了多个<code>cluster.indices.tombstones.size</code><span>&nbsp;</span>索引，节点再次加入集群集群 （2）master 节点丢失，数据节点重新加入新的集群等</p></blockquote><span id="OSC_h2_13"></span><h2>3.2 数据节点故障</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">数据节点灾难故障之后，无法恢复加入集群；可将数据物理复制到新的节点，然后按照 master 节点丢失的方式，将数据节点加入集群即可。</p><span id="OSC_h2_14"></span><h2>3.3 分片不能够自动分配</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">查看索引分片为什么无法分配，<code>POST _cluster/allocation/explain</code></p><span id="OSC_h3_15"></span><h3>3.3.1 分片正常</h3><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果分片数据正常，那么我们可以尝试重试分配分片任务;<code>POST _cluster/reroute?retry_failed</code></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">获取索引的 shard 在那些节点上，使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.0%2Findices-shards-stores.html" rel="nofollow" target="_blank">_shard_stores api</a></p><pre><code>GET indexName1/_shard_stores
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fcluster-reroute.html" rel="nofollow" target="_blank">cluster reroute</a>重新分配</p><pre><code># 尝试分配副本 
POST /_cluster/reroute
{
  "commands": [
    {
      "allocate_replica": {
        "index": "{indexName1}",
        "shard": {shardId},
        "node": "{nodes-9}"
      }
    }
  ]
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果是主分片无法分配，可以尝试如下命令进行分配</p><pre><code>POST /_cluster/reroute
{
  "commands": [
    {
      "allocate_stale_primary": {
        "index": "{indexName1}",
        "shard": {shardId},
        "node": {nodes-9},
        "accept_data_loss": true
      }
    }
  ]
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果主分片确实是无法分配，只能选择丢失该分片的数据，分配一个空的主分片</p><pre><code>POST /_cluster/reroute
{
  "commands": [
    {
      "allocate_empty_primary": {
        "index": "{indexName1}",
        "shard": {shardId},
        "node": "{nodes-9}",
        "accept_data_loss": true
      }
    }
  ]
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">es5.0 版本之前参考; https://www.elastic.co/guide/en/elasticsearch/reference/2.4/cluster-reroute.html</p><span id="OSC_h3_16"></span><h3>3.3.2 分片数据损坏</h3><span id="OSC_h4_17"></span><h4>shard corrupted</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">错误参考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdiscuss.elastic.co%2Ft%2Fcorrupted-elastic-index%2F135932" rel="nofollow" target="_blank">Corrupted elastic index</a></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F6.5%2Fshard-tool.html" rel="nofollow" target="_blank">shard-tool</a><span>&nbsp;</span>es6.5 版本引入，该操作需要 stop 节点<br> elasticsearch-shard 工具 es6.5 版本引入<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F33848" rel="nofollow" target="_blank">pr33848</a><br> elasticsearch-shard remove-corrupted-data 的 es7.0.0 引入<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F32281" rel="nofollow" target="_blank">pr32281</a></p><pre><code>bin/elasticsearch-shard remove-corrupted-data --index {indexName} --shard-id {shardId}
## 示列：修复索引 twitter 的 0 号分片
bin/elasticsearch-shard remove-corrupted-data --index twitter --shard-id 0

## 如果--index 和--shard-id 换成索引分片目录参数--dir，则直接修复 data 和 translog
bin/elasticsearch-shard remove-corrupted-data --dir /var/lib/elasticsearchdata/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">修复完成之后，启动节点，如果分片不能够自动分配,使用 reroute 命令进行 shard 分片</p><pre><code>POST /_cluster/reroute{
  "commands":[
    {
      "allocate_stale_primary":{
        "index":"index42",
        "shard":0,
        "node":"node-1",
        "accept_data_loss":false
      }
    }
  ]}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">5 版本之前可以通过索引级别配置，进行修复<br> index.shard.check_on_startup: fix ，该配置在 es6.5 版本移除<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F32279" rel="nofollow" target="_blank">pr32279</a></p><span id="OSC_h4_18"></span><h4>translog 损坏</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">修复 translog 操作，需要 stop 节点。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">修复工具 elasticsearch-translog es5.0.0 引入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F19342" rel="nofollow" target="_blank">pr19342</a><br> elasticsearch-shard remove-corrupted-data translog 的 es7.4.1 开始引入，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F47866" rel="nofollow" target="_blank">pr47866</a>elasticsearch-shard 可以直接清除 translog，也可以像上文中指定--dir 那样进行修复 translog</p><pre><code>bin/elasticsearch-shard remove-corrupted-data --index  --shard-id   --truncate-clean-translog
## 示列：修复索引 twitter 的 0 号分片
bin/elasticsearch-shard remove-corrupted-data --index twitter --shard-id 0 --truncate-clean-translog
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">清除完成之后使用 cluster reroute 进行恢复</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">5 版本之前可以通过索引级别配置，进行修复<br> index.shard.check_on_startup: fix ，该配置在 es6.5 版本移除<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F32279" rel="nofollow" target="_blank">pr32279</a></p><span id="OSC_h4_19"></span><h4><code>segments_N 文件丢失</code></h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">该种场景的文件损坏是最难修复的；官方还未提供工具，我们正在自己调研中</p><span id="OSC_h1_20"></span><h1>4 参考</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">[1]<span>&nbsp;</span>elasticsearch 集群启动流程<br> [2]https://www.elastic.co/guide/en/elasticsearch/reference/7.9/dangling-indices-list.html<br> [3]https://www.elastic.co/guide/en/elasticsearch/reference/7.10/node-tool.html</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">作者：京东科技，杨松柏</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">来源：京东云开发者社区，转载请注明来源</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:19:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/10920402</guid>
            <link>https://my.oschina.net/u/4090830/blog/10920402</link>
            <author>
                <![CDATA[京东云开发者]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CursusDB —— 面向文档的内存数据库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CursusDB 是一种面向文档的快速开源内存数据库，提供安全性、持久性、分布性、可用性和类似 SQL 的查询语言。</p><p>CursusDB 的设想是创建无限可扩展的东西，同时又不会真正减慢速度。假设你有 10 亿个文档存储在分布在 100 个节点的 1 个集合中，当集群同时在所有节点上启动非插入操作时，集群将在查询 1000 万个文档所需的时间内查询 10 亿个文档。这就是并行搜索的力量。Cursus 系统可同时在用户集合的多个部分中进行搜索。一个集群可以同时查询数千个节点。将主节点视为多个或一个集合的碎片。每个集合都会锁定插入、更新和删除，但由于 CursusDB 的分布式设计，它就像一个并发交换机，允许大量并发事务。一个集群或多个集群采取操作，这些操作作为请求同时转发到 1 个或多个节点。一致性和可靠性是设计 CursusDB 时的主要目标之一。</p><p style="text-align:start"><strong><span><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>特性</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></strong></p><ul><li>使用共享密钥和 OR TLS 保护集群和节点通信</li><li>运行时内存中的数据</li><li>并行搜索。同时搜索多个节点内的集合部分。</li><li>自动为所有节点生成唯一的所有文档的 $id 键</li><li>具有基本（R、RW）权限的数据库用户</li><li>集群和节点认证</li><li>专门针对读取的集群节点数据复制和同步</li><li>JSON 对象插入</li><li>非结构化集合</li><li>集群和客户端身份验证</li><li>节点（插入、更新、删除）实时转发给观察者</li><li>如果连接丢失，节点观察者自动重新连接</li><li>类似 SQL 的查询语言（CDQL - Cursus 文档查询语言）</li><li>低延迟</li><li>高可用</li><li>默认情况下使用共享密钥和用户确保安全</li><li>高度可配置</li><li>轻量级核心代码总共不到 6000 行代码</li><li>基于<code>log-max-lines</code>配置的文件日志记录和自动日志截断</li><li>自动重新连接任何丢失的节点或节点副本</li><li>如果 .curodeconfig 中的 automatic-backup 设置为 true，则自动备份节点</li><li>如果 .curodeconfig 中的 automatic-backup-cleanup 设置为 true，则自动清理节点备份。</li><li>如果配置了自动备份，则在数据损坏时自动恢复节点</li><li>节点数据 (.cdat) 和节点备份 (/backups/.cdat.{unixtime}) 是在关机或备份时通过序列化-加密 (chacha20poly1305)-压缩 (DEFLATE) 将内存中的数据序列化、加密并逐块压缩后创建的。</li><li>......</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:33:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/cursusdb</guid>
            <link>https://www.oschina.net/p/cursusdb</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 中文对话 0.2B 小模型 ChatLM-Chinese-0.2B]]>
            </title>
            <description>
                <![CDATA[<div align="center"><h1><a id="user-content-中文对话 02b 小模型-chatlm-chinese-02b" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D02b%E5%B0%8F%E6%A8%A1%E5%9E%8B-chatlm-chinese-02b"></a>中文对话 0.2B 小模型 ChatLM-Chinese-0.2B</h1><p>中文  | <a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/README.en.md">English</a></p></div><h1><a id="user-content-一介绍" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%80%E4%BB%8B%E7%BB%8D"></a>一、👋介绍</h1><p>现在的大语言模型的参数往往较大，消费级电脑单纯做推理都比较慢，更别说想自己从头开始训练一个模型了。本项目的目标是整理生成式语言模型的训练流程，包括数据清洗、tokenizer 训练、模型预训练、SFT 指令微调、RLHF 优化等。</p><p>ChatLM-mini-Chinese 为中文对话小模型，模型参数只有 0.2B（算共享权重约 210M），可以在最低 4GB 显存的机器进行预训练（<code>batch_size=1</code>，<code>fp16</code>或者<code> bf16</code>），<code>float16</code>加载、推理最少只需要 512MB 显存。</p><ul><li>公开所有预训练、SFT 指令微调、DPO 偏好优化数据集来源。</li><li>使用<code>Huggingface</code>NLP 框架，包括<code>transformers</code>、<code>accelerate</code>、<code>trl</code>、<code>peft</code>等。</li><li>自实现<code>trainer</code>，支持单机单卡、单机多卡进行预训练、SFT 微调。训练过程中支持在任意位置停止，及在任意位置继续训练。</li><li>预训练：整合为端到端的<code>Text-to-Text</code>预训练，非<code>mask</code>掩码预测预训练。
<ul><li>开源所有数据清洗（如规范化、基于 mini_hash 的文档去重等）、数据集构造、数据集加载优化等流程；</li><li>tokenizer 多进程词频统计，支持<code>sentencepiece</code>、<code>huggingface tokenizers</code>的 tokenizer 训练；</li><li>预训练支持任意位置断点，可从断点处继续训练;</li><li>大数据集（GB 级别）流式加载、支持缓冲区数据打乱，不利用内存、硬盘作为缓存，有效减少内存、磁盘占用。配置<code>batch_size=1, max_len=320</code>下，最低支持在 16GB 内存+4GB 显存的机器上进行预训练；</li><li>训练日志记录。</li></ul></li><li>SFT 微调：开源 SFT 数据集及数据处理过程。
<ul><li>自实现<code>trainer</code>支持 prompt 指令微调， 支持任意断点继续训练；</li><li>支持<code>Huggingface trainer</code>的<code>sequence to sequence</code>微调；</li><li>支持传统的低学习率，只训练 decoder 层的微调。</li></ul></li><li>偏好优化：使用 DPO 进行全量偏好优化。
<ul><li>支持使用<code>peft lora</code>进行偏好优化；</li><li>支持模型合并，可将<code>Lora adapter</code>合并到原始模型中。</li></ul></li><li>支持下游任务微调：<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract">finetune_examples</a>给出<strong>三元组信息抽取任务</strong>的微调示例，微调后的模型对话能力仍在。</li></ul><p>如果需要做基于小模型的检索增强生成（RAG），可以参考我的另一个项目<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese">Phi2-mini-Chinese</a>，代码见<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese%2Fblob%2Fmain%2Frag_with_langchain.ipynb">rag_with_langchain.ipynb</a></p><p>🟢<strong>最近更新</strong></p><details><summary><b>2024-01-07</b></summary>
- 添加数据清洗过程中基于 mini hash 实现的文档去重（在本项目中其实是数据集的样本去重），防止模型遇到多次重复数据后，在推理时吐出训练数据。<br>
- 添加`DropDatasetDuplicate`类实现对大数据集的文档去重。<br></details><details><summary><b>2023-12-29</b></summary>
- 更新模型代码（权重不变），可以直接使用`AutoModelForSeq2SeqLM.from_pretrained(...)`加载模型使用。<br>
- 更新 readme 文档。<br></details><details><summary><b>2023-12-18</b></summary>
- 补充利用`ChatLM-mini-0.2B`模型微调下游三元组信息抽取任务代码及抽取效果展示 。<br>
- 更新 readme 文档。<br></details><details><summary><b>2023-12-14</b></summary>
- 更新 SFT、DPO 后的模型权重文件。 <br>
- 更新预训练、SFT 及 DPO 脚本。 <br>
- 更新`tokenizer`为`PreTrainedTokenizerFast`。 <br>
- 重构`dataset`代码，支持动态最大长度，每个批次的最大长度由该批次的最长文本决定，节省显存。 <br>
- 补充`tokenizer`训练细节。 <br></details><details><summary><b>2023-12-04</b></summary>
- 更新`generate`参数及模型效果展示。<br>
- 更新 readme 文档。<br></details><details><summary><b>2023-11-28</b></summary>
- 更新 dpo 训练代码及模型权重。<br></details><details><summary><b>2023-10-19</b></summary>
- 项目开源， 开放模型权重供下载。 <br></details><h1><a id="user-content-二️chatlm-02b-chinese 模型训练过程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%8C%EF%B8%8Fchatlm-02b-chinese%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>二、🛠️ChatLM-0.2B-Chinese 模型训练过程</h1><h2><a id="user-content-21-预训练数据集" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#21-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"></a>2.1 预训练数据集</h2><p>所有数据集均来自互联网公开的<strong>单轮对话</strong>数据集，经过数据清洗、格式化后保存为 parquet 文件。数据处理过程见<code>utils/raw_data_process.py</code>。主要数据集包括：</p><ol><li>社区问答 json 版 webtext2019zh-大规模高质量数据集，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbrightmart%2Fnlp_chinese_corpus">nlp_chinese_corpus</a>。共 410 万，清洗后剩余 260 万。</li><li>baike_qa2019 百科类问答，见：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F107726">https://aistudio.baidu.com/datasetdetail/107726</a>，共 140 万，清醒后剩余 130 万。</li><li>中国医药领域问答数据集，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FToyhom%2FChinese-medical-dialogue-data">Chinese-medical-dialogue-data</a>，共 79 万，清洗后剩余 79 万。</li><li><del>金融行业问答数据，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F609821974">https://zhuanlan.zhihu.com/p/609821974</a>，共 77 万，清洗后剩余 52 万。</del><strong>数据质量太差，未采用。</strong></li><li>知乎问答数据，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fwangrui6%2FZhihu-KOL">Zhihu-KOL</a>，共 100 万行，清洗后剩余 97 万行。</li><li>belle 开源的指令训练数据，介绍：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>，下载：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2FBelleGroup">BelleGroup</a>，仅选取<code>Belle_open_source_1M</code>、<code>train_2M_CN</code>、及<code>train_3.5M_CN</code>中部分回答较短、不含复杂表格结构、翻译任务（没做英文词表）的数据，共 370 万行，清洗后剩余 338 万行。</li><li>维基百科（Wikipedia）词条数据，将词条拼凑为提示语，百科的前<code>N</code>个词为回答，使用<code>202309</code>的百科数据，清洗后剩余 119 万的词条提示语和回答。Wiki 下载：<a href="https://gitee.com/link?target=https%3A%2F%2Fdumps.wikimedia.org%2Fzhwiki%2F">zhwiki</a>，将下载的 bz2 文件转换为 wiki.txt 参考：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fapertium%2FWikiExtractor">WikiExtractor</a>。</li></ol><p>数据集总数量 1023 万：Text-to-Text 预训练集：930 万，评估集：2.5 万（因为解码较慢，所以没有把评估集设置太大）。<del>测试集：90 万。</del>
SFT 微调和 DPO 优化数据集见下文。</p><h2><a id="user-content-22-模型" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#22-%E6%A8%A1%E5%9E%8B"></a>2.2 模型</h2><p>T5 模型（Text-to-Text Transfer Transformer），详情见论文: <a href="https://gitee.com/link?target=https%3A%2F%2Farxiv.org%2Fabs%2F1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>。</p><p>模型源码来自 huggingface，见：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftransformers%2Fblob%2Fmain%2Fsrc%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py%23L1557">T5ForConditionalGeneration</a>。</p><p>模型配置见<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese%2Fblob%2Fmain%2Fconfig.json">model_config.json</a>，官方的<code>T5-base</code>：<code>encoder layer</code>和<code>decoder layer </code>均为为 12 层，本项目这两个参数修改为 10 层。</p><p>模型参数：0.2B。词表大小：29298，仅包含中文和少量英文。</p><h2><a id="user-content-23-训练过程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#23-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>2.3 训练过程</h2><p>硬件：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 预训练阶段：</span></span><span id="LC2" class="line">CPU: 28 vCPU Intel<span class="o">(</span>R<span class="o">)</span> Xeon<span class="o">(</span>R<span class="o">)</span> Gold 6330 CPU @ 2.00GHz</span><span id="LC3" class="line">内存：60 GB</span><span id="LC4" class="line">显卡：RTX A5000<span class="o">(</span>24GB<span class="o">)</span><span class="k">*</span> 2</span><span id="LC5" class="line"></span><span id="LC6" class="line"><span class="c"># sft 及 dpo 阶段：</span></span><span id="LC7" class="line">CPU: Intel<span class="o">(</span>R<span class="o">)</span> i5-13600k @ 5.1GHz</span><span id="LC8" class="line">内存：32 GB</span><span id="LC9" class="line">显卡：NVIDIA GeForce RTX 4060 Ti 16GB <span class="k">*</span> 1</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li><p><strong>tokenizer 训练</strong>： 现有<code>tokenizer</code>训练库遇到大语料时存在 OOM 问题，故全量语料按照类似<code>BPE</code>的方法根据词频合并、构造词库，运行耗时半天。</p></li><li><p><strong>Text-to-Text 预训练</strong>：学习率为<code>1e-4</code>到<code>5e-3</code>的动态学习率，预训练时间为 8 天。训练损失：</p></li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/train_loss.png" alt="traing loss" referrerpolicy="no-referrer"></p><ol start="3"><li><strong>prompt 监督微调（SFT）</strong>：使用<code>belle</code>指令训练数据集（指令和回答长度都在 512 以下），学习率为<code>1e-7</code>到<code>5e-5</code>的动态学习率，微调时间 2 天。微调损失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/sft_loss.png" alt="finetune loss" referrerpolicy="no-referrer"></p><ol start="4"><li><strong>dpo 直接偏好优化</strong>：数据集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>作为<code>chosen</code>文本，步骤<code>2</code>中 SFT 模型对数据集中的 prompt 做批量<code>generate</code>，得到<code>rejected</code>文本，耗时 1 天，dpo 全量偏好优化，学习率<code>le-5</code>，半精度<code>fp16</code>,共<code>2</code>个<code>epoch</code>，耗时 3h。dpo 损失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/dpo_loss.png" alt="dpo loss" referrerpolicy="no-referrer"></p><h2><a id="user-content-24-对话效果展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#24-%E5%AF%B9%E8%AF%9D%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA"></a>2.4 对话效果展示</h2><h3><a id="user-content-241-stream-chat" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#241-stream-chat"></a>2.4.1 stream chat</h3><p>默认使用<code>huggingface transformers</code>的 <code>TextIteratorStreamer</code>实现流式对话，只支持<code>greedy search</code>，如果需要<code>beam sample</code>等其他生成方式，请将<code>cli_demo.py</code>的<code>stream_chat</code>参数修改为<code>False</code>。
<img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/stream_chat.gif" alt="" referrerpolicy="no-referrer"></p><h3><a id="user-content-242-对话展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#242-%E5%AF%B9%E8%AF%9D%E5%B1%95%E7%A4%BA"></a>2.4.2 对话展示</h3><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/show1.png" alt="" referrerpolicy="no-referrer"></p><p>存在问题：预训练数据集只有 900 多万，模型参数也仅 0.2B，不能涵盖所有方面，会有答非所问、废话生成器的情况。</p><h1><a id="user-content-三使用说明" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%89%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"></a>三、📑使用说明</h1><h2><a id="user-content-31-快速开始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#31-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"></a>3.1 快速开始：</h2><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="kn">from</span><span class="nn">transformers</span><span class="kn">import</span><span class="n">AutoTokenizer</span><span class="p">,</span><span class="n">AutoModelForSeq2SeqLM</span></span><span id="LC2" class="line"><span class="kn">import</span><span class="nn">torch</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="n">model_id</span><span class="o">=</span><span class="s">'charent/ChatLM-mini-Chinese'</span></span><span id="LC5" class="line"><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="k">if</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span><span class="k">else</span><span class="s">'cpu'</span><span class="p">)</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="n">tokenizer</span><span class="o">=</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span></span><span id="LC8" class="line"><span class="n">model</span><span class="o">=</span><span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span><span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="n">txt</span><span class="o">=</span><span class="s">'如何评价 Apple 这家公司？'</span></span><span id="LC11" class="line"></span><span id="LC12" class="line"><span class="n">encode_ids</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">([</span><span class="n">txt</span><span class="p">])</span></span><span id="LC13" class="line"><span class="n">input_ids</span><span class="p">,</span><span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">]),</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'attention_mask'</span><span class="p">])</span></span><span id="LC14" class="line"></span><span id="LC15" class="line"><span class="n">outs</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">my_generate</span><span class="p">(</span></span><span id="LC16" class="line"><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC17" class="line"><span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC18" class="line"><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span></span><span id="LC19" class="line"><span class="n">search_type</span><span class="o">=</span><span class="s">'beam'</span><span class="p">,</span></span><span id="LC20" class="line"><span class="p">)</span></span><span id="LC21" class="line"></span><span id="LC22" class="line"><span class="n">outs_txt</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outs</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">(),</span><span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></span><span id="LC23" class="line"><span class="k">print</span><span class="p">(</span><span class="n">outs_txt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Apple 是一家专注于设计和用户体验的公司，其产品在设计上注重简约、流畅和功能性，而在用户体验方面则注重用户的反馈和使用体验。作为一家领先的科技公司，苹果公司一直致力于为用户提供最优质的产品和服务，不断推陈出新，不断创新和改进，以满足不断变化的市场需求。</span><span id="LC2" class="line">在 iPhone、iPad 和 Mac 等产品上，苹果公司一直保持着创新的态度，不断推出新的功能和设计，为用户提供更好的使用体验。在 iPad 上推出的 iPad Pro 和 iPod touch 等产品，也一直保持着优秀的用户体验。</span><span id="LC3" class="line">此外，苹果公司还致力于开发和销售软件和服务，例如 iTunes、iCloud 和 App Store 等，这些产品在市场上也获得了广泛的认可和好评。</span><span id="LC4" class="line">总的来说，苹果公司在设计、用户体验和产品创新方面都做得非常出色，为用户带来了许多便利和惊喜。</span><span id="LC5" class="line"></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-32-从克隆仓库代码开始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#32-%E4%BB%8E%E5%85%8B%E9%9A%86%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E5%BC%80%E5%A7%8B"></a>3.2 从克隆仓库代码开始</h2><p>本项目模型为<code>TextToText</code>模型，在预训练阶段、SFT 阶段、RLFH 阶段的<code>prompt</code>、<code>response</code>等字段，请务必加上<code>[EOS]</code>句子结束标记。<br>
本项目模型为<code>TextToText</code>模型，在预训练阶段、SFT 阶段、RLFH 阶段的<code>prompt</code>、<code>response</code>等字段，请务必加上<code>[EOS]</code>句子结束标记。<br>
本项目模型为<code>TextToText</code>模型，在预训练阶段、SFT 阶段、RLFH 阶段的<code>prompt</code>、<code>response</code>等字段，请务必加上<code>[EOS]</code>句子结束标记。</p><h3><a id="user-content-321-克隆项目" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#321-%E5%85%8B%E9%9A%86%E9%A1%B9%E7%9B%AE"></a>3.2.1 克隆项目：</h3><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone <span class="nt">--depth</span> 1 https://github.com/charent/ChatLM-mini-Chinese.git</span><span id="LC2" class="line"></span><span id="LC3" class="line"><span class="nb">cd </span>ChatLM-mini-Chinese</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-322-安装依赖" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#322-%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"></a>3.2.2 安装依赖</h3><p>本项目推荐使用<code>python 3.10</code>，过老的 python 版本可能不兼容所依赖的第三方库。</p><p>pip 安装：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pip <span class="nb">install</span><span class="nt">-r</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果 pip 安装了 CPU 版本的 pytorch，可以通过下面的命令安装 CUDA 版本的 pytorch：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># pip 安装 torch + cu118</span></span><span id="LC2" class="line">pip3 <span class="nb">install </span>torch <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu118</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>conda 安装：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">conda <span class="nb">install</span><span class="nt">--yes</span><span class="nt">--file</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-323-下载预训练模型及模型配置文件" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#323-%E4%B8%8B%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"></a>3.2.3 下载预训练模型及模型配置文件</h3><p>用<code>git</code>命令从<code>Hugging Face Hub</code>下载模型权重及配置文件，需要先安装<a href="https://gitee.com/link?target=https%3A%2F%2Fdocs.github.com%2Fzh%2Frepositories%2Fworking-with-files%2Fmanaging-large-files%2Finstalling-git-large-file-storage">Git LFS</a>，然后运行:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 使用 git 命令下载 huggingface 模型，先安装[Git LFS]，否则下载的模型文件不可用</span></span><span id="LC2" class="line">git clone <span class="nt">--depth</span> 1 https://huggingface.co/charent/ChatLM-mini-Chinese</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="nb">mv </span>ChatLM-mini-Chinese model_save</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>也可以直接从<code>Hugging Face Hub</code>仓库<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>手工下载，将下载的文件移动到<code>model_save</code>目录下即可。</p><h2><a id="user-content-33-tokenizer 训练" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#33-tokenizer%E8%AE%AD%E7%BB%83"></a>3.3 Tokenizer 训练</h2><p>原本打算直接用现成的<code>tokenizer</code>库训练的（如<code>sentencepiece</code>），但是数据集一大就容易 OOM。另外预训练数据集各个领域的语料不平衡，会产生很多不必要的合并。最后使用<code>jieba</code>分词对所有的预训练语料切词后统计词频，只保留出现 1500 次以上的字、词，参照<code>PreTrainedTokenizerFast</code>的<code>BPE model</code>的保存格式，构造<code>tokenzier</code>，最后转换为<code>PreTrainedTokenizerFast</code>。核心代码如下，详细的处理过程见<code>utils/train_tokenizer.py</code>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1"># 构造 merge 数组</span></span><span id="LC2" class="line"><span class="n">words_merge_list</span><span class="o">=</span><span class="p">[]</span></span><span id="LC3" class="line"><span class="k">for</span><span class="n">word</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span></span><span id="LC4" class="line"><span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span></span><span id="LC5" class="line"><span class="k">if</span><span class="n">n</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">:</span></span><span id="LC6" class="line"><span class="c1"># a, b 切分 12345 示例： 1 2345,  12 345,   123 45,   1234 5</span></span><span id="LC7" class="line"><span class="k">for</span><span class="n">i</span><span class="ow">in</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span></span><span id="LC8" class="line"><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]),</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="p">])</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="k">if</span><span class="n">a</span><span class="ow">in</span><span class="n">words_dict</span><span class="ow">and</span><span class="n">b</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">:</span></span><span id="LC11" class="line"><span class="n">words_merge_list</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>本项目还提供了使用预训练模型自带的<code>tokenizer</code>根据自己的语料重新训练<code>tokenizer</code>的例子，见<code>train_tokenizer.ipynb</code>。注意，重新训练<code>tokenizer</code>后，预训练模型的权重将无法使用，需要重新训练模型权重，因为<code>token</code>对应的<code>id</code>变了。</p><h2><a id="user-content-34-text-to-text-预训练" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#34-text-to-text-%E9%A2%84%E8%AE%AD%E7%BB%83"></a>3.4 Text-to-Text 预训练</h2><ol><li>预训练数据集示例</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"对于花园街，你有什么了解或看法吗？"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"花园街（是香港油尖旺区的一条富有特色的街道，位于九龙旺角东部，北至界限街，南至登打士街，与通菜街及洗衣街等街道平行。现时这条街道是香港著名的购物区之一。位于亚皆老街以南的一段花园街，也就是</span><span class="se">\"</span><span class="s2">波鞋街</span><span class="se">\"</span><span class="s2">整条街约 150 米长，有 50 多间售卖运动鞋和运动用品的店舖。旺角道至太子道西一段则为排档区，售卖成衣、蔬菜和水果等。花园街一共分成三段。明清时代，花园街是芒角村栽种花卉的地方。此外，根据历史专家郑宝鸿的考证：花园街曾是 1910 年代东方殷琴拿烟厂的花园。纵火案。自 2005 年起，花园街一带最少发生 5 宗纵火案，当中 4 宗涉及排档起火。2010 年。2010 年 12 月 6 日，花园街 222 号一个卖鞋的排档于凌晨 5 时许首先起火，浓烟涌往旁边住宅大厦，消防接报 4"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li><p>jupyter-lab 或者 jupyter notebook:</p><p>见文件<code>train.ipynb</code>，推荐使用 jupyter-lab，避免考虑与服务器断开后终端进程被杀的情况。</p></li><li><p>控制枱：</p><p>控制枱训练需要考虑连接断开后进程被杀的，推荐使用进程守护工具<code>Supervisor</code>或者<code>screen</code>建立连接会话。</p><p>首先要配置<code>accelerate</code>，执行以下命令， 根据提示选择即可，参考<code>accelerate.yaml</code>，<em>注意：DeepSpeed 在 Windows 安装比较麻烦</em>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">accelerate config</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>开始训练，如果要使用工程提供的配置请在下面的命令<code>accelerate launch</code>后加上参数<code>--config_file ./accelerate.yaml</code>，<em>该配置按照单机 2xGPU 配置。</em></p><p><em>预训练有两个脚本，本项目实现的 trainer 对应<code>train.py</code>，huggingface 实现的 trainer 对应<code>pre_train.py</code>，用哪个都可以，效果一致。本项目实现的 trainer 训练信息展示更美观、更容易修改训练细节（如损失函数，日志记录等），均支持断点继续训练，本项目实现的 trainer 支持在任意位置断点后继续训练，按<code>ctrl+c</code>退出脚本时会保存断点信息。</em></p><p>单机单卡：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer</span></span><span id="LC2" class="line">accelerate launch ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">python pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>单机多卡：
<code>2</code>为显卡数量，请根据自己的实际情况修改。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>从断点处继续训练：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train <span class="nt">--is_keep_training</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line"><span class="c"># 需要在`pre_train.py`中的`train`函数添加`resume_from_checkpoint=True`</span></span><span id="LC6" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li></ol><h2><a id="user-content-35-sft 微调" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#35-sft%E5%BE%AE%E8%B0%83"></a>3.5 SFT 微调</h2><p>SFT 数据集全部来自<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>大佬的贡献，感谢。SFT 数据集分别为：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Fgenerated_chat_0.4M">generated_chat_0.4M</a>、<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_0.5M_CN">train_0.5M_CN</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_2M_CN">train_2M_CN</a>，清洗后剩余约 137 万行。
sft 指令微调数据集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"解释什么是欧洲启示录"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"欧洲启示录（The Book of Revelation）是新约圣经的最后一卷书，也被称为《启示录》、《默示录》或《约翰默示录》。这本书从宗教的角度描述了世界末日的来临，以及上帝对世界的审判和拯救。 书中的主题包括来临的基督的荣耀，上帝对人性的惩罚和拯救，以及魔鬼和邪恶力量的存在。欧洲启示录是一个充满象征和暗示的文本，对于解读和理解有许多不同的方法和观点。"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>参考<code>data</code>目录下的示例<code>parquet</code>文件制作自己的数据集，数据集格式：<code>parquet</code>文件分两列，一列<code>prompt</code>文本，表示提示语，一列<code>response</code>文本，表示期待的模型输出。
微调细节见<code>model/trainer.py</code>下的<code>train</code>方法, <code>is_finetune</code>设置为<code>True</code>时，将进行微调，微调默认会冻结 embedding 层和 encoder 层，只训练 decoder 层。如需要冻结其他参数，请自行调整代码。</p><p>运行 SFT 微调：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本项目实现的 trainer， 添加参数`--is_finetune=True`即可, 参数`--is_keep_training=True`可从任意断点处继续训练</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py <span class="nt">--is_finetune</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer, 多 GPU 请用 accelerate launch --multi_gpu --num_processes gpu 个数 sft_train.py</span></span><span id="LC5" class="line">python sft_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-36-rlhf 强化学习人类反馈优化方法" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#36-rlhf%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"></a>3.6 RLHF（强化学习人类反馈优化方法）</h2><p>偏好方法这里介绍常见的两种：PPO 和 DPO，具体实现请自行搜索论文及博客。</p><ol><li><p>PPO 方法（近似偏好优化,Proximal Policy Optimization）<br>
步骤 1：使用微调数据集做有监督微调（SFT， Supervised Finetuning）。<br>
步骤 2：使用偏好数据集（一个 prompt 至少包含 2 个回复，一个想要的回复，一个不想要的回复。多个回复可以按照分数排序，最想要的分数最高）训练奖励模型（RM， Reward Model）。可使用<code>peft</code>库快速搭建 Lora 奖励模型。<br>
步骤 3：利用 RM 对 SFT 模型进行有监督 PPO 训练，使得模型满足偏好。</p></li><li><p>使用 DPO（直接偏好优化，Direct Preference Optimization）微调（<strong>本项目采用 DPO 微调方法，比较节省显存</strong>）
在获得 SFT 模型的基础上，无需训练奖励模型，取得正向回答（chosen）和负向回答（rejected）即可开始微调。微调的<code>chosen</code>文本来自原数据集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>，拒绝文本<code>rejected</code>来自 SFT 微调 1 个 epoch 后的模型输出，另外两个数据集：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FSkepsun%2Fhuozi_rlhf_data_json">huozi_rlhf_data_json</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fbeyond%2Frlhf-reward-single-round-trans_chinese">rlhf-reward-single-round-trans_chinese</a>，合并后共 8 万条 dpo 数据。</p><p>dpo 数据集处理过程见<code>utils/dpo_data_process.py</code>。</p></li></ol><p>DPO 偏好优化数据集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="w"></span><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"为给定的产品创建一个创意标语。，输入：可重复使用的水瓶。"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"chosen"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">保护地球，从拥有可重复使用的水瓶开始！</span><span class="se">\"</span><span class="s2">"</span><span class="p">,</span></span><span id="LC4" class="line"><span class="w"></span><span class="nl">"rejected"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">让你的水瓶成为你的生活伴侣，使用可重复使用的水瓶，让你的水瓶成为你的伙伴</span><span class="se">\"</span><span class="s2">"</span></span><span id="LC5" class="line"><span class="w"></span><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>运行偏好优化：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c">#  多 GPU 请用 accelerate launch --multi_gpu --num_processes gpu 个数 dpo_train.py</span></span><span id="LC2" class="line">python dpo_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-37-推理" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#37-%E6%8E%A8%E7%90%86"></a>3.7 推理</h2><p>确保<code>model_save</code>目录下有以下文件，这些文件都可以在<code>Hugging Face Hub</code>仓库<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>中找到：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">ChatLM-mini-Chinese</span><span id="LC2" class="line">├─model_save</span><span id="LC3" class="line">|  ├─config.json</span><span id="LC4" class="line">|  ├─configuration_chat_model.py</span><span id="LC5" class="line">|  ├─generation_config.json</span><span id="LC6" class="line">|  ├─model.safetensors</span><span id="LC7" class="line">|  ├─modeling_chat_model.py</span><span id="LC8" class="line">|  ├─special_tokens_map.json</span><span id="LC9" class="line">|  ├─tokenizer.json</span><span id="LC10" class="line">|  └─tokenizer_config.json</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li>控制枱运行：</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python cli_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li>API 调用</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python api_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>API 调用示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">curl <span class="nt">--location</span><span class="s1">'127.0.0.1:8812/api/chat'</span><span class="se">\</span></span><span id="LC2" class="line"><span class="nt">--header</span><span class="s1">'Content-Type: application/json'</span><span class="se">\</span></span><span id="LC3" class="line"><span class="nt">--header</span><span class="s1">'Authorization: Bearer Bearer'</span><span class="se">\</span></span><span id="LC4" class="line"><span class="nt">--data</span><span class="s1">'{</span></span><span id="LC5" class="line"><span class="s1">    "input_txt": "感冒了要怎么办"</span></span><span id="LC6" class="line"><span class="s1">}'</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/api_example.png" alt="api demo" referrerpolicy="no-referrer"></p><h2><a id="user-content-38-下游任务微调" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#38-%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83"></a>3.8 下游任务微调</h2><p>这里以文本中三元组信息为例，做下游微调。该任务的传统深度学习抽取方法见仓库<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。抽取出一段文本中所有的三元组，如句子<code>《写生随笔》是冶金工业 2006 年出版的图书，作者是张来亮</code>，抽取出三元组<code>(写生随笔,作者,张来亮)</code>和<code>(写生随笔,出版社,冶金工业)</code>。</p><p>原始数据集为：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F11384">百度三元组抽取数据集</a>。加工得到的微调数据集格式示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"请抽取出给定句子中的所有三元组。给定句子：《家乡的月亮》是宋雪莱演唱的一首歌曲，所属专辑是《久违的哥们》"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"[(家乡的月亮,歌手,宋雪莱),(家乡的月亮,所属专辑,久违的哥们)]"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>可以直接使用<code>sft_train.py</code>脚本进行微调，脚本<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract/finetune_IE_task.ipynb">finetune_IE_task.ipynb</a>里面包含详细的解码过程。训练数据集约<code>17000</code>条，学习率<code>5e-5</code>，训练 epoch<code>5</code>。微调后其他任务的对话能力也没有消失。</p><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/ie_task_chat.png" alt="信息抽取任务微调后的对话能力" referrerpolicy="no-referrer"></p><p>微调效果：
将<code>百度三元组抽取数据集</code>公开的<code>dev</code>数据集作为测试集，对比传统方法<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。</p><table><thead><tr><th align="left">模型</th><th align="center">F1 分数</th><th align="center">精确率 P</th><th align="center">召回率 R</th></tr></thead><tbody><tr><td align="left">ChatLM-Chinese-0.2B 微调</td><td align="center">0.74</td><td align="center">0.75</td><td align="center">0.73</td></tr><tr><td align="left">ChatLM-Chinese-0.2B 无预训练</td><td align="center">0.51</td><td align="center">0.53</td><td align="center">0.49</td></tr><tr><td align="left">传统深度学习方法</td><td align="center">0.80</td><td align="center">0.79</td><td align="center">80.1</td></tr></tbody></table><p>备注：<code>ChatLM-Chinese-0.2B 无预训练</code>指直接初始化随机参数，开始训练，学习率<code>1e-4</code>，其他参数和微调一致。</p><h1><a id="user-content-四引用" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E5%9B%9B%E5%BC%95%E7%94%A8"></a>四、🎓引用</h1><p>如果你觉得本项目对你有所帮助，欢迎引用。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">@<span class="n">misc</span>{<span class="n">Charent2023</span>,</span><span id="LC2" class="line"><span class="n">author</span>={<span class="n">Charent</span><span class="n">Chen</span>},</span><span id="LC3" class="line"><span class="n">title</span>={<span class="n">A</span><span class="n">small</span><span class="n">chinese</span><span class="n">chat</span><span class="n">language</span><span class="n">model</span><span class="n">with</span><span class="m">0</span>.<span class="m">2</span><span class="n">B</span><span class="n">parameters</span><span class="n">base</span><span class="n">on</span><span class="n">T5</span>},</span><span id="LC4" class="line"><span class="n">year</span>={<span class="m">2023</span>},</span><span id="LC5" class="line"><span class="n">publisher</span> = {<span class="n">GitHub</span>},</span><span id="LC6" class="line"><span class="n">journal</span> = {<span class="n">GitHub</span><span class="n">repository</span>},</span><span id="LC7" class="line"><span class="n">howpublished</span> = {\<span class="n">url</span>{<span class="n">https</span>://<span class="n">github</span>.<span class="n">com</span>/<span class="n">charent</span>/<span class="n">ChatLM</span>-<span class="n">mini</span>-<span class="n">Chinese</span>}},</span><span id="LC8" class="line">}</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h1><a id="user-content-五其他事项" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%94%E5%85%B6%E4%BB%96%E4%BA%8B%E9%A1%B9"></a>五、🤔其他事项</h1><p>本项目不承担开源模型和代码导致的数据安全、舆情风险或发生任何模型被误导、滥用、传播、不当利用而产生的风险和责任。</p>
]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:15:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/charent/ChatLM-mini-Chinese</guid>
            <link>https://gitee.com/charent/ChatLM-mini-Chinese</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | vivo 海量微服务架构最新实践]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section style="font-size: 15px;line-height: 1.6;"><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(219, 219, 219);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgba(0, 0, 0, 0.5);font-size: 14px;text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">作者：来自 vivo 互联网中间件团队</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="text-align: left;justify-content: flex-start;display: flex;flex-flow: row;margin-bottom: 10px;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;align-self: flex-start;flex: 0 0 auto;background-color: rgb(234, 241, 255);border-style: solid;border-width: 0px 0px 0px 4px;border-color: rgb(48, 97, 207) rgb(48, 97, 207) rgb(48, 97, 207) rgb(21, 151, 239);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">本文根据罗亮老师在「2023 vivo 开发者大会"现场演讲内容整理而成。公众号回复【2023 VDC】获取互联网技术分会场议题相关资料。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: left;" powered-by="xiumi.us"><section style="text-align: justify;line-height: 1.8;padding-right: 5px;padding-left: 5px;color: rgb(160, 160, 160);"><p style="text-wrap: wrap;">vivo 微服务平台为全球 5 亿+用户背后的全网十万级机器、万级微服务提供服务，在高效实践过程中，vivo 中间件平台团队输出了一套业务适用的微服务架构最佳实践--架构能力矩阵、高效的开源中间件组件全生命周期管理策略，走出了一条从开源到开源+自研的技术演进路径，通过微服务引擎升级和统一平台建设较好解决了面临的问题与挑战。</p></section></section><section style="margin-right: 0%;margin-bottom: -5px;margin-left: 0%;text-align: right;line-height: 1;font-size: 5px;transform: translate3d(5px, 0px, 0px);" powered-by="xiumi.us"><section style="width: 0px;display: inline-block;vertical-align: top;border-bottom: 0.6em solid rgb(160, 160, 160);border-right: 0.6em solid rgb(160, 160, 160);border-top: 0.6em solid transparent !important;border-left: 0.6em solid transparent !important;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>一、vivo 从 0 到 1 的微服务架构工程实践</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.1 为什么需要微服务及落地挑战</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">伴随业务的高速发展，业务的复杂度越来越高，用户规模和访问量也越来越大；项目的迭代速度越来越快，交付效率要求也越来越高。与此同时，服务的集群规模越来越大，部署架构越来越复杂，故障范围也越来越不可控。此外，突增的业务流量时刻考验着服务的水平扩容能力，创新业务的快速孵化也对服务的可扩展性提出了更高的要求。想要解决以上问题，业务架构会朝着微服务架构方向演进。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014317" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/0efbe187-74ec-4c6c-9f03-665534c87cb6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">正是在这样的背景下，vivo 于 2015 年开始微服务架构改造，在落地过程中碰到了以下问题：</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>一是</strong>：服务数量多，配置修改生效、服务发布等变更场景效率低下；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>二是</strong>：业务链路长，高可用保障难，问题与故障定位耗时长，服务的维护成本高；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>三是</strong>：大量的跨服务通讯，性能和访问体验优化提升难度大；</p><p style="text-wrap: wrap;"><strong>四是</strong>：一个业务链路涉及大量的上下游团队，对接沟通的协作成本高；</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">为了解决以上落地过程中的开发、运维、团队协作等难题，我们需要建设配套的微服务架构技术体系。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.2 vivo 微服务架构最佳实践-架构能力矩阵</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">建设一套微服务架构技术体系，助力业务又快又好地构建微服务工程，需要哪些技术能力？我们对微服务架构的主要业务场景进行了分析，在业务实践过程中，微服务主要会涉及同步、异步、定时任务三大核心业务场景。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014318" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/b246fe36-cacc-49af-a105-9c44ade187ea.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>同步调用</strong>场景：涉及的技术能力主要是 RPC 框架、注册中心、服务治理；</p><p style="margin-bottom: 10px;">在<strong>异步调用</strong>场景：涉及的技术能力主要是消息中间件；</p><p>在<strong>定时任务</strong>场景：涉及的技术能力主要是分布式任务调度。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了上面介绍的框架和系统，业务在微服务架构改造过程中，需要的能力全貌是怎样的？</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在深度参与业务微服务架构改造过程中，我们对最佳实践能力项进行了抽象，从而形成了 vivo 内部的微服务架构最佳实践总结-架构能力矩阵，总计近 30 项能力。为了更直观的呈现这些能力，我们从接入层、服务层、数据层的三层架构分层，开发、运维等 DevOps 的关键环节对架构能力进行了梳理，如下图所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014319" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d1fb2470-8f85-4233-b3f2-010196469737.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;"><br></p></section><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在开发环节：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>在开发接口时</strong>，我们要实现内外网接口分离，保障接口的安全性，为此我们要接入网关来隔离内外网接口；在接入层和服务层，我们可以通过治理平台来实现限流、熔断、降级等能力，保障业务的高可用。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在构建内部服务时</strong>，我们要尽可能实现服务无状态，通过 RPC 框架实现内部接口的 RPC 相互调用，具备异常重试能力，提升服务的鲁棒性；在编码过程中，我们通过接入配置中心实现代码与配置分离，具备运行时动态调整配置的能力，提高服务的变更效率。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在异步调用场景</strong>，我们可以通过接入消息中间件实现业务间的相互解耦、流量削峰；在定时任务场景，我们可以通过分布式任务调度系统，实现失败任务的自动转移能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，我们可以通过落地存储与计算分离能力，实现服务层和数据层的解耦，便于分层扩容，具备面向未来更大规模业务的扩展能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在数据层</strong>，通过落地读写分离、冷热分离等能力，提升系统性能，节省存储成本；同时将这些能力通过研发框架进行封装，便于业务侧复用。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在运维环节：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">我们可以借助 CDN 实现网站的动静分离访问，减小系统的请求压力；在日常运维过程中，我们要实现服务的可灰度、可回滚；服务节点无单点；同时借助容器技术快速实现弹性伸缩能力；提升系统的故障恢复速度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在部署时</strong>，通过部署与发布分离，可以较好规避发布变更时产生的问题，即服务部署成功，并且健康检查通过后再发布到生产环境，减小故障的影响范围。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在遇到严重的系统故障时</strong>，需要具备使用备份数据从零恢复的能力，同时对所有已知的故障场景要有对应的预案，提升系统的故障应对能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在数据运维上</strong>，我们要确保数据属主唯一，避免多个业务对同一个数据库进行访问；同时也要实现业务数据和大数据的存储隔离，避免相互影响。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了以上能力之外，我们<strong>还要</strong>实现业务的安全合规，建设覆盖 Metric、Trace、Log 的可观测能力体系，便于对故障问题的定位排查；在多机房层面，需要具备同城双活、异地多活等跨机房容灾能力。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.3 vivo 微服务平台能力</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">为了更好落地以上最佳实践，我们构建了一套从接入层、服务层、消息层、框架层到存储层的平台能力，完整的平台能力地图如下图所示：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014320" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/3b0a6fb7-b253-48eb-921d-5056d3d9204a.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>接入层</strong>，我们提供了四层流量网关和七层微服务 API 网关；在服务层提供了服务/流量治理平台、配置中心、注册中心、接口管理平台、分布式任务调度等系统。</p><p style="margin-bottom: 10px;">在<strong>消息层</strong>提供了消息中间件；在框架层提供了脚手架，可快速集成日志、配置、限流/熔断、MySQL/Redis 等 SDK，以及 RPC 框架。</p><p style="margin-bottom: 10px;">在<strong>存储层</strong>提供了 DaaS 平台，包含 MySQL、Redis、ElasticSearch、MongoDB、文件服务等系统能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">为了更好排查故障问题，我们在可观测领域构建了监控中心、日志中心、调用链等系统；此外，还有更好支撑服务构建、变更发布的 CICD 系统和 IT 基础设施的配置管理系统 CMDB。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">截止 2019 年，vivo 基本完成了从 0 到 1 的微服务平台能力烟囱式建设。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">快速构建这些能力的过程，离不开开源组件的赋能。例如微服务 API 网关背后的 zuul，注册中心背后的 ZooKeeper 和 etcd，RPC 框架的 Dubbo 和 bRPC；配置中心的 Apollo 和 Nacos，流量治理的 hystrix 和 sentinel，消息中间件的 RabbitMQ 和 RocketMQ，任务调度的 xxl-job；如下图所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014321" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d29efb2f-0028-42ec-b597-77c064aef74c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">在此，我们也通过 VDC(vivo 开发者大会) 平台，感谢开源社区的赋能，助力 vivo 微服务架构技术体系从 0 到 1 的快速构建。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.4 vivo 微服务现状</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">截止当前，vivo 的微服务平台为全球分布在 60+个国家/地区的 5 亿+用户提供服务；其中 vivo 现有万级的微服务，覆盖全网机器规模十万级，每天处理高达 8000 亿次的 RPC 调用次数，流量的峰值 QPS 达到千万级以上。<span style="text-align: center;letter-spacing: 0.034em;"></span></p><p><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014323" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/89e7af31-f3b1-48ab-8291-f6bdfc728e87.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在支撑如此规模的微服务过程中，特别是在 2020 年以后，我们碰到了较多的问题与挑战，为了解决这些问题，我们使用了微服务引擎升级和统一平台建设的解决方案；下面来一起看看我们碰到了哪些问题与挑战？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>二、微服务引擎升级与统一平台建设</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.1 面临的问题与挑战</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我们知道，注册中心和配置中心是微服务架构领域的技术基石；下面给大家说明下我们在这两个基石系统实践过程中遇到的<strong>问题与挑战</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014324" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/fd67ab5a-9617-4bd1-8251-85c51a7af775.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是注册中心，众所周知，ZK 是 CP 特性，在注册中心场景有较多不可用的问题，此外还有跨机房多活能力缺失，集群故障半径大等问题；写性能无法水平扩展，在大规模 Dubbo 服务场景中，接口级注册模型注册的数据量大，在业务高频变更期间网卡的带宽峰值会超过 1000Gbps。此外还有业务易混用，功能缺失；内部的多个技术栈使用不同的注册中心，跨技术栈调用的研发运维成本高等问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在配置中心场景，存在应用、组件配置的变更通道不统一，故障场景配置回滚慢，变更审计日志分散，业务恢复耗时长等问题；配置变更下发的时效不满足业务要求，内部存在多套配置中心，都需要和业务研发流程打通，存在审批、审计、回滚等功能没有对齐的问题；此外在功能和安全上，还需要实现内部的配置定时生效，配置加解密等需求，配置访问通道符合公司的安全要求。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">从以上的问题与挑战中可以看出，基于开源组件快速构建的微服务底层引擎在 vivo 的内部业务场景中存在较多的可用性、性能&amp;容量、研发运维、功能&amp;安全问题与挑战。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="font-size: 16px;color: rgb(65, 95, 255);" powered-by="xiumi.us"><p style="text-wrap: wrap;">2.2 注册中心引擎升级</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">为了解决以上的问题与挑战，我们需要进行技术升级，首先给大家介绍的是注册中心的<strong>解决方案</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014325" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bc83c41-6a74-4125-9924-22bc26ac900c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">针对 Dubbo 接口级服务发现导致 ZK 注册中心流量过大的问题，业界同行都在往应用级服务发现迁移来构建解决方案；通过 Dubbo 开源社区官网的介绍，我们可以看到，应用级服务发现是适应云原生，支持更大规模的服务发现模型；</p><p style="margin-bottom: 10px;">将 Dubbo 接口级服务发现模型升级为应用级，可降低单机 50% 的内存消耗，降低注册中心集群 90% 的存储与推送压力，从架构上支持百万实例集群规模；</p><p>因此我们需要将 Dubbo 框架服务发现模型从接口级升级为应用级，彻底解决注册数据量大，对注册中心请求压力大的问题，同时具备面向云原生微服务架构的扩展能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，针对注册中心的可用性、性能&amp;容量、研发运维等问题，我们需要建设满足 AP 特性、支持跨机房多活的统一注册中心，使用 Session+Data 分离架构，Data 层持久化数据，Session 层处理和客户端的长连接，无状态 Session 层能较好收敛客户端请求，实现读写流量隔离，具备较好的横向扩展能力，真正解决注册中心的性能、容量和扩展性问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>综上</strong>，我们需要构建 Dubbo 应用级服务发现能力，构建 Session+Data 分离的统一注册中心，内部的项目代号为 vns。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">从上面的技术方案分析中，我们可以看到，通过应用级注册可以彻底解决注册中心的流量突刺问题；通过 Session+Data 双层分离架构可以实现业务无感知的多集群拆分，有效缩小故障半径，那如何来<strong>落地</strong>呢？</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014326" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/444085d9-cd4f-491a-b55b-5ec48ccb6c28.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">我们首先想到的就是上图左侧的技术方案，通过构建暴露 gRPC 协议、支持应用级注册的 vns 系统，海量的 Dubbo 服务通过双注册来实现迁移；但是在经过详细的技术分析之后，我们发现该方案存在明显的<strong>耦合问题：</strong></p><p style="margin-bottom: 10px;">首先是 Dubbo 应用级注册升级的进展依赖 vns 系统的建设进度，Dubbo 框架依赖稳定的 vns SDK，Dubbo 框架和 vns 系统之间存在进度依赖问题；</p><p style="margin-bottom: 10px;">其次还存在回滚依赖问题，当 vns 系统因灰度异常回滚时，Dubbo 应用级注册升级进度也会同步回滚；</p><p style="margin-bottom: 10px;">同理当 Dubbo 流量切换异常回滚时，vns 的业务接入进度也会回退。</p><p style="margin-bottom: 10px;">此外，部分不迭代的业务可能需要继续使用接口级注册，无法实现 ZK 注册中心的完全下线。</p><p>为了解决以上问题，我们对技术方案进行了升级，改用通过 vns 系统暴露和支持 ZK 协议，实现 Dubbo 应用级注册升级和 vns 系统的能力建设解耦；当 vns 系统的能力建设进展还未达到生产环境要求时，我们可以通过引入一套新的 ZK 集群来支持 Dubbo 的应用级注册模型升级；当 vns 的能力成熟度达到生产环境的要求后，可以对引入的 ZK 集群进行替代，整个过程可以根据系统建设进展和可用性保障要求，进行可控的灰度放量和回滚操作，控制变更风险；最终，vns 通过暴露 ZK+gRPC 双协议满足业务的接入诉求。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在整个技术方案落地过程中，我们始终坚持业务导向原则，实现业务升级和迁移的零|低成本；采用稳妥、完善的升级迁移方案，确保过程可灰度、可回滚、可观测；大家可以看到，我们通过兼容 ZK 协议，最大限度的保障 Dubbo 业务的平滑升级，切换方案做到了可灰度可回滚可观测，在减少升级成本的同时，降低项目落地风险，最终实现 ZK 注册中心的完全下线。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.3 配置中心引擎升级</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介绍完注册中心，我们再来看看配置中心的解决方案，配置中心主要解决的是配置通道不统一，性能不达标，无法满足内部的业务需求等问题。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014327" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/4591d606-98ae-4070-823d-8aff13aa8f98.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">上图左侧是我们最新的配置中心技术架构图，右侧是统一配置通道的示意图，我们通过支持应用配置与组件配置的统一配置通道，实现了配置管理能力的收敛统一，在此基础上，建设一键审批/审计/回滚等能力，实现了和内部业务研发流程的打通，减少人力运维投入；此外，在新版配置中心上，我们也实现了较多的高可用、性能、安全、可观测能力增强等业务诉求；在配置中心升级过程中，我们追求业务的无感知升级，通过兼容原有配置中心对外开放的接口，实现了新系统的平滑升级，原有系统优雅下线。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，和注册中心的升级方案类似，在配置中心的技术方案设计中，我们也较好的遵循了业务导向原则。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.4 统一微服务平台建设</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介绍完注册中心和配置中心等微服务引擎的技术升级方案，我们再来看下从 0 到 1 快速构建的烟囱式微服务平台会面临哪些问题和挑战？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014328" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/2dcc08b7-f604-43a4-85d3-b7c2ae6eedf3.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">从上图左侧示意图中可以看到，我们快速构建的微服务平台存在 10 个以上的模块，每个模块都有独立的入口，用户使用平台的易用性很低；此外，这些模块在建设过程中，还需要重复对接云平台、单点登录、权限、工单、监控、CMDB 等公共服务系统；系统审计日志分散，不便于快速定位因变更引起的问题；综上，烟囱式微服务平台存在多入口，功能重复对接，运维、研发成本高，故障排查与恢复效率低，易用性不足等问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">要解决烟囱式微服务平台的问题，需要构建更合理的产品方案，我们对用户的使用现状进行了分析：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014329" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/f3ad12b5-778a-41ec-9496-bd59e3ab55a6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">通过系统埋点数据发现，烟囱式微服务平台中用户使用频率最高的两个系统分别是配置中心、服务治理。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">通过上图左侧的 PV/UV 饼状图数据，大家可以发现：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: center;"><p style="text-align: left;">配置中心的用户访问主要集中在配置的【查询与变更】、【变更记录与审批】和配置变更相关的 2 个页面上，服务治理的用户访问主要集中在【服务概览】、【服务查询】和服务相关的 2 个页面上。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">基于埋点数据，我们可以看到用户的访问集中在少数的几个功能上，通过整合各个系统模块高频使用的功能，建设统一的平台入口，实现系统间联动，这也给我们如何建设统一平台提供了较好的思路。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，在对各个模块的技术架构进行分析时，我们识别到了位于最底层、技术依赖程度最高的两个系统：配置中心、注册中心，这两个系统非常适合作为统一平台建设的技术底座。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014330" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bb1b5bf-7287-4f3d-884c-ea3b5ab7d7eb.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">区别于烟囱式微服务平台的多个系统模块独立对接 CICD 等研发平台，在统一微服务平台建设中，我们升级为统一平台对接 CICD 等研发平台；我们的建设思路是，以配置中心/注册中心为底座来建设统一微服务平台：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>：基于统一的配置通道与 CICD 等研发平台系统进行联动，建设一键审批、回滚能力，整合研发流程，降低对接成本；</p><p><strong>二是</strong>：通过统一平台的建设，实现平台间联动，建设高阶的自动化水平，支撑业务进一步提升持续服务能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.5 引擎升级&amp;统一平台建设总结</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">接下来，对我们前面讲到的内容做一个总结：在大规模、海量业务的微服务架构实践过程中，我们通过引擎升级和统一平台能力建设较好的解决了碰到的问题与挑战。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014331" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/16e89ce4-7fa5-497c-9a09-4b45df61c722.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在升级和建设过程中，我们需要保证现有业务的连续性，保障不发生因底层引擎升级和平台建设导致的可用性问题。因此，引擎升级和统一平台建设的工作需要建立在高可用保障的基础上；换句话来说，可用性是我们所有工作的底座。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在这个基础上，我们实现注册中心和配置中心的引擎升级，完成应用级注册模型升级；在这个过程中，解决底层引擎的扩展性、容量、性能、可维护性和安全性等问题；最后，我们要建设统一的微服务平台能力，实现平台间联动，构建自动/自助化使用能力；赋能业务。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，通过完整的方案介绍，在上图右侧我们呈现了微服务架构实践过程中的价值分层逻辑，即在可用性的基础上，提升系统的扩展性、容量、性能、可维护、安全性等能力；然后再在此基础上，交付更高的研发效率，更好的用户使用体验。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>三、微服务架构升级的总结与展望</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介绍完我们的解决方案后，最后来说明下我们对微服务架构升级的总结与思考，以及对未来的展望。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.1 拥抱开源的实用主义</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在构建微服务架构技术体系的过程中，我们始终坚持拥抱开源，迭代业务适用的技术平台；结合内部业务的实际情况，我们走出了一条从开源到开源+自研的研发路径。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014332" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8f26fca0-069c-4375-850c-26e6d7c79812.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在从 0 到 1 的平台能力建设过程中，我们引入开源组件进行能力快速构建，快速交付满足业务的需求；始终坚持业务适用原则，不过度设计，支撑业务的快速迭代；以上阶段，我们称之为「拿来主义」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在面向更大规模、海量业务实践过程中，为了解决碰到的问题与挑战，我们在开源的基础上进行增强，自研部分能力来解决亿级用户规模下内部业务的功能，性能，容量，研发流程打通等需求；这个阶段，我们称之为「实用主义」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在技术平台迭代过程中，我们始终坚持 2 个原则，一是简单有效原则，坚持用最简单的解决方案来解决问题；二是迭代和演进原则，坚持平台持续迭代和演进的原则；前期基于开源组件快速搭建能力，再基于实际的业务需求和痛点来落地自研架构；在这个过程中，始终坚持业务适用，不为了技术而技术，避免大而全的技术架构。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，也要说明一个常见的误区，我们为什么不完全自研？vivo 的微服务平台建设从开源社区获益良多，坚持不闭门造车，站在巨人肩膀上，持续引入优秀特性来支撑业务的快速发展，同时也会考虑将部分行业适用的通用优秀特性反馈给社区，和社区共同成长。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.2&nbsp;中间件组件全生命周期管理</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">大家可以看到，vivo 的微服务架构技术体系引入了较多的开源组件，在实践过程中，我们摸索出了一套完整的中间件组件全生命周期管理策略。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014333" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e436a13c-2e63-49df-a26a-8784d90e98a0.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我们先来看看业务的诉求和底层技术的<strong>特点</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是业务的诉求：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">业务期望更高的迭代交付效率；</span></p></li><li><p style="margin-bottom: 10px;">快速引入新技术，使用新技术助力业务创新，但很多时候新技术往往意味着成熟度不足，可能存在较多问题；</p></li><li><p style="margin-bottom: 10px;"><span style="letter-spacing: 0.034em;">业务的不断创新与发展，对组件的性能、容量要求越来越高；</span><br></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">对业务来说，高效迭代交付需求是第一位的。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(14, 14, 13);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">然而，底层技术有它自己的特点：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">技术的发展有它的客观规律，需要经历萌芽期 → 膨胀期 → 低谷期→ 复苏期→ 成熟期等多个阶段；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">缺</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">乏约束的技术体系必然随着时间推移而腐化，治理不及时会成为技术债务，阻塞业务发展；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">同类中间件组件的快速引入会有重复建设的效率问题；</span></span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">中间件组件的技术升级周期客观上都比较长。</span></span><span style="letter-spacing: 0.034em;"></span></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">实践证明，只有足够稳健的底层技术能力才能更好支撑业务的高效迭代。在这个过程中，如何兼顾效率与质量？尊重客观规律，确保整个过程都有明确的目标和方向，避免走偏，慢就是快。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我们认为，完善的中间件组件全生命周期管理策略，首先需要在所有的技术团队中形成价值共识；再通过组件扫描和组件地图等手段及时对组件全貌进行洞察；在组件的标准化治理和运营阶段实现有规范，补短板；同时在新技术引入时，通过完善的新技术引入规范，覆盖功能/性能/容量/扩展性/成熟度/使用成本等维度；在组件的版本治理上，使用基线版本治理方案，输出明确的使用标准/版本升级方案/版本收敛策略；最后，在组件的成熟度管理上，我们可以借助 Gartner(高德纳) 技术成熟度说明和组件能力矩阵，不断提升组件的成熟度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">综上，为更高效的支撑业务，在组件管理上我们使用了更加入宽松的引入策略，同时也会对组件的全生命周期进行严格管理，践行宽入严出策略，通过完善的中间件组件全生命周期管理助力业务跑的更快，走的更远。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.3 引擎升级探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">展望未来，我们会坚持和践行引擎升级和平台建设的<strong>持续迭代思路</strong>：</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">首先是对引擎升级的探索，通过引入新技术来解决当前碰到的研发效率、成本等痛点问题：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014334" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8bd511dc-cb3c-4056-a1f1-5fca3b345d4b.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在研发效率方向</strong>，存在的痛点问题如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>，组件 SDK 的升级周期长，碎片化问题严重；</p><p><strong>二是</strong>，当前 vivo 内部主要的是 Java、C++技术栈，新业务形态孵化可能会引入新的技术栈，需能够较好解决跨技术栈的问题。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">想要较好的解决以上问题，需要探索基于 Java Agent/SideCar 技术的标准 ServiceMesh 模式，将 RPC、MQ 等中间件能力下沉，透明化实现微服务治理、高可用等能力增强，同时组件具备热升级能力。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">此外，<strong>在成本方向</strong>，存在的痛点问题如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>， MQ 等重资源型应用的 CPU、存储资源利用率差异大；</p><p><strong>二是</strong>，部分事件驱动场景机器资源利用率低。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">要解决以上问题，我们可以通过升级 MQ 组件，落地存算分离技术，探索计算存储资源利用率优化方案。另外，还可以探索 Serverless 技术，实现平台化托管运维，降低资源成本，天然适合小程序、快应用等事件驱动业务场景。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>综上</strong>，在引擎升级探索上，我们会基于业务需求和痛点问题，探索和落地 ServiceMesh/Serverless/存算分离等云原生技术。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.4 平台建设探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">讲完引擎升级探索，我们再来看看在平台建设上的探索：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014335" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d67670bb-72fa-43e3-93a8-1c5f33dfca51.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">作为技术平台团队，我们在持续积极的探索「平台工程」理念，从现在的 DevOps 实践到平台工程，也是团队协作理念的再次升级。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我们知道，DevOps 于 2009 年出现，2015 年在国内火起来，它是一种文化、方法论，是敏捷理念从开发到运维的延伸。DevOps 的理念是：践行谁构建谁运行，开发运维一体化，实现业务的高效交付。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">但是，DevOps 在实际落地过程中存在以下问题：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">「DevOps 团队」的中心化与去中心化取舍问题</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">【<strong>中心化</strong>】指的是，独立的 DevOps 团队，即不在业务团队中配置 DevOps 能力，而把 DevOps 人员集中起来组建团队，这种完全中心化的模式本质上和 DevOps 文化相矛盾。同时根据康威定律，可能会制造新的效能瓶颈。「独立的 DevOps 团队」在 2014 年被 Thoughtworks「技术雷达」列为 Hold (停止采用)。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">【<strong>去中心化</strong>】指的是，将 DevOps 能力分散在业务团队，这种做法会将大量的和基础设施相关的工作职责划给业务团队；这种方式会随之出现基础设施和服务治理缺失、系统稳定性降低、研发和 DevOps 效能浪费等诸多问题。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">因此，想要践行好 DevOps，必须在中心化与去中心化之间取得平衡。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">此外，从平台能力上讲，DevOps 平台往往更侧重于建设流程和工具链，而在使用这些建设的工具技术平台过程中会大大增加业务开发团队的认知负荷，存在无法较好向业务开发团队屏蔽底层基础设施复杂性的问题。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">平台工程的概念，是在 2017 年首次出现，于 2022 年在国内兴起。平台工程的定义是，一套用来构建和运营支持软件交付和生命周期管理的自助式内部开发者平台的机制和架构；它的特点是：平台在演进中提供足够的透明度、敏捷性，在建设过程中形成适合业务架构的高效协作模式。在这一过程中逐步将知识体系固化到平台中，从而使得工程方式标准化、流程化和规模化并持续改善；它践行的理念是：一个可用的、高效的平台并非一个技术团队埋头苦干就可以产出的；恰恰相反，一个成功的平台工程需要企业各个组织部门合作、协调、推广并根据实际使用反馈不断迭代。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在具体实践中，平台工程约定了「业务团队」和「平台团队」两个团队，其中「业务团队」负责业务研发，「平台团队」负责平台建设；「平台团队」通过将技术知识沉淀到「平台工程」，隐藏和抽象底层基础设施的复杂性，实现基础设施即代码，为「业务团队」赋能增效；同时，基于「业务团队」在使用「平台工程」的过程中的不断反馈来持续改进平台的自助化产品能力，构建一整套覆盖 DevOps 全链路的简单易用平台产品；可以看到，平台工程是一种最佳实践，和我们当前的团队协作模式匹配度非常高。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014336" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e9b65caa-221d-4d7f-ae0c-c6722aa53455.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在平台建设的整体规划上：</strong></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>当前阶段</strong>：我们构建的统一微服务平台会持续探索「平台工程」理念，沉淀配置中心、注册中心等平台的技术知识与最佳实践，构建和打磨业务自助化使用的平台能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>展望未来</strong>：我们会通过明确的北极星指标，牵引平台提供更高的研发效率和更好的开发者体验。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在研发效率上</strong>，我们追求单位时间内更多的代码产出和需求交付；此外我们也追求更好的开发者体验，通过降低用户使用平台的打断次数和平台问题的人工支撑次数，提升业务团队和平台团队两个团队的开发体验。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在具体的落地路径上</strong>，我们始终以开发者用户为中心，针对研发工作中时间消耗较多的场景进行优化，通过北极星指标牵引，形成覆盖 IDE+PaaS 的平台工程实践路径，持续迭代优化平台能力，提升研发效率与开发者体验。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-right: 0%;margin-bottom: 20px;margin-left: 0%;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section><section style="display: inline-block;vertical-align: middle;width: 20%;align-self: center;flex: 0 0 auto;"><section style="text-align: center;color: rgb(45, 66, 87);font-size: 11px;" powered-by="xiumi.us"><p>END</p></section></section><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: left;" powered-by="xiumi.us"><section style="padding-left: 1em;padding-right: 1em;display: inline-block;text-align: center;"><span style="display: inline-block;padding: 0.3em 0.5em;border-radius: 0.5em;background-color: rgb(65, 94, 255);color: rgb(255, 255, 255);" title="" opera-tn-ra-cell="_$.pages:0.layers:0.comps:161.title1"><p>猜你喜欢</p></span></section><section style="border-width: 1px;border-style: solid;border-color: transparent;margin-top: -1em;padding: 20px 10px 10px;background-color: rgb(239, 239, 239);text-align: center;"><section style="font-size: 14px;text-align: left;" powered-by="xiumi.us"><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p><span style="font-size: 14px;letter-spacing: 0.578px;text-align: left;text-wrap: wrap;background-color: rgb(239, 239, 239);"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247498140%26idx%3D2%26sn%3D66854883c362d9145d89f72f267a7773%26chksm%3Debdb890edcac0018fa02b33bf9eff448548362f5174c829a6e20ee6ab656d9ea12b31c0a1e0f%26scene%3D21%23wechat_redirect" textvalue="Spring 七种事务传播性介绍" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Spring 七种事务传播性介绍</a></span><br></p></li><li><p><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497989%26idx%3D1%26sn%3Da98e270e4612356756966bd9d90d80ee%26chksm%3Debdb8997dcac0081e35a2c9ba681902e703f8c52406ee49fcedaafbba77b7dc3279f56305782%26scene%3D21%23wechat_redirect" textvalue="vivo 数据库备份恢复系统演化" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 数据库备份恢复系统演化</a></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497821%26idx%3D1%26sn%3D80e04511f5a5d5acfee4a44a8a8b3e31%26chksm%3Debdb88cfdcac01d954242fd24907b69c542e43fcb99ebe6a03d66858194e03ad14105281b62f%26scene%3D21%23wechat_redirect" textvalue="vivo 容器平台资源运营实践" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 容器平台资源运营实践</a></span></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497810%26idx%3D1%26sn%3Dfb5334c9637cdde4b5125f69ed32e89f%26chksm%3Debdb88c0dcac01d6faf82e4d44e8421616ec9128f46ea494339a599c346b13212b9f1d774886%26scene%3D21%23wechat_redirect" textvalue="Hudi 在 vivo 湖仓一体的落地实践" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Hudi 在 vivo 湖仓一体的落地实践</a></span></p></li></ul></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzI4NjY4MTU5Nw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/4g5IMGibSxt45QXJZicZ9gaNU2mRSlvqhQd94MJ7oQh4QFj1ibPV66xnUiaKoicSatwaGXepL5sBDSDLEckicX1ttibHg/0?wx_fmt=png" data-nickname="vivo 互联网技术" data-alias="vivoVMIC" data-signature="分享 vivo 互联网技术干货与沙龙活动，推荐最新行业动态与热门会议。" data-from="0" data-is_biz_ban="0"></mp-common-profile></section></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公众号 - vivo 互联网技术（vivoVMIC）。<br>如有侵权，请联系 support@oschina.cn 删除。<br>本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/10773883</guid>
            <link>https://my.oschina.net/vivotech/blog/10773883</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[一站式文件存储 X File Storage 发布 2.1.0 版本]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h3><img alt="logo" height="200px" src="https://x-file-storage.xuyanwu.cn/assets/logo.svg" referrerpolicy="no-referrer"><br><span>原名 X Spring File Storage 现已捐赠至 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdromara.org%2Fzh" target="_blank">dromara</a> 开源组织</span></h3><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.dromara.org" target="_blank">x-file-storage.dromara.org</a> | <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn" target="_blank">x-file-storage.xuyanwu.cn</a> | <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspring-file-storage.xuyanwu.cn" target="_blank">spring-file-storage.xuyanwu.cn</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcentral.sonatype.com%2Fsearch%3Fq%3Dorg.dromara.x-file-storage" target="_blank"><img src="https://img.shields.io/maven-central/v/org.dromara.x-file-storage/x-file-storage-core.svg?label=Maven%20Central" referrerpolicy="no-referrer"></a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.apache.org%2Flicenses%2FLICENSE-2.0" target="_blank"><img src="https://img.shields.io/badge/license-Apache%202-green.svg" referrerpolicy="no-referrer"></a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Findex.html" target="_blank"><img src="https://img.shields.io/badge/JDK-8+-blue.svg" referrerpolicy="no-referrer"></a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdromara%2Fx-file-storage" target="_blank"><img alt="github star" src="https://img.shields.io/github/stars/dromara/x-file-storage.svg?style=social" referrerpolicy="no-referrer"></a><a href="https://gitee.com/dromara/x-file-storage"><img alt="star" src="https://gitee.com/dromara/x-file-storage/badge/star.svg?theme=dark" referrerpolicy="no-referrer"></a><br><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjq.qq.com%2F%3F_wv%3D1027%26k%3DeGfeNqka" target="_blank"><img alt="" src="https://img.shields.io/badge/QQ%E7%BE%A4-515706495-orange" referrerpolicy="no-referrer"></a></p><h1><span>📚简介</span></h1><p>一行代码将文件存储到本地、FTP、SFTP、WebDAV、阿里云 OSS、华为云 OBS、七牛云 Kodo、腾讯云 COS、百度云 BOS、又拍云 USS、MinIO、 Amazon S3、GoogleCloud Storage、FastDFS、 Azure Blob Storage、Cloudflare R2、金山云 KS3、美团云 MSS、京东云 OSS、天翼云 OOS、移动，云 EOS、沃云 OSS、网易数帆 NOS、Ucloud US3、青云 QingStor、平安云 OBS、首云 OSS、IBM COS、其它兼容 S3 协议的存储平台。查看 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%25AD%2598%25E5%2582%25A8%25E5%25B9%25B3%25E5%258F%25B0" target="_blank">所有支持的存储平台</a></p><p>💡 通过 WebDAV 连接到 Alist 后，可以使用百度网盘、天翼云盘、阿里云盘、迅雷网盘等常见存储服务，查看 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Falist-doc.nn.ci%2Fdocs%2Fwebdav" target="_blank">Alist 支持的存储平台</a></p><p>GitHub：https://github.com/dromara/x-file-storage Gitee：https://gitee.com/dromara/x-file-storage</p><p>文档 1：https://x-file-storage.dromara.org 文档 2：https://x-file-storage.xuyanwu.cn 文档 3：https://spring-file-storage.xuyanwu.cn</p><hr><h1><span>📜更新内容</span></h1><ul><li><p>新增 FastDFS 存储平台</p></li><li><p>新增 Azure Blob Storage 存储平台</p></li><li><p>新增复制文件，支持跨存储平台复制</p></li><li><p>新增移动（重命名）文件，支持跨存储平台移动（重命名）</p></li><li><p>新增大文件手动分片上传（断点续传），1.0.0 版本早已支持大文件自动分片上传</p></li><li><p>新增计算哈希功能，上传下载时可以边处理边计算</p></li><li><p>上传无需强制获取文件大小，上传未知大小的文件更友好</p></li><li><p>优化 SpringBoot 自动配置兼容非 SpringWeb 环境</p></li><li><p>优化 FileKey 获取方式，避免空指针异常</p></li><li><p>优化上传代码结构</p></li><li><p>优化异常处理</p></li><li><p>优化进度监听器</p></li><li><p>修复上传时设置缩略图保存名称错误的 BUG</p></li><li><p>兼容低版本 SpringBoot(2.0.x) 的依赖注入</p></li><li><p>修复华为云 OBS 上传进度问题</p></li><li><p>修复 MultipartFile 存储到本地时，在某些情况下输入流未关闭的问题</p></li><li><p>修复，又拍云 USS 上传缩略图文件时 Response 未关闭的问题</p></li></ul><hr><h1><span>📦使用</span></h1><p>点击 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%25BF%25AB%25E9%2580%259F%25E5%2585%25A5%25E9%2597%25A8" target="_blank">快速入门</a> 查看全部存储平台的使用方法！</p><h4><span>🔧 配置</span></h4><p>这里以阿里云 OSS 为例，<code>pom.xml</code> 引入本项目，这里默认是 <code>SpringBoot</code> 环境，其它环境参考 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E8%2584%25B1%25E7%25A6%25BBSpringBoot%25E5%258D%2595%25E7%258B%25AC%25E4%25BD%25BF%25E7%2594%25A8" target="_blank">脱离 SpringBoot 单独使用</a></p><pre><code><span>&lt;!--&nbsp;引入本项目&nbsp;--&gt;</span><span>&lt;<span>dependency</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>groupId</span>&gt;</span>org.dromara.x-file-storage<span>&lt;/<span>groupId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>artifactId</span>&gt;</span>x-file-storage-spring<span>&lt;/<span>artifactId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>version</span>&gt;</span>2.1.0<span>&lt;/<span>version</span>&gt;</span><span>&lt;/<span>dependency</span>&gt;</span><span>&lt;!--&nbsp;引入&nbsp;阿里云&nbsp;OSS&nbsp;SDK，如果使用其它存储平台，就引入对应的&nbsp;SDK&nbsp;&nbsp;--&gt;</span><span>&lt;<span>dependency</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>groupId</span>&gt;</span>com.aliyun.oss<span>&lt;/<span>groupId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>artifactId</span>&gt;</span>aliyun-sdk-oss<span>&lt;/<span>artifactId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>version</span>&gt;</span>3.16.1<span>&lt;/<span>version</span>&gt;</span><span>&lt;/<span>dependency</span>&gt;</span></code></pre><p><code>application.yml</code> 配置文件中添加以下基础配置</p><pre><code><span>dromara:</span>
&nbsp;&nbsp;<span>x-file-storage:</span>&nbsp;<span>#文件存储配置</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>default-platform:</span>&nbsp;<span>aliyun-oss-1</span>&nbsp;<span>#默认使用的存储平台</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>aliyun-oss:</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>-</span>&nbsp;<span>platform:</span>&nbsp;<span>aliyun-oss-1</span>&nbsp;<span>#&nbsp;存储平台标识</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>enable-storage:</span>&nbsp;<span>true</span>&nbsp;&nbsp;<span>#&nbsp;启用存储</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>access-key:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>secret-key:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>end-point:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>bucket-name:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>domain:</span>&nbsp;<span>??</span>&nbsp;<span>#&nbsp;访问域名，注意「/」结尾，例如：https://abc.oss-cn-shanghai.aliyuncs.com/</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>base-path:</span>&nbsp;<span>test/</span>&nbsp;<span>#&nbsp;基础路径</span></code></pre><h4><span>🔨编码</span></h4><p>在启动类上加上<code>@EnableFileStorage</code>注解</p><pre><code><span>@EnableFileStorage</span><span>@SpringBootApplication</span><span>public</span>&nbsp;<span><span>class</span>&nbsp;<span>SpringFileStorageTestApplication</span>&nbsp;</span>{

&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;<span>static</span>&nbsp;<span>void</span>&nbsp;<span>main</span><span>(String[]&nbsp;args)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SpringApplication.run(SpringFileStorageTestApplication<span>.<span>class</span>,<span>args</span>)</span>;
&nbsp;&nbsp;&nbsp;&nbsp;}

}
</code></pre><h4><span>✨开始上传</span></h4><p>支持 File、MultipartFile、byte[]、InputStream、URL、URI、String、HttpServletRequest，大文件会自动分片上传。如果想支持更多方式，请阅读 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E6%2596%2587%25E4%25BB%25B6%25E9%2580%2582%25E9%2585%258D%25E5%2599%25A8" target="_blank">文件适配器</a> 章节</p><pre><code><span>@RestController</span><span>public</span>&nbsp;<span><span>class</span>&nbsp;<span>FileDetailController</span>&nbsp;</span>{

&nbsp;&nbsp;&nbsp;&nbsp;<span>@Autowired</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>private</span>&nbsp;FileStorageService&nbsp;fileStorageService;<span>//注入实列</span>

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上传文件
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>upload</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>//只需要这一行代码即可上传成功</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(file).upload();
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上传文件，成功返回文件&nbsp;url
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload2"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;String&nbsp;<span>upload2</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FileInfo&nbsp;fileInfo&nbsp;=&nbsp;fileStorageService.of(file)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPath(<span>"upload/"</span>)&nbsp;<span>//保存到相对路径下，为了方便管理，不需要可以不写</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setObjectId(<span>"0"</span>)&nbsp;&nbsp;&nbsp;<span>//关联对象 id，为了方便管理，不需要可以不写</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setObjectType(<span>"0"</span>)&nbsp;<span>//关联对象类型，为了方便管理，不需要可以不写</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.putAttr(<span>"role"</span>,<span>"admin"</span>)&nbsp;<span>//保存一些属性，可以在切面、保存上传记录、自定义存储平台等地方获取使用，不需要可以不写</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.upload();&nbsp;&nbsp;<span>//将文件上传到对应地方</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileInfo&nbsp;==&nbsp;<span>null</span>&nbsp;?&nbsp;<span>"上传失败！"</span>&nbsp;:&nbsp;fileInfo.getUrl();
&nbsp;&nbsp;&nbsp;&nbsp;}

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上传图片，成功返回文件信息
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;图片处理使用的是&nbsp;https://github.com/coobird/thumbnailator
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload-image"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>uploadImage</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(file)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.image(img&nbsp;-&gt;&nbsp;img.size(<span>1000</span>,<span>1000</span>))&nbsp;&nbsp;<span>//将图片大小调整到&nbsp;1000*1000</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.thumbnail(th&nbsp;-&gt;&nbsp;th.size(<span>200</span>,<span>200</span>))&nbsp;&nbsp;<span>//再生成一张&nbsp;200*200&nbsp;的缩略图</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.upload();
&nbsp;&nbsp;&nbsp;&nbsp;}

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上传文件到指定存储平台，成功返回文件信息
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload-platform"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>uploadPlatform</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(file)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPlatform(<span>"aliyun-oss-1"</span>)&nbsp;&nbsp;&nbsp;&nbsp;<span>//使用指定的存储平台</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.upload();
&nbsp;&nbsp;&nbsp;&nbsp;}

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;直接读取&nbsp;HttpServletRequest&nbsp;中的文件进行上传，成功返回文件信息
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;使用这种方式有些注意事项，请查看文档&nbsp;基础功能-上传&nbsp;章节
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload-request"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>uploadPlatform</span><span>(HttpServletRequest&nbsp;request)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(request).upload();
&nbsp;&nbsp;&nbsp;&nbsp;}
}
</code></pre><h4><span>🎨其它操作</span></h4><pre><code><span>//手动构造文件信息，可用于其它操作</span>
FileInfo&nbsp;fileInfo&nbsp;=&nbsp;<span>new</span>&nbsp;FileInfo()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPlatform(<span>"huawei-obs-1"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setBasePath(<span>"test/"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPath(<span>"aa/"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setFilename(<span>"image.png"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setThFilename(<span>"image.png.min.jpg"</span>);

<span>//文件是否存在</span><span>boolean</span>&nbsp;exists&nbsp;=&nbsp;fileStorageService.exists(fileInfo);
<span>//下载</span><span>byte</span>[]&nbsp;bytes&nbsp;=&nbsp;fileStorageService.download(fileInfo).bytes();
<span>//删除</span>
fileStorageService.delete(fileInfo);
<span>//其它更多操作</span></code></pre><p>如果将文件记录保存到数据库中，还可以更方便的根据 URL 进行操作了，详情请阅读 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%259F%25BA%25E7%25A1%2580%25E5%258A%259F%25E8%2583%25BD%3Fid%3D%25E4%25BF%259D%25E5%25AD%2598%25E4%25B8%258A%25E4%25BC%25A0%25E8%25AE%25B0%25E5%25BD%2595" target="_blank">保存上传记录</a> 章节</p><pre><code><span>//直接从数据库中获取&nbsp;FileInfo&nbsp;对象，更加方便执行其它操作</span>
FileInfo&nbsp;fileInfo&nbsp;=&nbsp;fileStorageService.getFileInfoByUrl(<span>"https://abc.def.com/test/aa/image.png"</span>);

<span>//文件是否存在</span><span>boolean</span>&nbsp;exists&nbsp;=&nbsp;fileStorageService.exists(<span>"https://abc.def.com/test/aa/image.png"</span>);
<span>//下载</span><span>byte</span>[]&nbsp;bytes&nbsp;=&nbsp;fileStorageService.download(<span>"https://abc.def.com/test/aa/image.png"</span>).bytes();
<span>//删除</span>
fileStorageService.delete(<span>"https://abc.def.com/test/aa/image.png"</span>);
<span>//其它更多操作</span></code></pre><p>点击 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%25BF%25AB%25E9%2580%259F%25E5%2585%25A5%25E9%2597%25A8" target="_blank">快速入门</a> 查看全部存储平台的使用方法！</p><hr><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 00:50:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276310</guid>
            <link>https://www.oschina.net/news/276310</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[《互联网法律白皮书（2023 年）》发布]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>2024 年 1 月 19 日，中国信息通信研究院（简称「中国信通院」）互联网法律研究中心举办第七届互联网法律研讨会，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSfy0OltG0u4n1YKNDSXmqg" target="_blank">发布</a>《互联网法律白皮书（2023 年）》。系统梳理过去一年国内外重要互联网立法活动，深入分析我国互联网立法的最新成果和国际互联网立法进展，对未来互联网立法工作进行了展望，希望能为社会各界了解互联网领域立法情况提供有价值的参考。</p><p><strong>白皮书核心观点</strong></p><ol><li><p>我国网络空间法治化迈向新阶段。我国加快推进互联网立法进程，在网络安全、数据安全、个人信息保护、互联网平台等方面明确管理要求，为构建网络综合治理体系提供了法治保障。2023 年，我国互联网法治建设继续顺应全球信息化发展大势，立足我国互联网发展实践，不断深化对依法治网的规律性认识，在保安全的基础上向促发展迈进，查漏补缺、完善制度、细化规则。</p></li><li><p>我国互联网法律体系日趋健全完善。我国深入贯彻落实党中央关于网络强国、数字中国的决策部署，积极推进互联网立法工作，不断完善相关法律制度规范，基本形成了具有中国特色的互联网法律体系。网络设施安全防护进一步加强，保障数据安全和促进数据价值释放法律制度同步推进，互联网平台发展的法治环境日益优化，新技术新模式发展逐步规范。</p></li><li><p>全球互联网立法展现新趋势新动向。2023 年，全球网络安全立法持续推进，数据跨境流动和数据共享流通成为新的立法关注点，互联网平台对网络信息内容的管理义务进一步强化，全球加快对人工智能等新技术新应用的立法应对，各国加快推进互联网立法进程为保障其国内发展、强化国际竞争提供制度保障。</p></li><li><p>构建适应数字经济和实体经济融合发展的互联网法律体系。在习近平新时代中国特色社会主义思想指引下，我国需适应数字经济发展要求，构建完善互联网法律体系，补齐法制短板，通过分类分级制度实现精准施策。应对人工智能等新技术发展趋势，为新技术的规范发展提供法治支撑。深入布局研究互联网立法前沿问题，探索构建数据基础法律制度。进一步健全互联网法律法规，提高我国数字经济治理体系和治理能力现代化水平，以法治护航数字经济行稳致远。</p></li></ol><p><img height="280" src="https://oscimg.oschina.net/oscnet/up-2e6e5be1f936754aa5edce38c01aada886b.png" width="500" referrerpolicy="no-referrer"></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-559d5e7fe744cce93a4ae209e492aee5880.png" width="500" referrerpolicy="no-referrer"></p><p><img height="272" src="https://oscimg.oschina.net/oscnet/up-7ae8f0a460439255c90efe36af5171f1195.png" width="500" referrerpolicy="no-referrer"></p><p><img height="271" src="https://oscimg.oschina.net/oscnet/up-ab6fa6ae131b9d94129e631761e1ee414c9.png" width="500" referrerpolicy="no-referrer"></p><p><img height="278" src="https://oscimg.oschina.net/oscnet/up-9998e54e517d56fe321f2268e243548c9f8.png" width="500" referrerpolicy="no-referrer"></p><p><img height="279" src="https://oscimg.oschina.net/oscnet/up-60418af84fcfa23ba48a3f631cff64713c5.png" width="500" referrerpolicy="no-referrer"></p><p><img height="277" src="https://oscimg.oschina.net/oscnet/up-6f1f74832bb661a34b0429d057d5aca4d93.png" width="500" referrerpolicy="no-referrer"></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-e099e03ebfcf5a5101d332999447594f51a.png" width="500" referrerpolicy="no-referrer"></p><p><img height="285" src="https://oscimg.oschina.net/oscnet/up-ef189c3cccb8a9ec7d7e5540ddac9d9026d.png" width="500" referrerpolicy="no-referrer"></p><p><img height="284" src="https://oscimg.oschina.net/oscnet/up-9a859a2d1c378f75b300162a287fa4ffd38.png" width="500" referrerpolicy="no-referrer"></p><p><img height="284" src="https://oscimg.oschina.net/oscnet/up-39b8a71bbc15699a837a443f84672b1781c.png" width="500" referrerpolicy="no-referrer">&nbsp;</p></div>
                                    ]]>
            </description>
            <guid isPermaLink="false">https://www.oschina.net/news/276402</guid>
            <link>https://www.oschina.net/news/276402</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
