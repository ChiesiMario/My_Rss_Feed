<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-最新资讯]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-最新资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 24 Feb 2024 11:08:09 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[Wubuntu：披着 Windows 11 外衣的 Ubuntu]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>众所周知，Ubuntu 是最受欢迎的 Linux 发行版之一。而微软拥抱 Linux 之后，Windows 成为了最好的 Linux 发行版（不是）。如果将两者结合，会碰撞出怎样的火花？</p><p><strong>Wubuntu，又称 "Windows Ubuntu"</strong>，是基于 Ubuntu 开发的操作系统，其最具特色之处在于<strong>完全复刻了 Windows 的所有外观和功能</strong>，而且运行时不需要具备 TPM、安全启动或任何其他硬件要求。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-b78011bce450db4cf20d1bb7cc559cd4cb6.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wubuntu.org%2F" target="_blank">https://www.wubuntu.org/</a></u></em></p></blockquote><p><span>此外，Wubuntu</span>&nbsp;通过集成 Wine 提供了与 Windows 应用的兼容性，开发者称 Wubuntu 支持运行 Windows 的 .exe 和 .msi&nbsp;程序，以及支持 Android 应用。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-bc1f4a6dcc383b64786c013151ada03e885.png" referrerpolicy="no-referrer"></p><p>Wubuntu 使用的技术栈：</p><p><img src="https://oscimg.oschina.net/oscnet/up-d127229369e54b269f10365f1fad568a5d1.png" referrerpolicy="no-referrer"></p><p>Wubuntu 运行效果：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-2e20ee135339d10eb33a0777b818ad6c9f1.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-ffec60452f4b93085c10c897e93761e16df.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-62e9f8b8a805f7ed3d4598c1d5c957a3119.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-1725fe7868e691dfed08e9814c6661a523e.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-61332db8d1390d860514df218a3d8a55444.png" referrerpolicy="no-referrer"></p><p>这界面不能说和 Windows 11 一模一样，只能说完全一致，微软法务部看了真的不会律师函警告吗？</p><p>此外，根据官网的信息，Wubuntu 提供免费版和专业版，其中专业版需要付费购买密钥才可使用，价格为 35 美元。但官方没有介绍两者的功能差异。</p><p>Wubuntu 下载地址：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wubuntu.org%2Findex.php%2Fget%2Fwubuntu" target="_blank">https://www.wubuntu.org/index.php/get/wubuntu</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 24 Feb 2024 10:07:14 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280083/wubuntu-windows-ubuntu</guid>
            <link>https://www.oschina.net/news/280083/wubuntu-windows-ubuntu</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[为什么 Chromebook 键盘采用小写字母？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Chromebook 键盘上的所有按键都是小写字母。自 2010 年推出第一款 Chromebook 原型机 CR-48 以来，他们就一直如此。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fdbc3ca95eb510cf632ffad2f3b12b6bf02.png" referrerpolicy="no-referrer"></p><p>谷歌最近<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.google%2Fproducts%2Fchromebooks%2Fchromebooks-lowercase-keyboard%2F" target="_blank">解释</a></u>了他们对「全小写键盘」的设计思路。当时参与早期 Chromebook 设计的 ChromeOS 团队高级主管 Alexander Kuscher 认为可以对按键进行精简，从而打造一款易于使用、对用户友好的键盘。</p><p>Chromebook 团队高级产品经理 Donny Reynolds 提出：</p><blockquote><p>我们已经习惯了键盘上的大写字母，但如果你进入一个文本开始编写文档，并开始在传统键盘上键入，按键就会与屏幕上显示的不一致，对吧？你按下大写'D'键，但出现的却是小写'd'。</p></blockquote><p>因此，他们决定不再为 Chromebook 键盘印上大写字母——确保用户「所见即所得」。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-f381533dbaf1652c58fb82ffd2f52aa40f7.png" referrerpolicy="no-referrer"></p><p>其实当时许多手机和平板电脑已经配备了简化的现代键盘——Android 手机在 2008 年首次亮相时就采用了小写键盘。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e678bff4959ef28477371fd4d648545eed9.png" referrerpolicy="no-referrer"></p><p><em>早在 2008 年初代 G1 推出时，Android 就开启了小写键盘革命</em>。</p><p>因此 Donny Reynolds 表示：「当我们开始制造 Chromebook 时，我们自问‘计算机如何才能与众不同，而且没有几十年来的包袱？看起来更友好是我们计算机设计的一个重要组成部分，我认为小写键盘确实已经成为几乎自己的标志性品牌，以帮助实现这一目标。」</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 24 Feb 2024 04:45:12 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280052/chromebooks-lowercase-keyboard</guid>
            <link>https://www.oschina.net/news/280052/chromebooks-lowercase-keyboard</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Databend 开源周报第 132 期]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><img src="https://oscimg.oschina.net/oscnet/up-42f77d1f307b752e9544a0efaded2715eae.png" alt="" referrerpolicy="no-referrer"></p><blockquote><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdatafuselabs%2Fdatabend" target="_blank">Databend</a> 是一款现代云数仓。专为弹性和高效设计，为您的大规模分析需求保驾护航。自由且开源。即刻体验云服务：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fapp.databend.cn" target="_blank">https://app.databend.cn</a> 。</p></blockquote><h2>What's On In Databend</h2><p>探索 Databend 本周新进展，遇到更贴近你心意的 Databend 。</p><h3>提供对 <code>CREATE [ OR REPLACE ]</code> 的全面支持</h3><p>Databend 现已提供对 <code>CREATE [ OR REPLACE ]</code> 语法糖的全面支持，以覆盖潜在的 <code>DROP IF EXISTS ...</code> + <code>CREATE ...</code> 用例。</p><p>目前支持该语法糖的对象包括：<code>DATABASE</code>、<code>TABLE</code>、<code>VIEW</code>、<code>AGGREGATING INDEX</code>、<code>STREAM</code>、<code>CONNECTION</code>、<code>FUNCTION</code>、<code>FILE FORMAT</code>、<code>MASKING POLICY</code> 等。</p><p>如果您想了解更多信息，欢迎联系 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdatabend.cn%2Fcontact-us%2F" target="_blank">Databend 团队</a>，或查看下面列出的资源。</p><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdatafuselabs%2Fdatabend%2Fissues%2F14299" target="_blank">Issue #14299 | tracking: CREATE OR REPLACE</a></li></ul><h2>Code Corner</h2><p>一起来探索 Databend 和周边生态中的代码片段或项目。</p><h3>利用 Databend Cloud 进行查询剖析</h3><p>Databend Cloud 提供可视化分析工具以简化对复杂查询的剖析和理解。</p><p>该剖析工具可以跟踪每个步骤的性能，从 TableScan 持续时间到 HashJoin 的详细信息，并监控数据外溢情况。帮助您轻松分析查询成本和时间，进行针对性优化。</p><p><img src="https://oscimg.oschina.net/oscnet/up-fc41fb6d19013fb622f80bea9d8bcac3ac8.png" alt="" referrerpolicy="no-referrer"></p><p>Databend 团队也充分利用该工具评估代码变更对查询执行的影响。例如 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdatafuselabs%2Fdatabend%2Fpull%2F14561" target="_blank">PR #14561 | feat: use materialized cte for standard stream</a> 。</p><h2>Highlights</h2><p>以下是一些值得注意的事件，也许您可以找到感兴趣的内容。</p><ul><li>支持 JSON 运算符 <code>#-</code> 。</li><li>在标准流中使用物化公用表表达式（Materialized CTE），以避免重复扫描。</li><li>阅读文档 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.databend.com%2Fguides%2Fdata-management%2F" target="_blank">Docs | Data Management</a> 了解如何利用 Databend 管理、恢复和保护您的数据。</li></ul><h2>What's Up Next</h2><p>我们始终对前沿技术和创新理念持开放态度，欢迎您加入社区，为 Databend 注入活力。</p><h3>支持多表插入</h3><p>Databend 计划支持多表插入以允许使用一条语句有条件地或无条件地插入多个表。</p><p>多表插入语句可以减少执行多个条件插入所需的表扫描和 SQL 。主要适用于数据仓库中的 ETL 过程，支持并行化和/或将非关系型数据转换为关系型格式。</p><pre><code class="language-sql">-- Unconditional multi-table insert
INSERT [ OVERWRITE ] ALL
  intoClause [ ... ]
&lt;subquery&gt;

-- Conditional multi-table insert
INSERT [ OVERWRITE ] { FIRST | ALL }
  { WHEN &lt;condition&gt; THEN intoClause [ ... ] }
  [ ... ]
  [ ELSE intoClause ]
&lt;subquery&gt;
</code></pre><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdatafuselabs%2Fdatabend%2Fissues%2F14565" target="_blank">Issue #14565 | Feature: Multi-table Inserts support</a></p><p>如果你对这个主题感兴趣，可以尝试解决其中的部分问题或者参与讨论和 PR review 。或者，你可以点击 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.databend.rs%2Fi-m-feeling-lucky" target="_blank">https://link.databend.rs/i-m-feeling-lucky</a> 来挑选一个随机问题，祝好运！</p><h2>Changelog</h2><p>前往查看 Databend 每日构建的变更日志，以了解开发的最新动态。</p><p>地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdatafuselabs%2Fdatabend%2Freleases" target="_blank">https://github.com/datafuselabs/databend/releases</a></p><h2>Contributors</h2><p>非常感谢贡献者们在本周的卓越工作。</p><p><img src="https://oscimg.oschina.net/oscnet/up-c21d62bccf01220d44b5608d571b364bb7e.png" alt="" referrerpolicy="no-referrer"></p><h2>Connect With Us</h2><p>Databend 是一款开源、弹性、低成本，基于对象存储也可以做实时分析的新式数仓。期待您的关注，一起探索云原生数仓解决方案，打造新一代开源 Data Cloud。</p><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdatabend.rs" target="_blank">Databend Website</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdatafuselabs%2Fdatabend%2Fdiscussions" target="_blank">GitHub Discussions</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2FDatafuse_Labs" target="_blank">Twitter</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.databend.rs%2Fjoin-slack" target="_blank">Slack Channel</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Sat, 24 Feb 2024 04:13:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/5489811/blog/11044054</guid>
            <link>https://my.oschina.net/u/5489811/blog/11044054</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2024 年，只有搞颜色的 P 站真正关心网站性能]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>2024 年，大家觉得一个网站 JS 文件的平均大小应该是多少？1MB、5MB、10MB，还是更加大呢？</p><p>近年来，层出不穷的现代化前端技术让人眼花缭乱，再加上终端设备的配置越来越高，许多网站似乎不用再过分担心性能问题 —— 常常打开网站就要下载超过 10M 的&nbsp;<span>JS 文件。</span></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-25992e8199a790e84d4276053e0858e1b77.png" referrerpolicy="no-referrer"></p><p>知名开源开发者 Nikita Prokopov 对常见网站的 JS 文件大小进行了统计（未压缩），结果有点令人出乎意料。</p><hr><h3><strong><span style="background-color:#e67e22">以静态页面为主的网站</span></strong></h3><ul><li><h4>Wikipedia, 0.2&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-1421231d12c5140f4dc29b93285f2916686.png" referrerpolicy="no-referrer"></p><ul><li><h4>Linear, 3&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e8a9b91df6faa5480f2103efd9cd244aa66.png" referrerpolicy="no-referrer"></p><ul><li><h4>Zoom, 6&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-63b072146754bdd245d2af6dcb63dc79c3e.png" referrerpolicy="no-referrer"></p><ul><li><h4>Vercel, 6&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-158a201b80b904688e2a4f9590f8345b4df.png" referrerpolicy="no-referrer"></p><ul><li><h4>Gitlab,<span style="background-color:#f1c40f"> 13&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3de67d966870abe2b064d536d3f770d66bc.png" referrerpolicy="no-referrer"></p><ul><li><h4>Medium, 3&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a132d36ae636b6435528e8d174ad233f7d2.png" referrerpolicy="no-referrer"></p><ul><li><h4>Quora, 4.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3a6c4a73ff80a50c4052aa7faeced43118b.png" referrerpolicy="no-referrer"></p><ul><li><h4>Pinterest, <span style="background-color:#f1c40f">10&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-bb825579fd8f6d787be8051e5b32e081fed.png" referrerpolicy="no-referrer"></p><hr><h3><strong><span style="background-color:#e67e22">以搜索功能为主的网站</span></strong></h3><ul><li><h4>StackOverflow, 3.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-860ef367515f1ea0ae55c567ed403a1a9c4.png" referrerpolicy="no-referrer"></p><ul><li><h4>NPM, 4&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e55316c24ebe2e2a885a576106ac801e0b9.png" referrerpolicy="no-referrer"></p><ul><li><h4>Airbnb, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a59cfa95b748bb2888d93b3b647a63ca416.png" referrerpolicy="no-referrer"></p><ul><li><h4>Booking.com, <span style="background-color:#f1c40f">12&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fee82c93858ef67b723612d9e1ae53314ed.png" referrerpolicy="no-referrer"></p><ul><li><h4>Google, 9&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-d677500f815f8156922beb9938670463a68.png" referrerpolicy="no-referrer"></p><h3><span style="background-color:#e67e22">具有简单交互的单应用网站</span></h3><ul><li><h4>Google Translate, 2.5&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-31a83dfa4eba4b19f6010f5479ce06fc03b.png" referrerpolicy="no-referrer"></p><ul><li><h4>ChatGPT, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-76109ab4826fd1e649be9e2d303e44be5ff.png" referrerpolicy="no-referrer"></p><h3><span style="background-color:#e67e22">视频/多媒体类网站</span></h3><ul><li><h4>Loom, 7&nbsp;MB</h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-35a3913d214fb0b4ee6edc2929f9cec5b77.png" referrerpolicy="no-referrer"></p><ul><li><h4>YouTube, <span style="background-color:#f1c40f">12&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-11c1827cfb0fcf18a4d1ab929ce533a8d42.png" referrerpolicy="no-referrer"></p><ul><li><h4>Pornhub, <span style="background-color:#16a085">&nbsp;1.4&nbsp;MB</span></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6531a939e4e613a4503e6bec46e4e5eb0eb.png" referrerpolicy="no-referrer"></p><p>目前看下来，维基百科网站的 JS 文件最小，仅有 0.2MB。Pornhub 次之，为 1.4MB。</p><p>但这俩在下面这个网站前面都是弟弟——</p><ul><li><h4><strong>jQuery, 0.1 MB</strong></h4></li></ul><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-a30ca6a8e2eb8cbc214df1411e42772b4c4.png" referrerpolicy="no-referrer"></p><hr><p>最后看看本站：</p><p><img src="https://oscimg.oschina.net/oscnet/up-8c7490be3117d1e5730f1bb2d6ab1bb8e77.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 09:26:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279994/js-bloat-2024</guid>
            <link>https://www.oschina.net/news/279994/js-bloat-2024</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Oracle 致力解决 Java 虚拟线程「Pinning」问题]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>虚拟线程在 2023 年 9 月发布的 JDK 21 中正式成为一项稳定功能。该功能在 Java 生态系统中反响极佳，但仍存在一些痛点。Oracle 日前在&nbsp;Inside Java 网站上详细<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finside.java%2F2024%2F02%2F21%2Fquality-heads-up%2F" target="_blank">介绍</a>了虚拟线程的「Pinning」问题。</p><p>最常见的两种情况是：(a) 虚拟线程在 synchronized method 中驻留（如执行 socket I/O）；(b) <span style="color:#333333">虚拟线程阻塞进入&nbsp;synchronized method</span>，因为对象的相关监视器被另一个线程持有。</p><p>在这两种情况下，载体或本地线程都不会被释放去做其他工作。因此可能会影响性能和可扩展性，并可能<span style="color:#333333">在某些情况下</span>导致饥饿和死锁。官方最近发布的一个<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Finside.java%2F2024%2F02%2F17%2Fvirtual-threads-next-steps%2F" target="_blank">Virtual Threads Next Steps</a>&nbsp;视频中则更详细地解释了其中的原因，并讨论了一些潜在的解决方案。</p><p><img height="196" src="https://oscimg.oschina.net/oscnet/up-6a2e93ce6802a570c1704258c9a589e998e.png" width="500" referrerpolicy="no-referrer"></p><p>项目团队正在努力解决这些问题。Java Project Loom 的新早期访问版本<span style="color:#4e4242">引入了对对象监视器实现的更改</span><span style="color:#333333">，但不适用这两种常见情况。因此 </span><span style="color:#4e4242">Loom&nbsp;</span><span style="color:#333333">团队正在寻求用户的帮助，以测试这些更新的对象监控器在使用虚拟线程的代码和大量同步的库中的可靠性和性能。可通过&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmail.openjdk.org%2Fpipermail%2Floom-dev%2F" target="_blank">Loom 邮件列表</a>&nbsp;<span style="color:#333333">报告或反馈问题。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 06:47:02 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279954/java-virtual-threads-pinning-issue</guid>
            <link>https://www.oschina.net/news/279954/java-virtual-threads-pinning-issue</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Koordinator v1.4 正式发布！为用户带来更多的计算负载类型和更灵活的资源管理机制]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h3_1"></span><h3>背景</h3><p style="text-align:justify">Koordinator 作为一个积极发展的开源项目，自 2022 年 4 月发布 v0.1.0 版本以来，经历了多次迭代，持续为 Kubernetes 生态系统带来创新和增强。项目的核心是提供混部工作负载编排、混部资源调度、混部资源隔离和混部性能调优的综合解决方案，帮助用户优化容器性能，并提升集群资源使用效率。</p><p style="text-align:justify">在过去的版本迭代中，Koordinator 社区不断壮大，已经得到了包括阿里巴巴、蚂蚁科技、Intel、小米、小红书、爱奇艺、360、有赞、趣玩、美亚柏科、PITS 等知名企业工程师的积极参与和贡献。每一个版本都是在社区共同努力下推进的，反映了项目在实际生产环境中解决问题的能力。</p><p style="text-align:justify"><strong>今天我们很高兴的向大家宣布，Koordinator v1.4.0 版本正式发布。</strong>在本次发布中，Koordinator 引入了 Kubernetes 与 YARN 负载混部、NUMA 拓扑对齐策略、CPU 归一化和冷内存上报等新特性，同时重点增强了弹性配额管理、宿主机非容器化应用的 QoS 管理、重调度防护策略等领域的功能。这些新增和改进点旨在更好地支持企业级 Kubernetes 集群环境，特别是对于复杂和多样化的应用场景。</p><p style="text-align:justify">v1.4.0 版本的发布，将为用户带来更多的计算负载类型支持和更灵活的资源管理机制，我们期待这些改进能够帮助用户应对更多企业资源管理挑战。在 v1.4.0 版本中，共有 11 位新加入的开发者参与到了 Koordinator 社区的建设，他们是&nbsp;<em>@shaloulcy，@baowj-678，@zqzten，@tan90github，@pheianox，@zxh326，@qinfustu，@ikaven1024，@peiqiaoWang，@bogo-y，@xujihui1985</em>，感谢期间各位社区同学的积极参与和贡献，也感谢所有同学在社区的持续投入。</p><span id="OSC_h3_2"></span><h3>版本功能特性解读</h3><span id="OSC_h4_3"></span><h4>1. 支持 K8s 与 YARN 混部</h4><p style="text-align:justify">Koordinator 已经支持了 K8s 生态内的在离线混部，然而在 K8s 生态外，仍有相当数量的大数据任务运行在传统的 Hadoop YARN 之上。YARN 作为发展多年的大数据生态下的资源管理系统，承载了包括 MapReduce、Spark、Flink 以及 Presto 等在内的多种计算引擎。</p><p style="text-align:justify">Koordinator 社区会同来自阿里云、小红书、蚂蚁金服的开发者们共同启动了 Hadoop YARN 与 K8s 混部项目 Koordinator YARN Copilot，支持将 Hadoop NodeManager 运行在 kubernetes 集群中，充分发挥不同类型负载错峰复用的技术价值。Koordinator YARN Copilot 具备以下特点：</p><ul><li><strong>面向开源生态</strong></li></ul><p style="text-align:justify">基于 Hadoop YARN 开源版本，不涉及对 YARN 的侵入式改造；</p><ul><li><strong>统一资源优先级和 QoS 策略</strong></li></ul><p style="text-align:justify">YARN NM 使用 Koordinator 的 Batch 优先级资源，遵循 Koordinator QoS 管理策略；</p><ul><li><strong>节点级别的资源共享</strong></li></ul><p style="text-align:justify">Koordinator 提供的混部资源，既可被 K8s Pod 使用，也可被 YARN task 使用，不同类型的离线应用可运行在同一节点。</p><p style="text-align:center"><img src="https://pic2.zhimg.com/80/v2-ac0aa93eb176110201a885bb0d474c31_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">关于 Koordinator YARN Copilot 的详细设计，以及在小红书生产环境的使用情况，请参考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttp%253A%2F%2Fmp.weixin.qq.com%2Fs%253Fspm%253Da2c6h.13046898.publish-article.4.5bb96ffaCJHZTt%2526__biz%253DMzUzNzYxNjAzMg%253D%253D%2526mid%253D2247559704%2526idx%253D1%2526sn%253D3aed8968e50c85f7af7d7e79387b9365%2526chksm%253Dfae7e1d7cd9068c10df63fa4cc9362ee259bcb9c5a4f7d68439d6f9779bb31492b072142582e%2526scene%253D21%2523wechat_redirect" target="_blank">往期文章：《Koordinator 助力云原生应用性能提升：小红书混部技术实践》</a>以及社区官方文档<strong>[1]</strong>。</p><span id="OSC_h4_4"></span><h4>2. 引入 NUMA 拓扑对齐策略</h4><p style="text-align:justify">运行在 Kubernetes 集群中的工作负载日益多样化。尤其是在机器学习等领域，对于高性能计算资源的需求持续上升。在这些领域中，不仅需要大量 CPU 资源，还经常需要 GPU 和 RDMA 等其他高速计算资源配合使用；并且，为了获得最佳的性能，这些资源往往需要在同一个 NUMA 节点，甚至同一个 PCIE 中。</p><p style="text-align:justify">Kubernetes 的 Kubelet 提供了 Topology Manager 来管理资源分配的 NUMA 拓扑，试图在 Kubelet 的 Admission 阶段从节点层面对齐多种资源的拓扑。然而，节点组件没有调度器的全局视角以及为 Pod 选择节点的时机，可能导致 Pod 被调度到无法满足拓扑对齐策略的节点上，从而导致 Pod 由于 Topology Affinity 错误无法启动。</p><p style="text-align:justify">为了解决这一问题，Koordinator 将 NUMA 拓扑选择和对齐的时机放在中心调度器中，从集群级别优化资源之间的 NUMA 拓扑。在本次发布的版本中，Koordinator 将 CPU 资源（包含 Batch 资源）的 NUMA 感知调度和 GPU 设备的 NUMA 感知调度作为 alpha 功能支持，整套 NUMA 感知调度快速演进中。</p><p style="text-align:justify">Koordinator 支持用户通过节点的 Label 配置节点上多种资源的 NUMA 拓扑对齐策略，可配置策略如下：</p><ul><li>None 是默认策略，不执行任何拓扑对齐。</li><li>BestEffort 表示节点不严格按照 NUMA 拓扑对齐来分配资源。只要节点的剩余总量满足 Pods 的需求，调度器总是可以将这样的节点分配给 Pods。</li><li>Restricted 表示节点严格按照 NUMA 拓扑对齐来分配资源，即调度器在分配多个资源时必须只选择相同的一个或多个 NUMA 节点，否则不应使用该节点；可以使用多个 NUMA 节点。例如，如果一个 Pod 请求 33C，并且每个 NUMA 节点有 32C，那么它可以被分配使用两个 NUMA 节点。如果这个 Pod 还需要请求 GPU/RDMA，那么它需要位于与 CPU 相同的 NUMA 节点上。</li><li>SingleNUMANode 与 Restricted 类似，也是严格按照 NUMA 拓扑对齐，但与 Restricted 不同的是，Restricted 允许使用多个 NUMA 节点，而 SingleNUMANode 只允许使用一个 NUMA 节点。</li></ul><p style="text-align:justify">举例，我们可以为 node-0 设置策略 SingleNUMANode：</p><pre><code>apiVersion: v1
kind: Node
metadata:
  labels:
    node.koordinator.sh/numa-topology-policy: "SingleNUMANode"
  name: node-0
spec:
  ...</code></pre><p style="text-align:justify">在生产环境中，用户可能已经开启了 Kubelet 的拓扑对齐策略，这个策略会由 koordlet 更新到 NodeResourceTopologyCRD 对象中的 TopologyPolicies 字段。当 Kubelet 的策略和用户在 Node 上设置的策略相冲突时，以 Kubelet 策略为准。Koordinator 调度器基本采用与 Kubelet Topology Manager 相同的 NUMA 对齐策略语义，Kubelet 策略 SingleNUMANodePodLevel 和 SingleNUMANodeContainerLevel 被映射为 SingleNUMANode。</p><p style="text-align:justify">在为节点配置好 NUMA 对齐策略的前提下，调度器可以为每个 Pod 选出许多个符合条件的 NUMA Node 分配结果。Koordinator 当前支持 NodeNUMAResource 插件配置 CPU 和内存资源的 NUMA Node 分配结果打分策略，包括 LeastAllocated 和 MostAllocated，默认为 LeastAllocated 策略，资源支持配置权重。调度器最终将选择得分最高的 NUMA Node 分配结果。如下例，我们配置 NUMA Node 分配结果打分策略为 MostAllocated：</p><pre><code>apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - pluginConfig:
      - name: NodeNUMAResource
        args:
          apiVersion: kubescheduler.config.k8s.io/v1beta2
          kind: NodeNUMAResourceArgs
          scoringStrategy:  # Here configure Node level scoring strategy
            type: MostAllocated
            resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              - name: "kubernetes.io/batch-cpu"
                weight: 1
              - name: "kubernetes.io/batch-memory"
                weight: 1
          numaScoringStrategy: # Here configure NUMA-Node level scoring strategy
            type: MostAllocated
            resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              - name: "kubernetes.io/batch-cpu"
                weight: 1
              - name: "kubernetes.io/batch-memory"
                weight: 1</code></pre><span id="OSC_h4_5"></span><h4>3. ElasticQuota 再进化</h4><p style="text-align:justify">为了充分地利用集群资源、降低管控系统成本，用户常常将多个租户的负载部署在一个集群中。在集群资源有限的情况下，不同租户之间必然会发生资源争抢。有的租户的负载可能一直被满足，而有的租户的负载一直无法得到执行。这就产生对公平性的诉求。配额机制是非常自然地保障租户间公平性的方式，给每个租户一个配额，租户可以使用配额内的资源，超过配额的任务将不被调度和执行。然而，简单的配额管理无法满足租户对云的弹性期待。用户希望除了配额之内的资源请求可以被满足外，配额之外的资源请求也可以按需地被满足。</p><p style="text-align:justify">在之前的版本中，Koordinator 复用了上游 ElasticQuota 的协议，允许租户设置 Min 表达其一定要满足的资源诉求，允许设置 Max 限制其最大可以使用的资源和表达在集群资源不足的情况下对集群剩余资源的使用权重。另外，Koordinator 观察到，一些租户可能通过 Min 申请了配额，但是实际的任务申请可能并没有充分利用该配额。由此，为了更近一步地提高资源利用率，Koordinator 允许租户间借用/归还资源。</p><p style="text-align:justify">除了提供弹性的配额机制满足租户按需诉求外，Koordinator 在 ElasticQuota 上增加注解将其组织成树的结构，方便用户表达树形的组织架构。</p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-0f15360828d04b31ebe10218c08a8758_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">上图是使用了 Koordinator 弹性配额的集群中常见的 Quota 结构树。Root Quota 是连接配额与集群中实际资源之间的桥梁。在之前的设计中，Root Quota 只在调度器逻辑中存在，在本次发布中，我们将 Root Quota 也通过 CRD 的形式暴露给用户，用户可以通过 koordinator-root-quota 这个 ElasticQuota CRD 查看 Root Quota 信息。</p><p style="text-align:justify"><strong>3.1 引入 Multi QuotaTree</strong></p><p style="text-align:justify">大型集群中的节点的形态是多样的，例如云厂商提供的 ECS VM 会有不同的架构，常见的是 amd64 和 arm64，相同架构又会有不同种类的机型，而且一般会把节点按可用区划分。不同类型的节点放到同一个 Quota Tree 中管理时，其特有的属性将丢失，当用户希望精细化管理机器的特有属性时，当前的 ElasticQuota 显得不够精确。为了满足用户灵活的资源管理或资源隔离诉求，Koordinator 支持用户将集群中的资源划分为多份，每一份由一个 Quota Tree 来管理，如下图所示：</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-e673d71606d710f3a447f0e504527d43_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">同时，为了帮助用户简化管理复杂性，Koordinator 在 v1.4.0 中，引入了 ElasticQuotaProfile 机制，用户可以通过 nodeSelector 快速的将节点关联到不同的 QuotaTree 中，如下实例所示：</p><pre><code>apiVersion: quota.koordinator.sh/v1alpha1
kind: ElasticQuotaProfile
metadata:
  labels:
    kubernetes.io/arch: amd64
  name: amd64-profile
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      kubernetes.io/arch: amd64 // 挑选 amd64 节点
  quotaName: amd64-root-quota   // 匹配的 root quota 名称
---
apiVersion: quota.koordinator.sh/v1alpha1
kind: ElasticQuotaProfile
metadata:
  labels:
    kubernetes.io/arch: arm64   
  name: arm64-profile
  namespace: kube-system
spec:
  nodeSelector:
    matchLabels:
      kubernetes.io/arch: arm64  // 挑选 arm64 节点
  quotaName: arm64-root-quota    // 匹配的 root quota 名称</code></pre><p style="text-align:justify">关联好 QuotaTree 之后，用户在每一个 QuotaTree 中与之前的 ElasticQuota 用法一致。当用户提交 Pod 到对应的 Quota 时，当前仍然需要用户完成 Pod NodeAffinity 的管理，以确保 Pod 运行在正确的节点上。未来，我们会增加一个特性帮助用户自动管理 Quota 到 Node 的映射关系。</p><p style="text-align:justify"><strong>3.2 支持 non-preemptible</strong></p><p style="text-align:justify">Koordinator ElasticQuota 支持把 ElasticQuota 中 min 未使用的部分共享给其他 ElasticQuota 使用从而提高资源利用效率，但当资源紧张时，会通过抢占机制把借用配额的 Pod 抢占驱逐走拿回资源。</p><p style="text-align:justify">在实际生产环境中，有一些在线服务如果从其他 ElasticQuota 中借用了这部分额度，后续又发生了抢占，是可能影响服务质量的。这类工作负载实质上是不能被抢占的。</p><p style="text-align:justify">为了实现这个机制，Koordinator v1.4.0 引入了新的 API，用户只需要在 Pod 上声明 quota.scheduling.koordinator.sh/preemptible: false 表示这个 Pod 不可以被抢占。</p><p style="text-align:justify">调度器调度时发现 Pod 声明了不可抢占，那么此类 Pod 的可用配额的上限不能超过 min，所以这里也需要注意的是，启用该能力时，一个 ElasticQuota 的 min 需要设置的合理，并且集群内有相应的资源保障。</p><p style="text-align:justify">这个特性不会破坏原有的行为。</p><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod-example
  namespace: default
  labels:
    quota.scheduling.koordinator.sh/name: "quota-example"
    quota.scheduling.koordinator.sh/preemptible: false
spec:
...</code></pre><p style="text-align:justify"><strong>3.3 其它改进</strong></p><p style="text-align:justify">1. Koordinator Scheduler 过去支持跨 Namespace 使用同一个 ElasticQuota 对象，但有一些场景下，希望只被一个或者多个有限的 Namespace 可以共享同一个对象，为了支持这个场景，用户可以在 ElasticQuota 上增加 annotation quota.scheduling.koordinator.sh/namespaces，对应的值为一个 JSON 字符串数组。</p><p style="text-align:justify">2. 性能优化：过去的实现中，当 ElasticQuota 发生变化时，ElasticQuota 插件会重建整棵 Quota 树，在 v1.4.0 版本中做了优化。</p><p style="text-align:justify">3. 支持忽略 Overhead：当 Pod 使用一些安全容器时，一般是在 Pod 中声明 Overhead 表示安全容器自身的资源开销，但这部分资源成本最终是否归于终端用户承担取决于资源售卖策略。当期望不用用户承担这部分成本时，那么就要求 ElaticQuota 忽略 overhead。在 v1.4.0 版本中，可以开启 featureGate ElasticQuotaIgnorePodOverhead 启用该功能。</p><span id="OSC_h4_6"></span><h4>4. CPU 归一化</h4><p style="text-align:justify">随着 Kubernetes 集群中节点硬件的多样化，不同架构和代数的 CPU 之间性能差异显著。因此，即使 Pod 的 CPU 请求相同，实际获得的计算能力也可能大不相同，这可能导致资源浪费或应用性能下降。CPU 归一化的目标是通过标准化节点上可分配 CPU 的性能，来保证每个 CPU 单元在 Kubernetes 中提供的计算能力在异构节点间保持一致。</p><p style="text-align:justify">为了解决该问题，Koordinator 在 v1.4.0 版本中实现了一套支持 CPU 归一化机制，根据节点的资源放大策略，调整节点上可分配的 CPU 资源数量，使得集群中每个可分配的 CPU 通过缩放实现算力的基本一致。整体的架构如下图所示：</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-b0e335f164d52e400107a5294a2a7dbb_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">CPU 归一化分为两个步骤：</p><p style="text-align:justify">1. CPU 性能评估，计算不同 CPU 的性能基准，可以参考工业级性能评测标准 SPEC CPU<strong>[2]</strong>，这部分 Koordinator 项目未提供；</p><p style="text-align:justify">2. 配置 CPU 归一化系数到 Koordinator，调度系统基于归一化系数来调度资源，这部分 Koordinator 提供。</p><p style="text-align:justify">将 CPU 归一化比例信息配置到 koord-manager 的 slo-controller-config 中，配置示例如下：</p><pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: slo-controller-config
  namespace: koordinator-system
data:
  cpu-normalization-config: |
    {
      "enable": true,
      "ratioModel": {
         "Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz": {
           "baseRatio": 1.29,
           "hyperThreadEnabledRatio": 0.82,
           "turboEnabledRatio": 1.52,
           "hyperThreadTurboEnabledRatio": 1.0
         },
         "Intel Xeon Platinum 8369B CPU @ 2.90GHz": {
           "baseRatio": 1.69,
           "hyperThreadEnabledRatio": 1.06,
           "turboEnabledRatio": 1.91,
           "hyperThreadTurboEnabledRatio": 1.20
         }
      }
    }
  # ...</code></pre><p style="text-align:justify">对于配置了 CPU 归一化的节点，Koordinator 通过 Webhook 拦截 Kubelet 对 Node.Status.Allocatable 的更新以实现 CPU 资源的缩放，最终在节点上呈现出归一后的 CPU 资源可分配量。</p><span id="OSC_h4_7"></span><h4>5. 改进的重调度防护策略</h4><p style="text-align:justify">Pod 迁移是一个复杂的过程，涉及审计、资源分配、应用启动等步骤，并且与应用升级、扩展场景以及集群管理员的资源操作和维护操作混合在一起。因此，如果同时有大量 Pods 正在进行迁移，可能会对系统的稳定性产生影响。此外，如果同一工作负载的许多 Pods 同时被迁移，也会影响应用的稳定性。此外，如果同时迁移多个作业中的 Pods，可能会造成惊群效应。因此，我们希望顺序处理每个作业中的 Pods。</p><p style="text-align:justify">Koordinator 在之前提供的 PodMigrationJob 功能中已经提供了一些防护策略来解决上述问题。在 v1.4.0 版本中，Koordinator 将之前的防护策略增强为仲裁机制。当有大量的 PodMigrationJob 可以被执行时，由仲裁器通过排序和筛选，来决定哪些 PodMigrationJob 可以得到执行。</p><p style="text-align:justify">排序过程如下：</p><ul><li>根据迁移开始时间与当前时间的间隔进行排序，间隔越小，排名越高。</li><li>根据 PodMigrationJob 的 Pod 优先级进行排序，优先级越低，排名越高。</li><li>按照工作负载分散 Jobs，使得同一作业中的 PodMigrationJobs 靠近。</li><li>如果作业中已有 Pods 正在迁移，则该 PodMigrationJob 的排名更高。</li></ul><p style="text-align:justify">筛选过程如下：</p><ul><li>根据工作负载、节点、命名空间等对 PodMigrationJob 进行分组和筛选。</li><li>检查每个工作负载中正在运行状态的 PodMigrationJob 数量，达到一定阈值的将被排除。</li><li>检查每个工作负载中不可用副本的数量是否超出了最大不可用副本数，超出的将被排除。</li><li>检查目标 Pod 所在节点上正在迁移的 Pod 数量是否超过单个节点的最大迁移量，超出的将被排除。</li></ul><span id="OSC_h4_8"></span><h4>6. 冷内存上报</h4><p style="text-align:justify">为提升系统性能，内核一般尽可能不让应用程序请求的页面缓存空闲，而是尽可能将其分配给应用程序。虽然内核分配了这些内存，但是应用可能不再访问，这些内存被称为冷内存。</p><p style="text-align:justify">Koordinator 在 1.4 版本中引入冷内存上报功能，主要为未来冷内存回收功能打下基础。冷内存回收主要用于应对两个场景：</p><ul><li>对于标准的 Kubernetes 集群，当节点内存水位过高时，突发的内存请求容器导致系统直接内存回收，操作系统的直接内存回收触发时会影响已经运行容器的性能，如果回收不及时极端场景可能触发整机 oom。保持节点内存资源的相对空闲，对提升运行时稳定性至关重要。</li><li>在混部场景中，高优先级应用程序请求但未使用的资源可以被低优先级应用程序回收利用。对内存而言，操作系统未回收的内存，是不能被 Koordinator 调度系统看到的。为了提高混部资源效率，回收容器未使用的内存页面可以提高整机的资源利用效率。</li></ul><p style="text-align:justify">Koordlet 在 Collector Plugins 中添加了一个冷页面回收器，用于读取由 kidled（Anolis 内核）、kstaled（Google）或 DAMON（Amazon）导出的 cgroup 文件 memory.idle_stat。该文件包含页面缓存中的冷页面信息，并存在于 memory 的每个层次结构中。目前 koordlet 已经对接了 kidled 冷页面收集器并提供了其他冷页面收集器接口。</p><p style="text-align:justify">在收集冷页面信息后，冷页面回收器将把收集到的指标（例如节点、Pod 和容器的热页面使用量和冷页面大小）存到 metriccache 中，最后该数据会被上报到 NodeMetric CRD 中。</p><p style="text-align:justify">用户可以通过 NodeMetric 启用冷内存回收和配置冷内存收集策略，当前提供了 usageWithHotPageCache、usageWithoutPageCache 和 usageWithPageCache 三种策略，更多的细节详见社区设计文档<strong>[3]</strong>。</p><span id="OSC_h4_9"></span><h4>7. 非容器化应用的 QoS 管理</h4><p style="text-align:justify">在企业容器化过程中，除了已经运行在 K8s 上的应用，可能还会存在一些非容器化的应用运行在主机上。为了更好兼容企业在容器化过程这一过渡态，Koordinator 开发了节点资源预留机制，可以在尚未容器化的应用预留资源并赋予特定的 QoS 特性。与 Kubelet 提供的资源预留配置不同，Koordinator 主要目标是解决这些非容器化应用与容器化应用运行时的 QoS 问题，整体的方案如下图所示：</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-5edba11d4ab8bac0dcef2bc16d12d4f6_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">目前，应用程序需要按照规范将进程启动到对应的 cgroup 中，Koordinator 未实现自动的 cgroup 搬迁工具。针对宿主机非容器化应用，支持 QoS 如下：</p><ul><li><strong>LS (Latency Sensitive)</strong><ul><li>CPU QoS(Group Identity)：应用按照规范将进程运行在 cgroup 的 cpu 子系统中，koordlet 根据 CPU QoS 的配置 resource-qos-config 为其设置 Group Identity 参数；</li><li>CPUSet Allocation：应用按照规范将进程运行在 cgroup 的 cpu 子系统中，koordlet 将为其设置 cpu share pool 中的所有 CPU 核心。</li></ul></li><li><strong>BE (Best-effort)</strong><ul><li>CPU QoS(Group Identity)：应用按照规范将进程运行在 cgroup 的 cpu 子系统中，koordlet 根据 CPU QoS 的配置为其设置 Group Identity 参数。</li></ul></li></ul><p style="text-align:justify">关于宿主机应用 QoS 管理的详细设计，可以参考社区文档<strong>[4]</strong>，后续我们将陆续增加其他 QoS 策略对宿主机应用的支持。</p><span id="OSC_h4_10"></span><h4>8. 其它特性</h4><p style="text-align:justify">除了上述新特性和功能增强外，Koordinator 在 v1.4.0 版本还做了一些如下的 bugfix 和优化：</p><ul><li><strong>RequiredCPUBindPolicy</strong></li></ul><p style="text-align:justify">精细化 CPU 编排支持 Required 的 CPU 绑定策略配置，表示严格按照指定的 CPU 绑定策略分配 CPU，否则调度失败。</p><ul><li><strong>CICD</strong></li></ul><p style="text-align:justify">Koordinator 社区在 v1.4.0 提供了一套 e2e 测试的 Pipeline；提供了 ARM64 镜像。</p><ul><li><strong>Batch 资源计算策略优化</strong></li></ul><p style="text-align:justify">支持了 maxUsageRequest 的计算策略，用于更保守地超卖高优资源；优化了节点上短时间大量 Pod 启停时，Batch allocatable 被低估的问题；完善了对 hostApplication、thirdparty allocatable、dangling pod used 等特殊情况的考虑。</p><ul><li><strong>其它</strong></li></ul><p style="text-align:justify">利用 libpfm4&amp;perf group 优化 CPI 采集、SystemResourceCollector 支持自定义的过期时间配置、BE Pod 支持根据 evictByAllocatable 策略计算 CPU 满足度、Koordlet CPUSetAllocator 修复了对于 LS 和 None Qos 的 Pod 的过滤逻辑、RDT 资源控制支持获得 sandbox 容器的 task IDs 等。</p><p style="text-align:justify">通过 v1.4.0 Release<strong>[5]</strong>页面，可以看到更多包含在 v1.4.0 版本的新增功能。</p><span id="OSC_h3_11"></span><h3>未来计划</h3><p style="text-align:justify">在接下来的版本中，Koordinator 目前规划了以下功能：</p><ul><li><strong>Core Scheduling</strong></li></ul><p style="text-align:justify">在运行时侧，Koordinator 开始探索下一代 CPU QoS 能力，通过利用 Linux Core Scheduling 等内核机制，增强的物理核维度的资源隔离，降低混部的安全性风险，相关工作详见 Issue #1728<strong>[6]</strong>。</p><ul><li><strong>设备联合分配</strong></li></ul><p style="text-align:justify">在 AI 大模型分布式训练场景中，不同机器 GPU 之间通常需要通过高性能网卡相互通信，且 GPU 和高性能网卡就近分配的时候性能更好。Koordinator 正在推进支持多种异构资源的联合分配，目前已经在协议上和调度器分配逻辑上支持联合分配；单机侧关于网卡资源的上报逻辑正在探索中。</p><p style="text-align:justify">更多信息，敬请关注 Milestone v1.5.0<strong>[7]</strong>。</p><span id="OSC_h3_12"></span><h3>结语</h3><p style="text-align:justify">最后，我们十分感谢 Koordinator 社区的所有贡献者和用户，是您们的积极参与和宝贵意见让 Koordinator 不断进步。我们期待您继续提供反馈，并欢迎新的贡献者加入我们的行列。</p><p style="text-align:justify"><strong>相关链接：</strong></p><p style="text-align:justify">[1] 社区官方文档</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fkoordinator.sh%2Fzh-Hans%2Fdocs%2Fnext%2Fdesigns%2Fkoordinator-yarn%2F%253Fspm%253Da2c6h.13046898.publish-article.5.5bb96ffaCJHZTt" target="_blank">https://koordinator.sh/zh-Hans/docs/next/designs/koordinator-yarn/</a></em></u></p><p style="text-align:justify">[2] SPEC CPU</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fwww.spec.org%2Fcpu2017%2F%253Fspm%253Da2c6h.13046898.publish-article.6.5bb96ffaCJHZTt" target="_blank">https://www.spec.org/cpu2017/</a></em></u></p><p style="text-align:justify">[3] 设计文档</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fblob%2Fmain%2Fdocs%2Fproposals%2Fkoordlet%2F20230728-support-cold-memory-compute.md%253Fspm%253Da2c6h.13046898.publish-article.7.5bb96ffaCJHZTt%2526file%253D20230728-support-cold-memory-compute.md" target="_blank">https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/koordlet/20230728-support-cold-memory-compute.md</a></em></u></p><p style="text-align:justify">[4] 社区文档</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fkoordinator.sh%2Fzh-Hans%2Fdocs%2Fnext%2Fuser-manuals%2Fhost-application-qos%2F%253Fspm%253Da2c6h.13046898.publish-article.8.5bb96ffaCJHZTt" target="_blank">https://koordinator.sh/zh-Hans/docs/next/user-manuals/host-application-qos/</a></em></u></p><p style="text-align:justify">[5] v1.4.0 Release</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Freleases%2Ftag%2Fv1.4.0%253Fspm%253Da2c6h.13046898.publish-article.9.5bb96ffaCJHZTt%2526file%253Dv1.4.0" target="_blank">https://github.com/koordinator-sh/koordinator/releases/tag/v1.4.0</a></em></u></p><p style="text-align:justify">[6] Issue #1728</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fissues%2F1728%253Fspm%253Da2c6h.13046898.publish-article.10.5bb96ffaCJHZTt" target="_blank">https://github.com/koordinator-sh/koordinator/issues/1728</a></em></u></p><p style="text-align:justify">[7] Milestone v1.5.0</p><p style="text-align:justify"><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flink.zhihu.com%2F%3Ftarget%3Dhttps%253A%2F%2Fgithub.com%2Fkoordinator-sh%2Fkoordinator%2Fmilestone%2F14%253Fspm%253Da2c6h.13046898.publish-article.11.5bb96ffaCJHZTt" target="_blank">https://github.com/koordinator-sh/koordinator/milestone/14</a></em></u></p><p style="text-align:justify"><em>作者：乔普</em></p><p style="text-align:justify"><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1423263%3Futm_content%3Dg_1000390303" target="_blank">原文链接</a></strong></p><p style="text-align:justify"><strong>本文为阿里云原创内容，未经允许不得转载。</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 06:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/yunqi/blog/11044520</guid>
            <link>https://my.oschina.net/yunqi/blog/11044520</link>
            <author>
                <![CDATA[阿里云云栖号]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源日报 | 目前的人工智能技术连猫的智能水平都没达到]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>欢迎阅读 OSCHINA 编辑部出品的开源日报，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.22</strong></span></h3><h2><strong><span style="color:#16a085">今日要点</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/279713/google-gemma-open-models" target="_blank">谷歌发布轻量级开源大语言模型 Gemma</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Gemma 是一款轻量级、先进的开源模型，供开发者和研究人员用于 AI 构建。Gemma 模型家族包括 2B（20 亿参数）和 7B（70 亿参数）两种尺寸，能够在不同的设备类型上运行，包括笔记本电脑、桌面电脑、IoT 设备、移动设备和云端。</p><p><img src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><h3><a href="https://www.oschina.net/news/279741/nightingale-7-0-0-beta-0-released" target="_blank">夜莺监控 V7 第一个 beta 版本</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">夜莺项目从 2024 开始开发 V7 版本，重点做体验优化，V7 和 V6 版本兼容可以平滑升级<span style="background-color:#ffffff; color:#333333">（V6 升级到 V7 只需要替换一下二进制重启即可，如果是容器部署，只需要更新镜像并重启）</span>，第一个 beta 的优化项包括：</p><ul><li>全站暗黑主题</li><li>优化边缘机房机器失联告警的实现逻辑，真正做到边缘机房告警自闭环</li><li>优化内置大盘、内置告警规则的列表页面 UI</li><li>全局回调地址页面展示优化，增加详尽的文档提示信息</li></ul><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="20240221141801" src="https://download.flashcat.cloud/ulric/20240221141801.png" referrerpolicy="no-referrer"></p><hr><h2><strong><span style="color:#16a085">今日观察</span></strong></h2><p><img height="554" src="https://oscimg.oschina.net/oscnet/up-14e1acb77ab8633d225b3117ebbc7dc7920.png" width="1522" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博<span>&nbsp;</span></span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1233486457%2FO1MdFaqZm" target="_blank">高飞</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-a2ff0599a45c140a6bf468a7d85524102fa.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博<span>&nbsp;</span></span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F1856404484%2FO1JFSAEg0" target="_blank">凤凰网科技</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推荐</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-da11d19579f9cc14d57a12c12f4479f067a.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>开源之声</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-33430582a60711cf56a923991e1fef04da9.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日项目榜</strong></span></h2><p>GitHub Trending</p><p><img src="https://oscimg.oschina.net/oscnet/up-291d0213846cc3efbd09b526da6bac5ae29.png" referrerpolicy="no-referrer"></p><blockquote><h4><strong><span style="background-color:#e67e22">在线阅读完整日报内容，访问：</span></strong><br><u><em><strong><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/s7n800w84o6guyv/014_kyezhNxOGD.pdf">开源日报第 014 期：目前的人工智能技术连猫的智能水平都没达到</a></strong></em></u></h4></blockquote><hr><p><strong>往期回顾</strong></p><ul><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC013%E6%9C%9F%EF%BC%9A%E7%AD%89%E5%88%B0%20Sora%20%E5%BC%80%E6%BA%90%E4%BA%86%E7%AB%8B%E5%88%BB%E6%8E%A8%E5%87%BA%E5%B1%9E%E4%BA%8E%E6%88%91%E4%BB%AC%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B.pdf">开源日报第 013 期：等到 Sora 开源了立刻推出属于我们自己的大模型</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">开源日报第 012 期：Sora 给中国 AI 带来的真实变化；Dart 3.3 发布</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">开源日报第 011 期：目前还没有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">开源日报第 010 期：Tauri v2 支持 Android 和 iOS，跨平台开发新选择</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">开源日报第 009 期：Vue.js 诞生 10 周年；扎克伯格解释 Meta 为什么要开源其 AI 技术</a></li><li><a href="https://www.oschina.net/news/277585">开源日报第 008 期：推动中国开源软硬件发展的经验与建议</a></li><li><a href="https://www.oschina.net/news/277415">开源日报第 007 期：「Linux 中国」 开源社区宣布停止运营</a></li><li><a href="https://www.oschina.net/news/277214">开源日报第 006 期：选择技术栈一定要选择开源的</a></li><li><a href="http://www.oschina.net/news/277040">开源日报第 005 期：RISC-V 万兆开源交换机发售；npm 存在大量武林外传视频</a></li><li><a href="https://www.oschina.net/news/276864">开源日报第 004 期：百度输入法在候选词区域植入广告；大神用 Excel 构建 CPU</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 03:52:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279923</guid>
            <link>https://www.oschina.net/news/279923</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Altman 回应 7 万亿美元半导体计划：所需投资远超想象]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">在英伟达发布了强劲的 2024 财年第四季度财报之后的几小时，英特尔首席执行官 Pat Gelsinger 和 OpenAI 首席执行官 Sam Altman 在加利福尼亚州圣何塞的一个会议中心展开了一场对话，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fapnews.com%2Farticle%2Fintel-openai-nvidia-chips-boom-dbf20077caafc9b33870f9f6d32d3794" target="_blank">畅谈</a>半导体在 AI 时代塑造社会所扮演的角色。</span></p><p><span style="color:#000000"><img alt="" height="375" src="https://oscimg.oschina.net/oscnet/up-67bd113ff9160357624584468c53d04258a.webp" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">在活动中，Gelsinger 询问了 Altman 有关最近报道的计划从中东地区筹集高达 7 万亿美元的资金，以支持 OpenAI 的一项半导体计划，并与英伟达展开竞争的传闻。对此，Altman 则反驳称，这种匿名人士未经证实的说法比比皆是，「我的主要工作不是到处修正这些错误的文章」。</span></p><p><span style="color:#000000">但 Altman 同时也承认，AI 的发展需要大量的资金。「事实是，我们认为世界将需要更多的 AI 计算（芯片）。这将需要全球范围内大量的投入，超出我们的想象。我们现在还没有一个具体数字。」</span></p><p><span style="color:#000000">他还强调了过去一年加快人工智能发展的重要性。他认为人工智能的进步将为人类带来更美好的未来，不过奥特曼也承认，在前进的过程中会有不利的一面。"我们正在走向这样一个世界：人工智能生成的内容将多于人类生成的内容。这将不仅仅是一个 good story，而是一个 net good story。"</span></p><p><span style="color:#000000">此外，Altman 重申了在 AI 领域进行监管的必要性，强调了政府在制定框架以降低潜在风险方面的关键作用。</span></p><p><span style="color:#000000">Gelsinger 预测，</span><span style="background-color:#ffffff; color:#000000">到 2030 年，英特尔将成为全球第二大代工企业，仅次于台积电。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 03:12:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279911/intel-openai-chips</guid>
            <link>https://www.oschina.net/news/279911/intel-openai-chips</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Apache ECharts 5.5.0 引入服务端渲染的新利器：1KB 的客户端轻量运行时]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Apache ECharts 5.5.0 版本已于 2024.2.18 正式发布。</p><p><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fecharts%2Freleases%2Ftag%2F5.5.0" target="_blank">https://github.com/apache/echarts/releases/tag/5.5.0</a></u></em></p><p><strong>主要变化</strong></p><ul><li><p>增强了代码的 ESM 识别，对&nbsp;Node.js&nbsp;环境开发更加友好；</p></li><li><p>为服务端渲染方案提供了一个 gzip 后仅 1KB 的轻量运行时，极大地降低了加载时间；</p></li><li><p>为数据下钻支持了过渡动画，开发者可以方便地实现多级数据的动画效果；</p></li><li><p>为饼图和极座标系图表增加了更多配置项，可以实现更丰富的样式；</p></li><li><p>新增阿拉伯语和荷兰语两种语言的翻译</p></li><li><p>……</p></li></ul><p>以下内容转自：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FIpGQS1GspyXzNe-u9F4B-A" target="_blank">https://mp.weixin.qq.com/s/IpGQS1GspyXzNe-u9F4B-A</a></u></em></p><hr><h2>增强的 ESM 支持</h2><p>为了让开发者在测试和 Node.js 环境使用更方便，我们在这个版本中对 ESM 的识别问题进行了优化。</p><p>以前，ECharts 只在 npm（npm 包的 lib 目录中）导出&nbsp;<code>*.esm</code>&nbsp;文件。虽然这在 bundlers 环境表现良好，但 Node.js 环境和一些基于 Node.js 的测试框架（如 vitest 和 jest）中的表现并不理想。</p><p>有了这个新功能，我们做了几个改变以改善这个问题：</p><ul><li><p>在&nbsp;<code>package.json</code>&nbsp;中添加了&nbsp;<code>"type": "module"</code></p></li><li><p>在&nbsp;<code>package.json</code>&nbsp;中添加了&nbsp;<code>"exports": {...}</code></p></li><li><p>在子目录中添加了一些只包含&nbsp;<code>"type": "commonjs"</code>&nbsp;的&nbsp;<code>package.json</code>&nbsp;文件</p></li></ul><p>这些改变意味着，像&nbsp;<code>echarts/core.js</code>&nbsp;这样的文件现在可以在像纯 Node.js、vitest、jest 和 create-react-app 这样的环境中解析为 ESM。</p><p>我们还确保了这个新功能与各种环境兼容，包括运行时（Node.js/vitest/jest（create-react-app）/ssr/…）和打包器（webpack/rollup/vite/esbuild/…）。</p><p>我们非常期待这一新功能，并相信它将极大地改善开发者的体验。</p><h2>服务端渲染 + 客户端轻量运行时</h2><p>Apache ECharts 功能强大，相应地，包体积也比较大。我们在之前的版本中也做了各种努力来改进这一点。开发者可以使用 TreeShaking 按需加载部分代码，以减少加载的代码量。从 Apache ECharts 5.3 版本起，我们支持了零依赖的服务端 SVG 字符串渲染方案，并支持图表的初始动画。这样，使用服务端渲染的结果作为首屏渲染的画面，可以大大减少首屏加载时间。</p><p>服务端渲染虽然是一种很有效减少包体积的解决方案，但如果需要在客户端实现一些交互，那么不得不仍旧加载 echarts.js，这可能会增加更多的加载时间。对于一些对页面加载速度要求较高的场景，这可能不是一个理想的选择。</p><p><strong>在 5.5.0 版本中，我们新增了客户端轻量运行时</strong>，客户端无需加载完整 ECharts 即可实现部分交互。这样，我们可以在服务端渲染图表，然后在客户端加载轻量运行时，实现一些常见的交互。这意味着，<strong>只需要加载&nbsp;4KB 的轻量运行时（gzip 后 1KB），即可实现带初始动画和部分常用交互形式的图表</strong>。这一改进将极大地提升页面加载速度，特别是对于移动端的体验。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-5c9857523986e92155d29f453c6307014c1.png" referrerpolicy="no-referrer"></p><p>以这个带标题的饼图为例，如果按客户端仅打包饼图和标题组件的方案，gzip 后需要 135KB；如果按服务端渲染的方案，渲染结果 SVG gzip 后 1 KB、客户端运行时 gzip 后 1KB，仅为前者体积的 1.5%。交互方面，后者也可以做到初始动画、鼠标移动到图表元素后的高亮，并且获取到点击事件，能够满足大部分的常见交互需求。</p><p>如需使用客户端轻量运行时方案，服务端代码和之前一样，但需要保证 ECharts 版本号在 5.5.0 以上。</p><pre><code>//&nbsp;服务端代码
const&nbsp;echarts&nbsp;=&nbsp;require('echarts');

//&nbsp;在&nbsp;SSR&nbsp;模式下第一个参数不需要再传入&nbsp;DOM&nbsp;对象
const&nbsp;chart&nbsp;=&nbsp;echarts.init(null,&nbsp;null,&nbsp;{
&nbsp;&nbsp;renderer:&nbsp;'svg',&nbsp;//&nbsp;必须使用&nbsp;SVG&nbsp;模式
&nbsp;&nbsp;ssr:&nbsp;true,&nbsp;//&nbsp;开启&nbsp;SSR
&nbsp;&nbsp;width:&nbsp;400,&nbsp;//&nbsp;需要指明高和宽，如果是根据客户端容器大小动态的，该值需要从客户端得到
&nbsp;&nbsp;height:&nbsp;300
});

//&nbsp;像正常使用一样&nbsp;setOption
chart.setOption({
&nbsp;&nbsp;//...
});

//&nbsp;输出字符串
const&nbsp;svgStr&nbsp;=&nbsp;chart.renderToSVGString();

//&nbsp;调用&nbsp;dispose&nbsp;以释放内存
chart.dispose();
chart&nbsp;=&nbsp;null;

//&nbsp;通过 HTTP Response 返回 svgStr 给前端或者缓存到本地（这里以 Express.js 为例）：
res.writeHead(200,&nbsp;{
&nbsp;&nbsp;'Content-Type':&nbsp;'application/xml'
});
res.write(svgStr);
res.end();

</code></pre><p>客户端将得到的 SVG 字符串添加到容器中，并绑定轻量运行时：</p><pre><code>&lt;div&nbsp;id="chart-container"&nbsp;style="width:800px;height:600px"&gt;&lt;/div&gt;

&lt;script&nbsp;src="https://cdn.jsdelivr.net/npm/echarts/ssr/client/dist/index.js"&gt;&lt;/script&gt;
&lt;script&gt;
const&nbsp;ssrClient&nbsp;=&nbsp;window['echarts-ssr-client'];

let&nbsp;isSeriesShown&nbsp;=&nbsp;{
&nbsp;&nbsp;a:&nbsp;true,
&nbsp;&nbsp;b:&nbsp;true
};

function&nbsp;updateChart(svgStr)&nbsp;{
&nbsp;&nbsp;const&nbsp;container&nbsp;=&nbsp;document.getElementById('chart-container');
&nbsp;&nbsp;container.innerHTML&nbsp;=&nbsp;svgStr;

&nbsp;&nbsp;//&nbsp;使用轻量运行时赋予图表交互能力
&nbsp;&nbsp;ssrClient.hydrate(main,&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;on:&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;click:&nbsp;(params)&nbsp;=&gt;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(params.ssrType&nbsp;===&nbsp;'legend')&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;点击图例元素，请求服务器进行二次渲染
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isSeriesShown[params.seriesName]&nbsp;=&nbsp;!isSeriesShown[params.seriesName];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fetch('...?series='&nbsp;+&nbsp;JSON.stringify(isSeriesShown))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.then(res&nbsp;=&gt;&nbsp;res.text())
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.then(svgStr&nbsp;=&gt;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;updateChart(svgStr);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;});
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;});
}

//&nbsp;通过&nbsp;AJAX&nbsp;请求获取服务端渲染的&nbsp;SVG&nbsp;字符串
fetch('...')
&nbsp;&nbsp;.then(res&nbsp;=&gt;&nbsp;res.text())
&nbsp;&nbsp;.then(svgStr&nbsp;=&gt;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;updateChart(svgStr);
&nbsp;&nbsp;});
&lt;/script&gt;
</code></pre><p>客户端轻量运行时必须配合 SVG 形式的服务端渲染结果使用，支持以下交互：</p><ul><li><p>图表初始动画（实现原理：服务端渲染的 SVG 带有 CSS 动画）</p></li><li><p>高亮样式（实现原理：服务端渲染的 SVG 带有 CSS 动画）</p></li><li><p>动态改变数据（实现原理：轻量运行时请求服务器进行二次渲染）</p></li><li><p>点击图例切换系列是否显示（实现原理：轻量运行时请求服务器进行二次渲染）</p></li></ul><p>可以发现，这能够满足大部分的交互场景需求。如果需要更复杂的交互，则客户端需要加载&nbsp;<code>echarts.js</code>&nbsp;实现完整功能。</p><p>完整的介绍请参见官网使用手册的「应用篇 - 跨平台方案 - 服务端渲染」。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279907/echarts-5-5-0</guid>
            <link>https://www.oschina.net/news/279907/echarts-5-5-0</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[和 Gmail 基本 HTML 视图说再见]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌已开始<u><a href="https://www.oschina.net/news/259383">停止支持 Gmail </a><a href="https://www.oschina.net/news/259383">基本 HTML 视图</a></u>。自 2024 年 2 月起，Gmail 会自动将用户从基本 HTML 视图转换为标准视图。</p><p><img src="https://oscimg.oschina.net/oscnet/up-0cdd4e28b49bf3c37718b40baee96da1d75.png" referrerpolicy="no-referrer"></p><p><em>基本 HTML 视图允许用户以简陋的方式查看电子邮件，但对所有的浏览器提供了最大的兼容性。</em></p><p>Gmail 更新了其<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fsupport.google.com%2Fmail%2Fanswer%2F15049%3Fhl%3Dzh-Hans" target="_blank">支持页面</a>，以反映 Gmail 将在截止日期后自动切换到标准视图。不仅如此，有用户在<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnews.ycombinator.com%2Fitem%3Fid%3D37558372" target="_blank">Hacker News</a>上发帖称，他们收到了一封来自 Google 的邮件，表明该功能已经结束。</p><blockquote><p>"我们写信通知您，从 2024 年 1 月初开始，桌面网页和移动网页的 Gmail Basic HTML 视图将被禁用。Gmail Basic HTML 视图是 Gmail 的旧版本，10 多年前就已被现代版本取代，不包含完整的 Gmail 功能。"</p></blockquote><p>即使在今天，当你尝试访问 HTML 版本时，Google 也会显示一条信息，称该版本是为"较慢的连接速度和传统浏览器"设计的，并要求你确认是否不想使用标准版本。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-cca03ff735c3d625769511865800215a87d.png" referrerpolicy="no-referrer"></p><p>HTML 版本缺少很多功能，如聊天、拼写检查、搜索过滤器、键盘快捷键和丰富的格式。但在连接性较差的地区或只想浏览电子邮件而不想使用任何额外功能的情况下，HTML 版还是很有用的。目前还不清楚 Google 是否计划添加低连接模式。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:40:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279900/goodbye-html-gmail</guid>
            <link>https://www.oschina.net/news/279900/goodbye-html-gmail</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[记一次 Rust 内存泄漏排查之旅]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>在某次持续压测过程中，我们发现 GreptimeDB 的 Frontend 节点内存即使在请求量平稳的阶段也在持续上涨，直至被 OOM kill。我们判断 Frontend 应该是有内存泄漏了，于是开启了排查内存泄漏之旅。</p><h2>Heap Profiling</h2><p>大型项目几乎不可能只通过看代码就能找到内存泄漏的地方。所以我们首先要对程序的内存用量做统计分析。幸运的是，GreptimeDB 使用的 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjemalloc%2Fjemalloc%2Fwiki%2FUse-Case%253A-Heap-Profiling" target="_blank">jemalloc 自带 heap profiling</a>，我们也<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fblob%2Fdevelop%2Fsrc%2Fcommon%2Fmem-prof%2FREADME.md" target="_blank">支持了导出 jemalloc 的 profile dump 文件</a>。于是我们在 GreptimeDB 的 Frontend 节点内存达到 300MB 和 800MB 时，分别 dump 出了其内存 profile 文件，再用 jemalloc 自带的 <code>jeprof</code> 分析两者内存差异（<code>--base</code> 参数），最后用火焰图显示出来：</p><p><img src="https://oscimg.oschina.net/oscnet/up-002154d38e6da2e50485895918972b1b8a1.png" alt="" referrerpolicy="no-referrer"></p><p>显然图片中间那一大长块就是不断增长的 500MB 内存占用了。仔细观察，居然有 thread 相关的 stack trace。难道是创建了太多线程？简单用 <code>ps -T -p</code> 命令看了几次 Frontend 节点的进程，线程数稳定在 84 个，而且都是预知的会创建的线程。所以「线程太多」这个原因可以排除。</p><p>再继续往下看，我们发现了很多 Tokio runtime 相关的 stack trace，而 Tokio 的 task 泄漏也是常见的一种内存泄漏。这个时候我们就要祭出另一个神器：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ftokio-rs%2Fconsole" target="_blank">Tokio-console</a>。</p><h2>Tokio Console</h2><p>Tokio Console 是 Tokio 官方的诊断工具，输出结果如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-1772a99dbc1b17d9b0ab49532e083a4234f.png" alt="" referrerpolicy="no-referrer"></p><p>我们看到居然有 5559 个正在运行的 task，且绝大多数都是 Idle 状态！于是我们可以确定，内存泄漏发生在 Tokio 的 task 上。 现在问题就变成了：GreptimeDB 的代码里，哪里 spawn 了那么多的无法结束的 Tokio task？</p><p>从上图的 "Location" 列我们可以看到 task 被 spawn 的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fblob%2Fdevelop%2Fsrc%2Fcommon%2Fruntime%2Fsrc%2Fruntime.rs%23L63" target="_blank">地方</a>：</p><pre><code class="language-rust">impl Runtime {
    /// Spawn a future and execute it in this thread pool
    ///
    /// Similar to Tokio::runtime::Runtime::spawn()
    pub fn spawn&lt;F&gt;(&amp;self, future: F) -&gt; JoinHandle&lt;F::Output&gt;
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        self.handle.spawn(future)
    }
}
</code></pre><p>接下来的任务是找到 GreptimeDB 里所有调用这个方法的代码。</p><h2><code>..Default::default()</code>！</h2><p>经过一番看代码的仔细排查，我们终于定位到了 Tokio task 泄漏的地方，并在 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb%2Fpull%2F1512" target="_blank">PR #1512</a> 中修复了这个泄漏。简单地说，就是我们在某个会被经常创建的 struct 的构造方法中，spawn 了一个可以在后台持续运行的 Tokio task，却未能及时回收它。对于资源管理来说，在构造方法中创建 task 本身并不是问题，只要在 <code>Drop</code> 中能够顺利终止这个 task 即可。而我们的内存泄漏就坏在忽视了这个约定。</p><p>这个构造方法同时在该 struct 的 <code>Default::default()</code> 方法当中被调用了，更增加了我们找到根因的难度。</p><p>Rust 有一个很方便的，可以用另一个 struct 来构造自己 struct 的方法，即 "<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdoc.rust-lang.org%2Fbook%2Fch05-01-defining-structs.html%23creating-instances-from-other-instances-with-struct-update-syntax" target="_blank">Struct Update Syntax</a>"。如果 struct 实现了 <code>Default</code>，我们可以简单的在 struct 的 field 构造中使用 <code>..Default::default()</code>。如果 <code>Default::default()</code> 内部有 「side effect」（比如我们本次内存泄漏的原因——创建了一个后台运行的 Tokio task），一定要特别注意：struct 构造完成后，<code>Default</code> 创建出来的临时 struct 就被丢弃了，一定要做好资源回收。</p><p>例如下面这个小例子：（<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fplay.rust-lang.org%2F%3Fversion%3Dstable%26mode%3Ddebug%26edition%3D2021%26gist%3Dc121ffd32d2ff0fa8e1241a62809bcef" target="_blank">Rust Playground</a>）</p><pre><code class="language-rust">struct A {
    i: i32,
}

impl Default for A {
    fn default() -&gt; Self {
        println!("called A::default()");
        A { i: 42 }
    }
}

#[derive(Default)]
struct B {
    a: A,
    i: i32,
}

impl B {
    fn new(a: A) -&gt; Self {
        B {
            a,
            // A::default() is called in B::default(), even though "a" is provided here.
            ..Default::default()
        }
    }
}

fn main() {
    let a = A { i: 1 };
    let b = B::new(a);
    println!("{}", b.a.i);
}
</code></pre><p>struct A 的 <code>default</code> 方法是会被调用的，打印出 <code>called A::default()</code>。</p><h2>总结</h2><ul><li>排查 Rust 程序的内存泄漏，我们可以用 jemalloc 的 heap profiling 导出 dump 文件；再生成火焰图可直观展现内存使用情况。</li><li>Tokio-console 可以方便地显示出 Tokio runtime 的 task 运行情况；要特别注意不断增长的 idle tasks。</li><li>尽量不要在常用 struct 的构造方法中留下有副作用的代码。</li><li><code>Default</code> 只应该用于值类型 struct。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-3d33a6c61f7c37725020cbe267d0c2d0f01.jpg" alt="" referrerpolicy="no-referrer"></p><h3>关于 Greptime</h3><p>Greptime 格睿科技于 2022 年创立，目前正在完善和打造时序数据库 GreptimeDB 和格睿云 GreptimeCloud 这两款产品。</p><p>GreptimeDB 是一款用 Rust 语言编写的时序数据库，具有分布式、开源、云原生、兼容性强等特点，帮助企业实时读写、处理和分析时序数据的同时，降低长期存储的成本。</p><p>GreptimeCloud 基于开源的 GreptimeDB，为用户提供全托管的 DBaaS，以及与可观测性、物联网等领域结合的应用产品。利用云提供软件和服务，可以达到快速的自助开通和交付，标准化的运维支持，和更好的资源弹性。GreptimeCloud 已正式开放内测，欢迎关注公众号或官网了解最新动态！</p><p>官网：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreptime.com%2F" target="_blank">https://greptime.com/</a></p><p>公众号：GreptimeDB</p><p>GitHub: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGreptimeTeam%2Fgreptimedb" target="_blank">https://github.com/GreptimeTeam/greptimedb</a></p><p>文档：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.greptime.com%2F" target="_blank">https://docs.greptime.com/</a></p><p>Twitter: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2FGreptime" target="_blank">https://twitter.com/Greptime</a></p><p>Slack: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgreptime.com%2Fslack" target="_blank">https://greptime.com/slack</a></p><p>LinkedIn: <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fgreptime%2F" target="_blank">https://www.linkedin.com/company/greptime/</a></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:23:03 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6839317/blog/11044292</guid>
            <link>https://my.oschina.net/u/6839317/blog/11044292</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Stability AI 发布 Stable Diffusion 3 早期预览版]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>AI 创业公司 Stability AI <u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fnews%2Fstable-diffusion-3" target="_blank">宣布</a></u>其最新一代的文本图像模型 Stable Diffusion 3 开放预览，该版本目前仅限部分用户参与测试，主要是为了在正式发布前收集与性能和安全性相关的用户反馈。感兴趣的用户可以申请加入等候名单。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-3fa2fd390f1fbdd7808ee91a4ed1a0ef4fd.png" referrerpolicy="no-referrer"></p><p>Stable Diffusion 3 早期预览版相比前代产品在图片质量、多主题展示和文字展示方面有大幅提升。Stable Diffusion 3 模型的参数规模从 8 亿，到 80 亿不等，其架构组合了 diffusion transformer（扩散变换架构）和 flow matching（流匹配），技术报告将在晚些时候公布。</p><p><img height="582" src="https://oscimg.oschina.net/oscnet/up-25784ed1b6a0b80ca5fc50b3c842456064f.png" width="3052" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-377f7b831888b11ab490f50cafe8931753d.png" referrerpolicy="no-referrer"></p><p>性能的具体提升内容包括：</p><ol><li>多主题提示处理能力： 新模型对于包含多个主题或元素的提示具有更好的理解和处理能力。这意味着用户可以在一个提示中描述更复杂的场景，而模型能够更准确地根据这些描述生成图像。</li><li>图像质量： Stable Diffusion 3 在生成的图像质量上有显著提高，包括更细腻的细节表现、更准确的颜色匹配以及更自然的光影处理。这些改进使得生成的图像更加逼真，更能捕捉到用户的创意意图。</li><li>拼写和文本处理能力： 这个版本在处理文本元素，尤其是在图像中直接展现的文本（如标语、标签等）时，有更好的拼写能力和文本理解。这包括更准确地识别和渲染用户提示中的文字，甚至是在复杂的视觉背景中。</li></ol><p>Stable Diffusion 3 的性能提升不仅基于其先进的扩散变换架构，还包括了以下关键的技术创新和改进：</p><ol><li>新型扩散变换器： Stable Diffusion 3 采用了一种新型的扩散变换技术，与 Sora 类似，这种新技术为模型提供了更强大的图像生成能力。 Transformer 是一种深度学习模型，专门设计来逐步构建图像的细节，从而生成高质量的视觉内容。</li><li>流匹配与其他改进： 模型还整合了流匹配技术和其他技术改进，进一步增强了生成图像的质量和多样性。流匹配技术有助于模型更好地理解和模拟图像中的动态元素和结构，使得生成的图像在视觉上更加连贯和自然。</li><li>利用 Transformer 的改进： Stable Diffusion 3 充分利用了 Transformer 技术的最新进展，这不仅使模型能够进一步扩展其能力，还使其能够接受多模态输入。这意味着模型能够处理更复杂和多样化的数据类型，如结合文本和图像的输入，从而在理解和生成图像内容方面提供更大的灵活性和精确度。</li></ol><p>加入等候名单：<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstability.ai%2Fstablediffusion3" target="_blank">https://stability.ai/stablediffusion3</a></em></u></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 23 Feb 2024 02:12:30 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279886/stable-diffusion-3-preview</guid>
            <link>https://www.oschina.net/news/279886/stable-diffusion-3-preview</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[「未来产业划定发展路线图」出炉]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span>近日，工业和信息化部、科技部、交通运输部、文化和旅游部等部门</span><span>联合印发</span><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMjM5OTUwMTc2OA%3D%3D%26mid%3D2650903280%26idx%3D1%26sn%3Db6edd0ce1c41aac431cd9f9a52eef8be%26chksm%3Dbccfb8578bb831412b6e96f801604afe81effbd68c11d7371507f00349b1b3accc4cc18e01b4%26scene%3D21%23wechat_redirect" target="_blank">《关于推动未来产业创新发展的实施意见》</a><span>，提出到 2025 年，未来产业技术创新、产业培育、安全治理等全面发展，部分领域达到国际先进水平，产业规模稳步提升；到 2027 年，未来产业综合实力显著提升，部分领域实现全球引领。</span></p><p>专家认为，《意见》充分把握全球科技创新和产业发展趋势，前瞻部署了生物制造、量子信息、氢能、核能、基因和细胞技术等多个细分赛道，将全面支撑推进新型工业化，加快形成新质生产力。</p><p style="margin-left:0; margin-right:0"><strong><span>全面布局新赛道</span></strong></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>未来产业由前沿技术驱动，尚处于孕育萌发阶段或产业化初期，是具有显著战略性、引领性、颠覆性和不确定性的前瞻性新兴产业。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">当前，新一轮科技革命和产业变革加速演进，重大前沿技术、颠覆性技术持续涌现，科技创新和产业发展融合不断加深，催生出元宇宙、人形机器人、脑机接口、量子信息等新产业发展方向。大力培育未来产业已成为引领科技进步、带动产业升级、开辟新赛道、塑造新质生产力的战略选择。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">我国具备工业体系完整、产业规模庞大、应用场景丰富等综合优势，为未来产业发展提供了丰厚的土壤。各省（区、市）积极培育未来产业，北京、上海、江苏、浙江等地出台了培育未来产业的政策文件。但我国未来产业发展也面临系统谋划不足、技术底座不牢等问题。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">针对这些问题，《意见》从技术创新、产品突破、企业培育、场景开拓、产业竞争力等方面提出到 2025 年和 2027 年的发展目标。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">赛迪研究院未来产业研究中心所长韩健介绍，到 2025 年要形成「一批+6 百」的目标体系，建设一批未来产业孵化器和先导区，突破百项前沿关键核心技术，形成百项标志性产品，打造百家领军企业，开拓百项典型应用场景，制定百项关键标准，培育百家专业服务机构，初步形成符合我国实际的未来产业发展模式。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">《意见》重在产业化落地。赛智产业研究院院长赵刚认为，《意见》提出以传统产业的高端化升级和前沿技术的产业化落地为主线，力争做到两年「打基础」，五年「大提升」，成为世界未来产业重要策源地。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">此外，《意见》还详细规划了六大方向超过 50 多个细分领域的未来产业发展，明确提出了下一代智能终端、信息服务产品、未来高端装备三类标志性产品发展路线。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">「设定未来产业发展目标既是我国推进新型工业化的自身现实需求，也是参与国际竞争的外部形势要求。从自身需求看，是我国引领科技进步、带动产业升级、培育新质生产力的战略选择；从外部需求看，是我国主动参与全球未来产业分工合作、深度融入全球创新网络的必然选择。」赵刚说。</p><p style="margin-left:0; margin-right:0"><strong><span>重点瞄准六大方向</span></strong></p><p><span>未来产业发展的核心是前沿技术创新突破。《意见》按照「技术创新—前瞻识别—成果转化」的思路，提出面向未来制造、未来信息、未来材料、未来能源、未来空间、未来健康六大重点方向，实施国家科技重大项目和重大科技攻关，发挥国家实验室、全国重点实验室等创新载体作用，鼓励龙头企业牵头成立创新联合体，体系化推进关键核心技术攻关。</span></p><p>赵刚分析，与优势产业、传统产业、战略性新兴产业相比，未来产业有 3 个明显特征。未来产业技术创新不是渐进式微创新，而是前瞻性、颠覆性重大创新，例如未来信息产业中的通用人工智能和量子信息、未来健康产业中的基因工程、未来材料产业中的超导材料等技术创新；未来产业生产要素配置不是传统要素线性叠加，而是现代要素相互融合和配置效率指数级提升，例如量子计算机能让计算能力实现成千上万倍增加；未来产业边界不是界限清晰，而是呈现出不同产业跨界融合和智能化、绿色化等发展特征，如智能制造、生物材料、人形机器人、脑机接口等。</p><p>对于这六大方向业内已有布局。早在 2016 年，字节跳动公司就成立了人工智能实验室，聚焦研究自然语言处理、机器学习、数据挖掘等方面。2023 年以来，字节跳动公司加码人工智能应用研究，旗下产品不断加入 AIGC（生成式人工智能）功能。比如，结合火山引擎智能创作云的 AIGC 能力，火山引擎视频云在商品营销、互动娱乐、在线教育、智能驾驶等场景引入数字人、虚拟直播间等，助力企业降本增效，提升用户体验。</p><p>「技术创新是经济长期持续增长的不竭动力，发展未来产业是高质量发展的前瞻性战略布局。今天对未来产业 20% 的投入和布局，将为以后带来 80% 的收益，从而建立起我国经济高质量发展的长效创新机制。」赵刚说。</p><p style="margin-left:0; margin-right:0"><strong><span>打造标志性创新产品</span></strong></p><p><span>《意见》提出，打造人形机器人、脑机接口、超大规模新型智算中心、第三代互联网等十大创新标志性产品。</span></p><p>赵刚分析，当颠覆式技术创新呈现出技术性能成倍提升、产品化成本大幅降低、应用场景广泛等特征后，创新产品就形成规模经济效应，具有巨大的市场前景。</p><p>当前，满足这 3 个特征的标志性产品主要有两类。一是通用人工智能产品。由于以 ChatGPT 为代表的通用人工智能技术取得重大进展，围绕通用人工智能技术创新形成的智能产品，如生成式人工智能产品、AI 手机和个人计算机、人形机器人、高级别智能网联汽车、智能装备、智能云服务、超大规模新型智算中心等智能产品和服务就具有较好前景。二是生物科技产品。由于细胞和基因工程等技术取得突破性进展，生物科技创新产品工程化能力加速提升，具有很好的市场前景，如基因编辑、合成生物等。其他一些前瞻性技术尽管在实验室获得了成功，但离大规模产品化和商业化还有很大差距，例如量子信息技术创新。</p><p>国际数据公司 IDC 预测，人工智能电脑在中国个人计算机市场中新机的装配比例将快速攀升，2027 年有望达 85%，成为市场主流。联想集团副总裁、中国区战略及业务拓展副总裁阿不力克木·阿不力米提表示，人工智能电脑是自然语言交互的个人 AI 助理。在过去 40 年发展历程中，联想不断推出变革用户体验的产品，未来还将和生态伙伴携手实现人工智能电脑快速普及，让 AI 惠及每一个人。</p><p>目前我国算力总规模排名全球第二位。但从结构看，通用算力占了大半，高性能算力占比有待提升。浪潮信息高级副总裁刘军表示，高质量算力采用先进的计算架构，具备高算效、高能效、可持续、可获得、可评估五大特征。其中，高算效是实测性能与资源利用率双重提升，是算力供需失衡、算力利用率低等矛盾的破解之道。而高能效是在最低碳排放前提下实现最大化算力输出，确保能源利用最优解。</p><p>脑机接口作为十大标志性产品之一，近年来在电极、算法、芯片等方面取得了重要进展。2023 年 9 月，中国信息通信研究院云计算与大数据研究所牵头在人工智能医疗器械创新合作平台成立脑机接口研究工作组。</p><p>中国信通院云计算与大数据研究所副所长闵栋介绍，脑机接口可应用于医疗、娱乐、智能生活、教育等领域。其中，医疗领域是主要阵地。脑机接口与医疗结合展现出广阔应用前景，为相关疾病诊疗和康复提供了全新手段。此外，脑机接口还可与虚拟现实、人机交互、人工智能等技术结合推动现有产业变革，如脑机接口应用于工业领域，可帮助人们通过意念操控机器人、无人车、工业产线等设备。</p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify">未来产业潜在价值巨大，需要资本持续投入。赵刚建议，要推动制造业转型升级基金、国家中小企业发展基金等加大投入，也可适时组建国家未来产业发展基金，并引导地方设立未来产业专项资金，发挥政府引导基金的引导性作用，吸引社会资本共同投资未来产业。同时，完善金融财税支持政策。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 05:58:15 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279756</guid>
            <link>https://www.oschina.net/news/279756</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[夜莺监控 V7 第一个 beta 版本来了]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>夜莺项目从 2024 开始开发 V7 版本，重点做体验优化，V7 和 V6 版本兼容可以平滑升级<span style="background-color:#ffffff; color:#333333">（V6 升级到 V7 只需要替换一下二进制重启即可，如果是容器部署，只需要更新镜像并重启）</span>，今天发布第一个 beta，此 beta 版本的优化项包括：</p><ul><li>全站暗黑主题</li><li>优化边缘机房机器失联告警的实现逻辑，真正做到边缘机房告警自闭环</li><li>优化内置大盘、内置告警规则的列表页面 UI</li><li>全局回调地址页面展示优化，增加详尽的文档提示信息</li></ul><p><span style="background-color:#ffffff; color:#333333">更多优化项正在开发中，V5、V6 用户可以放心升级，V7 会是一个更好的版本。升级之前记得备份以防万一。</span></p><h2>项目介绍</h2><p style="color:#333333; text-align:left">夜莺监控是一款开源云原生观测分析工具，采用 All-in-One 的设计理念，集数据采集、可视化、监控告警、数据分析于一体，与云原生生态紧密集成，提供开箱即用的企业级监控分析和告警能力。夜莺于 2020 年 3 月 20 日，在 github 上发布 v1 版本，已累计迭代 100 多个版本。</p><p style="color:#333333; text-align:left">夜莺最初由滴滴开发和开源，并于 2022 年 5 月 11 日，捐赠予中国计算机学会开源发展委员会（CCF ODC），为 CCF ODC 成立后接受捐赠的第一个开源项目。夜莺的核心研发团队，也是 Open-Falcon 项目原核心研发人员，从 2014 年（Open-Falcon 是 2014 年开源）算起来，也有 10 年了，只为把监控这个事情做好。</p><h2>项目截图</h2><p style="color:#333333; text-align:left"><img alt="20240221141801" src="https://download.flashcat.cloud/ulric/20240221141801.png" referrerpolicy="no-referrer"></p><p style="color:#333333; text-align:left"><img alt="20240221141817" src="https://download.flashcat.cloud/ulric/20240221141817.png" referrerpolicy="no-referrer"></p><h2>项目代码</h2><ul><li>后端：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fccfos%2Fnightingale" target="_blank">💡 https://github.com/ccfos/nightingale</a></li><li>前端：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fn9e%2Ffe" target="_blank">💡 https://github.com/n9e/fe</a></li></ul><p style="color:#333333; text-align:left">夜莺项目已收获 8000 多 github stars，1000 多 forks，100 多 contributors 参与其中，欢迎大家在<span>&nbsp;</span><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fccfos%2Fnightingale" target="_blank"><strong>GitHub</strong></a></strong><span>&nbsp;</span>上关注夜莺项目，及时获取项目更新动态，有任何问题，也欢迎提交 issues，以及提交 pull requests，开源社区需要大家一起参与才能有蓬勃的生命力。</p><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 04:15:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279741/nightingale-7-0-0-beta-0-released</guid>
            <link>https://www.oschina.net/news/279741/nightingale-7-0-0-beta-0-released</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源日报：等到 Sora 开源了立刻推出属于我们自己的大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>欢迎阅读 OSCHINA 编辑部出品的开源日报，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.21</strong></span></h3><h2><strong><span style="color:#16a085">今日要点</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/279326/linux-kernel-is-a-cna" target="_blank">2023 年度 Rust 调查报告</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000">有 93% 的受访者称自己是 Rust 用户，其中 49% 的人每天（或几乎每天）都会使用 Rust，相较上一年小幅增加 2 个百分点。在没有使用 Rust 的用户中，31% 的人表示主要原因时使用 Rust 有难度；67% 的人表示他们还没有机会优先学习 Rust，这也是最常见的原因。</span></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><span style="color:#000000"><img alt="" height="429" src="https://oscimg.oschina.net/oscnet/up-335afef97d65f03d34d0f08eb8831153557.png" width="500" referrerpolicy="no-referrer"></span></p><h3><a href="https://www.oschina.net/news/279389/dart-3-3-released" target="_blank">Go 1.22 正式发布</a></h3><p>Go 1.22 中新增的优化之一是改进了虚拟化，允许静态调度更多的接口方法调用。启用 PGO 后，大多数程序的性能将提高 2% 至 14%。 此外，Go 运行时中的内存优化可将 CPU 性能提高 1-3%，同时还可将大多数 Go 程序的内存开销减少约 1%。</p><p>新的 math/rand/v2 软件包提供了更简洁、更一致的应用程序接口，并使用了质量更高、速度更快的伪随机生成算法。&nbsp;</p><hr><h2><strong><span style="color:#16a085">今日观察</span></strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-9fc6f45ad8365450e0980fe7fe1855a56c5.png" referrerpolicy="no-referrer"></p><p>- 微博 <u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F6640496217%2FO1tM7BenZ" target="_blank">-赤犬龙之介-</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-d423a4b13075c233da7226ccd5235dc0a3e.png" referrerpolicy="no-referrer"></p><p>-&nbsp;<u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fishare.ifeng.com%2Fc%2Fs%2F8XMQgC7LlaK" target="_blank">21 世纪经济报道</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推荐</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-25f6c0a5f5becd775a0ad6bd1080893da87.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>开源之声</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-bbc6216c930e22f859d514d8becc057daae.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日项目榜</strong></span></h2><p>Gitee 榜单：</p><p><img src="https://oscimg.oschina.net/oscnet/up-5ab4f59bdf01add8c61aac9c617fa074d2c.png" referrerpolicy="no-referrer"></p><hr><p><strong>往期回顾</strong></p><ul><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">开源日报第 012 期：Sora 给中国 AI 带来的真实变化；Dart 3.3 发布</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">开源日报第 011 期：目前还没有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">开源日报第 010 期：Tauri v2 支持 Android 和 iOS，跨平台开发新选择</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">开源日报第 009 期：Vue.js 诞生 10 周年；扎克伯格解释 Meta 为什么要开源其 AI 技术</a></li><li><a href="https://www.oschina.net/news/277585">开源日报第 008 期：推动中国开源软硬件发展的经验与建议</a></li><li><a href="https://www.oschina.net/news/277415">开源日报第 007 期：「Linux 中国」 开源社区宣布停止运营</a></li><li><a href="https://www.oschina.net/news/277214">开源日报第 006 期：选择技术栈一定要选择开源的</a></li><li><a href="http://www.oschina.net/news/277040">开源日报第 005 期：RISC-V 万兆开源交换机发售；npm 存在大量武林外传视频</a></li><li><a href="https://www.oschina.net/news/276864">开源日报第 004 期：百度输入法在候选词区域植入广告；大神用 Excel 构建 CPU</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:47:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279735</guid>
            <link>https://www.oschina.net/news/279735</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[数百位名人签署公开信，呼吁制定反深度伪造立法]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>数百名 AI 界人士签署了一封<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenletter.net%2Fl%2Fdisrupting-deepfakes" target="_blank">公开信</a>，呼吁严格监管 AI 生成的冒名顶替或深度伪造（Deepfake）内容。</p><p><img height="274" src="https://oscimg.oschina.net/oscnet/up-df56f62fff93b887bf1198aca4ccf4bcb5e.png" width="500" referrerpolicy="no-referrer"></p><p>公开信指出，"深度伪造"是指未经同意或严重误导的人工智能生成的声音、图像或视频，合理的人会误以为是真实的。这不包括对图像或声音的轻微改动，也不包括容易识别为合成的无害娱乐或讽刺。</p><p>如今，深度伪造通常涉及性图像、欺诈或政治虚假信息。由于人工智能发展迅速，使得深度伪造变得更加容易，因此我们需要为数字基础设施的运行和完整性提供保障。「Deepfakes 对社会的威胁日益严重，政府必须在整个供应链中施加义务，以阻止 Deepfakes 的扩散。」</p><p>信中呼吁：</p><ul><li>将深度伪造的儿童性虐待材料（CSAM，又名儿童色情制品）完全定为刑事犯罪，无论所描绘的人物是真实的还是虚构的。</li><li>在任何情况下，如果有人制造或传播有害的深度伪造品，都需要受到刑事处罚。</li><li>要求软件开发商和分销商防止其音频和视频产品被用于制造有害的深度伪造品，如果他们的预防措施不充分，就要承担责任接受处罚。</li></ul><p>他们认为，如果设计得当，这些法律可以在不会造成过重负担的同时，培育有社会责任感的企业。</p><p>这封信中较为知名的签名者包括：</p><ul><li>Jaron Lanier</li><li>Frances Haugen</li><li>Stuart Russell</li><li>Andrew Yang</li><li>Marietje Schaake</li><li>Steven Pinker</li><li>Gary Marcus</li><li>Oren Etzioni</li><li>Genevieve smith</li><li>Yoshua Bengio</li><li>Dan Hendrycks</li><li>Tim Wu</li></ul><p>事实上，这也不是首次出现相关的呼吁。在本月早些时候正式提出之前，欧盟已就此类措施进行了多年辩论。外媒 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftechcrunch.com%2F2024%2F02%2F21%2Fhundreds-of-ai-luminaries-sign-letter-calling-for-anti-deepfake-legislation%2F" target="_blank">TechCrunch</a> 指出，也许正是欧盟愿意进行审议和落实，才激活了这些研究人员、创作者和管理人员的发言权。虽然此举不一定能推动真正的立法，但它确实是业界专家们如何看待这一争议问题的风向标。</p><p>更多详情可查看此处：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenletter.net%2Fl%2Fdisrupting-deepfakes" target="_blank">https://openletter.net/l/disrupting-deepfakes</a>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279733/disrupting-deepfakes</guid>
            <link>https://www.oschina.net/news/279733/disrupting-deepfakes</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[出门问问创始人李志飞点评谷歌开源大模型 Gemma：差点意思]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>谷歌在北京时间昨晚发布了<u><a href="https://www.oschina.net/news/279713/google-gemma-open-models">开源大模型 Gemma</a></u>，对标 Meta 旗下 Llama 2。</p><p><img src="https://oscimg.oschina.net/oscnet/up-fb557d3a75a71eccd7300352b8e419f6dd5.png" referrerpolicy="no-referrer"></p><p>出门问问创始人李志飞发表文章点评，<strong>称 Gemma 推出时间有点晚、开源力度不够、未放下高贵的头颅</strong>。</p><p>李志飞在文章中表示，相比于去年上半年就开源，现在可能要花数倍的努力进行模型的差异化以及推广的投入，才有可能在众多开源模型中脱颖而出。「面对 OpenAI 的强力竞争，只有杀敌一千、自损一千五。」</p><p>以下为李志飞全文：</p><blockquote><p>看到 Google 开源了小的语言模型 Gemma，直接狙击 Llama 2，回顾去年 5 月对 Google 关于开源和竞争的看法，几点思考如下：</p><p>1. 时间有点晚：相比于去年上半年就开源，现在可能要花数倍的努力进行模型的差异化以及推广的投入，才有可能在众多开源模型中脱颖而出。</p><p>2. 开源力度不够：感觉这次开源还是被动防御和略显扭捏的应对之策，不是进攻。比如说，开个 7B 的模型实在是太小儿科了，一点杀伤力都没有。应该直接开源一个超越市场上所有开源的至少 100B 的模型、1M 的超长上下文、完善的推理 infra 方案、外加送一定的 cloud credit。是的，再不歇斯底里 Google 真的就晚了。面对 OpenAI 的强力竞争，只有杀敌一千、自损一千五。</p><p>3. 未放下高贵的头颅：有种感觉，Google 觉得自己还是 AI 王者，放不下高贵的头颅，很多发布都有点不痛不痒，还是沿着过去研发驱动的老路而不是产品和竞争驱动，比如不停发论文、取新名字（多模态相关模型过去半年就发了 Palme、RT-2、Gemini、VideoPoet、W.A.L.T 等）、发布的模型又完整度不够，感觉就没有一个绝对能打的产品。Google 可能要意识到在公众眼中，他在 AI 领域已经是廉颇老矣溃不成军，经常起大早赶晚集（比如说这次 Sora 借鉴的 ViT、ViViT、NaVit、MAGVit 等核心组件技术都是它家写的论文）。</p><p>4. 希望亡羊补牢未为晚也：Google 作为一个僵化的大公司，动作慢一点可以理解，但是如果再不努力是不是就是 PC 互联网的 IBM、移动互联网的 Microsoft？ 作为 Google 的铁粉，还是希望他能打起精神一战，AI 产业需要强力的竞争才能不停向前发展，也需要他在前沿研究和系统的开源才能帮助一大众「贫穷」的 AI 创业公司。</p><p>5. 另外，除了对外开源外，Google 应该组成三个方阵面对大模型的竞争，详见去年 3 月发文。</p><p>回顾科技竞争史，PC 互联网时代的 IBM、移动互联网时代的 Microsoft、AGI 时代的 Google，新时代来临后，难道上一个时代科技霸主都难逃衰落的宿命？</p><p>当然，Microsoft 靠 Office SaaS、云和 OpenAI 又翻盘了。</p><p>历史的铁律，有被改写的可能吗？</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 03:15:33 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279728</guid>
            <link>https://www.oschina.net/news/279728</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenAI 员工的一天]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Chain Of Thought (CoT) 第一作者、从谷歌跳槽到 OpenAI 的 Jason Wei 分享了自己在 OpenAI 的一天：</p><blockquote><p>[9:00am] 起床</p><p>[9:30am] 搭乘 Waymo 前往 Mission SF，途中在 Tartine 买个牛油果吐司</p><p>[9:45am] 背诵 OpenAI 章程。向优化之神致敬，学习《The Bitter Lession》（强化学习之父 Rich Sutton 著）</p><p>[10:00am] 在 Google Meet 上开会，讨论如何在更多数据上训练更大的模型</p><p>[11:00am] 敲代码，在更多数据上训练更大的模型。搭档是 Hyung Won Chung</p><p>[12:00pm] 去食堂吃午饭（纯素且无麸质）</p><p>[1:00pm] 真正开始在大量数据上训练大模型</p><p>[2:00pm] 处理基础设施问题（我是脑子被驴踢了吗为啥要从 master 分支拉代码？）</p><p>[3:00pm] 监控模型训练进度，玩 Sora</p><p>[4:00pm] 给刚才训练的大模型上提示工程</p><p>[4:30pm] 短暂休息，坐在牛油果椅子上，思考 Gemini Ultra 的性能到底有多强</p><p>[5:00pm] 头脑风暴模型可能的算法改进</p><p>[5:05pm] 修改算法风险太高，pass。最安全的策略还是力大砖飞（增加算力和数据规模）</p><p>[6:00pm] 晚餐时间，和 Roon 一起享用蛤蜊浓汤</p><p>[7:00pm] 回家</p><p>[8:00pm] 喝点小酒，继续写码。Ballmer’s peak（酒精带来的编码高效阶段）即将到来</p><p>[9:00pm] 分析实验结果，我对 wandb 又爱又恨</p><p>[10:00pm] 启动实验，让它自己跑一晚上，第二天查看结果</p><p>[1:00am] 实验真正开始运行</p><p>[1:15am] 上床睡觉。在纳德拉和黄仁勋的守护下入梦。心想：压缩才是真谛。晚安</p><p><img src="https://oscimg.oschina.net/oscnet/up-3041206e9fa5084776ebedb71c760828888.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2F_jasonwei%2Fstatus%2F1760032264120041684" target="_blank">https://twitter.com/_jasonwei/status/1760032264120041684</a></u></em></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:46:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279722</guid>
            <link>https://www.oschina.net/news/279722</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[StarkNet 撒钱，程序员在 Web3 领了 14 万 ​​​]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>StarkNet 启动空投活动，GitHub 排名前 5000 开源项目的贡献者可领取价值 $200 奖励。</p><h2>背景</h2><ul><li>StarkNet 公链项目为了激励开发者参与其平台建设，启动了空投活动。</li><li>如果曾向 GitHub 上获得较多 Star 的项目提交过 PR ，就有资格领取 111.1 STRK 的空投奖励。</li><li>只需要使用 OAuth 2.0 登录，就可以直接领取。</li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-44f62dafa84c34781d15d46c71c7c8c862c.jpg" referrerpolicy="no-referrer"></p><p>有程序员晒出自己在这次空投活动获得的奖励——近 14 万人民币。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-96e62f68c4891cdbfd1f43b86ddfcdcbf79.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2Fstrrlthedev%2Fstatus%2F1760182038504837287" target="_blank">https://twitter.com/strrlthedev/status/1760182038504837287</a></u></em></p></blockquote><h2>领取规则</h2><ol><li>截止到 2023 年 11 月 15 日，至少对全球排名前 5000 的仓库提交过三次代码贡献。</li><li>其中至少有一次贡献是在 2018 年或之后完成的。</li></ol><h2>领取步骤</h2><p>领取地址：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fprovisions.starknet.io%2F" target="_blank">https://provisions.starknet.io/</a></u></em></p><ol><li>访问奖励领取页面并连接钱包（推荐使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchromewebstore.google.com%2Fdetail%2Fargent-x-starknet-wallet%2Fdlcobpjiigpikoobohmabehhmhfoodbb" target="_blank">Argent X</a>）。</li><li>通过 GitHub 登录，采用 OAuth 2.0 验证方式。</li><li>直接领取奖励。后续的治理投票和问卷调查可忽略不计。</li></ol></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:30:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279720</guid>
            <link>https://www.oschina.net/news/279720</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[华为：2 月 26 日将首发华为通信大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">华为官方微信公众号消息<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F7dnrCXQoOqWSfKBrCJWl6Q" target="_blank">显示</a>， 在 2 月 26 日华为产品与解决方案发布会上，即将首发华为通信大模型。</span></p><p><img height="677" src="https://oscimg.oschina.net/oscnet/up-59659387ba8ade9e0cf71c6f84d476a728e.png" width="500" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">华为称，2024 年，5G-A 商用元年正式开启！万兆时代已来，共同见证将 5G-A 带入现实。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 22 Feb 2024 02:23:42 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/279717</guid>
            <link>https://www.oschina.net/news/279717</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
