<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-最新资讯]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-最新资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 28 Feb 2024 10:56:05 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[零一万物发布 Yi 大模型 API 并启动公测，支持上下文 200K]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">零一万物通过其微信公众号<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FkSdGJh0xro9vm_LDDhMhgQ" target="_blank">宣布</a>，经过一段时间的开发和内测正式发布 Yi 大模型 API，同时启动邀测。目前，Yi 大模型 API 邀测名额限量开放中，申请成功即送 1000 万&nbsp;tokens。</span></p><p>此次邀测提供了两种模型：</p><ul><li>Yi-34B-Chat（0205）：支持聊天、问答、对话、写作、翻译等功能。</li><li>Yi-34B-Chat-200K：200K 上下文，多文档阅读理解、超长知识库构建小能手。</li></ul><h4><strong>模型优势</strong></h4><ul><li><strong>超长上下文</strong></li></ul><p>本次重磅出台 Yi-34B-Chat-200K API，加速大模型应用进入「长文本时代」。200K 支持处理约 20～30 万个中英文字符（例如，可以轻松处理整本《哈利•波特与魔法石》小说），适合用于多篇文档内容理解、海量数据分析挖掘和跨领域知识融合等，为各行各业提供了极大的便利。例如，金融分析师可以用它快速阅读报告并预测市场趋势、律师可以用它精准解读法律条文、科研人员可以用它高效提取论文要点、文学爱好者可以用它快速掌握作品精髓等，应用场景非常广泛。</p><p>例如，以下是 Yi-34B-Chat-200K 对经典文学作品《呼啸山庄》进行复杂角色和角色关系的归纳总结，该小说篇幅庞大（中文字数约 30 万字），且人物关系错综复杂，但它仍能精准地梳理和总结出人物之间的关系，展示了它在处理超长上下文时出色的复杂内容理解和分析能力。</p><p><img alt="" height="605" src="https://oscimg.oschina.net/oscnet/up-8036e1fcb930944e32404b4d2a35bd4bc33.png" width="300" referrerpolicy="no-referrer"></p><ul><li><strong>出色的指令遵循和创意内容生成能力</strong></li></ul><p>此前，零一万物发布并开源了 Yi-34B-Chat（1123），它的回复风格符合人类偏好，但在指令遵循上结果不够稳定。而此次新发布的 Yi-34B-Chat（0205）经过深度优化，性能得到大幅提升，不仅继承了符合人类偏好的回复风格，很擅长创意性内容创作，而且能够更好地理解复杂的用户需求，遵循多约束指令（指令遵循能力提升了近 30%），稳定生成指定格式的内容。</p><p>例如，以下是两个版本在指令遵循方面的测评对比。</p><p><strong>Prompt 1: 帮我输出一个俄国作家的书单，以 JSON 格式输出一个的 list，其中每一个 item 都要有两个 key，分别是书名和作家名字，请列出 3 本不同的书</strong></p><p><strong><img alt="" height="336" src="https://oscimg.oschina.net/oscnet/up-2d761ddabb83b7d03ee21cc74072f2431e1.jpg" width="500" referrerpolicy="no-referrer"></strong></p><p>Yi-34B-Chat（1123）输出的 JSON 文件格式略有不足（例如，第 8 行和第 12 行的引号），而 Yi-34B-Chat（0205）输出的 JSON 文件格式全部正确。</p><p><strong>Prompt 2: 判断下面这段话的情绪倾向，如果是正面的，回复数字 1；如果是负面的，回复数字 0：</strong></p><p><strong>这款手机真是物超所值，性能强大，电池续航长，外观设计也很有档次。我用了几个月，到现在还像新的一样。</strong></p><p><img height="169" src="https://oscimg.oschina.net/oscnet/up-e185cce9ff54b8737e02470009fe90a8d31.png" width="500" referrerpolicy="no-referrer"></p><p>Yi-34B-Chat（1123）虽然理解了问题，但是没有完全遵循指令，输出了较多冗余的分析。而 Yi-34B-Chat（0205）理解了问题，且正确遵循了用户指令。</p><h4><strong>API 优势</strong></h4><ul><li><strong>推理速度快</strong></li></ul><p>为了提升 API 性能，团队在 API 侧进行了推理优化，因此 Yi-34B-Chat 系列 API 具备较快的推理速度，这不仅缩短了处理时间，同时也保持了出色的模型效果。此外，优化的 API 接口显著降低了模型回复的延迟，进一步提高了用户体验的流畅性和响应速度。</p><ul><li><strong>兼容 OpenAI</strong></li></ul><p>Yi 大模型 API 与 OpenAI API 完全兼容，你只需修改少量代码，可以平滑迁移，即刻享受 Yi 大模型的超凡魅力。</p><pre><code>
import openai
from openai import OpenAI

API_BASE = "https://api.lingyiwanwu.com/v1"
API_KEY = "{{your key}}"

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key=API_KEY,
    base_url=API_BASE
)
completion = client.chat.completions.create(
    model="yi-34b-chat-200k",
    messages=[{"role": "user", "content": "Hi, who are you"}]
)
print(completion)</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 10:34:03 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280668</guid>
            <link>https://www.oschina.net/news/280668</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[WordPress 母公司 Automattic 计划出售数据给 OpenAI 等 AI 公司]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.404media.co%2Ftumblr-and-wordpress-to-sell-users-data-to-train-ai-tools%2F" target="_blank">404 Media 的一份报告显示</a></u>，Tumblr 和 WordPress.com 的所有者正在与人工智能公司 Midjourney 和 OpenAI 进行谈判，以提供从用户帖子中抓取的训练数据。</p><p><img height="1268" src="https://oscimg.oschina.net/oscnet/up-876500a00ea4d490602adc6cf3a01127f29.png" width="2282" referrerpolicy="no-referrer"></p><p>这份来自公司内部匿名消息人士的报告称，Automattic 与两家人工智能公司之间的交易「迫在眉睫」。过去一周，Tumblr 上流传着一些模糊的谣言，暗示与 Midjourney 的交易可能会为该网站带来新的收入来源。</p><p>根据报告，Automattic 计划在周三<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fautomattic.com%2F2024%2F02%2F27%2Fprotecting-user-choice%2F" target="_blank">推出一项新设置</a></u>，「允许用户选择不与包括人工智能公司在内的第三方共享数据」。</p><p>但它引用的内部帖子表明，该公司抓取了一份「初始数据转储」，其中包含「2014 年至 2023 年间 Tumblr 的所有公开帖子内容」，其中包括不会在博客上公开可见的内容——Automattic 称是错误抓取。目前尚不清楚这些数据做了什么，以及哪些数据已发送到 Midjourney 和 OpenAI。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 09:31:52 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280659/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools</guid>
            <link>https://www.oschina.net/news/280659/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[华为发布通信行业首个大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">2 月 26 日至 2 月 29 日举行的世界移动通信大会（MWC24）期间，华为发布了由其自主研发的服务于通信行业的大模型。</span></p><p><span style="color:#000000"><img height="282" src="https://oscimg.oschina.net/oscnet/up-7be3f3a47ce7d8c9ff61ba6653dd16045d4.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">根据介绍，华为通信大模型是一款基于人工智能的商用大模型，提供关键的智能化技术能力，用于优化通信网络性能、智能调度资源等，旨在实现在 5G 技术基础上演进而来的 5G-A 时代智能化目标。</span></p><p><span style="color:#000000">华为董事、ICT 产品与解决方案总裁杨超斌介绍，华为通信大模型支撑运营商智能化目标，面向不同角色，提供智能语言交互能力，提升员工知识水平和工作效率；面向不同运营运维场景，提供智能体应用，分析拆解复杂流程，编排操作方案，确保用户体验和满意度。</span></p><p><span style="color:#000000">华为通信大模型具有众多典型场景实践。如在敏捷业务发放案例中，通过放号助手的多模态精准评估，实现了快速用户放号；在用户体验保障案例中，通过大模型的寻优能力，实现了多目标体验保障；在辅助排障场景下，跨流程的质差分析和对话辅助处理，显著改善了故障处理效率。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 08:31:36 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280646</guid>
            <link>https://www.oschina.net/news/280646</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenAI 称《纽约时报》曾雇人入侵 ChatGPT]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">OpenAI 要求驳回《纽约时报》于去年 12 月对其提起的版权诉讼的大部分内容，指控《纽约时报》花钱请人入侵 OpenAI 的产品，以在该案中制造误导性证据。《纽约时报》曾起诉 OpenAI 和微软公司，称这两家公司非法使用其受版权保护的材料来训练 AI 模型，分流了《纽约时报》网站的流量。</span></p><blockquote><p><span style="color:#000000">「《纽约时报》投诉中的指控不符合其著名的严格新闻标准。随着本案的进展，真相将水落石出，那就是《纽约时报》花钱雇人入侵了 OpenAI 的产品。他们花了数以万计的尝试，才产生了构成原告证据 J 的高度反常的结果。他们只有通过使用公然违反 OpenAI 使用条款的欺骗性提示，瞄准并利用一个漏洞（OpenAI 已承诺解决该漏洞），才能做到这一点。即便如此，他们还不得不向该工具提供他们想要获取的文章的逐字段落，而这些文章几乎都已出现在多个公共网站上。正常人不会以这种方式使用 OpenAI 的产品。」</span></p></blockquote><p><img height="259" src="https://oscimg.oschina.net/oscnet/up-3b5642691a2dd2818f5e24e4c20c0da1846.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">OpenAI 认为，与投诉中的指控相反的是，ChatGPT 无论如何都不能替代《纽约时报》的订阅。因为在现实世界中，人们不会也不能为此目的使用 ChatGPT 或任何其他 OpenAI 产品。</span></p><p><span style="color:#000000">且《纽约时报》近年来一直在跟进 OpenAI 聊天机器人的发展，却从未提出过任何有关版权侵权的担忧。OpenAI 声称，他们在 2020 年就披露了《纽约时报》的文章被用于训练其 AI 模型的这一事实，但该报却在 ChatGPT 于 2022 年首次亮相后人气爆棚时才开始关心此事，提起诉讼要求赔偿数十亿美元。</span></p><p><span style="color:#000000">OpenAI 还回应了《纽约时报》对 ChatGPT 提供付费文章访问权限的担忧；表示这种只是一个"罕见的错误"，目前正在努力修复中。并声称，"《纽约时报》没有完整阐述所有的全部故事。"</span></p><p><span style="color:#000000">对此，《纽约时报》的首席法律顾问 Ian Crosby 则<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farstechnica.com%2Ftech-policy%2F2024%2F02%2Fopenai-accuses-nyt-of-hacking-chatgpt-to-set-up-copyright-suit%2F" target="_blank">反驳称</a>，OpenAI 离奇地将《纽约时报》寻求证据的行为误解成了黑客行为。并补充称，OpenAI 的抄袭规模远远大于起诉书中 100 多个示例。「开发新产品不能成为违反版权法的借口。」</span></p><p><span style="color:#000000">详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ffingfx.thomsonreuters.com%2Fgfx%2Flegaldocs%2Fbyvrkxbmgpe%2FOPENAI%2520MICROSOFT%2520NEW%2520YORK%2520TIMES%2520mtd.pdf" target="_blank">查看完整文件</a>。</span></p><p><strong><span style="color:#000000">相关阅读：</span></strong></p><ul><li><a href="https://www.oschina.net/news/274916/openai-and-journalism" target="_blank">OpenAI 称《纽约时报》的版权诉讼毫无根据</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 07:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280623/openai-new-york-times-lawsuit-hackin</guid>
            <link>https://www.oschina.net/news/280623/openai-new-york-times-lawsuit-hackin</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[蚂蚁百灵大模型推出 20 亿参数遥感模型 SkySense]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">蚂蚁百灵大模型推出了 20 亿参数多模态遥感基础模型 SkySense，这也是蚂蚁在多模态领域最新的研发成果。公开资料显示，SkySense 由蚂蚁 AI 创新研发部门 NextEvo 与武汉大学联合研发。</span></p><p><span style="color:#000000">SkySense 在总计 17 项国际权威公开数据集进行了测评，其测试任务类型包括了土地利用监测、高分辨率目标识别、地物变化检测等 7 种常见遥感感知任务，并与国际上已发布的包括 IBM 和 NASA 联合研发的 Prithvi 等共 18 个全球主流同类模型做了测试结果比较。</span></p><p><span style="color:#000000">数据显示，在 17 项测评中 SkySense 均名列第一。比如，在国际高清遥感地物检测榜单 FAIR1M 2.0 中，SkySense 平均精度（mAP）领先第二名超 3%。</span></p><p><span style="color:#000000"><img height="417" src="https://oscimg.oschina.net/oscnet/up-30d9098a9be808fb8043382fec2a6d88bcc.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">在蚂蚁百灵大模型多模态能力支持下，研发人员基于内部构建的 19 亿遥感影像数据集进行预训练，得到了 20.6 亿参数量的模型 SkySense。这也是迄今为止国际上参数规模最大、覆盖任务最全、识别精度最高的多模态遥感大模型。</span></p><p><span style="color:#000000">目前 SkySense 可广泛应用于城市规划、森林保护、应急救灾、绿色金融、农业监测等重要领域，目前通过蚂蚁内部 MEarth 平台提供数据与识别服务。&nbsp;</span></p><p><span style="color:#000000">据了解，蚂蚁集团正在计划开放 Skysense 模型参数，与行业共建，促进智能遥感技术与应用发展。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 06:15:40 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280605</guid>
            <link>https://www.oschina.net/news/280605</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[又一家硅谷明星公司误删库了]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>之前我们连续分析了两起误删库事件，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FShYpGMwrqige2bvmfO8ZRg" target="_blank">Linear 删库</a>，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FY7qAaYt2uIylqlPve9DGzg" target="_blank">GitLab 删库</a>。就在我们准备让这个主题告一段落时，业界又发生了一起删库事件。</p><p><img src="https://oscimg.oschina.net/oscnet/up-438613acc1f6667c828e7dbe14679e574cf.png" alt="file" referrerpolicy="no-referrer"></p><p>这次的主角是 Resend，也是最近硅谷冉冉升起的明星初创公司。想重塑邮件体验，挑战像 Mailchimp 这样的老牌玩家。</p><p><img src="https://oscimg.oschina.net/oscnet/up-59af844e75075e65e2d572ba7ba58a40bbd.png" alt="file" referrerpolicy="no-referrer"></p><p><strong>这次的删库事件依然是熟悉的配方，在执行数据库 schema 变更时，本来是针对本地环境执行，但结果命令发给了生产数据库，就这样把数据都删没了。</strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-a35feb6af09f25ccb81dd4a7d6e38a21a59.png" alt="file" referrerpolicy="no-referrer"></p><p>而在恢复的过程中，第一次恢复使用了错误的备份，导致浪费了 6 个小时。又经过额外的 5 小时备份，才把数据库恢复过来，但还是有 5 分钟的数据丢失了。Resend 也列出了一些后续措施：</p><ul><li>恢复 5 分钟丢失的数据</li><li>收回所有用户对生产环境的写权限</li><li>改进本地开发流程，以降低数据库 schema 变更的风险</li><li>提高故障演练的频率</li></ul><p>也因为 Resend 小有名气，所以也引来了 Hacker News 上网友们的锐评：</p><p><img src="https://oscimg.oschina.net/oscnet/up-f09cddcfcfafcdbbf0bc92d56ac41656d0c.png" alt="file" referrerpolicy="no-referrer"></p><p>太业余了，像 email 这种核心组件，还是交给更加成熟的 AWS SES，Postmark，Sendgrid 这些吧。</p><p><img src="https://oscimg.oschina.net/oscnet/up-df0428cac45dece8b9cd009e4d6d2a3c10f.png" alt="file" referrerpolicy="no-referrer"></p><p>或许这家公司根本就不该存在。</p><h2>如何避免</h2><p>笔者认为这个故障虽然有点低级。但连错数据库这个事情，不算少见。备份过程碰到意外，也很常见。当然低级的问题，解决起来也不难。</p><p>针对第一点，引入像 Bytebase 这样的变更审核工具，所有针对生产环境的变更操作都要通过 Bytebase，经过人工审核后才能发布。</p><p><img src="https://oscimg.oschina.net/oscnet/up-ba37741baf890b551db462c259fd8f6b5d0.png" alt="file" referrerpolicy="no-referrer"></p><p>针对第二点，首先是采用云上的托管数据库服务，因为他们提供了完整的数据备份和恢复功能。另外就是定期做灾备演练。</p><p>大家也引以为戒吧。</p><hr><p>💡 更多资讯，请关注 Bytebase 公号：Bytebase</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 06:00:40 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6148470/blog/11045039</guid>
            <link>https://my.oschina.net/u/6148470/blog/11045039</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[开源日报 | MariaDB 消亡史；写代码我有三不沾；V 神建议马斯克用 Linux]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>欢迎阅读 OSCHINA 编辑部出品的开源日报，每天更新一期。</p><h3><span style="color:#e67e22"><strong># 2024.2.27</strong></span></h3><h2><strong><span style="color:#16a085">今日要点</span></strong></h2><p><strong>OpenSource Daily</strong></p><h3><a href="https://www.oschina.net/news/280456/osi-delayed-open-source-publication-report" target="_blank">OSI 发布报告，研究 BSL 这样的 「延迟开源发布」</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Open Source Initiative（OSI）近期发布了一个报告《Delayed Open Source Publication: A Survey of Historical and Current Practices》（延迟开源发布：历史与当前实践调研）。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">Delayed Open Source Publication，简称 DOSP，延迟开源发布的意思。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">结论：DOSP 自开源运动早期以来一直在使用，公司通常利用它来保持商业优势，同时尽可能保留开源的优势。报告强调，DOSP 的实验性和多样性比预期的要多，且这种趋势可能会继续。</p><h3><a href="https://www.oschina.net/news/280524" target="_blank">马斯克抱怨微软 Windows 难用，V 神：加入 Linux！</a></h3><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img src="https://oscimg.oschina.net/oscnet/up-e5e820c2d6e02c58e0b272f4e8447623295.png" referrerpolicy="no-referrer"></p><hr><h2><strong><span style="color:#16a085">今日观察</span></strong></h2><p><img src="https://oscimg.oschina.net/oscnet/up-b918faeab5307706355aed13cc098be8979.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微信&nbsp;</span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkyMjQzOTkyMQ%3D%3D%26mid%3D2247484005%26idx%3D1%26sn%3D74c8079c36bc1a1a704a41f9d3dc48f9%26chksm%3Dc1f51bfbf68292ed01843d1451dc4170e814babd874c8b1661aa5c21e2bb9d99ad62da6ee348" target="_blank">IT 知识刺客</a></em></u></p><p><img src="https://oscimg.oschina.net/oscnet/up-7376d45caae71d8c9a249f35fb238c25d26.png" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#333333">- 微博&nbsp;</span><u><em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fweibo.com%2F2689166291%2FO2vRh7Twn" target="_blank">归零归零归 ww</a></em></u></p><hr><h2><span style="color:#16a085"><strong>今日推荐</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-505c94c6606e7aefd93a64ce5f75063c652.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>开源之声</strong></span></h2><p><img src="https://oscimg.oschina.net/oscnet/up-ddca260d14744131b56cecc93028c61a364.png" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-271b8a5abd0cf2553fb727bb13de466f0c2.png" referrerpolicy="no-referrer"></p><hr><h2><span style="color:#16a085"><strong>每日项目榜</strong></span></h2><p><strong><span style="background-color:#e67e22">每日 Gitee 精选</span></strong></p><p><img src="https://oscimg.oschina.net/oscnet/up-618659c15ef289866a177fc7ccfe8e89e53.png" referrerpolicy="no-referrer"></p><blockquote><h4><strong><span style="background-color:#e67e22">在线阅读完整日报内容，访问：</span></strong><br><u><em><strong><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/7r8dkz3232v4e7a/17_maria_db_v_linux_GoyNoM85IZ.pdf">开源日报第 017 期：MariaDB 消亡史；写代码我有三不沾；V 神建议马斯克用 Linux</a></strong></em></u></h4></blockquote><hr><p><strong>往期回顾</strong></p><ul><li><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/6typ9w3u98f5mxn/16_1_8_2efTeNfFjN.pdf">开源日报第 016 期：鸿蒙程序员平均月薪超 1 万 8；中美 AI 差距有多大？</a></li><li><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/92n4c9ryegpcq1z/015_sora_KcAkRNX93Y.pdf">开源日报第 015 期：为什么挡不住英伟达；Sora 不靠蛮力</a></li><li><a href="https://report.oschina.net/api/files/jhim80u9qm1ofsw/s7n800w84o6guyv/014_kyezhNxOGD.pdf">开源日报第 014 期：目前的人工智能技术连猫的智能水平都没达到</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC013%E6%9C%9F%EF%BC%9A%E7%AD%89%E5%88%B0%20Sora%20%E5%BC%80%E6%BA%90%E4%BA%86%E7%AB%8B%E5%88%BB%E6%8E%A8%E5%87%BA%E5%B1%9E%E4%BA%8E%E6%88%91%E4%BB%AC%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B.pdf">开源日报第 013 期：等到 Sora 开源了立刻推出属于我们自己的大模型</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC012%E6%9C%9F%EF%BC%9ASora%20%E7%BB%99%E4%B8%AD%E5%9B%BD%20AI%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%9C%9F%E5%AE%9E%E5%8F%98%E5%8C%96%EF%BC%9BDart%203.3%20%E5%8F%91%E5%B8%83.pdf">开源日报第 012 期：Sora 给中国 AI 带来的真实变化；Dart 3.3 发布</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC11%E6%9C%9F%EF%BC%9A%E7%9B%AE%E5%89%8D%E8%BF%98%E6%B2%A1%E6%9C%89%E2%80%9C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%89%88Linux%E2%80%9D.pdf">开源日报第 011 期：目前还没有「大模型版 Linux」</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5%E7%AC%AC010%E6%9C%9F%EF%BC%9ATauri%20v2%20%E6%94%AF%E6%8C%81%20Android%20%E5%92%8C%20iOS%EF%BC%8C%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%BC%80%E5%8F%91%E6%96%B0%E9%80%89%E6%8B%A9.pdf">开源日报第 010 期：Tauri v2 支持 Android 和 iOS，跨平台开发新选择</a></li><li><a href="https://oscimg.oschina.net/public_shard/%E5%BC%80%E6%BA%90%E6%97%A5%E6%8A%A5009%E6%9C%9F%EF%BC%9AVue.js%E8%AF%9E%E7%94%9F10%E5%91%A8%E5%B9%B4%EF%BC%9B%E6%89%8E%E5%85%8B%E4%BC%AF%E6%A0%BC%E8%A7%A3%E9%87%8AMeta%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%BC%80%E6%BA%90%E5%85%B6AI%E6%8A%80%E6%9C%AF.pdf">开源日报第 009 期：Vue.js 诞生 10 周年；扎克伯格解释 Meta 为什么要开源其 AI 技术</a></li><li><a href="https://www.oschina.net/news/277585">开源日报第 008 期：推动中国开源软硬件发展的经验与建议</a></li><li><a href="https://www.oschina.net/news/277415">开源日报第 007 期：「Linux 中国」 开源社区宣布停止运营</a></li><li><a href="https://www.oschina.net/news/277214">开源日报第 006 期：选择技术栈一定要选择开源的</a></li><li><a href="http://www.oschina.net/news/277040">开源日报第 005 期：RISC-V 万兆开源交换机发售；npm 存在大量武林外传视频</a></li><li><a href="https://www.oschina.net/news/276864">开源日报第 004 期：百度输入法在候选词区域植入广告；大神用 Excel 构建 CPU</a></li></ul><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 04:12:01 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280594</guid>
            <link>https://www.oschina.net/news/280594</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[国人独立开发的开源 Redis 客户端 ioredis 被 Redis 公司收购]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#333333">ioredis 作者&nbsp;</span><a href="https://my.oschina.net/u/1051352" target="_blank">@Luin</a><span style="background-color:#ffffff; color:#333333">&nbsp;宣布该项目已被 Redis 公司收购。</span>ioredis 是一个用于 Node.js 的 Redis 客户端，健壮、性能好、功能强大且全面。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-c7b9c050507e824beecc36c4767b126c712.png" referrerpolicy="no-referrer"></p></blockquote><p>目前&nbsp;ioredis 在 GitHub 的开源地址已迁移至&nbsp;Redis 公司旗下：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fredis%2Fioredis" target="_blank">https://github.com/redis/ioredis</a></u></em></p><p><img height="601" src="https://oscimg.oschina.net/oscnet/up-be0cf491613bb1c935a0bcfa6354508563c.jpg" width="1170" referrerpolicy="no-referrer"></p><p>两年前，ioredis <u><a href="https://www.oschina.net/news/208601">超过 </a></u>redis 成为了 Node.js 最流行的 Redis 客户端。当时&nbsp;<span style="background-color:#ffffff; color:#333333">ioredis 作者</span>还感叹&nbsp;redis 历经诸多波折终被 Redis 官方收购。</p><p><img src="https://oscimg.oschina.net/oscnet/up-b93256e806a38a0bede1296043170bf2947.png" referrerpolicy="no-referrer"></p><p><a href="https://my.oschina.net/u/1051352" target="_blank">@Luin</a><span style="background-color:#ffffff; color:#333333">&nbsp;曾表示&nbsp;</span>ioredis 是自己独立从零开发的项目，创建初衷也很「极客」——没找到满意的开源库，所以决定自己动手干。历经 9 年，从个人的&nbsp;side project 到被开源公司收购，吾辈楷模！</p><blockquote><p>2014 年底的时候我开始使用 Node.js 开发后端程序。为了连接 Redis ，所以研究了下市面上的 Redis 客户端库。当时最流行的库 redis 是由 Uber 的首席架构师 Matt Ranney 开发的。使用后发现这个库有一些让自己不满意的地方：</p><ol><li>不支持 Promise （当时 Promise 还是个非常新的概念）</li><li>命令语法不太美观（个人审美差异😄）</li><li>功能不齐全：缺少 Cluster 、Sentinel 等 Redis 新功能的支持。</li></ol><p>由于当时正好有点闲暇时间，就自己从零开发并开源了 ioredis 。</p></blockquote><p>延伸阅读：<em><u><a href="https://www.oschina.net/news/208601" target="_blank">ioredis 成为最流行的 Node.js Redis 库</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280584/redis-ltd-acquire-ioredis</guid>
            <link>https://www.oschina.net/news/280584/redis-ltd-acquire-ioredis</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[任天堂起诉 Switch 模拟器 Yuzu 开发者]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">美国任天堂公司正在对流行模拟器工具 Yuzu 背后的开发商 Tropic Haze LLC 提起诉讼，控告其「大规模侵犯任天堂和其他人版权作品知识产权」。</span></p><p><img height="315" src="https://oscimg.oschina.net/oscnet/up-7661078344ba837f79511462e7252c9b144.png" width="500" referrerpolicy="no-referrer"></p><p><span style="color:#000000">该公司在最新提交的一份法律<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdockets.justia.com%2Fdocket%2Frhode-island%2Fridce%2F1%3A2024cv00082%2F56980%3Fref%3Doverkill.wtf" target="_blank">文件</a>中指出，Tropic Haze LLC 非法规避了任天堂 Switch 游戏的软件加密和版权保护系统，从而助长了盗版行为，侵犯了《数字千年版权法案》（DMCA）规定的版权。并表示，其《塞尔达传说：王国之泪》在正式零售发售之前就因盗版被下载了超过一百万份。</span></p><p><span style="color:#000000">任天堂声称，Yuzu 的主要开发者已公开承认 Yuzu 网站向用户提供了指导，教他们如何入侵任天堂 Switch 游戏机，以及如何未经授权复制任天堂视频游戏。同时，任天堂还强调了 Yuzu 从中的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.patreon.com%2Fyuzuteam%3Fref%3Doverkill.wtf" target="_blank">获利</a></span><span style="color:#333333">；</span><span style="color:#000000">该项目目前已经得到了 7000 多名会员的支持，每月收入接近 3 万美元。</span></p><p><span style="color:#333333">因此，任天堂现在正在寻求获得损失赔偿，要求对每项违 </span><span style="color:#000000">DMCA&nbsp;</span><span style="color:#333333">反规避和反贩运条款的行为赔偿 2,500 美元，对每项侵犯版权的行为赔偿 150,000 美元。以及要求法院查封、扣押和销毁 Yuzu 模拟器的所有副本，和任天堂认为侵犯其版权的软件和硬件，并提出了"......立即将域名 yuzu-emu.org ......移交任天堂控制"的需求。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:39:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280580/nintendo-sue-yuzu-emulator</guid>
            <link>https://www.oschina.net/news/280580/nintendo-sue-yuzu-emulator</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[MFiX —— 开源多相流 CFD 软件]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>MFiX 是美国能源部开发的一款用于模拟颗粒流体多相流的开源软件，CFD 部分使用 SIMPLE 算法，而颗粒部分包含了 TFM、MPPIC 以及 DEM 等模型，且可以模拟连续相和离散相之间的传质传热。</p><p>MFiX 基于 fortran 语言开发，核心特性包括：并行、开源、跨平台、一键安装、用户图形界面、支持 TFM/DEM/PIC 多种模型。</p><ul><li>跨平台：该软件支持 Windows\Linux\macOS，使得用户既能在 windows 下学习、测试算例，也可以直接在超算平台上进行计算</li><li>一键安装：该开源软件可以通过 anaconda 方便的一键安装，解决了大规模开源软件难以编译安装的痛点</li><li>用户图形界面：该软件支持用户图形界面，可以方便的设置计算参数、监控计算结果。</li><li>Cutcell 网格：该软件采用 cutcell 网格处理复杂结构，可以通过 stl 文件或软件内置的几何结构生成器构建模拟计算区域。</li><li>物性数据库：软件自带物性数据库，方便传热、化学反应的计算</li><li>Stiff Chem Solver: 燃烧等化学反应的特征时间远远小于流动的特征时间，通过 stiff 化学求解器分步求解化学反应和流体流动，该软件可以方便的模拟多相反应流动</li><li>该软件自带多个演示算例，学习曲线平滑</li><li>模型丰富：采用 SIMPLE 算法的流体求解器、双流体 TFM 求解器、离散单元法 DEM 颗粒模拟、CFD-DEM 模拟、MP-PIC 模拟</li><li>MPI 多机并行，方便在超算平台上大规模计算</li></ul><p><strong>运行截图</strong></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-be3c669b3b0de4ac6c4bd620524919062f8.png" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:22:36 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/mfix</guid>
            <link>https://www.oschina.net/p/mfix</link>
        </item>
        <item>
            <title>
                <![CDATA[拓数派联手开源联盟 PG 分会，走进北京大学研究生公选课]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>为促进基础软件在中国高校的传播，进一步提高在校研究生对基础软件的学习和开发实践能力，培养数据库研发人才，<strong>拓数派联手开源联盟 PG 分会，走进北京大学</strong>，同国家特色化示范性软件学院：北京大学软件与微电子学院合作，进行了 2024 年《北京大学 PostgreSQL 内核开发：从入门到进阶》研究生公选课的打造与授课。</p><p style="text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-b22e29aa29909713ba2a7862c06e3909db1.png" referrerpolicy="no-referrer"></p><p style="text-align:center">公选课讲师及校方合影</p><p>本次课程由北京大学荆琦教授联合中国开源软件推进联盟（COPU）组织发起，面向国内一流技术企业收集优秀课程，已成功开展了 3 年。<strong>2024 年，拓数派联手 PG 开源分会精心组织了新学期的数据库内核开发从入门到进阶内容，经过评审成功进入北京大学研究生开源开发实践公选课框架。</strong> 公选课开课报告会已于 2 月 20 日成功举行。</p><p style="text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-fa96e68a8eefabc87ccf8c5eea797339cb6.png" referrerpolicy="no-referrer"></p><p style="text-align:center">公选课开课报告会合影</p><p>本次课程时长 16 周，共包括 32 个课时内容，针对数据库的数据加密、数据存取和优化器原理与实践三部分内容展开讲授。<strong>其中拓数派产品市场总监吴疆作为《优化器原理与实践》部分的讲师，结合云原生虚拟数仓 PieCloudDB Database 在云原生优化器的打造经验，</strong> 将以开源数据库 PostgreSQL 作为实操数据库，针对查询优化器的基本原理和工作流程展开授课。通过四周的学习，学生将学会如何使用统计信息和成本模型来评估不同的查询执行计划，并选择最佳的执行路径。我们还将讨论常见的查询优化技术，包括索引选择、连接算法和谓词下推等。</p><p style="text-align:center"><img alt="" src="https://oscimg.oschina.net/oscnet/up-590815f6a6a18c08c611006284b9f326ed2.png" referrerpolicy="no-referrer"></p><p style="text-align:center">吴疆在开课报告会上发表演讲</p><p>拓数派一直致力于促进产学研合作，通过校园行系列活动「校园 Pie」的组织与打造、高校课程的合作、联合实验室的创建等多种方式，希望通过前沿技术、产业界案例和应用的分享，促进学术界与产业界的进一步融合，为数据库从业人才的培养和交流平台的打造提供更多的支持。新的一年，拓数派将不断努力，在产品、商业和生态的打造上继续前行！</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:16:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280576</guid>
            <link>https://www.oschina.net/news/280576</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 小红书自主研发的跨平台播放器 REDPlayer]]>
            </title>
            <description>
                <![CDATA[<h1><a id="redplayer" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#redplayer"></a>REDPlayer</h1><p><img src="https://gitee.com/rte-dev/RedPlayer/raw/main/redplayer.jpg" alt="示例图片" referrerpolicy="no-referrer"></p><p><img src="https://img.shields.io/badge/release-v1.0.0-blue" alt="GitHub release (latest by date including pre-releases)" referrerpolicy="no-referrer"><img src="https://img.shields.io/badge/license-LGPL2.1-blue" alt="GitHub license" referrerpolicy="no-referrer"></p><h2><a id="about-redplayer" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#about-redplayer"></a>About REDPlayer</h2><p>REDPlayer 是一款由小红书自主研发的跨平台 (支持 Android、iOS、HarmonyOS 等平台) 播放器。不同于行业其他播放器，REDPlayer 具有结构简单、耦合度低、功能边界清晰等特点，提供了多种接入方式，技术人员可根据需要灵活选择，既可快速集成 SDK 使用，也可基于源码进行定制开发。</p><p>REDPlayer 的宗旨是让开发者可以快速明确地了解播放器的基本构造，并可根据个人需求进行简单扩展，满足不同用户的多样需求，可作为学生学习的基础工具，也可作为企业的商用平台。</p><p>REDPlayer 支持点播、直播场景下的多种协议和格式 (如 HLS、MP4、FLV 等)，并可二次扩展更多协议 (如：RTC 等)。每个模块均是解耦的，开发者可以根据需要挂载自定义模块，如自研解码器、渲染器等。</p><table><thead><tr><th>Platform</th><th>Build Status</th></tr></thead><tbody><tr><td>Android</td><td>Done</td></tr><tr><td>iOS</td><td>Done</td></tr><tr><td>others</td><td>In Coming</td></tr></tbody></table><h3><a id="quickstartdemo" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#quickstartdemo"></a>Quickstart/Demo</h3><ul><li><p>Android <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/android/README.md">Quickstart</a>/ <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/android/app/README.md">Demo</a></p></li><li><p>IOS <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/ios/README.md">Quickstart</a>/ <a href="https://gitee.com/rte-dev/RedPlayer/blob/main/source/ios/RedPlayerDemo">Demo</a></p></li><li><p>In coming...</p></li></ul><h3><a id="features" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#features"></a>Features</h3><table><thead><tr><th>Function</th><th>Function Description</th><th>Current Support Situation</th><th>Other Notes</th></tr></thead><tbody><tr><td>Rich Format</td><td>Supports rich audio and video formats such as FLV, HLS, MP4, MP3, and Vorbis</td><td>✅</td><td></td></tr><tr><td>DASH Protocol</td><td>Supports standard protocol DASH</td><td>✅</td><td>Optimized version of DASH for on-demand support in the later stage</td></tr><tr><td>HDR</td><td>Supports multiple HDR formats such as HDR10/HLG. Distribution and playback support are provided according to the model</td><td>✅</td><td></td></tr><tr><td>URL Playback</td><td>Supports playback of local and network videos via URL</td><td>✅</td><td></td></tr><tr><td>Log Reporting</td><td>Supports reporting player logs and statistics related to playback point information</td><td>✅</td><td></td></tr><tr><td>Abnormal Analysis</td><td>Supports obtaining corresponding abnormal information through log analysis</td><td>✅</td><td></td></tr><tr><td>H.264 Playback &amp; Hardware Decoding</td><td>Supports H.264 video sources and hardware decoding</td><td>✅</td><td></td></tr><tr><td>H.265 Playback &amp; Hardware Decoding</td><td>Supports H.265 video sources and hardware decoding</td><td>✅</td><td>Software decoding capabilities will be supported in the later stage</td></tr><tr><td>Automatic switching between software and hardware decoding</td><td>Automatically switches to software decoding when the terminal does not support hardware decoding</td><td>✅</td><td></td></tr><tr><td>Playback Control</td><td>Supports playback control functions such as start, end, pause, and resume</td><td>✅</td><td></td></tr><tr><td>Accurate Seeking</td><td>Supports accurate seeking to a specified position, which can be accurate to the frame level</td><td>✅</td><td></td></tr><tr><td>Dynamic Dropping</td><td>Start dynamic dropping when the frame rate exceeds 60 fps</td><td>✅</td><td></td></tr><tr><td>Replay</td><td>Supports manually triggered replay after the video ends</td><td>✅</td><td></td></tr><tr><td>Continue playing</td><td>Supports setting the continuous playing time point</td><td>✅</td><td></td></tr><tr><td>Loop Playback</td><td>Supports automatic replay after video playback ends</td><td>✅</td><td>Parameter configuration is required</td></tr><tr><td>Variable Speed Playback</td><td>Supports variable speed playback of 0.5-2 times, and the audio 实现 variable speed without changing the pitch</td><td>✅</td><td></td></tr><tr><td>Definition Adjustment</td><td>Supports switching between multiple definitions for on-demand and transcoding</td><td>✅</td><td></td></tr><tr><td>Seeking within the Cache</td><td>Supports seeking without clearing the buffer for cached video content</td><td>✅</td><td></td></tr><tr><td>Packing Mode</td><td>Supports picture cropping and filling</td><td>✅</td><td></td></tr><tr><td>Private DRM</td><td>Supports private DRM encryption schemes</td><td>✅</td><td></td></tr><tr><td>Adaptive Bitrate</td><td>When playing HLS/DASH, it supports automatically selecting the definition for playback through bandwidth prediction</td><td>✅</td><td>Currently only supports selection before playback, and does not support abr during playback</td></tr><tr><td>Volume Settings</td><td>Supports real-time adjustment of system volume and mute operation</td><td>✅</td><td></td></tr><tr><td>Pure Audio Playback</td><td>Supports playing audio only</td><td>✅</td><td></td></tr><tr><td>Preload</td><td>Supports setting the preload size to reduce the time spent on the first screen</td><td>✅</td><td></td></tr><tr><td>Play While Downloading</td><td>Supports playing while caching and downloading subsequent content, and you can set network policies</td><td>✅</td><td></td></tr><tr><td>Playback Callback</td><td>Supports playback status callback, first frame callback, playback completion or failure callback</td><td>✅</td><td></td></tr><tr><td>Retry on Playback Failure</td><td>Automatically retries on playback failure</td><td>✅</td><td>Only supports retries for non-4XX and 5XX classes</td></tr><tr><td>Real-time Download Speed</td><td>Supports getting real-time download speed</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>Encrypted Streaming PlayBack</td><td>Support for on-demand transcoding of encrypted streams</td><td>❌</td><td>Need for custom development</td></tr><tr><td>Screenshot Function</td><td>Support for capturing any frame of the playback picture</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>Thumbnail Preview</td><td>Support for previewing progress bar thumbnails (sprite map)</td><td>❌</td><td>Related to business, not currently supported</td></tr><tr><td>Set player size</td><td>Support for customizing the width and height of the player</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>External subtitles</td><td>Support for two docking modes of external subtitles: full-link solution and pure client solution</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>Client super-resolution</td><td>The client performs super-resolution enhancement on low-quality videos</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>H.266 playback</td><td>Support for video playback in H.266 encoding format</td><td>❌</td><td>Will be supported in later versions</td></tr><tr><td>AV1 playback</td><td>Support for video playback in AV1 encoding format</td><td>❌</td><td>Will be supported in later versions</td></tr></tbody></table><h3><a id="open-content" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#open-content"></a>Open Content</h3><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">```bash</span><span id="LC2" class="line"># Describe the main contents of current open source and the estimated time and contents of the next open source</span><span id="LC3" class="line">```</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ul><li><a href="https://gitee.com/rte-dev/RedPlayer/blob/main/CONTENTS.md">CONTENTS.md</a></li></ul><h3><a id="usage" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#usage"></a>Usage</h3><ul><li>You can directly integrate your project by calling the interface or compile independently.</li><li><a href="https://gitee.com/rte-dev/RedPlayer/blob/main/INTERFACES.md">INTERFACES.md</a></li></ul><h3><a id="build-environment" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#build-environment"></a>Build Environment</h3><ul><li><p><strong>Install Homebrew &amp; Git</strong></p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"> /bin/bash <span class="nt">-c</span><span class="s2">"</span><span class="si">$(</span>curl <span class="nt">-fsSL</span> https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span class="si">)</span><span class="s2">"</span></span><span id="LC2" class="line"> brew <span class="nb">install </span>git</span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li><li><p><strong>Build Android</strong></p><p><strong>Using Android SDK</strong></p><p><a href="https://gitee.com/link?target=https%3A%2F%2Fdeveloper.android.com%2Fstudio%3Fhl%3Den">Andrioid SDK</a> is android project base dependency. You should download and then config with the following command:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># add this line to your ~/.bash_profile or ~/.profile, the android sdk will work</span></span><span id="LC2" class="line"><span class="nb">export </span><span class="nv">ANDROID_SDK</span><span class="o">=</span>&lt;your sdk path&gt;</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># My build environment:</span></span><span id="LC5" class="line"><span class="c"># macOS 14.0</span></span><span id="LC6" class="line"><span class="c"># Android Studio Flamingo | 2022.2.1 Patch 2</span></span><span id="LC7" class="line"><span class="c"># gradle version: 7.5.0</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li><li><p><strong>Build iOS</strong></p><p><strong>Using CocoaPods</strong></p><p><a href="https://gitee.com/link?target=http%3A%2F%2Fcocoapods.org">CocoaPods</a> is a dependency manager for Cocoa projects. You can install it with the following command:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="nv">$ </span>gem <span class="nb">install </span>cocoapods</span><span id="LC2" class="line"></span><span id="LC3" class="line"><span class="c"># My build environment:</span></span><span id="LC4" class="line"><span class="c"># macOS 14.0</span></span><span id="LC5" class="line"><span class="c"># Xcode 15.2 (15C500b)</span></span><span id="LC6" class="line"><span class="c"># Cocoapods version: 1.10.2</span></span><span id="LC7" class="line"><span class="c"># Ruby 3.0.6p216</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li></ul><h3><a id="latest-changes" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#latest-changes"></a>Latest Changes</h3><ul><li><a href="https://gitee.com/rte-dev/RedPlayer/blob/main/NEWS.md">NEWS.md</a></li></ul><h3><a id="support" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#support"></a>Support</h3><ul><li>Please try to discuss technical issues (<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FRTE-Dev%2FRedPlayer%2Fissues">https://github.com/RTE-Dev/RedPlayer/issues</a>) publicly on github, and do not inquire privately by email. We will not reply one by one.</li></ul><h3><a id="licence" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#licence"></a>Licence</h3><h4><a id="self-licence" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#self-licence"></a>Self Licence</h4><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Copyright (c) 2024 xiaohongshu</span><span id="LC2" class="line">Licensed under LGPLv2.1 or later</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h4><a id="dependence-licence" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#dependence-licence"></a>Dependence Licence</h4><ul><li>ffmpeg: LGPL v2.1+</li><li>soundtouch: LGPL v2.1</li><li>libcurl: MIT License</li><li>c-ares: MIT License</li><li>glide: MIT License</li><li>Masonry: MIT License</li><li>openssl: Apache License 2.0</li><li>PictureSelector: Apache License 2.0</li></ul><h3><a id="law-and-rule" class="anchor" href="https://gitee.com/rte-dev/RedPlayer#law-and-rule"></a>Law And Rule</h3><p>All rights and explanations belong to Xiaohongshu，you should always ask your lawyer for these stuffs before use it in your product.</p>]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/rte-dev/RedPlayer</guid>
            <link>https://gitee.com/rte-dev/RedPlayer</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 语言大模型的浮点运算分配]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p><img height="352" src="https://oscimg.oschina.net/oscnet/203eda90-2b91-4ed0-9fbf-02459d2253d5.jpg" width="578" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px; text-align:left"><span>本文</span><span>通过实证分析展示了实际 LLM 模型的 FLOPS 分配情况，并与理论分析进行对比。</span><span>通过理论和实证相结合的方式，本文为理解和优化语言大模型的性能提供了有益见解。</span></p><p>&nbsp;</p><p><span>作者 Finbarr Timbers 是一名机器学习研究员，曾就职于 DeepMind。（以下内容由 OneFlow 编译发布，转载请联系授权。原文：<span style="background-color:#efefef">https://www.artfintel.com/p/where-do-llms-spend-their-flops</span></span><span><span>）</span></span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">作者 |&nbsp;Finbarr Timbers</span></strong></p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">OneFlow 编译</span></strong></p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">翻译｜宛子琳、杨婷</span></strong></p><p>&nbsp;</p><p><span style="color:#3f3f3f">本文对 LLM 的性能进行了理论分析，然后通过详细分析一个实际的 LLM，查看实证结果与理论之间的差异。首先是理论分析部分，我会借助</span><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247491309%26idx%3D1%26sn%3D407fefb7ec76a2c9fdfe1ae960f7de4d%26chksm%3Dfe4190dbc93619cd0b9bfecb979e142a125fd8548d323dd9bdc2cbeb619400202e6e1affced0%26scene%3D21%23wechat_redirect" target="_blank"><strong><span>Kipply 的优质博文</span></strong></a><span style="color:#3f3f3f">来填充细节。基本结论是：对于标准解码器模型，FLOPS（每秒浮点运算数）的分配如下（按每层计算）：</span></p><p>&nbsp;</p><ol><li><p><span style="color:#3f3f3f">6d^2 用于计算 QKV（Query（查询）、Key（键）和 Value（值））</span></p></li><li><p><span style="color:#3f3f3f">2d^2 用于计算注意力输出矩阵，softmax(Q @ K.T) @ V</span></p></li><li><p><span style="color:#3f3f3f">16d^2 用于运行前馈神经网络（FFN）&nbsp;</span></p></li></ol><p>&nbsp;</p><p><span style="color:#3f3f3f">总计 24d^2 FLOPS。从百分比看，25% 的时间用于计算 QKV，约 8% 的时间用于计算注意力输出矩阵，约 66% 的时间用于运行 FFN。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">那么用于注意力机制的时间呢？众所周知，注意力机制方程为：</span></p><p>&nbsp;</p><p style="text-align:center"><img height="84" src="https://oscimg.oschina.net/oscnet/fec217d2-600c-4ed3-850a-7407dd0b583b.jpg" width="315" referrerpolicy="no-referrer"></p><p style="text-align:left">&nbsp;</p><p style="text-align:left"><span style="color:#3f3f3f">假设你正在使用 KV 缓存，Q（查询）、K（键）和 V（值）都是 d 维向量（等价于（d，1）矩阵）。每个点积大约需要 2d 个 flops（</span><span style="color:#888888"><em><span>https://www.stat.cmu.edu/~ryantibs/convexopt-F18/scribes/Lecture_19.pdf</span></em></span><span style="color:#3f3f3f">），加上进行 d 次除法需要 d 个 flops，总计约为 5d 个 flops，四舍五入为零。</span></p><p>&nbsp;</p><p style="text-align:center"><img height="112" src="https://oscimg.oschina.net/oscnet/4c742fa6-51fa-4652-bcf4-06fcb9d9263e.png" width="253" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span style="color:#3f3f3f">当 d 等于 4096（在 Llama7b 中的取值），这仅为 0.005%，几乎可以忽略不计。这似乎表明注意力机制不重要，但事实并非如此。我们之所以使用 KV 缓存（以及 flash attention 等）正是因为它们非常重要，可以将其类比于米尔顿·弗里德曼的恒温器（</span><span style="color:#888888"><em><span>https://worthwhile.typepad.com/worthwhile_canadian_initi/2010/12/milton-friedmans-thermostat.html</span></em><span>，感谢 @bradchattergoon</span></span><span style="color:#3f3f3f">）：</span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span>假设一个房屋配备了一个运行良好的恒温器，那么我们能看到炉子燃烧的油量（M）与室外温度（V）之间存在强烈的负相关关系，同时炉子燃烧的油量（M）与室内温度（P）之间没有相关性，此外，室外温度（V）与室内温度（P）之间也没有相关性。</span></em></span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span>一位计量经济学家观察数据后得出结论：燃烧的油量对室内温度没有影响，室外温度对室内温度也没有影响。唯一的影响似乎是燃烧油量会降低室外温度。</span></em></span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span>观察相同的数据，第二位计量经济学家得出了完全相反的结论。他认为，室外温度（V）增加唯一的影响是会减少耗油量（M），而不会对室内温度（P）产生任何影响。</span></em></span></p><p>&nbsp;</p><p><span style="color:#888888"><em><span style="color:#888888">尽管两位计量经济学家得出了不同的结论，但他们一致认为燃烧油量（M）和室外温度（V）对室内温度（P）没有影响。基于这一共识，他们决定关闭炉子，不再浪费金钱购买燃油。</span></em></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">KV 缓存需要 O(T) 的内存（其中 T 是我们希望生成的词元数），因此内存需求成本较高，这一点可以参考公司股票（$NVDA）情况。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">KV 缓存有多大呢？对于每个词元，需要存储以下数量的字节（第一个 2 是因为我们假设使用 bf16 精度，因此每个参数占用 2 个字节；第二个 2 是因为需要同时存储 K 和 V 张量）：</span></p><p style="text-align:center"><img height="77" src="https://oscimg.oschina.net/oscnet/64861c4b-a4cb-4b5a-94e8-85c4b0b6f198.jpg" width="415" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span style="color:#3f3f3f">注意，根据假设，n_heads*d_head=d_model=d，因此字节数为 4*层数*d。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">GPT-3 有 96 层，d_model 为 12288，每个词元需要 4.72MB。因此，生成 2048 个词元需要 5.6GB 的内存。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">尽管如此，要使用给定模型生成给定长度的序列，我们仍需使用与 KV 缓存相同的内存量，只是在每次前向传播结束时将其丢弃。因此，我们并不需要更多内存。从某种意义上说，KV 缓存不占用内存（至少在 Jax 中是如此，除了一些繁琐的 bookkeeping 工作）。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">对于一些新兴架构（例如 Mistral 7B）又有何不同呢？Mistral 7b 使用了分组查询注意力（Llama2 也使用了类似的注意力机制，就好像这两个模型的作者存在某种联系。）和滑动窗口注意力。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">在分组查询注意力中，我们可以在多头之间共享一个 KV 投影（MQA），具体而言，可以是所有注意力头之间共享一个 KV 投影（MQA，</span><span style="color:#888888"><em><span>https://arxiv.org/abs/1911.02150</span></em></span><span style="color:#3f3f3f">），或者将其分成多个组（GQA，</span><span style="color:#888888"><em><span>https://arxiv.org/abs/2305.13245v</span></em><span>3</span></span><span style="color:#3f3f3f">）。这两种方法都等同于具有较小 d_model 的标准多头注意力（MHA）。在之前的 KV 缓存计算中，我们假设注意力头的数量乘以头的维度等于模型维度，但是在 MQA/GQA 中，我们放宽了这一假设。KV 缓存公式如下：</span></p><p>&nbsp;</p><p style="text-align:center"><img height="66" src="https://oscimg.oschina.net/oscnet/5062dc02-0be8-4534-b2b4-39320bbfa8a9.jpg" width="345" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span style="color:#3f3f3f">可以转换为：</span></p><p style="color:#494949">&nbsp;</p><p style="text-align:center"><img height="71" src="https://oscimg.oschina.net/oscnet/96864198-3330-4aab-8f30-22d5cd408e8e.jpg" width="332" referrerpolicy="no-referrer"></p><p style="color:#494949">&nbsp;</p><p><span><span style="color:#3f3f3f">其中，注意力头的数量乘以头的维度就是模型的有效维度。因此，可以看到，随着 KV 头数量的减少，KV 缓存大小呈线性减小（ 这也是 GQA/MQA 方法背后的关键动机之一）。</span></span></p><p>&nbsp;</p><p><span><span style="color:#3f3f3f">Llama{1,2} 模型参数如下：</span></span></p><p style="color:#494949">&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/7a2e216e-dbda-47b6-96f4-17d8c0b5f9b6.png" width="1060" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span><span><span style="color:#3f3f3f">Llama 2 中，每个词元所需的 KV 缓存如下：</span></span></span></p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/0acb1a24-9589-4d7e-bc1f-6be3cbe3a123.png" width="auto" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">在没有分组查询注意力（GQA）的情况下，34B 模型需要的 KV 缓存内存是原来的 5 倍，而 70B 模型需要的 KV 缓存内存是原来的 8 倍。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">Llama/Mistral 的另一个改进是滑动窗口注意力，它保证我们可以将 KV 缓存限制在窗口大小，对于 Llama7B 来说，窗口大小为 4096。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><strong><span style="color:#f6ab00">1</span></strong></p><p style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">性能驱动的架构变化</span></strong></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">如前所述，LLM 每层使用了 24d^2 个 flops。增加的层数将线性扩展 flops 和参数数量，增加模型宽度会二次方扩展模型大小。需要注意的是，这是因为参数的数量与 d_model 的平方成正比，因为我们的大多数层是从一个 d_model 输入向量转变为一个 d_model 的输出向量，所以权重矩阵的尺寸为 (d_model, d_model)。换句话说，计算量与参数的数量呈正比，增加 d_model 会使参数数量呈二次方增加。模型深度增加一倍会使参数数量翻倍，但模型宽度增加一倍会使参数数量增加四倍。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">尽管如此，更宽的模型的优势是能够更好地并行化。要计算第 N 层，必须首先计算前面的 N-1 层。这在高效并行化上十分困难，尤其是在训练期间，而通过张量并行化方法，跨多个 GPU 拆分单个层要容易得多。如果你主要关心时延，那么选择更宽的模型可能更合适。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><strong><span style="color:#f6ab00">2</span></strong></p><p style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">实证分析</span></strong></span></p><p style="margin-left:8px; margin-right:8px; text-align:center">&nbsp;</p><p><span style="color:#3f3f3f">我使用 Colab 进行了这项分析。（</span><span style="color:#888888"><em>https://colab.research.google.com/drive/1TH6AKsICZqlFoW1ph8h3wsF7q7qVMF8T?usp=sharing</em></span><span style="color:#3f3f3f">）</span></p><p><span style="color:#3f3f3f">以下是单次前向传播的高级概要（我的网站上有交互式概要：</span><span style="color:#888888"><em>https://finbarr.ca/static/llama-profile.svg</em></span><span style="color:#3f3f3f">）：</span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/d803a059-2155-4d20-bdfe-d09119f0d0a4.png" width="2378" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">可以看到，本次运行的总时间中有 4.88% 用於单次前向传播。在前向传播中，有 1.98% 的时间用于注意力机制，有 2.58% 的时间用于多层感知机（MLP）。在前向传播的总时间中，有 40% 的时间用于注意力层，53% 用于 MLP。在注意力层内部，时间分配在 4 个不同的线性层上，其中有 2 个线性层花费的时间大致相同（linear_1、linear_2），一个花费的时间多 50%（linear_3），另一个则是前两者的两倍（linear_0）。我猜测 linear_0 正在计算查询嵌入，而 linear_1/2 正在计算键和值嵌入。请注意，由于 KV 头的数量较少，计算速度要快得多！GQA（Query-aware Attention）带来了明显的差异，即便所使用的注意力机制（</span><span style="color:#888888"><em>xformers.ops.memory_efficient_attention</em></span><span style="color:#3f3f3f">）要求 QKV 嵌入被广播到相同的大小。</span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">理论分析预测，2/3 的时间将用于计算 FFN，1/3 将用于计算注意力机制。这基本符合我们上面所看到的情况。我们花在计算注意力机制上的时间略多于 MLP，但我怀疑这是因为 MLP 正在为 Torch 执行一个经良好优化的路径。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><strong><span style="color:#f6ab00">3</span></strong></p><p style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">性能变化</span></strong></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">接下来，我对 Llama2 进行了一系列实验，涉及模型宽度和深度的调整。以下是实验结果：</span></span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/0d787e3e-65db-48e6-9f1b-241cb3529be4.png" width="567" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">结果非常有趣。可以看到，隐藏维度为 1024 和 1536 的两个模型的速度基本没有变化（1.10 秒 vs1.11 秒），隐藏维度为 1024 和 2048 的模型只发生了轻微变化（1.15 秒 vs1.10 秒）。然而，当我们比较隐藏维度为 2048（1.15 秒）、3072（1.41 秒）和 4096（1.82 秒）的模型时，可以看到速度类似于线性扩展！</span></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">对此，我的看法是，调度 kernel 和实际执行矩阵乘法中存在较大的开销。这是在 T4 上运行的，尽管按现在的标准来看有些过时，但仍具有 65 TFLOPS 的 bf16 计算能力。如果我们将两个 1024x1024 的矩阵相乘，这就需要 1G FLOP 的计算能力，因此，理论上，我们可以每秒乘以 65000 个 1024x1024 的矩阵。实际上，我们只能得到其 60-80% 的性能，但仍然是每秒 40000 次矩阵乘法。如今的 GPU 拥有大量核心，T4 有 2560 个 CUDA 核心，每个核心的运行频率在 585 到 1590 MHz 之间。因此，任何能够并行化的任务都表现良好，但是那些需要顺序计算的任务则不会得到优化。我认为，这就是图中所看到的的情况——没有足够的并行性来充分利用 GPU。</span></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">Transformer 的深度使性能与预期完全一致：推理时间与深度呈线性增长。最深的模型可能存在一些噪声，但它的性能表现相当稳定。</span></span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/64122776-737e-4d29-bc3e-bdd23f972cf0.png" width="567" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#3f3f3f">接下来，我计算了生成更多词元所需的成本（对每个词元数量进行了 10 次运行，以减少噪音）：</span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/d3c4dac0-6e2f-4efa-b5d6-a3cfcc0740b7.png" width="auto" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span style="color:#3f3f3f">正如预期的那样，完全呈线性增长，因为 Llama2 使用了 KV 缓存。如果我们查看保留的内存，就会看到 KV 缓存与预期的一致（在某种程度上）：</span></span></p><p>&nbsp;</p><p style="color:#494949"><span><img height="auto" src="https://oscimg.oschina.net/oscnet/5e09bf5a-80bb-44e5-b570-fe8e6c840b03.png" width="571" referrerpolicy="no-referrer"></span></p><p>&nbsp;</p><p><span style="color:#a5a5a5"><span><span><span><span style="color:#3f3f3f">可以看到，模型每增加 20 个词元，内存占用就会增加约 2.1MB。由于该模型的 d_model 为 1024，有 8 个隐藏层，因此需要 4 * num_layers * d_model 字节的内存，即 4*8*1024 字节=每词元 32KB 的内存。理论上我们只需要 640KB 的内存。目前还不清楚额外的 3 倍开销是从哪里产生的。我怀疑是因为执行还不够高效。</span></span></span></span></span></p><p>&nbsp;</p><span id="OSC_h3_1"></span><h3>&nbsp;</h3><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">【语言大模型推理最高<strong><span>加速 11</span></strong>倍】</span></strong><span style="color:#3f3f3f">SiliconLLM 是由硅基流动开发的高效、易用、可扩展的 LLM 推理加速引擎，旨在为用户提供开箱即用的推理加速能力，显著降低大模型部署成本，加速生成式 AI 产品落地。（技术合作、交流请添加微信：SiliconFlow01）</span></p><p style="margin-left:8px; margin-right:8px">&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/838dbcc5-3a63-4cde-8f52-3b567a5f020a.png" referrerpolicy="no-referrer"></p><p style="text-align:left"><span style="color:#3f3f3f">SiliconLLM 的吞吐最高提升近<strong>4</strong>倍，时延最高降低近<strong>4</strong>倍</span></p><p style="text-align:center">&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/d7781a37-f9dd-4d5f-b5ef-7767cc2816af.png" referrerpolicy="no-referrer"></p><p style="text-align:left"><strong><span style="color:#3f3f3f">数据中心+PCIe</span></strong><span style="color:#3f3f3f">：SiliconLLM 的吞吐最高提升近<strong>5</strong>倍；<strong>消费卡场景</strong>：SiliconLLM 的吞吐最高提升近<strong>3</strong>倍</span></p><p style="text-align:center">&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/d42ca228-3463-4797-9dfb-454d4682d478.png" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px"><strong><span style="color:#3f3f3f">Sy</span><span style="color:#3f3f3f">stem Prompt 场景</span></strong><span style="color:#3f3f3f">：SiliconLLM 的吞吐最高提升<strong>11</strong>倍；<strong>MoE 模型</strong>：推理 SiliconLLM 的吞吐最高提升近<strong>10</strong>倍</span></p><p>&nbsp;</p><p><span style="color:#888888">其他人都在看</span></p><span id="OSC_h3_2"></span><h3>&nbsp;</h3><ul><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493221%26idx%3D1%26sn%3D0c75b8115f4d4a27c8a5d6505a0a4986%26chksm%3Dfe426853c935e145d21abd30e0ceb29486c9e306032dfcb3d071ce34d171425c11341a57d7bf%26scene%3D21%23wechat_redirect" target="_blank">800+页免费「大模型」电子书</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493108%26idx%3D1%26sn%3Db254dff8281096bf6f5462489e94658a%26chksm%3Dfe426bc2c935e2d404fb803985241ea05109aa9d7925a1876304a51c322fa5e6e57dd41e2e66%26scene%3D21%23wechat_redirect" target="_blank">语言大模型的推理技巧</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247491309%26idx%3D1%26sn%3D407fefb7ec76a2c9fdfe1ae960f7de4d%26chksm%3Dfe4190dbc93619cd0b9bfecb979e142a125fd8548d323dd9bdc2cbeb619400202e6e1affced0%26scene%3D21%23wechat_redirect" target="_blank">语言大模型的推理演算</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493321%26idx%3D1%26sn%3Dffc39c67080fefb01f1790e285a5085b%26chksm%3Dfe4268ffc935e1e9c8aeb5a095f08868a5eb8df90c1f46eb1e07044ec34ae6404d14b8b5926b%26scene%3D21%23wechat_redirect" target="_blank">语言大模型推理加速指南</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492811%26idx%3D1%26sn%3D916e330a2a4152dab3192635c3e475fa%26chksm%3Dfe426afdc935e3eb2f371ff5f56247c95800ce91a950a89bea871c26ddc4c3d13371acf03978%26scene%3D21%23wechat_redirect" target="_blank">语言大模型推理性能工程：最佳实践</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493149%26idx%3D1%26sn%3Dfd0369875ad89e8ad173935ec7b38126%26chksm%3Dfe42682bc935e13d91f1ae73cb0135cca0d815c0356baef29c20ab985c9279280e28ae956642%26scene%3D21%23wechat_redirect" target="_blank">迈向 100 倍加速：全栈 Transformer 推理优化</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493282%26idx%3D1%26sn%3D7f91a174ab4cccf16303aa3ce11afac7%26chksm%3Dfe426894c935e182d15b65dbf6d1e3d4d12398664ceae848f6fa79ee289dff1fad23a9c8aea5%26scene%3D21%23wechat_redirect" target="_blank">Mistral AI:LLM 推理的吞吐、时延及成本空间</a></p></li></ul><p><span style="color:#3f3f3f">试用 OneDiff:&nbsp;<strong><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgithub.com%2Fsiliconflow%2Fonediff" target="_blank">github.com/siliconflow/onediff</a></strong></span></p><span id="OSC_h2_3"></span><h2 style="margin-left:8px; margin-right:8px">&nbsp;</h2><p><img src="https://oscimg.oschina.net/oscnet/4d6d060f-6daf-4d9f-a3db-4de78e4b9745.png" width="578" referrerpolicy="no-referrer"></p><p>&nbsp;</p></div><p style="color:#858585">本文分享自微信公众号 - OneFlow（OneFlowTechnology）。<br> 如有侵权，请联系 support@oschina.cn 删除。<br> 本文参与「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源创计划</a>」，欢迎正在阅读的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 28 Feb 2024 02:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/oneflow/blog/11030216</guid>
            <link>https://my.oschina.net/oneflow/blog/11030216</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[墨干理工套件 V1.2.5 LTS 发布了]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(255, 255, 245, 0.86); margin-left:0; margin-right:0; text-align:start">墨干理工套件 V1.2.5LTS<span>本次更新主要是修复既有错误、改进用户体验。</span></p><blockquote><p style="color:var(--vp-c-text-2); margin-left:0; margin-right:0">注意：一个重大的变更是默认开启了实验选项<code>DO NOT use UTF-8 for CJK in TM format</code>，保持和 GNU TeXmacs 2.1.2 的兼容性。之前在墨干 V1.1.x 使用了 TM 格式中显示中文特性的用户，可以关闭这个选项以正确加载墨干 V1.1.x 生成的可以在纯文本编辑器中正常显示中文的文档。v1.1.x 的在 tm 文档中显示中文的特性可能会带来一些严重的兼容性问题，故而不建议用户使用可以在纯文本编辑器中正常显示中文的 TM 文档。从墨干 V1.2.6 开始，我们会实验性地支持 TMU 格式以 UTF-8 编码显示中文。TMU 格式经过多个版本迭代，最终才会对所有用户开放。</p></blockquote><p>墨干理工套件 V1.2.5 LTS 包含以下组件：</p><ul><li>墨干 V1.2.5LTS (Mogan Research v1.2.5 LTS)</li></ul><p>墨干 V1.2.5 LTS 这个版本标志着墨干作为 GNU TeXmacs 的一个发行版，基本完成了从 Qt 4 到 Qt 6 的升级，从 GNU Guile 1.8 到 S7 Scheme 的执行引擎切换，从 Autotools 到 xmake 的构建工具切换等重要的基础设施的建设。在这个坚实的基础上，从 V1.2.0 开始，我们以一月一版本的愉快的高强度研发迭代下，大大改善了墨干的可用性和易用性。</p><p>墨干 V1.2.5 LTS 是为期至少一年的长期支持版，其目的是服务教育工作者。一年的长期支持，是为了覆盖 2024 年的春季学期和秋季学期。欢迎教育工作者们使用墨干制作试卷、幻灯片、讲义以及撰写书籍。在未来的一个月内，我们会发布 V1.2.5.1，以修复 V1.2.5 中仍旧存在的错误。后续的补丁版本会按需发布，欢迎教育工作者们使用墨干并反馈错误。</p><p>墨干 V1.2.5 LTS 保留了对 Qt 5 的支持，这是为了以一种比较低的维护成本为老旧系统提供构建墨干的可行性：比如 Windows 7，macOS 10.x，龙芯生态的旧世界。</p><p>从墨干 V1.2.6 开始，我们移除了对 Qt 5 的支持，轻装上阵；另外，我们也会放慢研发迭代的节奏，每两个月发布一个版本，作为 TeXmacs 发行版中的先行者，稳步迭代进化。</p><h2>影响用户体验的详细更新<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmogan.app%2Fzh%2Fguide%2FChangeLog_v1.2.5.html%23%25E5%25BD%25B1%25E5%2593%258D%25E7%2594%25A8%25E6%2588%25B7%25E4%25BD%2593%25E9%25AA%258C%25E7%259A%2584%25E8%25AF%25A6%25E7%25BB%2586%25E6%259B%25B4%25E6%2596%25B0" target="_blank">​</a></h2><ul><li>安装包 
  <ul><li>提供 Ubuntu 20.04 的 deb 安装包</li><li>提供 Debian 12 (bookworm) 的 deb 安装包</li><li>提供 Windows 平台的便携版安装包</li></ul></li><li>幻灯片模式 
  <ul><li>修复在 macOS 平台切换幻灯片主题无法切换幻灯片背景图的问题</li></ul></li><li>快捷键 
  <ul><li>macOS：Command 和+/-的组合无论是否按下 Shift 都可以放大或者缩小</li><li>修复未知快捷键被作为文本插入到文档中的问题</li></ul></li><li>用户界面 
  <ul><li>修复在 macOS 平台双击打开含有中文字符的文件失败的问题</li></ul></li><li>排版引擎 
  <ul><li>修复在文档中删除内容出现残影的问题</li><li>修复在幻灯片设置部分内容为双栏时出错的问题</li><li>number 原语支持汉字的数字：一、二、三等</li></ul></li><li>性能优化 
  <ul><li>改进解析 TeXmacs 文档的性能</li><li>改进渲染 Pine 幻灯片主题的性能</li></ul></li><li>PDF 导入 
  <ul><li>新增<code>文件-&gt;导入-&gt;可编辑 PDF</code>用于导入可编辑 PDF，而不是直接使用<code>文件-&gt;打开</code>来打开可编辑 PDF</li></ul></li><li>版本控制 
  <ul><li>修复在 Windows 平台进入<code>版本-&gt;历史</code>之后，无法查看文档历史版本的问题</li></ul></li><li>图像插件 
  <ul><li>改进对 Postscript 图像格式的支持（需要用户手动安装 Ghostscript）</li><li>改进对 SVG 图像格式的支持（需要用户手动安装 Inkscape）</li></ul></li><li>会话插件 
  <ul><li>改进 Scheme 会话中中文变量名、中文字符串的支持</li></ul></li><li>代码插件 
  <ul><li>重新添加了 Python 的代码高亮</li></ul></li></ul><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 27 Feb 2024 12:39:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280535/mogan-1-2-5-released</guid>
            <link>https://www.oschina.net/news/280535/mogan-1-2-5-released</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[aiohttp < 3.9.2 路径遍历漏洞]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h2>漏洞描述</h2><p>aiohttp 是一个开源的用于 asyncio 和 Python 的异步 HTTP 客户端/服务器框架。</p><p>当使用 aiohttp 作为 Web 服务器并设置静态路由时，若 follow_symlinks 选项设为 True，则不会验证指定文件路径是否位于根目录内。攻击者可以通过构造恶意的请求，访问服务器任意文件。</p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fray-project%2Fray%EF%BC%88ray" target="_blank">https://github.com/ray-project/ray（ray</a> dashboard）等多个项目中并未正确配置该参数，也会受到漏洞影响。</p><table><tbody><tr><th>漏洞名称</th><th>aiohttp &lt; 3.9.2 路径遍历漏洞</th></tr></tbody><tbody><tr><td>漏洞类型</td><td>路径遍历</td></tr><tr><td>发现时间</td><td>2024-01-30</td></tr><tr><td>漏洞影响广度</td><td>广</td></tr><tr><td>MPS 编号</td><td>MPS-rxvm-9042</td></tr><tr><td>CVE 编号</td><td>CVE-2024-23334</td></tr><tr><td>CNVD 编号</td><td>-</td></tr></tbody></table><h2>影响范围</h2><p>aiohttp@[1.0.5, 3.9.2)</p><p>python-aiohttp@影响所有版本</p><p>aiohttp@[1.0.5, 3.9.2)</p><h2>修复方案</h2><p>将 aiohttp 升级至 3.9.2 及以上版本</p><p>如果应用程序的静态资源服务不需要符号链接功能，将 follow_symlinks 设置为 False</p><h2>参考链接</h2><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oscs1024.com%2Fhd%2FMPS-rxvm-9042" target="_blank">https://www.oscs1024.com/hd/MPS-rxvm-9042</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnvd.nist.gov%2Fvuln%2Fdetail%2FCVE-2024-23334" target="_blank">https://nvd.nist.gov/vuln/detail/CVE-2024-23334</a></p><h2>免费情报订阅&amp;代码安全检测</h2><p>OSCS 是国内首个开源软件供应链安全社区，社区联合开发者帮助全球顶级开源项目解决安全问题，并提供实时的安全漏洞情报，同时提供专业的代码安全检测工具为开发者免费使用。社区开发者可以通过配置飞书、钉钉、企业微信机器人获取一手的情报。</p><p>免费代码安全检测工具： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.murphysec.com%2F%3Fsrc%3Dosc" target="_blank">https://www.murphysec.com/?src=osc</a></p><p>免费情报订阅： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oscs1024.com%2Fcm%2F%3Fsrc%3Dosc" target="_blank">https://www.oscs1024.com/cm/?src=osc</a></p><p>具体订阅方式详见： <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oscs1024.com%2Fdocs%2Fvuln-warning%2Fintro%2F%3Fsrc%3Dosc" target="_blank">https://www.oscs1024.com/docs/vuln-warning/intro/?src=osc</a></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-4aeef4048430ca1baea7afb51fe0f5dc3dd.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 27 Feb 2024 11:10:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280528</guid>
            <link>https://www.oschina.net/news/280528</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[马斯克抱怨微软 Windows 难用，V 神：加入 Linux！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>特斯拉 CEO 埃隆・马斯克在社交平台说道，上周末他购买了一台新款的 Windows 11 笔记本电脑，却发现必须创建微软账户 (MSA) 才能使用系统，这让他感到非常愤怒，认为这变相地让微软的人工智能 (AI) 访问了他的数据。</p><blockquote><p><img src="https://oscimg.oschina.net/oscnet/up-e5e820c2d6e02c58e0b272f4e8447623295.png" referrerpolicy="no-referrer"><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftwitter.com%2FVitalikButerin%2Fstatus%2F1762363524918227423" target="_blank">https://twitter.com/VitalikButerin/status/1762363524918227423</a></u></em></p></blockquote><p>对此，<span style="background-color:#ffffff; color:#333333">以太坊联合创始人 Vitalik Buterin</span>&nbsp;建议马斯克改用 Linux 桌面发行版。</p><p>X 上的一些用户称赞 V 神推广了开源软件。但也有人指出，Linux 可能不是马斯克的最佳选择，因为他使用 PC 的主要目的是玩游戏。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 27 Feb 2024 09:59:34 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280524</guid>
            <link>https://www.oschina.net/news/280524</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[GPL 抗辩成功——织梦 CMS「系列」版权纠纷迎来重大转折]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><blockquote><p style="text-align:justify"><span style="color:#27ae60"><strong>摘要</strong></span></p><p style="text-align:justify"><span style="color:#4e5f70">原告：上海卓卓网络科技有限公司（以下简称：卓卓公司）</span></p><p style="text-align:justify"><span style="color:#4e5f70">被告：****医院</span></p><p style="text-align:justify"><span style="color:#4e5f70">事件：****医院使用 DedeCMSV5.7-sp1 软件开发网站，卓卓公司以拥有 DedeCMS Biz V1.0 以及后续多个版本的著作权为由，认为医院侵犯了自己的著作权，要求***医院赔偿 5800 元的授权许可费和 8700 元的诉讼费用。</span></p><p style="text-align:justify"><span style="color:#4e5f70">判决：一审法院认为 DedeCMSV5.7-sp1 中包含 GPL 协议下开源的代码，整体应遵守 GPL 协议，****医院使用软件不用支付授权费用，但仍需遵守署名权的要求。</span></p></blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>持续近 3 年、涉及 300 万用户的织梦 CMS 「系列」版权纠纷案在</span></span></span><span><span><span>近</span></span></span><span><span><span>日迎来第一份抗辩成功判决。上海卓卓网络科技有限公司自 2021 年起，以「织梦商业网站内容管理系统【简称：DedeCMS Biz】V1.0」著作权方的身份，在全国各地起诉多个网站中含有 DedeCMS 相关代码的公司，要求赔偿。</span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>2023 年 9 月 11 日，卓卓公司诉****医院侵害计算机软件著作权纠纷案件立案。2024 年 2 月 19 日，江苏省无锡市中级人民法院</span></span></span><span><span><span>作出</span></span></span><span><span><span>一审判决书，认可了 GPL 的</span></span></span><span><span><span>「</span></span></span><span><span><span>传染性</span></span></span><span><span><span>」</span></span></span><span><span><span>，涉案软件（具体版本为 DedeCMSV5.7-sp1）整体按 GPL 对外许可，但要求使用者（被告）在网页底部添加原告网址链接，不得侵犯原告的署名权。</span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>一审判决书在判决书送达之日起十五日，当事人没有提起上诉的，就会生效。据案件知情人士分析，卓卓公司肯定会上诉。</span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>开源中国</span></span></span><span><span><span>从知情人处</span></span></span><span><span><span>获取并查阅判决书。据判决书显示，卓卓公司提出，卓卓通过受让取得了 DedeCMS Biz V1.0 的软件著作权，又在 DedeCMS Biz V1.0 版本的基础上迭代出 DedeCMSV5.5、DedeCMSV5.6、DedeCMSV5.7 等版本。****医院名下网站相关网页相关源代码与卓卓公司享有著作权的涉案软件代码相同，证据包括授权协议中的卓卓公司名称、Powered By DedeCMS、织梦内容管理系统 DEDECMS 的 logo/mark 等等。但****医院并未向卓卓公司购买正版涉案软件，也从未获得过卓卓公司商业使用授权许可。因此卓卓要求****医院支付涉案软件 DedeCMS 软件的授权许可费 5800 元，以及卓卓公司为诉讼所指出的 8700 元费用，共计 14500 元。</span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>而****医院辩称：卓卓虽然通过受取得 DedeCMS Biz V1.0 的软件著作权，但没有证据能够证明卓卓公司是织梦内容管理系统 DedeCMS 软件的著作权人。此外，DedeCMS 是一款以 GPL 协议对外许可发布的开源软件，DedeCMS 后续版本包括涉案权利软件都是在 DedeCMSV3 版本基础上迭代升级的，因此受 GPL 约束。根据 GPL 协议禁止添加商业使用的限制条款，也不允许著作权人就软件本身收取授权许可费。</span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>此外，围绕案件，卓卓公司和****医院还列举了多项诉求与证据，详情可查看一审判决书（尚未生效）。</span></span></span></span></span></span></p><div><blockquote><div><span style="color:#4e5f70">此份判决书由知情人提供并脱密：https://report.oschina.net/api/files/jhim80u9qm1ofsw/79ci47r1rrt9yqm/2023_02_482_cEvUNytTpS.pdf</span></div></blockquote></div><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>法院总结此案件争议焦点有两个：</span></span></span></span></span></span></p><div><blockquote><div><span style="color:#4e5f70">综合双方的诉辩主张，并经双方确认，本院对本案的争议焦点归纳为：</span></div><div><span style="color:#4e5f70">一、涉案权利软件的著作权人是否是卓卓公司；</span></div><div><span style="color:#4e5f70">二、****医院是否有权依据 GPL 协议免费使用涉案权利软件</span></div></blockquote></div><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>针对第一点，法院认为：</span></span></span></span></span></span></p><blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span style="color:#4e5f70">涉案权利软件的著作权人是卓卓公司。</span></p></blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>针对第二点，法院认为：</span></span></span></span></span></span></p><blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span>&nbsp;</span><span><span><span><span style="color:#646a73">涉案软件 DedeCMSV5.7-sp1&nbsp;是包含采用 GPLV2.0 及以后版本做为协议的 sphinxclient 的派生作品。……由此本案中卓卓公司将涉案软件进行发布代表卓卓公司已经接受 GPL 协议。</span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#646a73">……</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span style="color:#646a73">涉案软件应当遵守 GPL 协议，卓卓公司在涉案软件的许可协议中的「商业用途需获得授权」的条款与 GPL 协议</span></span></span><span><span><span style="color:#646a73">相</span></span></span><span><span><span style="color:#646a73">抵触，卓卓公司有义务按照 GPL 协议将涉案软件整体授权给获得许可的人，****医院因 GPL 协议获得了对涉案软件使用的授权，并未侵犯卓卓公司的复制权，卓卓公司无权对此行为请求支付授权费用。</span></span></span></span></span><span>&nbsp;</span></p></blockquote><p><span>不过，法院认为****医院需在网站注明来源是卓卓公司。</span></p><blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span>&nbsp;</span><span><span><span><span style="color:#646a73">卓卓公司在许可协议中载明用户应在使用涉案软件建成网站的主页标注网站链接 www.dedecms.com，该条款并不构成对下游接收者对软件复制、分发、修改权利的限制，应当有效。</span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span><span style="color:#646a73">……</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span style="color:#646a73">GPL 协议作为许可协议有双务性，被许可人在行使复制、发布修改开源软件的权利时，也需要按照协议要求承担相应义务，****医院使用涉案软件 DedeCMS 建成了网站，但未在主页标注卓卓公司创作印记或官网链接，违反了该附加条款，侵害了卓卓公司的署名权，损害了卓卓公司的身份权益。故卓卓公司请求****医院赔偿损失并赔礼道歉，具有事实和法律依据，本院予以支持。</span></span></span></span></span><span>&nbsp;</span></p></blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>最终，法院判定由****医院赔偿卓卓公司经济损失及合理维权开支共 800 元，并在判决生效之日起十日内在其公司网站主页发布为期三十日的赔礼道歉声明。在关于赔偿金额的表述中，法院的考量因素中还提到两点值得关注：</span></span></span></span></span></span></p><blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span style="color:#4e5f70">第三，卓卓公司主张****医院在内的用户需遵守涉案软件的许可协议，但卓卓公司在使用他人代码时却未遵守他人软件的 GPL 许可协议，其行为本身有违诚信原则，具有不正当性。卓卓公司自身对涉案软件的著作权也存在管理不周的情况，软件源代码中记录的版权信息、署名都未直接指向卓卓公司，源代码中不同位置的许可协议条款存在不一致的情形。</span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span style="color:#4e5f70">第四，卓卓公司以涉案软件为权利基础，在全国法院提起大量侵害计算机软件著作权纠纷案件，并因此获得较大收益，该种维权模式既不利于有效打击侵权源头，又大量占用解决纠纷的公共资源，不宜提倡和鼓励。</span></p></blockquote><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>此外，据企查查信息显示，上海卓卓发起了多起侵害计算机软件著作权纠纷诉讼，自 2 月 28 日-3 月 30 日将陆续有案件开庭。</span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><img height="522" src="https://static.oschina.net/uploads/space/2024/0227/170855_5DFc_4489239.png" width="600" referrerpolicy="no-referrer"></p><p style="margin-left:.0001pt; margin-right:0; text-align:left"><span><span><span><span><span><span>开源中国将持续关注 DedeCMS 系列版权纠纷案件进展，本周内也将梳理此系列纠纷的时间线，欢迎知情者私信爆料。</span></span></span></span></span></span></p></div></div>
                                    ]]>
            </description>
            <pubDate>Tue, 27 Feb 2024 09:10:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280518/gpl-dedecms</guid>
            <link>https://www.oschina.net/news/280518/gpl-dedecms</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[白宫敦促开发者改用内存安全的编程语言]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">白宫国家网络主任办公室 (ONCD) <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.whitehouse.gov%2Foncd%2Fbriefing-room%2F2024%2F02%2F26%2Fpress-release-technical-report%2F" target="_blank">发布</a>了一份报告，呼吁科技界主动减少网络空间的攻击面；通过改用 Rust 等内存安全编程语言，减少内存安全漏洞的数量来提高软件安全性。同时鼓励研究界解决软件可测量性问题，以便开发出更好的测量网络安全质量的诊断方法。</span></p><p><img height="390" src="https://oscimg.oschina.net/oscnet/up-7cb1183993aa2d8ce4f54a8bb5787fa09a7.png" width="300" referrerpolicy="no-referrer"></p><p><span style="color:#000000">ONCD 例举了历史上一些著名的网络攻击事件，包括：1988 年的 Morris 蠕虫病毒、2003 年的 Slammer 蠕虫病毒、2014 年的 Heartbleed 漏洞、2016 年的 Trident 漏洞、2023 年的 Blastpass 漏洞。并指出，所有这些问题的背后都有一个共同的根本原因，即内存安全漏洞。</span></p><p><span style="color:#000000">报告称：「35 年来，三十五年来，内存安全漏洞一直困扰着数字生态系统，但情况本不必如此。消除整类软件漏洞的挑战是一个紧迫而复杂的问题。展望未来，必须采取新方法来减轻这种风险。」</span></p><p><span style="color:#000000">「减少内存安全漏洞的最高杠杆方法是保护网络空间的构建模块之一：编程语言。使用内存安全编程语言可以消除大多数内存安全错误。」</span></p><p><span style="color:#000000">在此之前，</span><span style="background-color:#ffffff"><span style="color:#000000">美国国家安全局 (NSA) 曾于 2022 年 11 月发布了关于软件开发人员如何防止软件内存安全问题的</span><a href="https://www.oschina.net/news/217425/nsa-memory-safe-programming-language" target="_blank">指南。</a></span><span style="background-color:#ffffff; color:#000000">美国网络安全与基础设施安全局 (CISA)<span>&nbsp;</span></span><span style="color:#070707"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>也在 2023 年 12 月发布了类似<a href="https://www.oschina.net/news/269933/cisa-the-case-for-memory-safe-roadmaps">报告</a></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="color:#000000"><span style="background-color:#ffffff">，</span>要求过渡到内存安全编程语言，通过消除与内存相关的漏洞来减少软件产品的攻击面。</span></p><p><span style="color:#000000">ONCD 报告以美国总统拜登于 2023 年 3 月签署的</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Fwhite-house-releases-new-us-national-cybersecurity-strategy%2F" target="_blank">国家网络安全战略</a><span style="color:#000000">为基础，将网络安全的责任从个人和小型企业转移到技术公司和联邦政府等更有能力管理不断变化的威胁的大型组织身上。并在与整个联邦政府的安全设计计划和研发工作保持一致的同时更进一步，涵盖了由 CISA、NSA、FBI 和 NIST 领导的计划和研发工作。</span></p><p><span style="color:#000000">报告中有关内存安全的工作还补充了美国国会对此主题的兴趣。此外，美国参议院国土安全和政府事务委员会主席 Gary Peters (D-MI) 和美国参议员 Ron Wyden (D-OR) 也向 ONCD 强调了他们在内存安全方面的立法努力。</span></p><p><span style="color:#000000">更多详情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.whitehouse.gov%2Fwp-content%2Fuploads%2F2024%2F02%2FFinal-ONCD-Technical-Report.pdf" target="_blank">查看完整报告</a>。</span></p><p><strong><span style="color:#000000">相关阅读：</span></strong></p><ul><li><a href="https://www.oschina.net/news/217425/nsa-memory-safe-programming-language" target="_blank">美国国家安全局建议从 C/C++ 切换到内存安全语言</a></li><li><a href="https://www.oschina.net/news/269933/cisa-the-case-for-memory-safe-roadmaps" target="_blank">美国 CISA 建议放弃 C/C++，消除内存安全漏洞</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Tue, 27 Feb 2024 08:33:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280505/white-house-memory-safe-programming-languages</guid>
            <link>https://www.oschina.net/news/280505/white-house-memory-safe-programming-languages</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[南京大学将开通全国高校首家 AI 课程]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">南京大学正式发布全国高校首家面向 3700 余名新生开设的「1+X+Y」三层次的人工智能通识核心课总体方案。</span></p><p><span style="color:#000000">该方案包含 1 门必修的人工智能通识核心课，X 门人工智能素养课，Y 门各学科与人工智能深度融合的前沿拓展课。其中，1 门人工智能通识核心课面向对象为 2024 年起面向全体本科新生。</span></p><p><span style="color:#000000"><img height="256" src="https://oscimg.oschina.net/oscnet/up-c46928165fcc29d00ea987b62dd6db02a66.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">南京大学党委书记、中国科学院院士谭铁牛指出，当今世界，由人工智能引领的新一轮科技革命和产业变革方兴未艾。在移动互联网、大数据、超级计算、传感网、脑科学等新理论新技术驱动下，人工智能已经对经济发展、社会进步、全球治理等各方面产生重大而深远的影响。只有紧跟时代步伐，把握时代脉搏，才能顺势而上，应势而为，把创新主动权、发展主动权牢牢掌握在自己手中。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 27 Feb 2024 08:03:42 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280497</guid>
            <link>https://www.oschina.net/news/280497</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[0201-0225 开放签团队工作日记]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="margin-left:0.0001pt; margin-right:0px"><span><span><span><span><span><span><span>2022 年底</span></span></span><span><span><span>团队决定</span></span></span><span><span><span>以全新的产品运营和设计思路重回电子签章行业，重新做</span></span></span><span><span><span>电子签章</span></span></span><span><span><span>产品。至于当时如何离开电子签章，又是如何回来的，具体原因等后面再敍。在这么多年的创业的过程中，我们团队经历了从迷茫无助到方向坚定（我们认为的），从一点点构建基础技术架构到基本成熟，有太多的不容易，每一个不容易都可以是个故事，具体的也在将来一一再敍，这次单说最近的一些工作感受和工作概况。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>经过努力</span></span></span><span><span><span>，23 年底（12 月 15 日）</span></span></span><span><span><span>产品</span></span></span><span><span><span>上线</span></span></span><span><span><span>后</span></span></span><span><span><span>，我们深知自己在</span></span></span><span><span><span>市场竞争中与头部企业仍存在功能层面的差距</span></span></span><span><span><span>，不敢妄想有什么好的反馈和成果。但是首</span></span></span><span><span><span>月</span></span></span><span><span><span>便</span></span></span><span><span><span>迎来了</span></span></span><span><span><span>付费用户</span></span></span><span><span><span>（企业版）和近</span></span></span><span><span><span>百</span></span></span><span><span><span>个开源用户</span></span></span><span><span><span>，</span></span></span><span><span><span>这完全出乎我和同事的意料</span></span></span><span><span><span>。刚开始我们以为这些用户至少要在 3-5 个月内才能积累到。事实证明我们错了，我们保守了，但是方向貌似对了（还需要更多的付出和积累）。在与客户沟通过程中，很快就收集到</span></span></span><span><span><span>首批客户集中提出</span></span></span><span><span><span>的众多需求，主要体现在</span></span></span><span><span><span>移动端签署、API 集成、</span></span></span><span><span><span>国产化</span></span></span><span><span><span>及优化交互体验</span></span></span><span><span><span>四大方面。也有很多我们在设计过程中没有考虑到的，没有考虑到的方面对我们来说尤其珍贵，价值巨大。</span></span></span><span><span><span>所以</span></span></span><span><span><span>我们在年前</span></span></span><span><span><span>加快工作节奏，</span></span></span><span><span><span>年后规划新年一季度目标。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>首要任务是</span></span></span><span><span><span>移动端开发，并承诺于春节后第一周交付新功能。</span></span></span><span><span><span>这段时间的工作节奏是这样的：</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>1、</span></span></span><span><span><span>临近春节</span></span></span><span><span><span>（</span></span></span><span><span><span>农历 28 日</span></span></span><span><span><span>）</span></span></span><span><span><span>我们完成了功能开</span></span></span><span><span><span>发，勉强通过冒烟测试</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>2、</span></span></span><span><span><span>年后进行系统和功能测试时，</span></span></span><span><span><span>出乎意料的事情接踵而至</span></span></span><span><span><span>，</span></span></span><span><span><span>出现了</span></span></span><span><span><span>移动端链接逻辑</span></span></span><span><span><span>跳转混乱</span></span></span><span><span><span>、文件签署内存异常、</span></span></span><span><span><span>签署</span></span></span><span><span><span>图片丢失、签署控件重复等</span></span></span><span><span><span>问题</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>3、</span></span></span><span><span><span>测试同学「大壮」在群里</span></span></span><span><span><span>发飙了，</span></span></span><span><span><span>讲述上线风险和延期上线的请求</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>4、不动声色的产品负责人老胡看到</span></span></span><span><span><span>请求</span></span></span><span><span><span>后</span></span></span><span><span><span>一直未回复（他的性格很刚强，表面不说，内心很要强）</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>5、老胡</span></span></span><span><span><span>开始</span></span></span><span><span><span>着手</span></span></span><span><span><span>理清工作任务，</span></span></span><span><span><span>逐条分析 BUG，确定优先级。</span></span></span><span><span><span>···········（结果：原定计划</span></span></span><span><span><span>（2 月 25 日）</span></span></span><span><span><span>未完成上线）只好协调大家</span></span></span><span><span><span>周六日</span></span></span><span><span><span>继续</span></span></span><span><span><span>通宵奋战</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>6、</span></span></span><span><span><span>直至</span></span></span><span><span><span>2 月 26 日</span></span></span><span><span><span>凌晨五点成功修复所有问题并上线新版本</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>这个过程真是酸爽，自从决定做开放签以来，首先内心是非常欣慰的，工作状态也超好，整个团队也是热情澎湃的，甚至自然而然的解决了一些团队管理问题。</span></span></span><span><span><span>同样的产品不同的公司，都是为了服务客户</span></span></span><span><span><span>和理想在</span></span></span><span><span><span>奋斗</span></span></span><span><span><span>、在</span></span></span><span><span><span>熬夜，感谢</span></span></span><span><span><span>团</span></span></span><span><span><span>队成员的努力付出</span></span></span><span><span><span>！加油！（会想尽一切办法和努力给大家加鸡腿，让我们</span></span></span><span><span><span>的</span></span></span><span><span><span>产品更好</span></span></span><span><span><span>，</span></span></span><span><span><span>团队更顽强</span></span></span><span><span><span>，客户更放心</span></span></span><span><span><span>........）</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:justify"><span><span><span><span><span><span><span>值得欣喜的是，按照约定我们成功完成版本更新</span></span></span><span><span><span>，</span></span></span><span><span><span>并与两家新客户签约。接下来，我们将采取敏捷迭代策略，小步快跑地满足需求</span></span></span><span><span><span>。</span></span></span><span><span><span>同时大胆创新签约场景模式，使更多企业在真实场景下实现高效合规</span></span></span><span><span><span>签署，让电子签更简单不是说说而已</span></span></span><span><span><span>。</span></span></span></span></span></span></span></p><p style="margin-left:.0001pt; margin-right:0; text-align:right"><span><span><span><span><span><span><span>2024 年 02 月 27 日</span></span></span></span></span></span></span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 27 Feb 2024 07:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/280492</guid>
            <link>https://www.oschina.net/news/280492</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
