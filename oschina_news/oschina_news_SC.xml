<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[开源中国-最新资讯]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[开源中国-最新资讯 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Mon, 30 Oct 2023 03:14:50 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[jQuery 4.0 开发进度：已完成 99%]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>根据 jQuery 的 GitHub 里程碑状态，其 4.0.0 版本的开发进度已完成 99%。</p><p><img height="1018" src="https://static.oschina.net/uploads/space/2023/1030/105430_Ecrh_2720166.png" width="2170" referrerpolicy="no-referrer"></p><p><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjquery%2Fjquery%2Fmilestone%2F7" target="_blank">https://github.com/jquery/jquery/milestone/7</a></u></em></p><p>可以看到，目前待处理的 issue 仅剩一个，其内容是升级与 ESLint 相关的软件包，以及修复 linting 错误。已经处理完毕的 issue 共计 163 个，内容包括核心变更、构建变更、与 Ajax 相关的改动等，详情查看&nbsp;<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjquery%2Fjquery%2Fmilestone%2F7%3Fclosed%3D1" target="_blank">https://github.com/jquery/jquery/milestone/7?closed=1</a></u>。</p><p><img src="https://static.oschina.net/uploads/space/2023/1030/111012_2yQh_2720166.png" referrerpolicy="no-referrer"></p><blockquote><p>jQuery 是一个快速、小型且功能丰富的 JavaScript 库。通过易于使用的 API（可在多种浏览器中使用），使 HTML 文档的遍历和操作、事件处理、动画和 Ajax 等操作变得更加简单。结合了多功能性和可扩展性，jQuery 改变了数百万人编写 JavaScript 的方式。</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 03:11:46 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264038/jquery-4-0-milestone-99-percent-complete</guid>
            <link>https://www.oschina.net/news/264038/jquery-4-0-milestone-99-percent-complete</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[B 站：全年「AIGC」相关视频播放量 90 亿]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="color:#000000">哔哩哔哩 (B 站) 日前在其首届「bilibili 超级科学晚」会上，发布了一个「五大科学焦点榜单」—— AIGC、室温超导、脑机接口、黑洞、可控核聚变入选。</span></p><p><span style="color:#000000">并公布数据称，过去一年有 2.43 亿用户在 B 站进行学习，是中国在校大学生人数的 5.5 倍。B 站泛知识内容消费人群中，有 72% 为 00 后。</span></p><p><span style="color:#000000">科学和知识品类占 B 站用户搜索排名第 2 位，相关内容播放量占 B 站 41%，00 后正在成为科学内容消费主力。全年「AIGC」相关视频播放量 90 亿，播放时长达 140 亿分钟。全站 UP 主围绕 ChatGPT、文心一言、盘古气象等多个大模型发布动向，共投稿 330 万支视频，是无可争议的 2023 年热度最高科学技术领域。</span></p><p><span style="color:#000000">截至目前，B 站累计入驻知识类 UP 主 300 余万人。</span></p><p><span style="color:#000000"><img alt="" height="1596" src="https://oscimg.oschina.net/oscnet/up-8324fab9d8b85f93db34425ec9fc991cf4b.jpg" width="350" referrerpolicy="no-referrer"></span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 03:11:46 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264039</guid>
            <link>https://www.oschina.net/news/264039</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[KubeSphere 社区双周报 | KubeKey 支持 Web UI]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>KubeSphere 社区双周报主要整理展示新增的贡献者名单和证书、新增的讲师证书以及两周内提交过 commit 的贡献者，并对近期重要的 PR 进行解析，同时还包含了线上/线下活动和布道推广等一系列社区动态。</p><p>本次双周报涵盖时间为：2023.10.13-2023.10.26。</p><h2>贡献者名单</h2><p><img src="https://oscimg.oschina.net/oscnet/up-1a76ec80c43e7bcb348d4c83f3acd397fe2.gif" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-a1db65d7a7a9dd815a333a3f0f7cac20dde.png" alt="" referrerpolicy="no-referrer"></p><h2>新晋 KubeSphere Contributor</h2><p>两周内共有 7 位新晋 KubeSphere Contributor，包括在社区分享最佳实践经验的小伙伴，感谢各位对 KubeSphere 社区的贡献！</p><table><thead><tr><th>GitHub ID</th><th>证书</th></tr></thead><tbody><tr><td>Hanmo123</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-Hanmo123.png" target="_blank">下载证书</a></td></tr><tr><td>JoeDerby</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-JoeDerby.png" target="_blank">下载证书</a></td></tr><tr><td>SongJXin</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-SongJXin.png" target="_blank">下载证书</a></td></tr><tr><td>gunine</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-gunine.png" target="_blank">下载证书</a></td></tr><tr><td>jongwooo</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-jongwooo.png" target="_blank">下载证书</a></td></tr><tr><td>studyingwang23</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-studyingwang23.png" target="_blank">下载证书</a></td></tr><tr><td>Leirong Luo</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-luoleirong.png" target="_blank">下载证书</a></td></tr></tbody></table><h2>近期更新</h2><h3>KubeSphere</h3><h4>1. 修复 K8s 1.26+ 环境中网管地址未正确展示的问题</h4><p>相关 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fpull%2F5950" target="_blank">https://github.com/kubesphere/kubesphere/pull/5950</a></p><p>贡献者：hongzhouzi</p><h4>2. 修复应用更新时间不正确的问题</h4><p>相关 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fpull%2F5948" target="_blank">https://github.com/kubesphere/kubesphere/pull/5948</a></p><p>贡献者：king-119</p><h4>3. 镜像 tag 列表默认按名称排序</h4><p>相关 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fpull%2F5957" target="_blank">https://github.com/kubesphere/kubesphere/pull/5957</a></p><p>贡献者：zhou1203</p><h3>KubeKey</h3><h4>1. 支持 Web UI</h4><p>相关 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubekey%2Fpull%2F2007" target="_blank">https://github.com/kubesphere/kubekey/pull/2007</a></p><p>贡献者：shijilin0116</p><h2>社区动态</h2><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg4NTU0MzEyMg%3D%3D%26mid%3D2247526348%26idx%3D1%26sn%3Dbc88cb295e3d769a8bf7f2f94b17dab7%26chksm%3Dcfa57e71f8d2f7670a55352fd93aafb57f990c2ab407e0996c63888aeb085aa765d61507e31c%26token%3D1638355988%26lang%3Dzh_CN%23rd" target="_blank">ARM 版 openEuler 22.03 部署 KubeSphere v3.4.0 不完全指南</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg4NTU0MzEyMg%3D%3D%26mid%3D2247526431%26idx%3D1%26sn%3D035ab125b4109ab78c685b5eba12ab4d%26chksm%3Dcfa571a2f8d2f8b435a22c90e1fec21e4ebd58270e9e2ddd1d292faf77ce79c06026b2402bf0%26token%3D1638355988%26lang%3Dzh_CN%23rd" target="_blank">11 月 4 日成都站 Meetup 分享内容详情曝光！</a></li></ul><blockquote><p>本文由博客一文多发平台 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenwrite.cn%3Ffrom%3Darticle_bottom" target="_blank">OpenWrite</a> 发布！</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 03:03:46 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4197945/blog/10138824</guid>
            <link>https://my.oschina.net/u/4197945/blog/10138824</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Vercel 发布免费开源字体 Geist]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Vercel 公司发布了一款免费开源字体 ——<strong>「<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvercel.com%2Ffont" target="_blank">Geist</a>」</strong>，称专门面向设计师和开发者而设计。</p><blockquote><p><em>开源地址：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvercel%2Fgeist-font" target="_blank">https://github.com/vercel/geist-font</a></u><br> 下载地址：</em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvercel%2Fgeist-font%2Freleases%2Ftag%2F1.0.0" target="_blank">https://github.com/vercel/geist-font/releases/tag/1.0.0</a></u></p></blockquote><p>Geist 字体由 Vercel 与 Basement Studio 联手打造，包含<strong> Geist Sans 和 Geist Mono</strong>，分别对应的是<strong>无衬线字体</strong>和<strong>等宽字体</strong>。</p><p>根据官方的介绍，Geist Sans 作为无衬线字体，其设计理念是易读和简洁。它是一种现代的几何字体，基于经典的瑞士字体设计原则打造，被设计用于标题、徽标、海报和其他大尺寸显示屏。</p><p>Geist Mono 则是 Geist Sans 的完美搭档。作为等宽字体，它被设计用于代码编辑器、图表、终端和其他以文本为基础的代码界面。目前提供的版本优先考虑可读性，并支持无缝集成到编码环境中。</p><h4><strong>Geist 字体预览效果</strong></h4><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e79e19a1fd7bb59d7c6f9780a4b6b40cefd.png" referrerpolicy="no-referrer"></p><p>此外，Geist 受到以下字体的影响和启发：Inter、Univers、SF Mono、SF Pro、Suisse International、ABC Diatype Mono 和 ABC Diatype。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:35:56 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264031/vercel-geist-font</guid>
            <link>https://www.oschina.net/news/264031/vercel-geist-font</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Weaviate —— 开源向量数据库]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="project_detail_above_text_link_1" data-tracepid="project_detail_above_text_link"><a style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代 <img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p><span><span><span>Weaviate 是一个开源向量数据库，</span></span></span><span style="background-color:#ffffff; color:#1f2328">具有强大、可扩展、云原生且快速的特点。</span><span><span><span>可存储对象和向量，允许将向量搜索与结构化过滤与云原生数据库的容错性和可扩展性相结合，所有这些都可以通过 GraphQL、REST 和各种语言客户端进行访问。</span></span></span></p><p><span style="color:#000000">允许你存储来自你最喜欢的 ML 模型的数据对象和向量嵌入，并无缝扩展到数十亿个数据对象。</span></p><p style="margin-left:0px; margin-right:0px; text-align:start"><span><span><span><span><span style="color:#000000"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><strong>简而言之</strong>：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li>Weaviate 是一个开源<a href="https://weaviate.io/blog/what-is-a-vector-database" target="_blank">向量数据库</a>。</li><li><a href="https://weaviate.io/developers/weaviate/concepts/vector-index">Weaviate 允许你通过使用向量索引</a>来根据数据对象的语义属性来存储和检索数据对象。</li><li>Weaviate 可以独立使用<span style="background-color:#ffffff; color:#000000">&nbsp;(aka<span>&nbsp;</span></span>bring your vectors<span style="background-color:#ffffff; color:#000000">)<span>&nbsp;</span></span>，也可以与各种可以为您进行向量化并扩展核心功能的<a href="https://weaviate.io/developers/weaviate/modules">模块一起使用。</a></li><li>Weaviate 具有<a href="https://weaviate.io/developers/weaviate/api/graphql">GraphQL-API</a>，可轻松访问你的数据。</li><li>Weaviate 速度很快（查看<a href="https://weaviate.io/developers/weaviate/benchmarks">开源基准测试</a>）。</li></ul><p>Weaviate 是一个低延迟向量数据库，对不同媒体类型（文本、图像等）提供开箱即用的支持。它提供语义搜索、问答提取、分类、可定制模型 (PyTorch/TensorFlow/Keras) 等。Weaviate 以 Go 语言从头开始构建，同时存储对象和向量，从而将向量搜索与结构化过滤和云原生数据库的容错性结合起来。所有这些都可以通过 GraphQL、REST 和各种客户端编程语言进行访问。</p><p style="margin-left:0px; margin-right:0px; text-align:start"><span><span><span><span><span style="color:#000000"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>Weaviate 可以轻松使用最先进的 AI 模型，同时提供专用向量数据库的可扩展性、易用性、安全性和成本效益。最为显着地：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>快速查询</strong><br>
Weaviate 通常在不到 100 毫秒的时间内对数百万个对象执行最近邻 (NN) 搜索。<a href="https://weaviate.io/developers/weaviate/benchmarks">可以在我们的基准</a>页面上找到更多信息。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>使用 Wea​​viate 模块摄取任何媒体类型</strong><br>
使用最先进的 AI 模型推理（例如 Transformer）在搜索和查询时访问数据（文本、图像等），让 Weaviate 管理数据矢量化过程为你 - 或提供你自己的向量。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>组合向量和标量搜索</strong><br>
Weaviate 可以进行高效的组合向量和标量搜索。例如，「过去 7 天内发表的与 COVID-19 大流行相关的文章」。Weaviate 存储对象和向量，并确保两者的检索始终高效。不需要第三方对象存储。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>实时且持久的</strong><br>
Weaviate 让你可以搜索数据，即使当前正在导入或更新数据。此外，每次写入都会写入预写日志 (WAL)，以便立即持久写入 - 即使发生崩溃也是如此。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>水平可扩展性</strong></span></span></span></span><br><span style="background-color:#ffffff; color:#000000">Scale</span>&nbsp;<span><span><span><span>Weaviate 满足你的确切需求，例如最大摄取量、最大可能的数据集大小、每秒最大查询数等。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>高可用性</strong><br>
已列入<a href="https://weaviate.io/developers/weaviate/roadmap">路线图</a>，并计划于今年晚些时候发布。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>成本效益</strong><br>
非常大的数据集不需要完全保存在 Weaviate 的内存中。同时，可以利用可用内存来提高查询速度。这样可以有意识地进行速度/成本权衡，以适应每个用例。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>对象之间的类似图形的连接</strong><br>
以类似图形的方式在对象之间建立任意连接，以类似于数据点之间的真实连接。使用 GraphQL 遍历这些连接。</span></span></span></span></p></li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:32:56 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/weaviate</guid>
            <link>https://www.oschina.net/p/weaviate</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推荐 | 基于 LLM 大语言模型的知识库问答系统 FastGPT]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-fastgpt" class="anchor" href="https://gitee.com/xindian/FastGPT#fastgpt"></a>FastGPT</h1><p>FastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景！</p><h2><a id="user-content--在线体验" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E5%9C%A8%E7%BA%BF%E4%BD%93%E9%AA%8C"></a>🛸 在线体验</h2><p>🎉 <a href="https://gitee.com/link?target=https%3A%2F%2Ffastgpt.run%2F">fastgpt.run</a>（服务器在新加坡，部分地区可能无法直连）</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro1.png" alt="Demo" referrerpolicy="no-referrer"></td><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro2.png" alt="Demo" referrerpolicy="no-referrer"></td></tr><tr><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro3.png" alt="Demo" referrerpolicy="no-referrer"></td><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro4.png" alt="Demo" referrerpolicy="no-referrer"></td></tr></tbody></table><h2><a id="user-content--开发" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E5%BC%80%E5%8F%91"></a>👨‍💻 开发</h2><p>项目技术栈: NextJs + TS + ChakraUI + Mongo + Postgres（Vector 插件）<br><a href="https://gitee.com/xindian/FastGPT/blob/dev4/docSite/i18n/zh-Hans/docusaurus-plugin-content-docs/current/quick-start/dev.md">本地开发 Quick Start</a></p><h2><a id="user-content--部署" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E9%83%A8%E7%BD%B2"></a>🚀 部署</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fsealos.io%2Fdocs%2Fexamples%2Fai-applications%2Finstall-fastgpt-on-desktop">官方推荐 Sealos 部署</a> 无需服务器，代理和域名，高可用。</li><li><a href="https://gitee.com/xindian/FastGPT/blob/dev4/docSite/i18n/zh-Hans/docusaurus-plugin-content-docs/current/deploy/docker.md">docker-compose 部署</a> 单机版。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1tV4y1y7Mj%2F%3Fvd_source%3D92041a1a395f852f9d89158eaa3f61b4">由社区贡献的宝塔部署和本地运行教程</a> 单机版。</li></ul><h2><a id="user-content--roadmap" class="anchor" href="https://gitee.com/xindian/FastGPT#-roadmap"></a><img class="emoji" alt=":point_right:" style="vertical-align: middle" src="https://cn-assets.gitee.com/assets/emoji/point_right-8d392cf32998e3bca12bb7b4ee10dae0.png" width="14" height="14" referrerpolicy="no-referrer"> RoadMap</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fkjqvjse66l.feishu.cn%2Fdocx%2FRVUxdqE2WolDYyxEKATcM0XXnte">FastGpt RoadMap</a></li></ul><h2><a id="user-content-️-交流群" class="anchor" href="https://gitee.com/xindian/FastGPT#%EF%B8%8F-%E4%BA%A4%E6%B5%81%E7%BE%A4"></a>🏘️ 交流群</h2><p>添加 wx 进入：<br><img src="https://otnvvf-imgs.oss.laf.run/wx300.jpg" alt="Demo" referrerpolicy="no-referrer"></p><h2><a id="user-content-powered-by" class="anchor" href="https://gitee.com/xindian/FastGPT#powered-by"></a>Powered by</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fmsgbyte%2Ftushan">TuShan: 5 分钟搭建后台管理系统</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Flabring%2Flaf">Laf: 3 分钟快速接入三方应用</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Flabring%2Fsealos">Sealos: 快速部署集群应用</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fsongquanpeng%2Fone-api">One API: 令牌管理 &amp; 二次分发，支持 Azure</a></li></ul><h2><a id="user-content--其他" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E5%85%B6%E4%BB%96"></a>👀 其他</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fkjqvjse66l.feishu.cn%2Fdocx%2FHtrgdT0pkonP4kxGx8qcu6XDnGh">FastGpt 常见问题</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1jo4y147fT%2F">docker 部署教程视频</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1xh4y1t7fy%2F">公众号接入视频教程</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1Wo4y1p7i1%2F">FastGpt 知识库演示</a></li></ul><h2><a id="user-content-第三方生态" class="anchor" href="https://gitee.com/xindian/FastGPT#%E7%AC%AC%E4%B8%89%E6%96%B9%E7%94%9F%E6%80%81"></a>第三方生态</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fluolin-ai%2FFastGPT-Enterprise-WeChatbot">luolinAI: 企微机器人，开箱即用</a></li></ul><h2><a id="user-content--star-history" class="anchor" href="https://gitee.com/xindian/FastGPT#-star-history"></a>🌟 Star History</h2><p><a href="https://gitee.com/link?target=https%3A%2F%2Fstar-history.com%2F%23labring%2FFastGPT%26Date"><img src="https://api.star-history.com/svg?repos=labring/FastGPT&amp;type=Date" alt="Star History Chart" referrerpolicy="no-referrer"></a></p>]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:26:56 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/xindian/FastGPT</guid>
            <link>https://gitee.com/xindian/FastGPT</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 智能问答技术在百度搜索中的应用]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p><img src="https://oscimg.oschina.net/oscnet/up-7e074847d19344a1cd39fd61d566f8ba83b.png" alt="" referrerpolicy="no-referrer"></p><p>作者 | Xiaodong</p><blockquote><p>导读</p><p>本文主要介绍了智能问答技术在百度搜索中的应用。包括机器问答的发展历程、生成式问答、百度搜索智能问答应用。欢迎大家加入百度搜索团队，共同探索智能问答技术的发展方向，文末有简历投递方式。</p></blockquote><blockquote><p><em>全文 6474 字，预计阅读时间 17 分钟。</em></p></blockquote><h1><strong>01 什么是机器问答</strong></h1><p>机器问答，就是让计算机软件系统自动回答人类提出的描述性问题。例如问：「王小丫的主持的节目叫什么」，我们可以在百度搜索框里输入任意用自然语言描述的问题，并在搜索的首位结果中可以直接得相关答案，如下图所示：</p><p><img src="https://oscimg.oschina.net/oscnet/up-963d02166a9d2429174f6b8653991ec8f94.png" alt="图片" referrerpolicy="no-referrer"></p><p>区别于传统搜索引擎根据多个关键词反馈检索的网页链接，机器问答根据自然语言描述的问题直接获取答案，可以极大地提高大家获取信息的效率。机器问答在生活中无处不在，经统计，有约 40% 的搜索需求、约 30% 的对话需求都跟机器问答相关。</p><p>那么，百度搜索的机器问答应用现状如何？目前首条结果可以直接满足大部分的问答需求，并且，在百度搜索中，不限定用户问题领域，是一个开放式的问答系统，可以询问任何信息。</p><h2><strong>1.1 机器问答的发展历程</strong></h2><p>机器问答的发展历程如下，与机器学习发展相吻合。</p><p><img src="https://oscimg.oschina.net/oscnet/up-9a139b0eb31cf87b33168f4ad87086aa2cd.png" alt="图片" referrerpolicy="no-referrer"></p><p><strong>从模型方法的发展上看：</strong></p><p>2013 年以前，大家主要做一些特征工程相关工作，即给定一个问题和一些候选答案，设计多种字面匹配特征，并计算问题与答案之间词的匹配度，例如 BM25 等算法。</p><p>2014~2015 年，随着深度学习的发展，大家会使用神经网络来计算问题和答案间表示的语义距离，例如 CNN、RNN 等。</p><p>2016~2017 年，大家会使用 Attention 网络结构设计各类模型结构，进一步刻画问题和答案间的深层语义匹配关系。</p><p>2018~2021 年，研究主要集中在训练模型上，会使用一些更大、效果更好的预训练模型来完成复杂的问答匹配任务。</p><p>自 2022 年开始，大家更多关注生成模型的应用。</p><p><strong>从数据集的发展上看：</strong></p><p>2013 年，MCTest 出现，以选择题和完形填空形式为主。</p><p>2016 年，SQuAD 诞生，这是第一个大型阅读理解数据信息，会根据用户问题从提供的一篇文章中进行答案抽取。</p><p>2017 年，百度发布了 DuReader 数据集，这是首个中文的阅读理解的数据集。</p><p>2018 年，HotputQA 等发布，更加深入研究了多跳推理、常识推理等复杂的问答场景。</p><h2><strong>1.2 机器问答建模</strong></h2><p>目前的主流范式：Retriever + Reader</p><p>Retriever = 基于 query 查询候选。即给定一个 query，获得该 query 的相关候选，可能是网页、视频、表格、知识图谱等。</p><p>Reader = 从给定候选中获取答案信息。即在给定候选的基础上，结合 query 进一步进行答案抽取。</p><p>百度搜索就是一个非常强的一个 Retriever ，它可以提供相关候选查询，所以我们的研究工作更多集中在 Reader 上，即基于搜索结果如何更好地完成答案抽取。</p><p><img src="https://oscimg.oschina.net/oscnet/up-d2ba5b11127a99934c18d5825cba6ab00b1.png" alt="图片" referrerpolicy="no-referrer"></p><p>早期的 Reader，主要基于传统的特征工程方法，是一个很复杂的系统化 pipeline 流程：先分析 query 获得期望的答案类型、实体信息、问题类型等，并根据这些信息从候选库里检索若干候选，并设计复杂的匹配特征来计算 query 和候选的相关性打分，并设计排序函数进行排序，得到排序最高的答案，过程如下图。</p><p><img src="https://oscimg.oschina.net/oscnet/up-74c5ecc4978e073d0b1799f4735a3b37278.png" alt="图片" referrerpolicy="no-referrer"></p><p>这个流程是管道串联的，每一步都存在误差的积累，整个训练流程也不可整体迭代，维护成本较高。后来，大家希望找到一种更加端到端的方法来解决以上问题，机器阅读理解（Machine Reading Comprehension，MRC）被提出出来。</p><p>MRC 的任务的定义是：输入 Question+Document，直接用一个模型替代复杂流程，输出 Answer。早期的 MRC 工作会设计一些比较复杂的网络结构，来对问题和答案之间的关系进行建模。一个比较经典的方法是 BiDAF，它的输入层是对整个 document 和 query 分别映射到 enbedding 表示上，各自通过 LSTM 等网络来学习问题和文档上下文的表示，之后通过 Attention 交互层，采用双向注意力对 query 和 document 的关系进行建模，在此基础上再通过 LSTM 网络获取更丰富的上下文表示，最终输出层预测每个位置作为答案开始和终止的概率，概率最高的片段被抽取作为答案。</p><p><img src="https://oscimg.oschina.net/oscnet/up-c79c34662eb1f0af4c7b094a66cf7656acb.png" alt="图片" referrerpolicy="no-referrer"></p><p>早期的模型结构设计呈现百花齐放的状态，以期更好解决问题和答案的建模。</p><p>后来，预训练模型逐渐发展起来，大家意识到，复杂的模型结构设计并不太必要，transformer 就是目前为止最好的模型结构，这样可以释放更多研究精力到预训练工作中，更多关注预训练的任务设计、 loss 函数、预训练的数据等。</p><p>在这种情况下，产生了多种预训练模型，比如说最早的 BERT 和百度的<a href="https://www.oschina.net/action/visit/ad?id=1191" title="ERNIE">ERNIE</a>等，这些预训练模型会使 MRC 更加简单，大家会把 query 和 document 整体作为一个序列进行输入，query 和 document 之间可以用一些特殊符号进行分割。经过预训练模型的语义表示建模，最后依旧预测答案开始和结束的位置并进行抽取。</p><p><img src="https://oscimg.oschina.net/oscnet/up-b91feb0c3746117307b1b707dbc9b2341c0.png" alt="图片" referrerpolicy="no-referrer"></p><h1><strong>02 生成式问答</strong></h1><p>近期生成式技术的发展非常火热，也有非常多的工作发表。</p><p>早期一个比较有代表性的生成式 Reader，是 2017 年的 S-NET，它是针对 MS-MARCO 数据集专门设计的，该数据集的特点是答案来自多篇文章并且与原文中词汇不一定相同。</p><p>针对这样的任务，很自然的想法是用生成的方式来解决这个问题。它设计了一套两阶段的流程，第一阶段是答案抽取模型，跟我们上面介绍的模型非常一致，并额外引入了 passage 排序任务对候选文章进行相关性排序。第二阶段是生成模型，输入得到抽取结果，生成答案的总结，如下图所示。</p><p><img src="https://oscimg.oschina.net/oscnet/up-eac42697ab12c4dd61e9625a1bbf3ab4bcf.png" alt="图片" referrerpolicy="no-referrer"></p><p>可以看出，早期的这些工作跟我们现在所使用的生成式问答流程非常相似，我们还会加一个检索模块，就是我们刚才最早提到的 Retriever，然后就是候选抽取、排序、生成。但是，这个工作还是依赖于额外信息来做参考总结。大家会想，是不是可以有一个生成模型，直接生成答案，而不依赖于我们输入额外的信息知识？</p><p>2019 年的 T5 模型首先解决了这个问题，当时它是采用了一种「预训练+迁移学习」的思路，将不同 NLP 任务统一到生成范式下，来统一完成问答、机器翻译、情感分析、对话等一系列任务，且通过百亿参数量的大模型（在当时算是比较大的规模）中存储的知识直接回答问题。它也验证了不同生成模型的结构，包括 Encoder-Decoder 方式的、Decoder-only 的和混合式的。</p><p>但是，T5 这类模型虽然可以完成一些简单问答，但还不足以达到可直接使用的商用状态，它的参数量及训练方式还存在改进空间，对于一些通用问题也不能直接取得非常好的效果。直到 ChatGPT 的出现，它会采用更大的参数规模（千亿级），并有更强的人类回复对齐能力，去理解用户指令，从而完成更加复杂的问答。可以说，ChatGPT 是已达商用级别的对话和问答产品。</p><h1><strong>03 百度搜索的智能问答应用</strong></h1><p>百度搜索的问答场景是丰富多样的。答案抽取方式也有多种，比如说我们可以从百科或者网页通过信息抽取的方式得到一些知识图谱，在知识图谱上来进行答案提取；更通用的方式是从网页文本中，通过阅读理解直接抽取答案；还可以通过对一些半结构化的数据，比如表格，来进一步的提取信息，并组织成更结构化的方式展现。不止是文本，也包括对视频内容的理解和抽取。</p><p><img src="https://oscimg.oschina.net/oscnet/up-4f1d8cf290673670a6b13f27ff8f32c76f0.png" alt="图片" referrerpolicy="no-referrer"></p><p>面临着这样一个丰富多样的问答场景，我们会有哪些挑战呢？</p><p><strong>挑战 1</strong>：机器问答面临复杂语义理解、推理、上下文建模难点？</p><p><strong>挑战 2</strong>：面对搜索的高流量和机器问答对复杂模型的需求，如何实现快速响应？</p><p><strong>挑战 3</strong>：开放领域的搜索场景下网页数据非常复杂，答案质量参差不齐（错误、片面），如何提供正确且高质量的答案？</p><p>、</p><h2><strong>3.1 解决复杂语义理解、推理、上下文建模难点</strong></h2><p>比如最开始的这个例子，如下图所示，答案中提到一个「她」，就需要做指代消解、对上下文的理解，并且上下文篇幅可能很长，通过深层次的理解才能知道所需的是一个答题节目，而不是其他节目。这个问题的解决依赖一些很复杂的模型。</p><p><img src="https://oscimg.oschina.net/oscnet/up-b0523ebaa060da3070ea6a4d3811d62d1bd.png" alt="图片" referrerpolicy="no-referrer"></p><p>我们采用的解决方案是「大模型+预训练」。</p><p>在预训练中，我们会使用非常丰富的数据，包括几个阶段：</p><ul><li><p>首先，用 T 级别通用文本进行 Pretrain 学习基础语言模型；</p></li><li><p>并且，使用百 G 级业务日志进行 Post-pretrain 实现领域和目标迁移；</p></li><li><p>此外，进行细致的数据挖掘，通过 G 级人工标注数据进行 Finetune 拟合业务效果；</p></li><li><p>最后，通过远程监督数据增强、标注数据质量识别、薄弱数据自动挖掘和定向标注、用户行为指引，实现数据和模型的闭环反馈。</p></li></ul><p>而在大模型方面：</p><ul><li><p>使用百亿级参数量模型，提升知识记忆和语言理解能力</p></li><li><p>通过长序列建模，充分理解上下文</p></li></ul><p>例如，我们正在使用的一个模型，我们称之为 DocMRC 模型，它模拟人做阅读理解答题，阅读整个文章，逻辑如下图所示。</p><p><img src="https://oscimg.oschina.net/oscnet/up-efaa14b51d1240a2fed3ef2c93b114b90d9.png" alt="图片" referrerpolicy="no-referrer"></p><p>输入层支持长序列建模，将整个 doc segment sents 进行切分；特别的是，我们在每句话前插入 token 表示，CLS 用来汇聚每个句子的表示，整体输入浅层词级模型结构来学习局部表示；基于这个表示经过层次化结构学习深层上下文关系；最后输出 CLS 特殊 token 表示标注，输出答案。</p><p>输出层会有两种输出：一种是针对问题输出偏摘要等多句话答案介绍，会使用句子层的输出，然后做序列标注的输出；另一种是强调答案中的关键内容，可能是几个实体，会将 token 表示做序列标注预测。</p><h2><strong>3.2 提升整体模型的速度，实现快速响应</strong></h2><p>搜索每天的用户流量非常大，前面也提到，我们需要用到较大或较复杂模型，整个模型的耗时以及资源消耗也是非常大的。那么，有没有其他方式来提升整体模型的速度，实现快速响应及资源平衡？</p><p>刚才介绍的层次化的建模，对模型结构的优化，是一种解决方案。</p><p>另外有一种通用的方式：知识蒸馏，知识蒸馏是将大模型的知识提炼给单个小模型，在效果接近的情况下提升推理速度。这里我们采用了一种「多 teacher 多阶段蒸馏」模式。</p><p>针对问答的业务场景，我们会训练多个不同的 teacher，通过不同 teacher 的集成来提升学习目标的上限。然后对于多个 teacher 蒸馏，一种基线方案是将每个 teacher 的打分或 loss 加权直接做平均，让 student 拟合，但是我们认为这种方式可能并不能确保达到非常极致的效果。我们期望根据不同样本动态做出选择（因为不同 teacher 的侧重有差异），设计了一种多阶段蒸馏的模式，并在其中根据数据动态选择 teacher，如下图所示。</p><p><strong>第一阶段</strong>，Teacher 模型训练，训练多 teacher 提升学习上限；</p><p><strong>第二阶段</strong>，无监督蒸馏，无标数据很难判断 teacher 的好坏，所以采用 teacher 间投票的方式，依据梯度方向动态选择 teacher，剔除可能的噪声 teacher；</p><p><strong>第三阶段</strong>，有监督蒸馏，依据标注样本对 teacher 动态赋权。</p><p><img src="https://oscimg.oschina.net/oscnet/up-1d06f3320489bb36099a988b3c28560be5c.png" alt="图片" referrerpolicy="no-referrer"></p><p>通过这样一种多阶段多 teacher 蒸馏的方式，我们最终得到一个效果非常好的 student 模型，甚至超过单个大模型效果。</p><h2><strong>3.3 如何提供正确且高质量的答案</strong></h2><p>搜索场景的问答数据非常复杂，答案质量也参差不齐，很多网页中可能存在一些错误信息或片面介绍，如何提供正确且高质量的答案是我们面临的第三个挑战。</p><p>如下图所示，是搜索中场景的复杂答案的例子。左侧是冗长答案，用户无法快速抓住重点，这种情况下需要一种方式进行总结，用户才能快速理解的答案关键信息，提升满足效率。抽取式答案提取方式已经无法满足，我们需要用生成技术对答案进行深层次压缩总结。</p><p><img src="https://oscimg.oschina.net/oscnet/up-3b0cb10d8ed43151c7cfb9dba4b1f9792a7.png" alt="图片" referrerpolicy="no-referrer"></p><p>另外，对於单篇文章中提取的答案可能不够全面，我们需要从多篇网页中做答案总结，也需要生成模型，如下图所示。我们从多篇文章中总结答案，并在答案中标注来源，用户可以清晰看到答案出处。</p><p><img src="https://oscimg.oschina.net/oscnet/up-743951b5f984c8a0ac47dc29838f43e96c9.png" alt="图片" referrerpolicy="no-referrer"></p><p>综上，如果要生成全面、高效、正确的答案，就需要有一个更好的生成模型。目前的大语言模型非常多，但怎样的大语言模型才能完成搜索场景的问答任务呢？</p><h1><strong>04 检索增强生成</strong></h1><p>目前大语言模型直接做问答还有几个问题：</p><p><strong>第一</strong>，大预言模型难以记住所有知识，对于一些偏长尾知识可能有错误或者不知道的情况；</p><p><strong>第二</strong>，大语言模型的知识容易过时、更新困难，对于新知识无法及时感知；</p><p><strong>第三</strong>，大语言模型的输出难以验证，目前用户的信赖感较差，我们无法完全信赖生成模型直接生成的答案。</p><p><strong>所以在这种情况下，大家希望能有一些方式来进行一些辅助的答案验证。</strong></p><h2><strong>4.1 检索增强生成流程</strong></h2><p>针对搜索问答场景，我们设计了检索增强生成方案，已在百度搜索落地。检索增强生成是基于搜索引擎补充相关信息，可有效缓解大模型幻觉，来提升答案的正确性、时效性以及可信度。整体流程分为几个阶段：</p><p>1、文档检索阶段，会检索得到多种参考来源；</p><p>2、答案抽取阶段，会把文章抽取关键信息，减轻生成模型负担；</p><p>3、prompt 组成阶段，会根据获取的参考来源来回答问题，并提供具体要求，比如说在答案内容中序号标注来源；</p><p>4、答案生成阶段，将 prompt 输入生成大模型中，最终得到搜索结果。</p><p><img src="https://oscimg.oschina.net/oscnet/up-4433c15aa52ec41c714f2962fbc7f36d8bf.png" alt="图片" referrerpolicy="no-referrer"></p><p>如上图所示，可以看到右侧答案是总结了多篇文章的一个结果，并且也会在其中标注上参考来源，这就是我们期望给用户提供的答案。</p><h2><strong>4.2 生成大模型训练流程</strong></h2><p>我们生成大模型的训练流程分为四个阶段，如下图所示，前两个阶段跟目前主流的生成大模型训练比较接近，后两个阶段我们做了检索增强生成问答场景下的特殊适配。</p><p><img src="https://oscimg.oschina.net/oscnet/up-2d170b5a9d03db57ccca02b3f9648bfe4a9.png" alt="图片" referrerpolicy="no-referrer"></p><p><strong>第一阶段</strong>，通用预训练，我们会有一些通用的网页语料以及垂类语料，比如书籍、表格、对话等，来获得通用的预训练基础模型；</p><p><strong>第二阶段</strong>，进行指令微调，我会提供一些通用的指令，使得模型拥有理解指令的能力；</p><p><strong>第三阶段</strong>，标注业务指令，并用其做具体的微调，使其能理解搜索场景下的多结果组织的问答场景；</p><p><strong>第四阶段</strong>，基于用户行为反馈做细致微调，以及通过强化学习等方式，提高生成答案的质量。</p><h2><strong>4.3 通过指令拆解，学习复杂指令</strong></h2><p>搜索的业务场景指令非常复杂，我们会提出非常具体的要求，并提供参考来源。那么如何让生成模型来理解这种复杂的指令？一种解决方案是标注很多这类复杂指令，并输入到生成模型中，但这种方式并不一定是最佳的。如果模型学习这类指令偏多了，反而无法达到更好泛化效果，造成模型效果下降。有没有其他的方式？</p><p><strong>这里，我们借鉴推理链（CoT）的思想，提出通过指令拆解的方式，学习检索生成场景下复杂指令。</strong></p><p>上述复杂指令通常可以通过三步简单步骤完成：</p><p>第一步，选择能用来回答问题的搜索结果；</p><p>第二步，根据选择的搜索结果进行答案的组织和生成；</p><p>第三步，用编号的形式，加上参考来源。</p><p>可以看出，对于很复杂的指令，我们可以通过多步拆解变成多个简单指令，我们会让模型先去学习并理解简单指令，之后可能不用太多复杂指令的数据，就能使模型在复杂指令上的表现达到一个非常好的水平。</p><h2><strong>4.4 推理加速及降低资源消耗</strong></h2><p>对于一些判别式模型，可以用蒸馏或一些其他的技术来做。但对于生成模型来说，模型尺寸小了对效果的影响较大，蒸馏并不特别适用，需要有一些其他的加速手段。近期业内有很多相关的工作研究，例如 Inference with Reference，就是针对检索增强生成的业务场景，通过检测固定 prefix，从参考中复制固定长度文本作为候选序列，验证如与模型输出一致则实现并行解码多步，如下图所示。</p><p><img src="https://oscimg.oschina.net/oscnet/up-78ee3ddc4ee98c5d2055bc1c54dc1d0b8d1.png" alt="图片" referrerpolicy="no-referrer"></p><p>另外也有一些更加通用的生成加速的手段，例如可以用小模型快速生成多步，把小模型的预测结果直接输入大模型，大模型验证是否解码一致，类似前一个工作也可以实现加速，但要求是尽量使我们的小模型和大模型效果接近，预测准确的概率会更大，加速比就会更大。</p><p>最后我给大家留一个问题，大家可以想一想： <strong>「下一代的搜索引擎会是什么样子？」</strong> 很期待你的回答，也欢迎与我们一起探讨。</p><p><a href="https://www.oschina.net/action/GoToLink?url=mailto%3A%E4%BA%A4%E6%B5%81%26%E7%AE%80%E5%8E%86%E6%8A%95%E9%80%92%E9%82%AE%E7%AE%B1%EF%BC%9Asti01%40baidu.com" target="_blank">交流&amp;简历投递邮箱：sti01@baidu.com</a></p><p>——END——</p><p><strong>推荐阅读</strong></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247572187%26idx%3D1%26sn%3De62d1cf576fcc45232b67028321a9dd0%26chksm%3Dc03feaa7f74863b11423311a7158d1ae375b5fded421b83756d0af9cbb4db6d5dfc034ac1be4%26scene%3D21%23wechat_redirect" target="_blank">通过 Python 脚本支持 OC 代码重构实践（一）：模块调用关系分析</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571891%26idx%3D1%26sn%3De5ab3e3ad26b8e92b5387e5905d17805%26chksm%3Dc03fe9cff74860d911187fc6e1b70da54e5c9b05453ef602576d2efc03d95b6fda158003ada7%26scene%3D21%23wechat_redirect" target="_blank">CVPR2023 优秀论文 | AIGC 伪造图像鉴别算法泛化性缺失问题分析</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571772%26idx%3D1%26sn%3D3f9c022e989d2e8e2f7af2e6c7f7db76%26chksm%3Dc03fe940f7486056ca76cb5e0d6e1175d25ec77186ed8f7f26887ae5cb8d24c750acfafdaadb%26scene%3D21%23wechat_redirect" target="_blank">一文搞定专属码的设计与开发</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571660%26idx%3D1%26sn%3D867bb1f6c7e9cb68da34a95923a06a5e%26chksm%3Dc03fe8b0f74861a6972c085bcabc20b71647b02a4b312980722ad37ac843f519432393f6b657%26scene%3D21%23wechat_redirect" target="_blank">AI 原生应用速通指南</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571596%26idx%3D1%26sn%3Dcc698b9be371c3d0316236551cf73a9d%26chksm%3Dc03fe8f0f74861e6e27850cbee6bad70d2e88f1e9fcaf474db0736232108a8a743d0223fdf40%26scene%3D21%23wechat_redirect" target="_blank">代码理解技术应用实践介绍</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:21:56 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/10123217</guid>
            <link>https://my.oschina.net/u/4939618/blog/10123217</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Linux Mint]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Linux Mint 团队在最新月度报告中<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.linuxmint.com%2F%3Fp%3D4591" target="_blank">提到</a></u>，他们已经开始着手开发对 Wayland 的支持。</p><p>团队称这项工作是他们在很长一段时间内必须面对的主要挑战之一，虽然他们不期待 Wayland 能很快取代 Xorg 作为默认值，无论是在 21.3 中，还是在 22.x 中，但仍然希望做好准备。</p><p>按照计划，Cinnamon 6.0 计划在今年的 Mint 21.3 中推出，<strong>并将提供实验性的 Wayland 支持</strong>。用户可以从登录界面在默认 Cinnamon 会话（在 Xorg 上运行）和 Cinnamon on Wayland 之间进行选择。</p><p>下图是 Cinnamon on Wayland 的运行截图：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-21b9daccab538fe5909df6cef6b172695f5.png" referrerpolicy="no-referrer"></p><p>Linux Mint 团队表示，他们可能在 2026 年实现对 Wayland 的稳定支持/默认支持。鉴于 Linux Mint 坚持以 Ubuntu LTS 为基础，因此并不指望在 Ubuntu 24.04 LTS 之前就能支持 Wayland。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 28 Oct 2023 04:19:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263917/linux-mint-wayland-progress</guid>
            <link>https://www.oschina.net/news/263917/linux-mint-wayland-progress</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[智谱 AI 推出第三代基座大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>2023 年 10 月 27 日，智谱 AI 于 2023 中国计算机大会（CNCC）上，推出了<strong>全自研的第三代基座大模型 ChatGLM3</strong>及相关系列产品。</p><p><img height="281" src="https://static.oschina.net/uploads/space/2023/1028/102320_GQzP_2720166.jpg" width="500" referrerpolicy="no-referrer"></p><p>以下汇总摘录自官方公告：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSVq458IhrR2GGezA9goumw" target="_blank">https://mp.weixin.qq.com/s/SVq458IhrR2GGezA9goumw</a></u></p><hr><h4><strong>全新技术升级</strong></h4><p><strong>1. 更强大的性能：</strong></p><p>今年以来，这是我们第三次对 ChatGLM 基座模型进行了深度优化。我们采用了独创的多阶段增强预训练方法，更丰富的训练数据和更优的训练方案，使训练更为充分。</p><p>评测显示，与 ChatGLM 二代模型相比，在 44 个中英文公开数据集测试中，ChatGLM3 在国内同尺寸模型中排名首位。其中，MMLU 提升 36%、CEval 提升 33%、GSM8K 提升 179% 、BBH 提升 126%。</p><p><strong>2. 瞄向 GPT-4V 的技术升级：</strong></p><p>瞄向 GPT-4V，ChatGLM3 本次实现了若干全新功能的迭代升级，包括：</p><p><strong>多模态理解</strong>能力的 CogVLM，看图识语义，在 10 余个国际标准图文评测数据集上取得 SOTA；</p><p><strong>代码增强</strong>模块 Code Interpreter 根据用户需求生成代码并执行，自动完成数据分析、文件处理等复杂任务；</p><p><strong>网络搜索增强</strong>WebGLM，接入搜索增强，能自动根据问题在互联网上查找相关资料并在回答时提供参考相关文献或文章链接。</p><p>ChatGLM3 的<strong>语义能力与逻辑能力</strong>大大增强。</p><p><strong>3. 全新的 Agent 智能体能力：</strong></p><p>ChatGLM3 本次集成了自研的 AgentTuning 技术，激活了模型智能体能力，尤其在智能规划和执行方面，相比于 ChatGLM 二代提升 1000%；开启国产大模型原生支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理、操作系统等复杂场景。</p><p><strong>4. Edge 端侧模型：</strong></p><p>ChatGLM3 本次推出可手机部署的端测模型 ChatGLM3-1.5B 和 ChatGLM3-3B，支持包括 Vivo、小米、三星在内的多种手机以及车载平台，甚至支持移动平台上 CPU 芯片的推理，速度可达 20 tokens/s。</p><p>精度方面 ChatGLM3-1.5B 和 ChatGLM3-3B 在公开 Benchmark 上与 ChatGLM2-6B 模型性能接近。</p><p><strong>5. 更高效推理/降本增效：</strong></p><p>基于最新的高效动态推理和显存优化技术，我们当前的推理框架在相同硬件、模型条件下，相较于目前最佳的开源实现，包括伯克利大学推出的 vLLM 以及 Hugging Face TGI 的最新版本，推理速度提升了 2-3 倍，推理成本降低一倍，每千 tokens 仅 0.5 分，成本最低。</p><h4><strong>新一代「智谱清言」上线</strong></h4><p>在全新升级的 ChatGLM3 赋能下，生成式 AI 助手智谱清言已成为国内首个具备代码交互能力的大模型产品（Code Interpreter）。</p><p>传送门：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchatglm.cn%2Fmain%2Fcode" target="_blank">https://chatglm.cn/main/code</a></u></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102833_nbbu_2720166.png" referrerpolicy="no-referrer"></p><p>在这一能力的加持下，ChatGLM3 可支持图像处理、数学计算、数据分析等使用场景。以下分别为：</p><p><strong>处理数据生成图表</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102853_C5gK_2720166.png" referrerpolicy="no-referrer"></p><p><strong>画图</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102920_iPm4_2720166.png" referrerpolicy="no-referrer"></p><p><strong>上传 SQL 代码分析</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102938_Bkdu_2720166.png" referrerpolicy="no-referrer"></p><p>随着 WebGLM 大模型能力的加入，智谱清言现具有搜索增强能力。智谱清言可以帮助用户整理出相关问题的网上文献或文章链接，并整理出答案。</p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102955_TE2Q_2720166.jpg" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103014_ein5_2720166.png" referrerpolicy="no-referrer"></p><p>CogVLM 模型则提高了智谱清言的中文图文理解能力，取得了接近 GPT-4V 的图片理解能力。它可以回答各种类型的视觉问题，并且可以完成复杂的目标检测，并打上标签，完成自动数据标注。</p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103030_N8vg_2720166.png" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103042_fWLm_2720166.jpg" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103052_suIU_2720166.jpg" referrerpolicy="no-referrer"></p><hr><p>据介绍，自 2022 年初，智谱 GLM 系列模型已支持在升腾、神威超算、海光 DCU 架构上进行大规模预训练和推理，当前已支持 10 余种国产硬件生态，包括升腾、神威超算、海光 DCU、海飞科、沐曦曦云、算能科技、天数智芯、寒武纪、摩尔线程、百度昆仑芯、灵汐科技、长城超云等。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 28 Oct 2023 02:31:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263818</guid>
            <link>https://www.oschina.net/news/263818</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[网易云课堂 Service Worker 运用与实践]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p><img src="https://oscimg.oschina.net/oscnet/up-ac820c57f42ee1b582f9a6e7a1787375e7a.png" alt="" referrerpolicy="no-referrer"></p><h1>前言</h1><p>本文首先会简单介绍下前端的常见缓存方式，再引入 Service Worker 的概念，针对其原理和如何运用进行介绍。然后基于 google 推出的第三方库 Workbox，在产品中进行运用实践，并对其原理进行简要剖析。</p><blockquote><p>作者：刘放</p></blockquote><blockquote><p>编辑：Ein</p></blockquote><h1>前端缓存简介</h1><p>先简单介绍一下现有的前端缓存技术方案，主要分为 http 缓存和浏览器缓存。</p><h2>http 缓存</h2><p>http 缓存都是第二次请求时开始的，这也是个老生常谈的话题了。无非也是那几个 http 头的问题：</p><h3>Expires</h3><p>HTTP1.0 的内容，服务器使用 Expires 头来告诉 Web 客户端它可以使用当前副本，直到指定的时间为止。</p><h3>Cache-Control</h3><p>HTTP1.1 引入了 Cathe-Control，它使用 max-age 指定资源被缓存多久，主要是解决了 Expires 一个重大的缺陷，就是它设置的是一个固定的时间点，客户端时间和服务端时间可能有误差。 所以一般会把两个头都带上，这种缓存称为强缓存，表现形式为： <img src="https://oscimg.oschina.net/oscnet/up-5d55a7877b12164c2b7f2fe4e870e072dc2.png" alt="" referrerpolicy="no-referrer"></p><h3>Last-Modified / If-Modified-Since</h3><p>Last-Modified 是服务器告诉浏览器该资源的最后修改时间，If-Modified-Since 是请求头带上的，上次服务器给自己的该资源的最后修改时间。然后服务器拿去对比。</p><p>若资源的最后修改时间大于 If-Modified-Since，说明资源又被改动过，则响应整片资源内容，返回状态码 200；</p><p>若资源的最后修改时间小于或等于 If-Modified-Since，说明资源无新修改，则响应 HTTP 304，告知浏览器继续使用当前版本。</p><h3>Etag / If-None-Match</h3><p>前面提到由文件的修改时间来判断文件是否改动，还是会带来一定的误差，比如注释等无关紧要的修改等。所以推出了新的方式。</p><p>Etag 是由服务端特定算法生成的该文件的唯一标识，而请求头把返回的 Etag 值通过 If-None-Match 再带给服务端，服务端通过比对从而决定是否响应新内容。这也是 304 缓存。</p><h2>浏览器缓存</h2><h3>Storage</h3><p>简单的缓存方式有 cookie，localStorage 和 sessionStorage。这里就不详细介绍他们的区别了，这里说下通过 localStorage 来缓存静态资源的优化方案。 localStorage 通常有 5MB 的存储空间，我们以微信文章页为例。 查看请求发现，基本没有 js 和 css 的请求，因为它把全部的不需要改动的资源都放到了 localStorage 中： <img src="https://oscimg.oschina.net/oscnet/up-aa2899a96564193e2509884484b4f1eb12b.png" alt="" referrerpolicy="no-referrer"> 所以微信的文章页加载非常的快。</p><h3>前端数据库</h3><p>前端数据库有 WebSql 和 IndexDB，其中 WebSql 被规范废弃，他们都有大约 50MB 的最大容量，可以理解为 localStorage 的加强版。</p><h3>应用缓存</h3><p>应用缓存主要是通过 manifest 文件来注册被缓存的静态资源，已经被废弃，因为他的设计有些不合理的地方，他在缓存静态文件的同时，也会默认缓存 html 文件。这导致页面的更新只能通过 manifest 文件中的版本号来决定。所以，应用缓存只适合那种常年不变化的静态网站。如此的不方便，也是被废弃的重要原因。</p><p>PWA 也运用了该文件，不同于 manifest 简单的将文件通过是否缓存进行分类，PWA 用 manifest 构建了自己的 APP 骨架，并运用 Servie Worker 来控制缓存，这也是今天的主角。</p><h1>Service Worker</h1><p>Service Worker 本质上也是浏览器缓存资源用的，只不过他不仅仅是 Cache，也是通过 worker 的方式来进一步优化。 他基于 h5 的 web worker，所以绝对不会阻碍当前 js 线程的执行，sw 最重要的工作原理就是：</p><p>1、后台线程：独立于当前网页线程；</p><p>2、网络代理：在网页发起请求时代理，来缓存文件。</p><h2>兼容性</h2><p><img src="https://oscimg.oschina.net/oscnet/up-c50376e8514a0eda4c04fcf7bf1af3f24aa.png" alt="" referrerpolicy="no-referrer"> 可以看到，基本上新版浏览器还是兼容滴。之前是只有 chrome 和 firefox 支持，现在微软和苹果也相继支持了。</p><h2>成熟程度</h2><p>判断一个技术是否值得尝试，肯定要考虑下它的成熟程度，否则过一段时间又和应用缓存一样被规范抛弃就尴尬了。 所以这里我列举了几个使用 Service Worker 的页面：</p><ul><li>淘宝</li><li>网易新闻</li><li>考拉</li></ul><p>所以说还是可以尝试下的。</p><h2>调试方法</h2><p>一个网站是否启用 Service Worker，可以通过开发者工具中的 Application 来查看：</p><p><img src="https://oscimg.oschina.net/oscnet/up-b631c480eabb3662b08968b60c0466ceefd.png" alt="" referrerpolicy="no-referrer"></p><p>被 Service Worker 缓存的文件，可以在 Network 中看到 Size 项为 from Service Worker：</p><p><img src="https://oscimg.oschina.net/oscnet/up-a94d21f7c7ca175656166c4224fae4ba3c9.png" alt="" referrerpolicy="no-referrer"></p><p>也可以在 Application 的 Cache Storage 中查看缓存的具体内容：</p><p><img src="https://oscimg.oschina.net/oscnet/up-7db236cf39ff32cf4e8f86a591a08431b07.png" alt="" referrerpolicy="no-referrer"></p><p>如果是具体的断点调试，需要使用对应的线程，不再是 main 线程了，这也是 webworker 的通用调试方法：</p><p><img src="https://oscimg.oschina.net/oscnet/up-72a1c5ec7411b0e92530a737fe53db2b158.png" alt="" referrerpolicy="no-referrer"></p><h2>使用条件</h2><p>sw 是基于 HTTPS 的，因为 Service Worker 中涉及到请求拦截，所以必须使用 HTTPS 协议来保障安全。如果是本地调试的话，localhost 是可以的。 而我们刚好全站强制 https 化，所以正好可以使用。</p><h2>生命周期</h2><p>大概可以用如下图片来解释：</p><p><img src="https://oscimg.oschina.net/oscnet/up-385c15f6dc80d598f67579d5c0308bb98e5.png" alt="" referrerpolicy="no-referrer"></p><h3>注册</h3><p>要使用 Service Worker，首先需要注册一个 sw，通知浏览器为该页面分配一块内存，然后 sw 就会进入安装阶段。 一个简单的注册方式：</p><pre><code>(function() {
    if('serviceWorker' in navigator) {
        navigator.serviceWorker.register('./sw.js');
    }
})()
</code></pre><p>当然也可以考虑全面点，参考网易新闻的注册方式：</p><pre><code>"serviceWorker" in navigator &amp;&amp; window.addEventListener("load",
    function() {
        var e = location.pathname.match(/\/news\/[a-z]{1,}\//)[0] + "article-sw.js?v=08494f887a520e6455fa";
        navigator.serviceWorker.register(e).then(function(n) {
            n.onupdatefound = function() {
                var e = n.installing;
                e.onstatechange = function() {
                    switch (e.state) {
                        case "installed":
                            navigator.serviceWorker.controller ? console.log("New or updated content is available.") : console.log("Content is now available offline!");
                            break;
                        case "redundant":
                            console.error("The installing service worker became redundant.")
                    }
                }
            }
        }).
        catch(function(e) {
            console.error("Error during service worker registration:", e)
        })
    })
</code></pre><p>前面提到过，由于 sw 会监听和代理所有的请求，所以 sw 的作用域就显得额外的重要了，比如说我们只想监听我们专题页的所有请求，就在注册时指定路径：</p><pre><code>navigator.serviceWorker.register('/topics/sw.js');
</code></pre><p>这样就只会对 topics/下面的路径进行优化。</p><h3>installing</h3><p>我们注册后，浏览器就会开始安装 sw，可以通过事件监听：</p><pre><code>//service worker 安装成功后开始缓存所需的资源
var CACHE_PREFIX = 'cms-sw-cache';
var CACHE_VERSION = '0.0.20';
var CACHE_NAME = CACHE_PREFIX+'-'+CACHE_VERSION;
var allAssets = [
    './main.css'
];
self.addEventListener('install', function(event) {

    //调试时跳过等待过程
    self.skipWaiting();


    // Perform install steps
    //首先 event.waitUntil 你可以理解为 new Promise，
    //它接受的实际参数只能是一个 promise，因为,caches 和 cache.addAll 返回的都是 Promise，
    //这里就是一个串行的异步加载，当所有加载都成功时，那么 SW 就可以下一步。
    //另外，event.waitUntil 还有另外一个重要好处，它可以用来延长一个事件作用的时间，
    //这里特别针对于我们 SW 来说，比如我们使用 caches.open 是用来打开指定的缓存，但开启的时候，
    //并不是一下就能调用成功，也有可能有一定延迟，由于系统会随时睡眠 SW，所以，为了防止执行中断，
    //就需要使用 event.waitUntil 进行捕获。另外，event.waitUntil 会监听所有的异步 promise
    //如果其中一个 promise 是 reject 状态，那么该次 event 是失败的。这就导致，我们的 SW 开启失败。
    event.waitUntil(
        caches.open(CACHE_NAME)
            .then(function(cache) {
                console.log('[SW]: Opened cache');
                return cache.addAll(allAssets);
            })
    );

});
</code></pre><p>安装时，sw 就开始缓存文件了，会检查所有文件的缓存状态，如果都已经缓存了，则安装成功，进入下一阶段。</p><h3>activated</h3><p>如果是第一次加载 sw，在安装后，会直接进入 activated 阶段，而如果 sw 进行更新，情况就会显得复杂一些。流程如下：</p><p>首先老的 sw 为 A，新的 sw 版本为 B。 B 进入 install 阶段，而 A 还处于工作状态，所以 B 进入 waiting 阶段。只有等到 A 被 terminated 后，B 才能正常替换 A 的工作。</p><p><img src="https://oscimg.oschina.net/oscnet/up-5d04b8ca78c8e2f2bc8f02e992c6a540426.png" alt="" referrerpolicy="no-referrer"></p><p>这个 terminated 的时机有如下几种方式：</p><p>1、关闭浏览器一段时间；</p><p>2、手动清除 Service Worker；</p><p>3、在 sw 安装时直接跳过 waiting 阶段</p><pre><code>//service worker 安装成功后开始缓存所需的资源
self.addEventListener('install', function(event) {
    //跳过等待过程
    self.skipWaiting();
});
</code></pre><p>然后就进入了 activated 阶段，激活 sw 工作。</p><p>activated 阶段可以做很多有意义的事情，比如更新存储在 Cache 中的 key 和 value：</p><pre><code>var CACHE_PREFIX = 'cms-sw-cache';
var CACHE_VERSION = '0.0.20';
/**
 * 找出对应的其他 key 并进行删除操作
 * @returns {*}
 */
function deleteOldCaches() {
    return caches.keys().then(function (keys) {
        var all = keys.map(function (key) {
            if (key.indexOf(CACHE_PREFIX) !== -1 &amp;&amp; key.indexOf(CACHE_VERSION) === -1){
                console.log('[SW]: Delete cache:' + key);
                return caches.delete(key);
            }
        });
        return Promise.all(all);
    });
}
//sw 激活阶段,说明上一 sw 已失效
self.addEventListener('activate', function(event) {


    event.waitUntil(
        // 遍历 caches 里所有缓存的 keys 值
        caches.keys().then(deleteOldCaches)
    );
});
</code></pre><h3>idle</h3><p>这个空闲状态一般是不可见的，这种一般说明 sw 的事情都处理完毕了，然后处于闲置状态了。</p><p>浏览器会周期性的轮询，去释放处于 idle 的 sw 占用的资源。</p><h3>fetch</h3><p>该阶段是 sw 最为关键的一个阶段，用于拦截代理所有指定的请求，并进行对应的操作。</p><p>所有的缓存部分，都是在该阶段，这里举一个简单的例子：</p><pre><code>//监听浏览器的所有 fetch 请求，对已经缓存的资源使用本地缓存回复
self.addEventListener('fetch', function(event) {
    event.respondWith(
        caches.match(event.request)
            .then(function(response) {
                //该 fetch 请求已经缓存
                if (response) {
                    return response;
                }
                return fetch(event.request);
                }
            )
    );
});
</code></pre><p>生命周期大概讲清楚了，我们就以一个具体的例子来说明下原生的 serviceworker 是如何在生产环境中使用的吧。</p><h2>举个栗子</h2><p>我们可以以网易新闻的 wap 页为例,其针对不怎么变化的静态资源开启了 sw 缓存，具体的 sw.js 逻辑和解读如下：</p><pre><code>'use strict';
//需要缓存的资源列表
var precacheConfig = [
    ["https://static.ws.126.net/163/wap/f2e/milk_index/bg_img_sm_minfy.png",
        "c4f55f5a9784ed2093009dadf1e954f9"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/change.png",
        "9af1b102ef784b8ff08567ba25f31d95"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-download.png",
        "1c02c724381d77a1a19ca18925e9b30c"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-login-dark.png",
        "b59ba5abe97ff29855dfa4bd3a7a9f35"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-refresh.png",
        "a5b1084e41939885969a13f8dbc88abd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-video-play.png",
        "065ff496d7d36345196d254aff027240"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon.ico",
        "a14e5365cc2b27ec57e1ab7866c6a228"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.eot",
        "e4d2788fef09eb0630d66cc7e6b1ab79"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.svg",
        "d9e57c341608fddd7c140570167bdabb"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.ttf",
        "f422407038a3180bb3ce941a4a52bfa2"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.woff",
        "ead2bef59378b00425779c4ca558d9bd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/index.5cdf03e8.js",
        "6262ac947d12a7b0baf32be79e273083"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/index.bc729f8a.css",
        "58e54a2c735f72a24715af7dab757739"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-bohe.png",
        "ac5116d8f5fcb3e7c49e962c54ff9766"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-mail.png",
        "a12bbfaeee7fbf025d5ee85634fca1eb"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-manhua.png",
        "b8905b119cf19a43caa2d8a0120bdd06"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-open.png",
        "b7cc76ba7874b2132f407049d3e4e6e6"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-read.png",
        "e6e9c8bc72f857960822df13141cbbfd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-site.png",
        "2b0d728b46518870a7e2fe424e9c0085"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_no_pic.png",
        "aef80885188e9d763282735e53b25c0e"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_pc.png",
        "42f3cc914eab7be4258fac3a4889d41d"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_standard.png",
        "573408fa002e58c347041e9f41a5cd0d"]
];
var cacheName = 'sw-precache-v3-new-wap-index-' + (self.registration ? self.registration.scope : '');

var ignoreUrlParametersMatching = [/^utm_/];

var addDirectoryIndex = function(originalUrl, index) {
    var url = new URL(originalUrl);
    if (url.pathname.slice(-1) === '/') {
        url.pathname += index;
    }
    return url.toString();
};
var cleanResponse = function(originalResponse) {
    // If this is not a redirected response, then we don't have to do anything.
    if (!originalResponse.redirected) {
        return Promise.resolve(originalResponse);
    }
    // Firefox 50 and below doesn't support the Response.body stream, so we may
    // need to read the entire body to memory as a Blob.
    var bodyPromise = 'body' in originalResponse ?
        Promise.resolve(originalResponse.body) :
        originalResponse.blob();
    return bodyPromise.then(function(body) {
        // new Response() is happy when passed either a stream or a Blob.
        return new Response(body, {
            headers: originalResponse.headers,
            status: originalResponse.status,
            statusText: originalResponse.statusText
        });
    });
};
var createCacheKey = function(originalUrl, paramName, paramValue,
                              dontCacheBustUrlsMatching) {
    // Create a new URL object to avoid modifying originalUrl.
    var url = new URL(originalUrl);
    // If dontCacheBustUrlsMatching is not set, or if we don't have a match,
    // then add in the extra cache-busting URL parameter.
    if (!dontCacheBustUrlsMatching ||
        !(url.pathname.match(dontCacheBustUrlsMatching))) {
        url.search += (url.search ? '&amp;' : '') +
            encodeURIComponent(paramName) + '=' + encodeURIComponent(paramValue);
    }
    return url.toString();
};
var isPathWhitelisted = function(whitelist, absoluteUrlString) {
    // If the whitelist is empty, then consider all URLs to be whitelisted.
    if (whitelist.length === 0) {
        return true;
    }
    // Otherwise compare each path regex to the path of the URL passed in.
    var path = (new URL(absoluteUrlString)).pathname;
    return whitelist.some(function(whitelistedPathRegex) {
        return path.match(whitelistedPathRegex);
    });
};
var stripIgnoredUrlParameters = function(originalUrl,
                                         ignoreUrlParametersMatching) {
    var url = new URL(originalUrl);
    // Remove the hash; see https://github.com/GoogleChrome/sw-precache/issues/290
    url.hash = '';
    url.search = url.search.slice(1) // Exclude initial '?'
        .split('&amp;') // Split into an array of 'key=value' strings
        .map(function(kv) {
            return kv.split('='); // Split each 'key=value' string into a [key, value] array
        })
        .filter(function(kv) {
            return ignoreUrlParametersMatching.every(function(ignoredRegex) {
                return !ignoredRegex.test(kv[0]); // Return true iff the key doesn't match any of the regexes.
            });
        })
        .map(function(kv) {
            return kv.join('='); // Join each [key, value] array into a 'key=value' string
        })
        .join('&amp;'); // Join the array of 'key=value' strings into a string with '&amp;' in between each
    return url.toString();
};

var hashParamName = '_sw-precache';
//定义需要缓存的 url 列表
var urlsToCacheKeys = new Map(
    precacheConfig.map(function(item) {
        var relativeUrl = item[0];
        var hash = item[1];
        var absoluteUrl = new URL(relativeUrl, self.location);
        var cacheKey = createCacheKey(absoluteUrl, hashParamName, hash, false);
        return [absoluteUrl.toString(), cacheKey];
    })
);
//把 cache 中的 url 提取出来,进行去重操作
function setOfCachedUrls(cache) {
    return cache.keys().then(function(requests) {
        //提取 url
        return requests.map(function(request) {
            return request.url;
        });
    }).then(function(urls) {
        //去重
        return new Set(urls);
    });
}
//sw 安装阶段
self.addEventListener('install', function(event) {
    event.waitUntil(
        //首先尝试取出存在客户端 cache 中的数据
        caches.open(cacheName).then(function(cache) {
            return setOfCachedUrls(cache).then(function(cachedUrls) {
                return Promise.all(
                    Array.from(urlsToCacheKeys.values()).map(function(cacheKey) {
                        //如果需要缓存的 url 不在当前 cache 中,则添加到 cache
                        if (!cachedUrls.has(cacheKey)) {
                            //设置 same-origin 是为了兼容旧版本 safari 中其默认值不为 same-origin,
                            //只有当 URL 与响应脚本同源才发送 cookies、 HTTP Basic authentication 等验证信息
                            var request = new Request(cacheKey, {credentials: 'same-origin'});
                            return fetch(request).then(function(response) {
                                //通过 fetch api 请求资源
                                if (!response.ok) {
                                    throw new Error('Request for ' + cacheKey + ' returned a ' +
                                        'response with status ' + response.status);
                                }
                                return cleanResponse(response).then(function(responseToCache) {
                                    //并设置到当前 cache 中
                                    return cache.put(cacheKey, responseToCache);
                                });
                            });
                        }
                    })
                );
            });
        }).then(function() {

            //强制跳过等待阶段,进入激活阶段
            return self.skipWaiting();

        })
    );
});
self.addEventListener('activate', function(event) {
    //清除 cache 中原来老的一批相同 key 的数据
    var setOfExpectedUrls = new Set(urlsToCacheKeys.values());
    event.waitUntil(
        caches.open(cacheName).then(function(cache) {
            return cache.keys().then(function(existingRequests) {
                return Promise.all(
                    existingRequests.map(function(existingRequest) {
                        if (!setOfExpectedUrls.has(existingRequest.url)) {
                            //cache 中删除指定对象
                            return cache.delete(existingRequest);
                        }
                    })
                );
            });
        }).then(function() {
            //self 相当于 webworker 线程的当前作用域
            //当一个 service worker 被初始注册时，页面在下次加载之前不会使用它。 claim() 方法会立即控制这些页面
            //从而更新客户端上的 serviceworker
            return self.clients.claim();

        })
    );
});

self.addEventListener('fetch', function(event) {
    if (event.request.method === 'GET') {
        // 标识位,用来判断是否需要缓存
        var shouldRespond;
        // 对 url 进行一些处理,移除一些不必要的参数
        var url = stripIgnoredUrlParameters(event.request.url, ignoreUrlParametersMatching);
        // 如果该 url 不是我们想要缓存的 url,置为 false
        shouldRespond = urlsToCacheKeys.has(url);
        // 如果 shouldRespond 未 false,再次验证
        var directoryIndex = 'index.html';
        if (!shouldRespond &amp;&amp; directoryIndex) {
            url = addDirectoryIndex(url, directoryIndex);
            shouldRespond = urlsToCacheKeys.has(url);
        }
        // 再次验证,判断其是否是一个 navigation 类型的请求
        var navigateFallback = '';
        if (!shouldRespond &amp;&amp;
            navigateFallback &amp;&amp;
            (event.request.mode === 'navigate') &amp;&amp;
            isPathWhitelisted([], event.request.url)) {
            url = new URL(navigateFallback, self.location).toString();
            shouldRespond = urlsToCacheKeys.has(url);
        }
        // 如果标识位为 true
        if (shouldRespond) {
            event.respondWith(
                caches.open(cacheName).then(function(cache) {
                    //去缓存 cache 中找对应的 url 的值
                    return cache.match(urlsToCacheKeys.get(url)).then(function(response) {
                        //如果找到了,就返回 value
                        if (response) {
                            return response;
                        }
                        throw Error('The cached response that was expected is missing.');
                    });
                }).catch(function(e) {
                    // 如果没找到则请求该资源
                    console.warn('Couldn\'t serve response for "%s" from cache: %O', event.request.url, e);
                    return fetch(event.request);
                })
            );
        }
    }
});
</code></pre><p>这里的策略大概就是优先在 Cache 中寻找资源，如果找不到再请求资源。可以看出，为了实现一个较为简单的缓存，还是比较复杂和繁琐的，所以很多工具就应运而生了。</p><h1>Workbox</h1><p>由于直接写原生的 sw.js，比较繁琐和复杂，所以一些工具就出现了，而 Workbox 是其中的佼佼者，由 google 团队推出。</p><h2>简介</h2><p>在 Workbox 之前，GoogleChrome 团队较早时间推出过 sw-precache 和 sw-toolbox 库，但是在 GoogleChrome 工程师们看来，workbox 才是真正能方便统一的处理离线能力的更完美的方案，所以停止了对 sw-precache 和 sw-toolbox 的维护。</p><h2>使用者</h2><p>有很多团队也是启用该工具来实现 serviceworker 的缓存，比如说：</p><ul><li>淘宝首页</li><li>网易新闻 wap 文章页</li><li>百度的 Lavas</li></ul><h2>基本配置</h2><p>首先，需要在项目的 sw.js 文件中，引入 Workbox 的官方 js，这里用了我们自己的静态资源：</p><pre><code>importScripts(
    "https://edu-cms.nosdn.127.net/topics/js/workbox_9cc4c3d662a4266fe6691d0d5d83f4dc.js"
);
</code></pre><p>其中 importScripts 是 webworker 中加载 js 的方式。</p><p>引入 Workbox 后，全局会挂载一个 Workbox 对象</p><pre><code>if (workbox) {
    console.log('workbox 加载成功');
} else {
    console.log('workbox 加载失败');
}
</code></pre><p>然后需要在使用其他的 api 前，提前使用配置</p><pre><code>//关闭控制枱中的输出
workbox.setConfig({ debug: false });
</code></pre><p>也可以统一指定存储时 Cache 的名称：</p><pre><code>//设置缓存 cachestorage 的名称
workbox.core.setCacheNameDetails({
    prefix:'edu-cms',
    suffix:'v1'
});
</code></pre><h2>precache</h2><p>Workbox 的缓存分为两种，一种的 precache，一种的 runtimecache。</p><p>precache 对应的是在 installing 阶段进行读取缓存的操作。它让开发人员可以确定缓存文件的时间和长度，以及在不进入网络的情况下将其提供给浏览器，这意味着它可以用于创建 Web 离线工作的应用。</p><h3>工作原理</h3><p>首次加载 Web 应用程序时，Workbox 会下载指定的资源，并存储具体内容和相关修订的信息在 indexedDB 中。</p><p>当资源内容和 sw.js 更新后，Workbox 会去比对资源，然后将新的资源存入 Cache，并修改 indexedDB 中的版本信息。</p><p>我们举一个例子：</p><pre><code>workbox.precaching.precacheAndRoute([
    './main.css'
]);
</code></pre><p><img src="https://oscimg.oschina.net/oscnet/up-d8a79ba50b83bfbc538b960e07f0c707b17.png" alt="" referrerpolicy="no-referrer"></p><p>indexedDB 中会保存其相关信息</p><p><img src="https://oscimg.oschina.net/oscnet/up-b9f1c514f24a2b5121771fa9b59fb0cc82b.png" alt="" referrerpolicy="no-referrer"></p><p>这个时候我们把 main.css 的内容改变后，再刷新页面，会发现除非强制刷新，否则 Workbox 还是会读取 Cache 中存在的老的 main.css 内容。</p><p>即使我们把 main.css 从服务器上删除，也不会对页面造成影响。</p><p>所以这种方式的缓存都需要配置一个版本号。在修改 sw.js 时，对应的版本也需要变更。</p><h3>使用实践</h3><p>当然了，一般我们的一些不经常变的资源，都会使用 cdn，所以这里自然就需要支持域外资源了，配置方式如下：</p><pre><code>var fileList = [
    {
        url:'https://edu-cms.nosdn.127.net/topics/js/cms_specialWebCommon_js_f26c710bd7cd055a64b67456192ed32a.js'
    },
    {
        url:'https://static.ws.126.net/163/frontend/share/css/article.207ac19ad70fd0e54d4a.css'
    }
];


//precache 适用于支持跨域的 cdn 和域内静态资源
workbox.precaching.suppressWarnings();
workbox.precaching.precacheAndRoute(fileList, {
    "ignoreUrlParametersMatching": [/./]
});
</code></pre><p>这里需要对应的资源配置跨域允许头，否则是不能正常加载的。且文件都要以版本文件名的方式，来确保修改后 Cache 和 indexDB 会得到更新。</p><p>理解了原理和实践后，说明这种方式适合于上线后就不会经常变动的静态资源。</p><h2>runtimecache</h2><p>运行时缓存是在 install 之后，activated 和 fetch 阶段做的事情。</p><p>既然在 fetch 阶段发送，那么 runtimecache 往往应对着各种类型的资源，对于不同类型的资源往往也有不同的缓存策略。</p><h3>缓存策略</h3><p>Workbox 提供的缓存策划有以下几种，通过不同的配置可以针对自己的业务达到不同的效果：</p><h3>Stale While Revalidate</h3><p>这种策略的意思是当请求的路由有对应的 Cache 缓存结果就直接返回，</p><p>在返回 Cache 缓存结果的同时会在后台发起网络请求拿到请求结果并更新 Cache 缓存，如果本来就没有 Cache 缓存的话，直接就发起网络请求并返回结果，这对用户来说是一种非常安全的策略，能保证用户最快速的拿到请求的结果。</p><p>但是也有一定的缺点，就是还是会有网络请求占用了用户的网络带宽。可以像如下的方式使用 State While Revalidate 策略：</p><pre><code>workbox.routing.registerRoute(
    new RegExp('https://edu-cms\.nosdn\.127\.net/topics/'),
    workbox.strategies.staleWhileRevalidate({
        //cache 名称
        cacheName: 'lf-sw:static',
        plugins: [
            new workbox.expiration.Plugin({
                //cache 最大数量
                maxEntries: 30
            })
        ]
    })
);
</code></pre><h3>Network First</h3><p>这种策略就是当请求路由是被匹配的，就采用网络优先的策略，也就是优先尝试拿到网络请求的返回结果，如果拿到网络请求的结果，就将结果返回给客户端并且写入 Cache 缓存。</p><p>如果网络请求失败，那最后被缓存的 Cache 缓存结果就会被返回到客户端，这种策略一般适用于返回结果不太固定或对实时性有要求的请求，为网络请求失败进行兜底。可以像如下方式使用 Network First 策略：</p><pre><code>//自定义要缓存的 html 列表
var cacheList = [
    '/Hexo/public/demo/PWADemo/workbox/index.html'
];
workbox.routing.registerRoute(
    //自定义过滤方法
    function(event) {
        // 需要缓存的 HTML 路径列表
        if (event.url.host === 'localhost:63342') {
            if (~cacheList.indexOf(event.url.pathname)) return true;
            else return false;
        } else {
            return false;
        }
    },
    workbox.strategies.networkFirst({
        cacheName: 'lf-sw:html',
        plugins: [
            new workbox.expiration.Plugin({
                maxEntries: 10
            })
        ]
    })
);
</code></pre><h3>Cache First</h3><p>这个策略的意思就是当匹配到请求之后直接从 Cache 缓存中取得结果，如果 Cache 缓存中没有结果，那就会发起网络请求，拿到网络请求结果并将结果更新至 Cache 缓存，并将结果返回给客户端。这种策略比较适合结果不怎么变动且对实时性要求不高的请求。可以像如下方式使用 Cache First 策略：</p><pre><code>workbox.routing.registerRoute(
    new RegExp('https://edu-image\.nosdn\.127\.net/'),
    workbox.strategies.cacheFirst({
        cacheName: 'lf-sw:img',
        plugins: [
            //如果要拿到域外的资源，必须配置
            //因为跨域使用 fetch 配置了
            //mode: 'no-cors',所以 status 返回值为 0，故而需要兼容
            new workbox.cacheableResponse.Plugin({
                statuses: [0, 200]
            }),
            new workbox.expiration.Plugin({
                maxEntries: 40,
                //缓存的时间
                maxAgeSeconds: 12 * 60 * 60
            })
        ]
    })
);
</code></pre><h3>Network Only</h3><p>比较直接的策略，直接强制使用正常的网络请求，并将结果返回给客户端，这种策略比较适合对实时性要求非常高的请求。</p><h3>Cache Only</h3><p>这个策略也比较直接，直接使用 Cache 缓存的结果，并将结果返回给客户端，这种策略比较适合一上线就不会变的静态资源请求。</p><h2>举个栗子</h2><p>又到了举个栗子的阶段了，这次我们用淘宝好了，看看他们是如何通过 Workbox 来配置 Service Worker 的：</p><pre><code>//首先是异常处理
self.addEventListener('error', function(e) {
  self.clients.matchAll()
    .then(function (clients) {
      if (clients &amp;&amp; clients.length) {
        clients[0].postMessage({ 
          type: 'ERROR',
          msg: e.message || null,
          stack: e.error ? e.error.stack : null
        });
      }
    });
});

self.addEventListener('unhandledrejection', function(e) {
  self.clients.matchAll()
    .then(function (clients) {
      if (clients &amp;&amp; clients.length) {
        clients[0].postMessage({
          type: 'REJECTION',
          msg: e.reason ? e.reason.message : null,
          stack: e.reason ? e.reason.stack : null
        });
      }
    });
})
//然后引入 workbox
importScripts('https://g.alicdn.com/kg/workbox/3.3.0/workbox-sw.js');
workbox.setConfig({
  debug: false,
  modulePathPrefix: 'https://g.alicdn.com/kg/workbox/3.3.0/'
});
//直接激活跳过等待阶段
workbox.skipWaiting();
workbox.clientsClaim();
//定义要缓存的 html
var cacheList = [
  '/',
  '/tbhome/home-2017',
  '/tbhome/page/market-list'
];
//html 采用 networkFirst 策略，支持离线也能大体访问
workbox.routing.registerRoute(
  function(event) {
    // 需要缓存的 HTML 路径列表
    if (event.url.host === 'www.taobao.com') {
      if (~cacheList.indexOf(event.url.pathname)) return true;
      else return false;
    } else {
      return false;
    }
  },
  workbox.strategies.networkFirst({
    cacheName: 'tbh:html',
    plugins: [
      new workbox.expiration.Plugin({
        maxEntries: 10
      })
    ]
  })
);
//静态资源采用 staleWhileRevalidate 策略，安全可靠
workbox.routing.registerRoute(
  new RegExp('https://g\.alicdn\.com/'),
  workbox.strategies.staleWhileRevalidate({
    cacheName: 'tbh:static',
    plugins: [
      new workbox.expiration.Plugin({
        maxEntries: 20
      })
    ]
  })
);
//图片采用 cacheFirst 策略，提升速度
workbox.routing.registerRoute(
  new RegExp('https://img\.alicdn\.com/'),
  workbox.strategies.cacheFirst({
    cacheName: 'tbh:img',
    plugins: [
      new workbox.cacheableResponse.Plugin({
        statuses: [0, 200]
      }),
      new workbox.expiration.Plugin({
        maxEntries: 20,
        maxAgeSeconds: 12 * 60 * 60
      })
    ]
  })
);

workbox.routing.registerRoute(
  new RegExp('https://gtms01\.alicdn\.com/'),
  workbox.strategies.cacheFirst({
    cacheName: 'tbh:img',
    plugins: [
      new workbox.cacheableResponse.Plugin({
        statuses: [0, 200]
      }),
      new workbox.expiration.Plugin({
        maxEntries: 30,
        maxAgeSeconds: 12 * 60 * 60
      })
    ]
  })
);
</code></pre><p>可以看出，使用 Workbox 比起直接手撸来，要快很多，也明确很多。</p><h2>原理</h2><p>目前分析 Service Worker 和 Workbox 的文章不少，但是介绍 Workbox 原理的文章却不多。这里简单介绍下 Workbox 这个工具库的原理。</p><p>首先将几个我们产品用到的模块图奉上：</p><p><img src="https://oscimg.oschina.net/oscnet/up-b22e014db049eea325d28de53c9b6b9cd76.png" alt="" referrerpolicy="no-referrer"></p><p>简单提几个 Workbox 源码的亮点。</p><h3>通过 Proxy 按需依赖</h3><p>熟悉了 Workbox 后会得知，它是有很多个子模块的，各个子模块再通过用到的时候按需 importScript 到线程中。 <img src="https://oscimg.oschina.net/oscnet/up-12e98258edc4bd13e5ad48a74c9835ede68.png" alt="" referrerpolicy="no-referrer"></p><p>做到按需依赖的原理就是通过 Proxy 对全局对象 Workbox 进行代理：</p><pre><code>new Proxy(this, {
  get(t, s) {
    //如果 workbox 对象上不存在指定对象，就依赖注入该对象对应的脚本
    if (t[s]) return t[s];
    const o = e[s];
    return o &amp;&amp; t.loadModule(`workbox-${o}`), t[s];
  }
})
</code></pre><p>如果找不到对应模块，则通过 importScripts 主动加载：</p><pre><code>/**
 * 加载前端模块
 * @param {Strnig} t 
 */
loadModule(t) {
  const e = this.o(t);
  try {
    importScripts(e), (this.s = !0);
  } catch (s) {
    throw (console.error(`Unable to import module '${t}' from '${e}'.`), s);
  }
}
</code></pre><h3>通过 freeze 冻结对外暴露 api</h3><p>Workbox.core 模块中提供了几个核心操作模块，如封装了 indexedDB 操作的 DBWrapper、对 Cache Storage 进行读取的 Cache Wrapper，以及发送请求的 fetchWrapper 和日志管理的 logger 等等。</p><p>为了防止外部对内部模块暴露出去的 api 进行修改，导致出现不可预估的错误，内部模块可以通过 Object.freeze 将 api 进行冻结保护：</p><pre><code>var _private = /*#__PURE__*/Object.freeze({
    DBWrapper: DBWrapper,
    WorkboxError: WorkboxError,
    assert: finalAssertExports,
    cacheNames: cacheNames,
    cacheWrapper: cacheWrapper,
    fetchWrapper: fetchWrapper,
    getFriendlyURL: getFriendlyURL,
    logger: defaultExport
  });
</code></pre><h1>总结</h1><p>通过对 Service Worker 的理解和 Workbox 的应用，可以进一步提升产品的性能和弱网情况下的体验。有兴趣的同学也可以对 Workbox 的源码细细评读，其中还有很多不错的设计模式和编程风格值得学习。</p><p><img src="https://oscimg.oschina.net/oscnet/up-a546d4d52ff6cdf625f4d4a4890fd454bec.png" alt="" referrerpolicy="no-referrer"></p><p><strong>-END-</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 10:45:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/youdaotech/blog/5054309</guid>
            <link>https://my.oschina.net/youdaotech/blog/5054309</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Wasmer 开源 WinterJS：Rust 编写的 Service Worker]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Wasmer 团队开源了一款用 Rust 编写的<strong> JavaScript Service Worker：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwasmer.io%2Fposts%2Fannouncing-winterjs-service-workers" target="_blank">WinterJS</a></u></strong>。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6382fe02fb5cbb80e1cb6951156b73e1143.png" referrerpolicy="no-referrer"></p><p><em>WinterJS 开源地址：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwasmerio%2Fwinterjs" target="_blank">https://github.com/wasmerio/winterjs</a></u></em></p><p>据介绍，WinterJS 使用 SpiderMonkey 运行时执行 JavaScript（与 Firefox 使用的运行时相同），并遵循 WinterCG 规范，目的是最大限度地兼容 Cloudflare Workers、Deno Deploy 和 Vercel 等其他服务（因此命名为 WinterJS）。</p><p>WinterJS 除了速度极快，还能通过 WASIX <strong>编译成 WebAssembly</strong>，因此完全支持在 Wasmer 上运行。</p><ul><li><strong>使用示例</strong></li></ul><p><strong>创建<code>serviceworker.js</code>文件，并返回 "hello world"</strong></p><pre><code class="language-javascript">$ wasmer run wasmer/winterjs --net --mapdir /app:. /app/serviceworker.js</code></pre><pre><code class="language-javascript">addEventListener('fetch', (req) =&gt; {
  req.respondWith(`hello world from ${req.request.url.href}`);
});</code></pre><blockquote><p>Wasmer 是支持 WASI 和 Emscripten 的通用 WebAssembly 运行时，提供基于 WebAssembly 的超轻量级容器，专注于支持在任何平台上运行 WASM 代码：从桌面端到云端、以及 IoT 设备，并且能嵌入在任何编程语言中。</p><p><img alt="" src="https://static.oschina.net/uploads/space/2023/0627/173716_02s8_2720166.png" referrerpolicy="no-referrer"></p><p>Wasmer 凭借其多样化的支持和专注于从通用桌面应用程序到 「便携式 ML/AI 应用程序」 的领域，目前仍然是领先的 WASM 运行时之一。</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 10:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263968/winterjs-service-workers</guid>
            <link>https://www.oschina.net/news/263968/winterjs-service-workers</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[扎克伯克：Meta 明年投入更多工程和计算资源到 AI 领域]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>当地时间 10 月 25 日，在 2023 财年第三季度财报电话会上，Meta CEO 扎克伯格强调，相信生成式 AI 的相关技术将让人们使用各种应用程序的方式变得更有意义。在未来，Meta 甚至有可能会利用 AI 来根据用户的兴趣为他们直接生成内容。</p><p>扎克伯格表示，AI 将帮助使用 Meta 各大应用的创作者提升内容质量和生产效率，而随着时间的推移，AI 参与生成的内容在用户消费内容中的占比将会越来越大。</p><p>对于公司的后续发展，扎克伯格表示在 2024 年，<strong>就工程和计算资源而言，AI 将成为 Meta 最大的投资领域</strong>。此外，扎克伯格补充道，为了避免引入大量的新员工，<strong>公司将降低一些非 AI 项目的优先级，并将相关人员转向从事 AI 工作</strong>。</p><p>上月曾报道过，<u><a href="https://www.oschina.net/news/257670/meta-building-llm-rival-openais-gpt4">Meta 正在构建</a></u>新开源大模型，据称性能超越 Llama 2、比肩 GPT-4，最终目标是加速开发下一代生成式人工智能模型，使其能够生成更多类似人类的表达。</p><p><img alt="" src="https://static.oschina.net/uploads/space/2023/0911/152426_g2gp_2720166.png" referrerpolicy="no-referrer"></p><p>长期以来，Meta 一直在采用开源方法公开其大模型产品，是业内众所周知的最大贡献者之一。仅今年它就向人工智能社区发布了大量人工智能模型和训练数据集。其中包括针对编程任务优化的 Code Llama 大语言模型； 可实现数百种语言通用按需翻译的 SeamlessM4T 模型； 用于创作音乐和声音的生成式人工智能模型 AudioCraft；语音生成人工智能模型 Voicebox。它还推出了 I-JEPA（一种可以像人类一样学习的计算机视觉模型）和 FACET（一种基准数据集，旨在帮助研究人员审核计算机视觉模型的偏差）。</p><hr><p>延伸阅读</p><ul><li><a href="https://www.oschina.net/news/256830/meta-ai-belebele">Meta AI 多语言阅读理解数据集 Belebele，涵盖 122 种语言变体</a></li><li><a href="https://www.oschina.net/news/255350/meta-code-llama">Meta 开源基于 Llama 2 的 AI 代码生成大模型：Code Llama</a></li><li><a href="https://www.oschina.net/news/255168/meta-seamless-m4t">Meta 推出&nbsp;SeamlessM4T，可转录和翻译近 100 种语言</a></li><li><a href="https://www.oschina.net/news/252174/audiocraft-generative-ai-for-music-and-audio">Meta 发布开源 AI 工具 AudioCraft，文本自动生成音乐</a></li><li><a href="https://www.oschina.net/news/249944/meta-llama-2">Meta 放大招：发布开源大语言模型 Llama 2，可免费商用</a></li><li><a href="https://www.oschina.net/news/245895/meta-voicebox-generative-ai-model-speech">Meta 发布语音生成 AI 模型：Voicebox</a></li><li><a href="https://www.oschina.net/news/245705/meta-musicgen">Meta 开源音乐生成模型 MusicGen</a></li><li><a href="https://www.oschina.net/news/242331/mate-multilingual-model-speech">Meta 开源大模型：支持 4000+ 语言识别，1100+ 种语音文本转换</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 09:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263757</guid>
            <link>https://www.oschina.net/news/263757</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Ubuntu 24.04 进入开发阶段，代号 Noble Numbat]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="color:#000000">Canonical 的 Utkarsh Gupta 在一封发送给 Ubuntu 开发邮件列表的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.ubuntu.com%2Farchives%2Fubuntu-devel%2F2023-October%2F042835.html" target="_blank">电子邮件中宣布</a>，Ubuntu 24.04 现已开放供开发，并透露了该版本的代号为「Noble Numbat」。</span></p><blockquote><p><span style="color:#000000">我们很高兴地宣布，Noble Numbat 现已开放开发。自动同步已启用，并将很快运行。和往常一样，我们预计在初始阶段会有大量的构建和自动测试涌入，这将导致一些延迟现象的出现。请协助修复出现的任何故障。</span></p></blockquote><p><span style="color:#000000">根据百度百科，Numbat（袋食蚁兽）是分布于澳大利亚西南部的一种小型有袋动物，几乎只以白蚁为食，每天可以吃约 20000 只白蚁。目前仅在少数地区存活，属于濒危物种，已被列入《世界自然保护联盟濒危物种红色名录》。袋食蚁兽的体型小而吻长，牙齿多达 52 枚，超过任何陆生哺乳动物的齿数，齿细，排成长列，长而能伸的舌（长约 10 厘米），用以捕捉白蚁。</span></p><p><span style="color:#000000"><img alt="" height="286" src="https://oscimg.oschina.net/oscnet/up-050f5f99cc77c3757686112b409fb2558f7.jpg" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">Ubuntu 24.04 将是 Ubuntu 自 2006 年以来的第 10 个 LTS 版本。Ubuntu 的 LTS 版本将获得 5 年的安全更新、错误修复和精选应用程序更新。Ubuntu Pro 则会在此基础上额外增加 5 年的安全保障，为现代的 LTS 版本提供了长达十年的支持。</span></p><p><span style="color:#000000">目前对于 Ubuntu 24.04 中将包含的新功能和改进仍然知之甚少。但 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2023%2F10%2Fubuntu-24-04-development-open" target="_blank">OMG! Ubuntu</a> 指出，对于长期支持版本而言，Ubuntu 在主要新功能、用户界面的巨大变化等方面往往会比较保守，主要会更加专注于坚实、稳定的体验。</span></p><p><span style="color:#000000">可以确定的是，24.04 肯定会配备新的 Linux 内核（6.7 或 6.8，视时间而定）、GNOME 46（预计将在三月份发布）。Canonical 的 Oliver Grawert 还透露，一个不可变的、snap-based Ubuntu 24.04 镜像将于 4 月份提供下载（但不会是默认推荐下载）。</span></p><p><span style="color:#000000">Ubuntu 24.04 计划于 2024 年 4 月 25 日正式发布。其功能冻结阶段定于 2024 年 2 月 29 日，beta 版本计划于 2024 年 4 月 4 日发布。</span></p><p><img height="501" src="https://oscimg.oschina.net/oscnet/up-78523d04d02ccf21bda56aa13e0dcd0b317.png" width="300" referrerpolicy="no-referrer"></p><p><span style="color:#000000">可在此查看具体的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdiscourse.ubuntu.com%2Ft%2Fnoble-numbat-release-schedule%2F35649" target="_blank">发布时间表</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 09:29:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263754/ubuntu-24-04-noble-numbat</guid>
            <link>https://www.oschina.net/news/263754/ubuntu-24-04-noble-numbat</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Next.js 14 发布：Server Actions 已稳定、部分预渲染进入预览]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Vercel 公司在 Next.js Conf 2023 上宣布了&nbsp;<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14" target="_blank">Next.js 14</a></u>。</p><blockquote><p>Vercel 是流行的开源前端框架 Next.js 背后的公司，Next.js 提供了包括服务器端渲染和为 Web 应用程序生成静态网站在内的功能。Vercel 作为一个开放的云平台提供了网站托管服务，让开发者能够在上面开发、预览和发布 Web 应用，同时优化了前端开发者的开发和部署体验。</p></blockquote><p><img alt="" height="338" src="https://oscimg.oschina.net/oscnet/up-facb05348dbe78400c4b01b68c0fceaa5d3.png" width="600" referrerpolicy="no-referrer"></p><p><strong>Next.js 14 主要变化：</strong></p><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23nextjs-compiler-turbocharged" target="_blank"><strong>Turbopack</strong></a>: App &amp; 页面路由通过了 5000 项测试 
  <ul><li>本地服务器启动速度提升&nbsp;<strong>53%</strong></li><li>使用 Fast Refresh 进行代码更新的速度提升&nbsp;<strong>94%</strong></li></ul></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23forms-and-mutations" target="_blank"><strong>Server Actions (Stable)</strong></a>: 渐进式的增强突变 
  <ul><li>重新验证缓存数据</li><li>支持简单的函数调用</li><li>本地支持表单</li></ul></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23partial-prerendering-preview" target="_blank"><strong>Partial Prerendering (Preview)</strong></a>: 快速初始化静态响应 + 流式动态内容</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23nextjs-learn-course" target="_blank"><strong>Next.js Learn (New)</strong></a>: 针对 App Router、身份验证、数据库等内容的全新免费课程</li></ul><hr><ul><li><strong>Turbopack 通过 5000 项集成测试</strong></li></ul><p>底层采用 Rust 编写的构建引擎 Turbopack 已通过<code>next dev</code>&nbsp;的 5,000 项集成测试，这些测试包括 7 年的错误修复。</p><p>Vercel 称开发者现在应该使用<code>next dev -turbo</code>会得到更快、更可靠的性能。该公司还表示，一旦 Turbopack 所有测试都通过，它将进入稳定状态（目前通过了 90% 的测试）。</p><ul><li><strong>Server Actions</strong></li></ul><p>在 Next.js 14 中，Next.js 团队通过稳定版本的 Server Actions 改进了开发者在编写数据变更方面的体验。</p><p>Server Actions 允许开发者定义异步服务器函数，使用 Server Actions 来重新验证缓存数据、重定向到不同的路由、设置和读取 cookie 等等。</p><p>现在，只需在 React 组件中定义一个函数，就能在服务器上安全地执行操作。</p><p>下面是一个简易示例：</p><pre><code>export default function Page() {
  async function create(formData: FormData) {
    'use server';
    const id = await createItem(formData);
  }
 
  return (
    &lt;form action={create}&gt;
      &lt;input type="text" name="name" /&gt;
      &lt;button type="submit"&gt;Submit&lt;/button&gt;
    &lt;/form&gt;
  );
}</code></pre><p>这不仅减少代码量，还减少了更改数据和重新渲染页面所需的网络往返次数，从而提升用户体验。</p><ul><li><strong>部分预渲染 (Partial Prerendering)</strong></li></ul><p>Next.js 团队正在为 Next.js 开发的」部分预渲染「是一种针对具有快速初始静态响应的动态内容的编译器优化。</p><p>Partial Prerendering 基于十年来对服务器端渲染 (SSR)、静态网站生成 (SSG) 和增量静态重验证 (ISR) 的研究和开发。</p><p>详情查看<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14" target="_blank"><u>发布公告</u></a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 08:48:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263948/next-js-14</guid>
            <link>https://www.oschina.net/news/263948/next-js-14</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[libnop - C++ 本机对象协议]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="project_detail_above_text_link_1" data-tracepid="project_detail_above_text_link"><a style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代 <img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>libnop 是一个仅用于序列化和反序列化 C++数据类型的头库，无需外部代码生成器或运行时支持库。唯一的强制性要求是一个支持 C++14 标准的编译器。</p><hr><p style="color:#1f2328; text-align:start"><strong>libnop 有以下目标：</strong></p><ul><li>使简单的序列化任务变得容易，使复杂的任务变得易于处理。</li><li>在 C++语言中移除对代码生成器和模式文件描述数据类型、格式和协议的依赖。</li><li>避免运行序列化操作时可能需要的额外运行时间。</li><li>提供现代功能，如双向二进制兼容性、数据验证、类型安全性和类型可替代性。</li><li>以最少的工作量处理内部类型、常见的 STL 类型和容器以及用户定义的类型。</li><li>生成易于分析的代码。</li><li>避免动态内存的分配时使用。</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 07:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/libnop</guid>
            <link>https://www.oschina.net/p/libnop</link>
        </item>
        <item>
            <title>
                <![CDATA[Next.js 支持在前端代码中写 SQL，开倒车还是遥遥领先？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>下面这张图来自近日举办的&nbsp;Next.js Conf 2023，里面的代码使用了名为<strong>「Server Actions」</strong>的特性——在前端代码中使用 SQL 语句直接操作数据库。</p><blockquote><p>Next.js 是流行的开源前端框架，其开发商是知名创业公司 Vercel。</p><p>Next.js 提供了包括服务器端渲染和为 Web 应用程序生成静态网站在内的功能。Vercel 作为一个开放的云平台提供了网站托管服务，让开发者能够在上面开发、预览和发布 Web 应用，同时优化了前端开发者的开发和部署体验。</p></blockquote><p><img alt="" height="533" src="https://oscimg.oschina.net/oscnet/up-e254f1c847ae20e8c530b34f9021da3a4d0.png" width="400" referrerpolicy="no-referrer"></p><p>在最新发布的 Next.js 14 中，Server Actions 已到达稳定阶段。其团队表示，Server Actions 改进了开发者在编写数据变更方面的体验。</p><blockquote><p><em><u><a href="https://www.oschina.net/news/263948/next-js-14" target="_blank">Next.js 14 发布：Server Actions 已稳定、部分预渲染进入预览</a></u></em></p></blockquote><p>Server Actions 允许开发者定义异步服务器函数，他们可以使用 Server Actions 重新验证缓存数据、重定向到不同的路由、设置和读取 cookie 等等。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-77eb78e36979830a06c1f44ed2476bb4db1.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0ad921306554c0716b4b4b0a8bedb71b8ba.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-badb8152ef443eb8272d03c220e0450c723.png" referrerpolicy="no-referrer"></p><p>不过目前看来，大多数人对它的评价似乎并不太好 ——</p><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1029/122449_wEe4_2720166.png" referrerpolicy="no-referrer"></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 04:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263921/nextjs-server-actions</guid>
            <link>https://www.oschina.net/news/263921/nextjs-server-actions</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[国外物价高，6 美元只能买 50 个 GitHub stars]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>《Wired》杂志发表文章<em>"<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wired.com%2Fstory%2Fgithub-stars-black-market-coders-cheat%2F" target="_blank">The GitHub Black Market That Helps Coders Cheat the Popularity Contest</a></u>"</em>，介绍了交易 GitHub Stars 的地下黑市。</p><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1028/161939_TSqR_2720166.png" referrerpolicy="no-referrer"></p></blockquote><p>GitHub 平台托管项目的受欢迎程度能够为部分程序员和创业公司打开一扇大门，他们通过 Stars 获得关注度、影响力和声誉。然而地下黑市出售的 Stars 提供了」以假乱真「的方式来让他们进行作弊。这些虚假 Stars 在某种程度上能帮助程序员和创业公司在联络投资人或找工作时留下好印象。</p><p>据介绍，在交易 GitHub Stars 的平台上，支付价值&nbsp;6 美元的以太币即可购买 50 个 Stars。除了 Stars，其他可量化的指标——如 Forks、Watchers 和 Follower 也可单独或组合购买。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img src="https://static.oschina.net/uploads/space/2023/1024/174616_4UDX_2720166.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><em>via<span>&nbsp;</span><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaddhi.shop%2Fproduct%2Fbuy-github-followers%2F" target="_blank">https://baddhi.shop/product/buy-github-followers/</a></u></em></p><p>文章写道，此前初创公司、程序员和投资者在决定雇用谁、为谁工作或投资谁时，会使用这些指标来筛选有潜力的程序员和初创公司。</p><p>但真正决定成功的不仅是这些指标，投资者开始意识到这种评估方式并不可靠，正在改变对 GitHub Stars 等指标的依赖，GitHub 平台也在打击这些专门用于刷数据的虚假账号。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 10:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263862/github-stars-black-market-coders-cheat</guid>
            <link>https://www.oschina.net/news/263862/github-stars-black-market-coders-cheat</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[小米 14 开机动画显示澎湃 OS 基于 Android]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>小米 14、澎湃 OS 等一大波新品已经正式登场<strong>。澎湃 OS 发布前，不少人都争论，它不是小米自研的系统，对此雷军还特意表示，确实不是。</strong></p><p>小米的澎湃 OS 由两部分组成：一部分是基于安卓系统进行深度进化的，这使得澎湃 OS 可以与安卓系统保持同步，并且能够使用安卓软件。</p><p>另一部分则是小米自研发的 Vela 系统，主要用于实现小米产品之间的互联互通。</p><p>这种系统架构使得澎湃 OS 能够兼顾兼容性和自主性，既满足了用户对丰富应用的需求，又能够提供更好的硬件软件一体化体验。</p><p><strong>发布会后，有网友从现场展示的新机看到，小米澎湃 OS 开机页面动画还是和以前一样，也有显示 "Powered by android"，这也算是证实了雷军之前的说法。</strong></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-4b428f25973338129e02686c3a80ff9faf1.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 05:17:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263829</guid>
            <link>https://www.oschina.net/news/263829</link>
            <author>
                <![CDATA[来源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenSSL 3.2 发布首个 Beta]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>OpenSSL 3.2 首个 Beta 版本已发布。</p><p>OpenSSL 3.2 实现了针对 QUIC 的初步客户端，QUIC 是 Google 开发的通用传输层网络协议，后来被 IETF 采用。 对于 OpenSSL 3.3 和明年的 OpenSSL 3.4，他们的目标是进一步完成此实现。</p><p>此外还增加了对 TLS 1.3 中 Brainpool 曲线的支持、原始公钥 (RFC7250) 支持、使用 Brotli 和 Zstd 进行证书压缩的支持、SM4-XTS 支持、确定性 ECDSA 签名、AES-GCM-SIV、混合公钥加密 (HPKE) ），以及其他特性。</p><p>OpenSSL 3.2 还将默认的 SSL/TLS 安全级别从 1 更改为 2。</p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopenssl%2Fopenssl%2Freleases%2Ftag%2Fopenssl-3.2.0-beta1" target="_blank">下载地址</a> | <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopenssl%2Fopenssl%2Fblob%2Fmaster%2FNEWS.md" target="_blank">更新说明</a></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 04:08:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263824/openssl-3-2-0-beta1-released</guid>
            <link>https://www.oschina.net/news/263824/openssl-3-2-0-beta1-released</link>
            <author>
                <![CDATA[来源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2023 CCF 中国开源大会开源商业化分论坛顺利召开]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 请你来轰趴啦！1028 苏州源创会，一起寻宝 AI 时代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>10 月 21 日至 22 日，由中国计算机学会（CCF）、开放原子开源基金会主办的 2023 CCF 中国开源大会在长沙顺利举行。其中，开源商业化分论坛由开源中国承办，开源中国董事长马越担任主席。来自开源原生商业公司的诸多专家就开源项目商业化最佳实践展开分享，为更多开发者和企业提供可借鉴的经验，共同推动开源生态建设，助力开源生态发展。</p><p><strong>开源中国董事长马越</strong>以《中国开源商业发展的现状及思考》为题发表主旨演讲。他指出，当前开源创业公司有「七大恨」：没有品牌、没有流量、没有销售能力、没有资质、没有交付能力、没有现金流、没有资本渠道。这些都严重阻碍了创业公司进一步发展壮大。而解决开源创业公司「七大恨」的关键就在于开源创业联合体。所谓开源创业联合体，就是提供通用服务模型的价值流平台，实现集成和自动化 IT 价值链的插件开放平台，融合市场各类开源或商业生态能力，落地客户场景服务。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-729f0faa336b257003127266ac12c4b722d.png" width="800" referrerpolicy="no-referrer"></p><p>最后马越提议，希望能够集众多开源力量共建这样的插件开放平台，繁荣开源商业生态，互通有无。开源中国已积累了十几年的商业化经验以及商业化能力，旗下 Gitee 平台也已经入驻了 27 万多家中小团队，服务中国 600 多家 100 亿元估值以上的大企业。未来，开源中国将通过该插件平台将这些经验和能力赋能更多开源企业。</p><p><strong>CCF 开源发展委员会常委谭中意</strong>就 「AI to B 的开源和商业化」这两大方向展开探讨。谭中意认为，当前 LLM to B 业务的难点在于，需要找到一个 Killer 场景——有足够的商业回报，能覆盖大模型 Finetune 和 Serving 以及 LLM 应用开发和运维的成本。但是 LLM 技术存在先天上的约束：一是无法避免幻觉的问题，To C 业务需要满足网信办规定，合规成本很高；二是无法避免概率的问题，To B 的严肃场景可能不太适合。因此，突破口可能在于：一是企业内部，对生成内容更讲究创意或者实时 Check 的场景，比如内部研发代码生成工具、游戏行业的场景；二是电商领域，比如促销、广告投放等，因为这是离钱最近的潜力市场，且容易形成数据闭环。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-cf8531b4308c6c6399ab200699cb0ef9b7d.png" width="800" referrerpolicy="no-referrer"></p><p>至于开源在 LLM 产业的关键作用，主要有两个：一是降低成本，开源是降低整个产业创新成本的关键，即 AI 民主化。这需要整个学术界和产业界一起努力，来把成本降下来，而且开源底座模型是整个大生态最重要部分；二是建立信任，人工智能要让人信任，它必须公开透明。一旦这个问题解决了，一个十万亿规模的市场可能起飞。</p><p><strong>PingCAP 副总裁刘松</strong>分享了 TiDB 从开源到 Serverless 的商业化演进逻辑。据其介绍，PingCAP 的商业化之路可以总结为「四部曲」：创建一个满足时代刚需的开源项目，开源产品获得规模化的用户部署与反馈，打造一个全球化的商业化模式，持续创造满足极致用户需求的产品形态。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-c12ba68303c5e846f95912614c315de728c.png" width="800" referrerpolicy="no-referrer"></p><p>刘松表示，面对「开源 + 云」互相推动和演进的新技术环境，TiDB 演进方向从技术领先走向了「技术+体验」领先。当前，PingCAP 围绕 TiDB 构建了三大产品形态：TiDB 企业版、TiDB Cloud（全托管）、TiDB Cloud Serverless。其中，TiDB Cloud 提供全托管的 DBaaS （Database-as-a-Service）服务，极大地降低了云数据库的使用门槛；TiDB Cloud Serverless 基于云原生/多云的设计，采用 AI-Ready 的架构，实现极致低成本、极致弹性，拥有自动化的资源调度能力以及灵活集成 AI 能力等特性。</p><p><strong>统信软件解决方案中心专家任紫东</strong>以《中国开源操作系统商业发展探索》为题展开分享。任紫东认为，2020 年是国产 Linux 里程碑之年。2019 年前，我国有 10 多家国产操作系统企业，随着政策引导和市场竞争战略的选择，国产操作系统厂商进行了全新的业态整合，2020 年起，初步形成两家主流国产操作系统企业，统信就是其中之一。及至 2022 年，产业链的商业形态形成。头部操作系统公司规模也呈现「干百十」特征：千人以上的操作系统开发队伍，百人以上的内核研发团队，十人以上的开源合规律师团队。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-d9d35f21d4f87673628b2337118d6fb42dc.png" width="800" referrerpolicy="no-referrer"></p><p>当前，统信 UOS 生态是国内最大的自主操作系统生态圈之一。统信己基于服务器操作系统完成诸多主流国外商业软件的适配，涵盖了 Oracle、IBM、SAP、微软等厂商的主流数据库产品，Google、FaceBook、百度、华为等厂商的主流 AI 人工智能类产品，以及 Oracle、IBM 等厂商的主流中间件产品。</p><p>开源社区做得好，怎么变成钱？在以《白鲸 DataOps 开源矩阵商业化之路》为题的演讲中，<strong>白鲸开源 CEO 郭炜</strong>提到，白鲸花了 8 个月时间，摸索了一套开源商业转化流程，积累数千万的商业 Pipeline 以及上百个线索，而投入资源不过是一名销售人员，没有任何市场费用。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-c3550e7f84e4a38252e13b77a70f32bcd94.png" width="800" referrerpolicy="no-referrer"></p><p>郭炜把这些成果归结于几大原则：「别人做中石油，我做中石化」，即做所有人的朋友；开源项目定位要清晰，商业功能痛点要明确，因此白鲸开源采取了「开源矩阵+OpenCore」的路线；开源商业软件要重视「行业属性」，分行业洞察痛点，口碑营销；不忘初心，牢记使命，不断升级开源版，商业版才有机会；勇于探索，拥抱新技术，将大模型融入软件，等等。</p><p>最后他提到，开源风口并没有过去，而且温度刚刚好。面对经济下行周期、资本趋于冷静以及收入体量要求更高等挑战，也应看到行业展现出来更多的机会，比如恶性竞争减少，互联网公司开始付费买工具，订阅制更容易被接受，海外市场逐步增长等等。</p><p><strong>TDengine 联合创始人&amp; 商业化 VP 李广</strong>分享了 TDengine 是如何从开源时序数据库到工业大数据处理平台的。他表示，一款软件开源，就意味着可信、可控。TDengine 的商业逻辑就是重塑 2B 销售模式，以开源建社区与品牌，以开源建 GTM 路径。通过开源扩大影响力，树立品牌，形成开发者社区，构建竞争壁垒，快速获得市场反馈，快速迭代，快速打造生态，获得用户信任。另一方面，将传统的 2B 销售演变为 2C 的模式，将传统的登门拜访演变为线上销售，将资源型销售转化为技术和产品型销售。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-a48a9ecbd7606788031401fba0b8a83267d.png" width="800" referrerpolicy="no-referrer"></p><p>当前，TDengine 提供三种产品与服务，一是开源社区版——TDengine OSS，主要是为了建立开发者社区，建立生态；二是企业版——TDengine Enterprise，支持独立部署并按照 TBL（Term Based License）年度服务订阅或者永久 Licenese 模式销售；三是云服务版——TDengine Cloud，在阿里云、AWS、华为云等云平台上直接提供 SaaS 服务，根据数据量和时长计费。</p><p>最后他表示，开源软件的商业化逻辑已经发生了变化，从关注增长转向强调利润。2B 软件群龙纷争的时代结束，只有深入行业黑土地才能生存。</p><p><strong>筑栈（KodeRover / Zadig）创始人 &amp; CEO 李倩</strong>分析了公司为什么选择深耕中国而不是出海。据悉，在商业化过程中，KodeRover 也面临过不少问题，比如花了时间打磨产品，但用户付费意愿不强烈；有需求有预算的大客户找上门，却因为自身团队规模过小无法为其提供大型服务等等，不过 KodeRover 最终也地制定了相应策略。李倩将公司的商业化思路总结为：技术上用开源铸造好基建，打造产品力、品牌力；商业上，中国场景助力「新 IT」 （比如硬科技创新、旧行业升级重整）升级，创造客户价值；可持续创造价值，广泛链接，为客户提供最优质的解决方案。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-e0510f159c4ed52d504290463b06b22c059.png" width="800" referrerpolicy="no-referrer"></p><p>李倩提到，Zadig 是生产软件的软件，目的是交付数字业务。 Zadig 开源两年，企业安装总量近 3 万，目前是国内云原生 DevOps 领域落地最广泛的平台，成为包括字节飞书、极氪、路特斯、小鹏、七牛云、WiFi 万能钥匙、易快报、iMile、TT 语音、锅圈、药师帮、大参林、老百姓大药房、 益丰大药房、小天才等标杆企业的数千家企业研发工程师每日深度、高频使用的软件交付平台。</p><p><strong>EMQ 映云科技联合创始人兼 CPO 金发华</strong>以《EMQ —— 开源数实融合基础软件的商业化》为题发表演进。据了解，作为全球领先的物联网基础软件提供商，EMQ 创立并主导 LF Edge eKuiper、NanoMQ、Neuron 等多个全球知名边缘软件开源项目。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-086857ea6bb66926b0c38eb9fb66172ceae.png" width="800" referrerpolicy="no-referrer"></p><p>金发华表示，Hosting(Cloud) 模式是未来产品的一个方向。当前，EMQ Cloud 商业服务有三种。一是 Serverless，轻松几步即可获得一个安全可伸缩的 ServerlessMQTT 服务。全托管特性让用户无需关心基础设施和资源管理，特别适用于个人开发者、中小型项目、开发测试环境以及技术框架的评估。二是专有版，独立部署的全托管 MQTT 服务，具有更高的性能保障和可定制能力，尤其适用于对性能、稳定性要求较高的企业级项目。三是 BYOC (Bring Your Own Cloud)，用户在自己的云上部署 EMQX 集群,并交由 EMQX 团队托管，适用于有严格数据安全和合规性要求的企业级项目，最大限度地利用现有的云资源。</p><p><strong>天际科技投资副总裁江志桐</strong>阐述了开源与 AI 时代下的投资逻辑。她表示，在 AI 2.0 时代，从全球市场来看，基于大模型未来增长预期，资本给予显著溢价。当前通用模型格局明确，出现了微软、谷歌双龙头企业，工具层、应用层、垂直领域涌现大量独角兽。全球市场都在关注大模型商业化的落地，围绕效率、创意、情感陪伴 2C/2B 的应用生态繁荣。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-d423c2f9a083841554cb3cea84bb20c4056.png" width="800" referrerpolicy="no-referrer"></p><p>与此同时，开源正在加速 AI 2.0 落地，加速 AI 生态繁荣。开源是大模型基础设施必然选择，延伸出的服务、应用具有巨大商业机会。AI 开源时代的投资策略以核心人物为中心，布局早期，发挥产业资源优势，关键人物、网络效应、稀缺数据，以及软硬一体都有可能成为企业护城河的因素，也是 AI 开源时代投资重点。</p><p>那么中国市场的机会在哪里呢？江志桐认为，主要在于两方面，一是基础设施，二是垂直行业。国内大模型的应用还在快速成长，基于开源加速模型落地后，整个 AI 生态里面也会出现能够对标全球市场的公司。总之，国内 AI 市场还处于巨头形成的阶段。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 02:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/10136947</guid>
            <link>https://my.oschina.net/u/3859945/blog/10136947</link>
            <author>
                <![CDATA[原创]]>
            </author>
        </item>
    </channel>
</rss>
