<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[開源中國-最新資訊]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[開源中國-最新資訊 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 13 Dec 2023 02:18:09 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[HyperDX —— 開發者友好的 Datadog 替代品]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>HyperDX&nbsp;是<span style="background-color:#ffffff; color:#1f2328">一個</span>基於雲的生產監控和調試工具<span style="background-color:#ffffff; color:#1f2328">，統一會話重放、日誌、指標、跟蹤和錯誤。</span>通過將日誌、指標、跟蹤、異常和會話重播集中並關聯到一處，幫助工程師更快地找出生產中斷的原因。Datadog 和 New Relic 的開源且開發人員友好的替代方案。</p><ul><li>端到端關聯，只需點擊幾下即可從瀏覽器會話重放到日誌和跟蹤</li><li>由 Clickhouse 提供支持的極快性能</li><li>直觀的全文搜索和屬性搜索語法（例如<code>level:err</code>）</li><li>自動對數十億個事件中的事件模式進行聚類</li><li>儀錶板高基數事件，無需複雜的查詢語言</li><li>只需點擊幾下即可設置警報</li><li>自動解析 JSON/結構化日誌</li><li>OpenTelemetry native</li></ul><p><img height="702" src="https://static.oschina.net/uploads/space/2023/0920/163847_4VuB_4252687.png" width="1220" referrerpolicy="no-referrer"></p></div>
                                                                ]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 02:09:07 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/hyperdx</guid>
            <link>https://www.oschina.net/p/hyperdx</link>
        </item>
        <item>
            <title>
                <![CDATA[小米迴應餘承東「龍骨轉軸」抄襲華為言論]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>「小米公司發言人」官方微博近日發佈聲明，就餘承東所述「龍骨轉軸」抄襲華為言論做出澄清稱：</p><blockquote><p>「近日，餘承東先生無端針對我司龍骨轉軸技術發佈不實言論，與事實嚴重不符。我們請餘承東先生遵循「科學與嚴謹」的基本規則，請勿再抹黑同行、誤導公眾。」</p></blockquote><p>聲明指出，無論是設計思路還是機械結構，小米自研的龍骨轉軸與餘承東所宣稱的所謂雙旋水滴較鏈都完全不同。</p><p>且龍骨轉軸於 2020 年 9 月 18 日申請專利，並於 2021 年 1 月 5 日獲得專利授權，在 2023 年 8 月於 XiaomiMIXFold 了上首發應用。雙旋水滴較鏈則於 2019 年 12 月 13 日申請的專利，2021 年 6 月 18 日才公開。「由此可知，餘承東先生的言論，完全不符合事實。」</p><p><img alt="" height="1349" src="https://oscimg.oschina.net/oscnet/up-31b05e591ed808bedd4150f1214bc51f38d.jpg" width="500" referrerpolicy="no-referrer"></p><p>專利圖：</p><p><img alt="" height="368" src="https://static.oschina.net/uploads/space/2023/1213/100457_fh3o_4252687.jpg" width="500" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 02:05:07 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270776</guid>
            <link>https://www.oschina.net/news/270776</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推薦 | 物聯網智能網關係統，物聯大師]]>
            </title>
            <description>
                <![CDATA[<h1><a id="物聯大師" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E7%89%A9%E8%81%94%E5%A4%A7%E5%B8%88"></a>物聯大師</h1><p><strong>注意，[V3.0]版本與<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Ftree%2Fv2">V2.0</a>
和<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Ftree%2Fv1">V1.0</a>有較大差異，不可以直接升級！！！</strong></p><h3><a id="説明文檔--演示 demo-賬號密碼-admin-123456" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3--%E6%BC%94%E7%A4%BAdemo-%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81-admin-123456"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fiot-master.com%2Fmanual">説明文檔</a><a href="https://gitee.com/link?target=http%3A%2F%2Fdemo.iot-master.com%3A8080%2F">演示 demo</a> 賬號密碼 admin 123456</h3><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Factions%2Fworkflows%2Fgo.yml"><img src="https://github.com/zgwit/iot-master/actions/workflows/go.yml/badge.svg" alt="Go" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Factions%2Fworkflows%2Fcodeql-analysis.yml"><img src="https://github.com/zgwit/iot-master/actions/workflows/codeql-analysis.yml/badge.svg" alt="Go" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fcodecov.io%2Fgh%2Fzgwit%2Fiot-master"><img src="https://codecov.io/gh/zgwit/iot-master/branch/main/graph/badge.svg?token=AK5TD8KQ5C" alt="codecov" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fpkg.go.dev%2Fgithub.com%2Fzgwit%2Fiot-master"><img src="https://pkg.go.dev/badge/github.com/zgwit/iot-master.svg" alt="Go Reference" referrerpolicy="no-referrer"></a><a href="https://gitee.com/link?target=https%3A%2F%2Fgoreportcard.com%2Freport%2Fgithub.com%2Fzgwit%2Fiot-master"><img src="https://goreportcard.com/badge/github.com/zgwit/iot-master" alt="Go Report Card" referrerpolicy="no-referrer"></a></p><p>物聯大師是<a href="https://gitee.com/link?target=https%3A%2F%2Flabs.zgwit.com">無錫真格智能科技有限公司</a>
推出的開源且免費的物聯網操作系統，內置 MQTT、TCP Server/Client、UDP Server/Client、串口等接入服務，
系統集成標準 Modbus，水務（SL651、SZY206），電力（DL/T645、IEC101、102、103、104、61850）以及一些主流 PLC 協議，
系統可以通過插件支持數據採集、公式計算、定時控制、異常報警、自動控制策略、流量監控、遠程調試、Web 組態等功能，
適用於大部分物聯網或工業互聯網應用場景。
系統採用 Golang 編程實現，支持多種操作系統和 CPU 架構，可以運行在智能網關上，也可以安裝在現場的電腦或工控機上，還能部署到雲端服務器。</p><p>項目摒棄複雜的平台架構思維，遠離微服務，從真實需求出發，注重用戶體驗，做到簡捷而不簡單，真正解決物聯網缺乏靈魂的問題。</p><p>我們的宗旨是：<strong>讓物聯網實施變成一件簡單的事情!!!</strong></p><h2><a id="項目的優勢" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%BC%98%E5%8A%BF"></a>項目的優勢</h2><ul><li>開源免費，商業應用也不限制</li><li>單一程序文件，不需要配置運行環境，不依賴第三方服務，放服務器上就能跑</li><li>極小內存佔用，對於一百節點以內的物聯網項目，只需要幾十兆內存足夠了，<del>比起隔壁 Java 動輒大幾百兆內存簡直太省了</del></li><li>支持工控機和智能網關，邊緣計算也沒問題</li><li>支持大屏展示，Web 組態，3D 數據孿生 <del>畢竟很多物聯網項目都是面子工程</del></li><li>在線產品庫、模板庫、組件庫，小白也能分分鐘搞得有模有樣【還在努力建設中】</li></ul><h2><a id="項目示例" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E9%A1%B9%E7%9B%AE%E7%A4%BA%E4%BE%8B"></a>項目示例</h2><p><img src="https://iot-master.com/web1.jpg" alt="web" referrerpolicy="no-referrer"><img src="https://iot-master.com/hmi-editor.png" alt="scada" referrerpolicy="no-referrer"></p><h2><a id="諮詢服務" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E5%92%A8%E8%AF%A2%E6%9C%8D%E5%8A%A1"></a>諮詢服務</h2><p><strong>本公司目前提供免費的物聯網方案諮詢服務，結合我們十多年的行業經驗，給您提供最好的建議，請聯繫 15161515197（微信同號）</strong></p><blockquote><p>PS. 提供此服務的主要目的是讓用戶少走彎路，為物聯網行業的健康發展盡綿薄之力。
總結一下常見的彎路：</p><ol><li>前期使用某個物聯網雲平台，後期沒辦法繼續，二次開發受限</li><li>花了幾千元買了工業網關，用着一百元 DTU 的功能</li><li>找多個外包公司，低價拿單，結果做出屎一樣的東西</li><li>盲目使用開源項目，最終被開源項目所累</li><li>硬件選型失敗，效果差強人意</li><li>自身技術人員能力有限，架構設計有問題</li><li>不支持高併發量，市場爆發了，平台反而跟不上</li><li>等等</li></ol></blockquote><h2><a id="聯繫方式" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E8%81%94%E7%B3%BB%E6%96%B9%E5%BC%8F"></a>聯繫方式</h2><ul><li>郵箱：<a href="mailto:jason@zgwit.com">jason@zgwit.com</a></li><li>手機：<a>15161515197</a>(微信同號)</li></ul><table><thead><tr><th>技術交流羣</th><th>微信</th></tr></thead><tbody><tr><td><img src="https://iot-master.com/tech.png" alt="微信羣" referrerpolicy="no-referrer"></td><td><img src="https://iot-master.com/jason.jpg" alt="微信" referrerpolicy="no-referrer"></td></tr></tbody></table><h2><a id="開源協議" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE"></a>開源協議</h2><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fzgwit%2Fiot-master%2Fblob%2Fmain%2FLICENSE">GPL v3</a></p><p>補充：任何組織或個人都可以免費使用或做二次開發，但不得用於商業售賣，如有需求請聯繫我們。</p><h3><a id="官方插件" class="anchor" href="https://gitee.com/zgwit_labs/iot-master#%E5%AE%98%E6%96%B9%E6%8F%92%E4%BB%B6"></a>官方插件</h3><p><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Forgs%2Fiot-master-contrib%2Frepositories">插件庫</a></p><table><thead><tr><th>插件</th><th>完成</th><th>正式版</th></tr></thead><tbody><tr><td>歷史統計【內置】</td><td>✅</td><td>⬜</td></tr><tr><td>異常報警【內置】</td><td>✅</td><td>⬜</td></tr><tr><td>Influxdb 時序數據庫</td><td>✅</td><td>⬜</td></tr><tr><td>Modbus 通訊協議</td><td>✅</td><td>⬜</td></tr><tr><td>WebRTC 接入攝像頭</td><td>✅</td><td>⬜</td></tr><tr><td>Web 組態</td><td>✅</td><td>⬜</td></tr><tr><td>3D 數據孿生</td><td>⬜</td><td>⬜</td></tr><tr><td>阿里雲通知</td><td>✅</td><td>⬜</td></tr><tr><td>DLT645-2007，電力規約</td><td>⬜</td><td>⬜</td></tr><tr><td>西門子 PLC，S7 系統，PPI，MPI，FetchWrite</td><td>✅</td><td>⬜</td></tr><tr><td>三菱 PLC</td><td>✅</td><td>⬜</td></tr><tr><td>歐姆龍 PLC，Hostlink，Fins</td><td>✅</td><td>⬜</td></tr><tr><td>TDEngine</td><td>⬜</td><td>⬜</td></tr><tr><td>OpenTSDB</td><td>⬜</td><td>⬜</td></tr><tr><td>流式計算</td><td>⬜</td><td>⬜</td></tr><tr><td>報表引擎</td><td>⬜</td><td>⬜</td></tr></tbody></table>]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 01:58:16 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/zgwit_labs/iot-master</guid>
            <link>https://gitee.com/zgwit_labs/iot-master</link>
        </item>
        <item>
            <title>
                <![CDATA[AutoMQ 社區雙週精選第二期（11.20-12.01）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h2>本期概要</h2><p>在開源的第二個雙週裏，作為一個成長中的開源項目，AutoMQ 做了很多的優化和重構，以下是相關重點動態的總結。<br> AutoMQ Kafka：寫鏈路耗時優化、快慢讀隔離、Spot 實例強制回收容災。<br> AutoMQ RocketMQ：歷史數據冷讀優化、LogCache 讀寫耗時優化、發佈 v0.0.3-alpha 版本、發佈 Helm Chart、發佈文檔站。</p><h2>AutoMQ Kafka 精選動態</h2><h3>寫鏈路耗時優化</h3><p>原來所有的寫入和回調都會放到一個單線程線程池去進行處理來確保數據安全，該方式存在線程上下文切換通信、單線程處理排隊兩個問題。本次優化將寫入流程中的數據結構改造成線程併發安全模式，使得不同 stream 之間可以併發進行寫入，AutoMQ Kafka 客戶端平均寫入耗時<strong>下降 0.3ms</strong>。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F728" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/728</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F729" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/729</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F743" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/743</a></p><h3>快慢讀隔離</h3><p>隔離從 Cache 讀取的快讀和從 S3 的讀取的慢讀，避免慢讀佔滿快讀的線程池影響快讀。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-kafka%2Fpull%2F472" target="_blank">https://github.com/AutoMQ/automq-for-kafka/pull/472</a></p><p><strong>Spot 實例強制回收容災</strong></p><p>在上期精選中提及進度的 Spot 實例強制回收容災已經完成。Spot（競價實例）相比按需實例可以便宜至多 90 %，但問題是它可能不經通知就強制回收。該特性支持 Spot 實例強制回收的情況下，仍舊可以將數據卷掛載到存活的機器，進行<strong>秒級容災恢復</strong>。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-kafka%2Fissues%2F447" target="_blank">https://github.com/AutoMQ/automq-for-kafka/issues/447</a></p><h2>AutoMQ RocketMQ 精選動態</h2><h3>Stream 模塊性能優化</h3><h4>歷史數據冷讀優化</h4><p>歷史數據追趕讀優化，Fetch 請求（50MB &amp; 50 stream）冷讀穿透到 S3 場景，單次 Fetch 耗時從 4s 優化到 100ms。即使是完全穿透冷讀，S3 讀取吞吐效率也是很高的。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F766" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/766</a></p><h4>LogCache 讀寫耗時優化</h4><p>增加上次 Cache 讀取位點記錄，避免每次從 LogCache 讀取數據都需要二分查找定位，10W 個消息下 10W 次查詢時間從 71s 優化到 86ms。<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F731" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/731</a> 通過讀寫鎖，將 LogCache 升級成線程併發安全的數據結構，提升 LogCache 讀取併發效率。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F701" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/701</a></p><h3>發佈 v0.0.3-alpha 版本</h3><p>這個版本包含了以下功能和優化： 1）穩定性與性能提升：修復了潛在的 OOM 問題以及提升 stream 模塊性能，詳見 Changelog：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fcompare%2Fv0.0.2-alpha...v0.0.3-alpha" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/compare/v0.0.2-alpha...v0.0.3-alpha</a> 2）工程化建設：引入 Nightly build 和&nbsp;E2E test CI <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fhub.docker.com%2Fr%2Fautomqinc%2Fautomq-for-rocketmq%2Ftags" target="_blank">https://hub.docker.com/r/automqinc/automq-for-rocketmq/tags</a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Factions%2Fworkflows%2Fbuild-ci.yml" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/actions/workflows/build-ci.yml</a> 3）可觀測性提升：為 stream 模塊引入 Metrics；為 Proxy、Store 模塊引入 Trace <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-rocketmq%2Fpull%2F766" target="_blank">https://github.com/AutoMQ/automq-for-rocketmq/pull/766</a></p><h3>發佈 Helm Chart</h3><p>現在可以使用 Helm Chart 快速在 Kubernetes 中創建 AutoMQ RocketMQ 集羣。</p><pre><code class="language-cs">$ helm repo add automq https://charts.automq.com
$ helm search repo automq                                                                                                                                                            
NAME                            CHART VERSION   APP VERSION     DESCRIPTION                                                                                                           
automq/automq-for-rocketmq      0.0.4           v0.0.3-alpha    A Helm chart for automq-for-rocketmq
</code></pre><p>部署該 Chart 會創建一個 AutoMQ RocketMQ Broker 以及依賴的 MySQL 與 Minio 組件。後續會陸續加入可選的可觀測性依賴組件。</p><h3>發佈文檔站</h3><p>介紹了 AutoMQ RocketMQ 基本使用方式，包含：本地構建、使用 docker compose 部署、在 Kubernetes 上部署。以及使用 CLI 運維集羣模式，管理 Topic 等資源。 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdocs.automq.com%2Fzh%2Fdocs%2Fautomq-rocketmq%2FRmuXwhb5Xi9zjCkrInRcCz0UnTe" target="_blank">https://docs.automq.com/zh/docs/automq-rocketmq/RmuXwhb5Xi9zjCkrInRcCz0UnTe</a></p><h2>More Things</h2><p>與小紅書的同學共創對象存儲跨地域容災方案 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ%2Fautomq-for-kafka%2Fissues%2F477" target="_blank">https://github.com/AutoMQ/automq-for-kafka/issues/477</a></p><p>以上是第二期《雙週精選》的內容，歡迎關注我們的公眾號，我們會定期更新 AutoMQ 社區的進展。同時，也誠邀各位開源愛好者持續關注我們社區，跟我們一起構建雲原生消息中間件！</p><p><strong>END</strong></p><h3>關於我們</h3><p>AutoMQ 是一家專業的消息隊列和流存儲軟件服務供應商。AutoMQ 開源的 AutoMQ Kafka 和 AutoMQ RocketMQ 基於雲對 Apache Kafka、Apache RocketMQ 消息引擎進行重新設計與實現，在充分利用雲上的競價實例、對象存儲等服務的基礎上，兌現了雲設施的規模化紅利，帶來了下一代更穩定、高效的消息引擎。此外，AutoMQ 推出的 RocketMQ Copilot 專家系統也重新定義了 RocketMQ 消息運維的新範式，賦能消息運維人員更好的管理消息集羣。&nbsp;</p><p>🌟&nbsp;GitHub 地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAutoMQ" target="_blank">https://github.com/AutoMQ</a></p><p>💻&nbsp;官網：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.automq.com" target="_blank">https://www.automq.com</a></p><p>👀&nbsp;B 站：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspace.bilibili.com%2F3546572478482870" target="_blank">AutoMQ 官方賬號</a></p><p>🔍&nbsp;視頻號：AutoMQ&nbsp;</p><p><strong>👉 掃二維碼</strong>加入我們的社區羣</p><p><img src="https://oscimg.oschina.net/oscnet/up-c4c6b2be9441c750e268dd2d48294131af7.png" alt="" referrerpolicy="no-referrer"></p><p>關注我們，一起學習更多雲原生乾貨</p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 13 Dec 2023 01:47:16 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/6990971/blog/10320900</guid>
            <link>https://my.oschina.net/u/6990971/blog/10320900</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[雲原生週刊：Kubernetes v1.29 新特性一覽]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h2>開源項目推薦</h2><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwerf%2Fkubedog" title="kubedog" target="_blank">kubedog</a></h3><p>Kubedog 是一個用於在 CI/CD 部署管道中監視和跟蹤 Kubernetes 資源的庫。</p><p>這個庫被用於 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwerf%2Fwerf" title="werf CI/CD" target="_blank">werf CI/CD</a> 工具中，在部署過程中跟蹤資源。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frunwhen-contrib%2Frunwhen-local" title="RunWhen Local" target="_blank">RunWhen Local</a></h3><p>runwhen-local 是一個工具，用於在本地環境中運行 runwhen 腳本。runwhen 是一個靈活的任務調度工具，可以根據條件和時間表來執行任務。通過 runwhen-local，開發者可以在本地測試和調試 runwhen 腳本，以確保其正確運行。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubewharf%2Fkubegateway" title="KubeGateway" target="_blank">KubeGateway</a></h3><p>kube-gateway 是字節跳動內部管理海量 kubernetes 集羣的最佳實踐。 它是為 kube-apiserver 的 HTTP2 流量專門設計並定製的七層負載均衡代理。 目標是為海量的大規模 kubernetes 集羣（千級 node 以上）提供靈活的穩定的流量治理方案。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fflannel-io%2Fflannel" title="flannel" target="_blank">flannel</a></h3><p>Flannel 是為 Kubernetes 設計的一種簡單且易於配置的第三層網絡結構的解決方案。</p><h2>文章推薦</h2><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmoelove.info%2F2023%2F12%2F10%2FKubernetes-v1.29-%25E6%2596%25B0%25E7%2589%25B9%25E6%2580%25A7%25E4%25B8%2580%25E8%25A7%2588%2F" target="_blank">Kubernetes v1.29 新特性一覽</a></h3><p>這篇文章介紹了 Kubernetes v1.29 版本的新特性。該版本包含了 49 個主要的更新，其中有 19 個增強功能進入 Alpha 階段，19 個升級到 Beta 階段，還有 11 個升級到穩定版。</p><p>文章重點介紹了兩個重要的特性：基於 CEL 的 CRD 規則校驗和為動態和靜態分配預留 NodePort 端口範圍。基於 CEL 的 CRD 規則校驗是一種在 CRD 聲明中編寫校驗規則的方式，簡化了開發和維護成本。而為動態和靜態分配預留 NodePort 端口範圍的特性解決了在創建 NodePort 時可能產生的端口衝突問題。總體而言，Kubernetes v1.29 版本的新特性為用戶提供了更好的功能擴展和更可靠的輸入校驗。</p><h3>[Kubernetes：Pod 和 WorkerNodes – 控制 Pod 在節點上的放置</h3><p>](<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Frtfm.co.ua%2Fen%2Fkubernetes-pods-and-workernodes-control-the-placement-of-the-pods-on-the-nodes%2F" target="_blank">https://rtfm.co.ua/en/kubernetes-pods-and-workernodes-control-the-placement-of-the-pods-on-the-nodes/</a>)</p><p>這篇文章介紹了在 Kubernetes 中如何控制 Pods 在 WorkerNodes 上的部署位置。它提供了四種主要的方法來實現這種控制：</p><ul><li>配置節點</li><li>Taints 和 Tolerations</li><li>配置 Pod 本身</li><li>Pod 親和性和反親和性</li></ul><p>此外，文章還提到了 Pod 拓撲分佈約束（Pod Topology Spread Constraints），即根據失敗域（regions、可用區或節點）的規則來放置 Pod。</p><p>文章還提供了一些使用 kubectl explain 命令來查看相關參數和資源文檔的技巧。</p><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmedium.com%2F%40geoffrey.muselli%2Fargocd-multi-tenancy-strategy-94d72183c94" title="ArgoCD：多租戶策略" target="_blank">ArgoCD：多租戶策略</a></h3><p>這篇文章介紹了使用 ArgoCD 實現多租戶策略的方法。在使用 ArgoCD 時，通常會允許所有用戶自由操作，直到進入生產環境後才意識到某個人通過刪除應用程序而刪除了命名空間或 CRD。為瞭解決這個問題，需要使用訪問控制和多租戶策略。文章詳細介紹瞭如何利用 ArgoCD 的原生功能實現多租戶策略，並提供了一個示例來演示如何在大型組織中使用企業敏捷框架（例如 SAFe）來實施。文章還討論了 ArgoCD 中的 AppProject、RBAC 和命名空間等概念，以及如何配置和使用它們來實現多租戶策略。最後，文章提供了一個具體的示例，展示瞭如何根據團隊和項目的需求來配置 AppProject 和 RBAC。</p><h2>雲原生動態</h2><h3><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.cncf.io%2Fblog%2F2023%2F12%2F05%2Fkyverno-completes-third-party-security-audit%2F" title="Kyverno 完成第三方安全審計" target="_blank">Kyverno 完成第三方安全審計</a></h3><p>Kyverno 項目宣佈完成了第三方安全審計。該審計是由 Ada Logics 與 Kyverno 維護人員、開源技術改進基金合作進行，由 CNCF 資助。</p><p>該安全審計是一個全面的安全審計，有以下四個目標：</p><ul><li>為 Kyverno 定義一個正式的威脅模型。</li><li>對代碼進行手動安全漏洞審計。</li><li>根據威脅模型評估 Kyverno 的模糊測試套件。</li><li>針對 SLSA 評估 Kyverno 的供應鏈風險。</li></ul><blockquote><p>本文由博客一文多發平台 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenwrite.cn%3Ffrom%3Darticle_bottom" target="_blank">OpenWrite</a> 發佈！</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 11:05:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4197945/blog/10320847</guid>
            <link>https://my.oschina.net/u/4197945/blog/10320847</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[.NET 8 極致性能優化 - Reflection（反射）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1><span><strong><span style="color:#3c70c6">前言</span></strong></span></h1><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>反射一直是性能的瓶頸，所以無論哪個.NET 版本反射的優化必然少不了。主要是集中在兩個方面優化，分配和緩存。.NET8 自然也不例外。本篇看下。</span></p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">原文:<u><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5NDYwNjU4MA%3D%3D%26mid%3D2247485722%26idx%3D1%26sn%3Da126d8687afbc4b980533ec7fd239026%26chksm%3Dc01c4481f76bcd97a92c031859b0327a4460f7b4c73dad11cb0f45fa9c283954e5c95f442eec%26token%3D322944710%26lang%3Dzh_CN%23rd" rel="nofollow" target="_blank">.NET8 極致性能優化 Reflection</a></strong></u></p><span id="OSC_h1_2"></span><h1><span><strong><span style="color:#3c70c6">概述</span></strong></span></h1><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>比如針對 GetCustomAttributes 通過反射獲取屬性的優化，以下例子</span></p><pre><code><span><em>// dotnet run -c Release -f net7.0 --filter "*" --runtimes net7.0 net8.0</em></span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">object</span>[] <span style="color:#dd1144">GetCustomAttributes</span>()</span> =&gt; <span style="color:#ca7d37">typeof</span>(C).GetCustomAttributes(<span style="color:#ca7d37">typeof</span>(MyAttribute), inherit: <span style="color:#0e9ce5">true</span>);</span></code><code><span>    [<span style="color:#afafaf">My(Value1 = 1, Value2 = 2)</span>]</span></code><code><span><span style="color:#ca7d37">class</span><span style="color:#dd1144">C</span> { }</span></code><code><span>    [<span style="color:#afafaf">AttributeUsage(AttributeTargets.All)</span>]</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">MyAttribute</span> : <span style="color:#dd1144">Attribute</span></span></code><code><span>    {</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">int</span> Value1 { <span style="color:#ca7d37">get</span>; <span style="color:#ca7d37">set</span>; }</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">int</span> Value2 { <span style="color:#ca7d37">get</span>; <span style="color:#ca7d37">set</span>; }</span></code><code><span>    }</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET7 和.NET8 明顯的差異，它主要是優化了</span><span>避免分配一個 object[1]數組來設置屬性的值</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>運行時</th><th>平均值</th><th>比率</th><th>分配</th><th>分配比率</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetCustomAttributes</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1,287.1 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">296 B</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetCustomAttributes</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">994.0 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.77</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">232 B</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.78</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">其它的比如減少反射堆棧中的分配，比如通過更自由的 spans。改進了 Type 上的泛型處理，從而提升各種與泛型相關的成員性能，比如 GetGenericTypeDefinition，它的結果現在被緩存在了 Type 對象上​​​​​​​</p><pre><code><span><em>// dotnet run -c Release -f net7.0 --filter "*" --runtimes net7.0 net8.0</em></span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span> Type _type = <span style="color:#ca7d37">typeof</span>(List&lt;<span style="color:#ca7d37">int</span>&gt;);</span></code><code><span>&nbsp;&nbsp;&nbsp;&nbsp;<span><span style="color:#ca7d37">public</span>&nbsp;Type&nbsp;<span style="color:#dd1144">GetGenericTypeDefinition</span>()</span>&nbsp;=&gt;&nbsp;_type.GetGenericTypeDefinition();</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET7 和.NET8 如下</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>運行時</th><th>平均值</th><th>比</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetGenericTypeDefinition</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">47.426 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">GetGenericTypeDefinition</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">3.289 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.07</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span><span style="background-color:#ffffff">這</span>些<span style="background-color:#ffffff">都是細枝末節，影響反射性能最大的一塊是 MethodBase.Invoke。</span><span style="background-color:#ffffff">當在編譯的時候，知道方法的簽名並且通過反射來調用方法。</span><span style="background-color:#ffffff">就可以通過使用</span></span><span style="background-color:#ffffff">CreateDelegate</span><span>來獲取和緩存該方法的委託，然後通過該委託執行所有的調用。從而實現性</span><span>能最佳化，但是如果在編譯的時候你不知道</span><span>方法的簽名，則需要依賴動態的方法。比如 MethodBase.Invoke，這個方法降低性能並且更耗</span><span>時。一些比較瞭解.NET 開</span><span>發的人員會用 emit 避免這種開銷。.NET7 裏面採用這種方式。.NET8 裏面，為許多這樣的情況進行了改進，以前，emitter 總是生成可以容納 ref/out 參數的代碼，但許多方法不提供這樣的參數，當不需要考慮這些因素時，生成的代碼可以更高效。</span>​​​​​​​</p><pre><code><span><em>// If you have .NET 6 installed, you can update the csproj to include a net6.0 in the target frameworks, and then run:</em></span></code><code><span><em>//     dotnet run -c Release -f net6.0 --filter "*" --runtimes net6.0 net7.0 net8.0</em></span></code><code><span><em>// Otherwise, you can run:</em></span></code><code><span><em>//     dotnet run -c Release -f net7.0 --filter "*" --runtimes net7.0 net8.0</em></span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Attributes;</span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Running;</span></code><code><span><span style="color:#ca7d37">using</span> System.Reflection;</span></code><code><span>BenchmarkSwitcher.FromAssembly(<span style="color:#ca7d37">typeof</span>(Tests).Assembly).Run(args);</span></code><code><span>[<span style="color:#afafaf">HideColumns(<span>"Error"</span>, <span>"StdDev"</span>, <span>"Median"</span>, <span>"RatioSD"</span>)</span>]</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span style="color:#ca7d37">private</span> MethodInfo _method0, _method1, _method2, _method3;</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args1 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">1</span> };</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args2 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">2</span>, <span style="color:#0e9ce5">3</span> };</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args3 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">4</span>, <span style="color:#0e9ce5">5</span>, <span style="color:#0e9ce5">6</span> };</span></code><code><span>    [<span style="color:#afafaf">GlobalSetup</span>]</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Setup</span>()</span></span></code><code><span>    {</span></code><code><span>        _method0 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod0"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method1 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod1"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method2 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod2"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method3 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod3"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>    }</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method0</span>()</span> =&gt; _method0.Invoke(<span style="color:#0e9ce5">null</span>, <span style="color:#0e9ce5">null</span>);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method1</span>()</span> =&gt; _method1.Invoke(<span style="color:#0e9ce5">null</span>, _args1);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method2</span>()</span> =&gt; _method2.Invoke(<span style="color:#0e9ce5">null</span>, _args2);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>] <span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Method3</span>()</span> =&gt; _method3.Invoke(<span style="color:#0e9ce5">null</span>, _args3);</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod0</span>()</span> { }</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod1</span>(<span><span style="color:#ca7d37">int</span> arg1</span>)</span> { }</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod2</span>(<span><span style="color:#ca7d37">int</span> arg1, <span style="color:#ca7d37">int</span> arg2</span>)</span> { }</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod3</span>(<span><span style="color:#ca7d37">int</span> arg1, <span style="color:#ca7d37">int</span> arg2, <span style="color:#ca7d37">int</span> arg3</span>)</span> { }</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET6 以及 7 和 8 的情況分別如下：</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>運行時</th><th>平均值</th><th>比率</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">91.457 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">7.205 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.08</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">5.719 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.06</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method1</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">132.832 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method1</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">26.151 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.20</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method1</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">21.602 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.16</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method2</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">172.224 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method2</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">37.937 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.22</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method2</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">26.951 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.16</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method3</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 6.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">211.247 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method3</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 7.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">42.988 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.20</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">Method3</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">.NET 8.0</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">34.112 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.16</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">這裏有一些問題，每次調用都會涉及到一些性能開銷，每次調用都會重複。如果我們可以提取這些重複性的工作，對它們進行緩存。就可以實現更好的性能。.NET8 裏面通過 MethodInvoker 和 ConstructorInvoker 類型中實現了這些功能。這些並沒有包含所有 MethodBase.Invoke 處理的不常見錯誤（如特別識別和處理 Type.Missing），但對於其他所有情況，它為優化在構建時未知簽名的方法的重複調用提供了一個很好的解決方案。​​​​​​​</p><pre><code><span><em>// dotnet run -c Release -f net8.0 --filter "*"</em></span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Attributes;</span></code><code><span><span style="color:#ca7d37">using</span> BenchmarkDotNet.Running;</span></code><code><span><span style="color:#ca7d37">using</span> System.Reflection;</span></code><code><span>BenchmarkSwitcher.FromAssembly(<span style="color:#ca7d37">typeof</span>(Tests).Assembly).Run(args);</span></code><code><span>[<span style="color:#afafaf">HideColumns(<span>"Error"</span>, <span>"StdDev"</span>, <span>"Median"</span>, <span>"RatioSD"</span>)</span>]</span></code><code><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">class</span><span style="color:#dd1144">Tests</span></span></code><code><span>{</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span> _arg0 = <span style="color:#0e9ce5">4</span>, _arg1 = <span style="color:#0e9ce5">5</span>, _arg2 = <span style="color:#0e9ce5">6</span>;</span></code><code><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">readonly</span><span style="color:#ca7d37">object</span>[] _args3 = <span style="color:#ca7d37">new</span><span style="color:#ca7d37">object</span>[] { <span style="color:#0e9ce5">4</span>, <span style="color:#0e9ce5">5</span>, <span style="color:#0e9ce5">6</span> };</span></code><code><span><span style="color:#ca7d37">private</span> MethodInfo _method3;</span></code><code><span><span style="color:#ca7d37">private</span> MethodInvoker _method3Invoker;</span></code><code><span>    [<span style="color:#afafaf">GlobalSetup</span>]</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">Setup</span>()</span></span></code><code><span>    {</span></code><code><span>        _method3 = <span style="color:#ca7d37">typeof</span>(Tests).GetMethod(<span style="color:#dd1144">"MyMethod3"</span>, BindingFlags.NonPublic | BindingFlags.Static);</span></code><code><span>        _method3Invoker = MethodInvoker.Create(_method3);</span></code><code><span>    }</span></code><code><span>    [<span style="color:#afafaf">Benchmark(Baseline = true)</span>] </span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MethodBaseInvoke</span>()</span> =&gt; _method3.Invoke(<span style="color:#0e9ce5">null</span>, _args3);</span></code><code><span>    [<span style="color:#afafaf">Benchmark</span>]</span></code><code><span><span><span style="color:#ca7d37">public</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MethodInvokerInvoke</span>()</span> =&gt; _method3Invoker.Invoke(<span style="color:#0e9ce5">null</span>, _arg0, _arg1, _arg2);</span></code><code><span><span><span style="color:#ca7d37">private</span><span style="color:#ca7d37">static</span><span style="color:#ca7d37">void</span><span style="color:#dd1144">MyMethod3</span>(<span><span style="color:#ca7d37">int</span> arg1, <span style="color:#ca7d37">int</span> arg2, <span style="color:#ca7d37">int</span> arg3</span>)</span> { }</span></code><code><span>}</span></code></pre><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>.NET8 的情況如下</span></p><table cellspacing="0" style="-webkit-text-stroke-width:0px; background-color:#ffffff; border-collapse:collapse; box-sizing:border-box !important; color:rgba(0, 0, 0, 0.9); display:table; font-family:system-ui,-apple-system,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif; font-size:17px; font-style:normal; font-variant-caps:normal; font-variant-ligatures:normal; font-weight:400; letter-spacing:0.544px; margin:0px 0px 10px; max-width:100%; orphans:2; outline:0px; overflow-wrap:break-word !important; padding:0px; text-align:justify; text-decoration-color:initial; text-decoration-style:initial; text-decoration-thickness:initial; text-transform:none; white-space:normal; widows:2; width:676.989px; word-spacing:0px"><tbody><tr><th>方法</th><th>平均值</th><th>比率</th></tr></tbody><tbody><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">MethodBaseInvoke</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">32.42 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">1.00</td></tr><tr><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">MethodInvokerInvoke</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">11.47 ns</td><td style="border-collapse:collapse; border-color:#c0c0c0; border-style:solid; border-width:1px">0.35</td></tr></tbody></table><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span>這些類型被 Microsoft.Extensions.DependencyInjection.Abstractions 中的 ActivatorUtilities.CreateFactory 方法使用，以進一步提高 DI 服務構建性能。通過添加額外的緩存層進一步改進，進一步避免每次構建時的反射。</span></p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">&nbsp;</p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left">作者:jianghupt</p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><strong>歡迎關注公眾號 (jianghupt），文章首發地。</strong></p><p style="color:#4a4a4a; margin-left:0; margin-right:0; text-align:left"><span><img alt="" height="430" src="https://oscimg.oschina.net/oscnet/up-3243ba74c89867eabc4277de83aa83aa7bb.png" width="430" referrerpolicy="no-referrer"></span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:50:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/5407571/blog/10320411</guid>
            <link>https://my.oschina.net/u/5407571/blog/10320411</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[FastUI —— 更快地構建更好的 UI]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 是一種構建由聲明式 Python 代碼來構建 Web 應用程序用戶界面的新方法。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>這意味着：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><strong>如果你是一名 Python 開發人員</strong>，可以使用 React 構建響應式 Web 應用程序，而無需編寫任何 JavaScript 代碼，也無需接觸<code>npm</code>。</li><li><strong>如果你是前端開發人員</strong>，可以專注於構建真正可重用的神奇組件，無需為每個視圖複製粘貼組件。</li><li><strong>對於每個人來説&nbsp;</strong>—— 真正的關注點分離，後端定義了整個應用程序；而前端可以自由地僅實現用戶界面</li></ul><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 的核心是一組匹配的&nbsp;<a href="https://docs.pydantic.dev/">Pydantic</a>&nbsp;模型和 TypeScript </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>interfaces<span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>，允許你定義用戶界面。其在構建時由 TypeScript 和 Pyright/mypy 進行驗證，並在運行時由 Pydantic 進行驗證。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>FastUI 由 4 部分組成：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><a href="https://pypi.python.org/pypi/fastui"><code>fastui</code>PyPI 包</a>— UI 組件的 Pydantic 模型和一些實用程序。雖然它與<a href="https://fastapi.tiangolo.com/">FastAPI</a>配合良好，但它不依賴於 FastAPI，並且其中大部分可以與任何 Python Web 框架一起使用。</li><li><a href="https://www.npmjs.com/package/@pydantic/fastui"><code>@pydantic/fastui</code>npm 包</a>— 一個 React TypeScript 包，讓你在實現自己的組件時重用 FastUI 的機制和類型</li><li><a href="https://www.npmjs.com/package/@pydantic/fastui-bootstrap"><code>@pydantic/fastui-bootstrap</code>npm 包</a> — 使用&nbsp;<a href="https://getbootstrap.com/">Bootstrap</a>&nbsp;實現/定製所有 FastUI 組件</li><li><a href="https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt"><code>@pydantic/fastui-prebuilt</code>npm 包</a>（在&nbsp;<a href="https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt">jsdelivr.com CDN</a>&nbsp;上提供）提供了 FastUI React 應用程序的預構建版本，因此你無需安裝任何 npm 包或自行構建任何內容即可使用它。Python 包提供了一個簡單的 HTML 頁面來服務此應用程序。</li></ul><p style="text-align:start"><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>以下是一個簡單但完整的 FastAPI 應用程序，它使用 FastUI 來顯示一些用戶配置文件：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><pre><code>from datetime import date

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastui import FastUI, AnyComponent, prebuilt_html, components as c
from fastui.components.display import DisplayMode, DisplayLookup
from fastui.events import GoToEvent, BackEvent
from pydantic import BaseModel, Field

app = FastAPI()


class User(BaseModel):
    id: int
    name: str
    dob: date = Field(title='Date of Birth')


# define some users
users = [
    User(id=1, name='John', dob=date(1990, 1, 1)),
    User(id=2, name='Jack', dob=date(1991, 1, 1)),
    User(id=3, name='Jill', dob=date(1992, 1, 1)),
    User(id=4, name='Jane', dob=date(1993, 1, 1)),
]


@app.get("/api/", response_model=FastUI, response_model_exclude_none=True)
def users_table() -&gt; list[AnyComponent]:
    """
    Show a table of four users, `/api` is the endpoint the frontend will connect to
    when a user fixes `/` to fetch components to render.
    """
    return [
        c.Page(  # Page provides a basic container for components
            components=[
                c.Heading(text='Users', level=2),  # renders `&lt;h2&gt;Users&lt;/h2&gt;`
                c.Table[User](  # c.Table is a generic component parameterized with the model used for rows
                    data=users,
                    # define two columns for the table
                    columns=[
                        # the first is the users, name rendered as a link to their profile
                        DisplayLookup(field='name', on_click=GoToEvent(url='/user/{id}/')),
                        # the second is the date of birth, rendered as a date
                        DisplayLookup(field='dob', mode=DisplayMode.date),
                    ],
                ),
            ]
        ),
    ]


@app.get("/api/user/{user_id}/", response_model=FastUI, response_model_exclude_none=True)
def user_profile(user_id: int) -&gt; list[AnyComponent]:
    """
    User profile page, the frontend will fetch this when the user visits `/user/{id}/`.
    """
    try:
        user = next(u for u in users if u.id == user_id)
    except StopIteration:
        raise HTTPException(status_code=404, detail="User not found")
    return [
        c.Page(
            components=[
                c.Heading(text=user.name, level=2),
                c.Link(components=[c.Text(text='Back')], on_click=BackEvent()),
                c.Details(data=user),
            ]
        ),
    ]


@app.get('/{path:path}')
async def html_landing() -&gt; HTMLResponse:
    """Simple HTML page which serves the React app, comes last as it matches all paths."""
    return HTMLResponse(prebuilt_html(title='FastUI Demo'))</code></pre></div>
                                                                ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:29:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/fastui</guid>
            <link>https://www.oschina.net/p/fastui</link>
        </item>
        <item>
            <title>
                <![CDATA[🎁有獎問答 | 聊聊 NGINX 向雲原生演進那點兒事]]>
            </title>
            <description>
                <![CDATA[<h1 class="header article-title"><a href="https://www.oschina.net/question/4700705_2331501">高手問答第 311 期 —— 聊聊 NGINX 向雲原生演進那點兒事</a><div class="ui red label horizontal" data-tooltip="置頂">頂</div></h1><div class="extra ui horizontal list meta-wrap"><div class="item"><a href="https://my.oschina.net/u/4700705" class="__user"><span>小白兔愛吃大灰狼</span></a> 發佈於，昨天 11:35
                    </div><div class="item">閲讀 408</div><div class="item collect-btn " data-id="2331501" data-user-id="4700705" data-obj-type="2" data-max="99" data-tag-required="" data-current-user-id="" data-recommend-tags=""><i class="star outline icon"></i> 收藏 <span data-collect-count="" data-id="2331501" data-obj-type="2">1</span></div><div class="item comment-count"><a href="https://www.oschina.net/question/4700705_2331501#comments" class="normal"><i class="comment outline icon"></i> 答案 <span data-article-reply-count="">5</span></a></div></div><div class="tags"><a class="ui horizontal label" href="https://www.oschina.net/question/topic/masteronline" target="_blank"><img src="https://static.oschina.net/uploads/logo/masteronline_9WTeU.png" referrerpolicy="no-referrer">高手問答</a></div><div class="content" id="articleContent"><p><span><span>據 Gartner 預測，到 2025 年，雲原生架構將成為超過 95% 的新數字計劃基礎，高於 2021 年的不到 40%，雲原生架構市場佔有率不斷提高。而如今，全球半數以上（55%） 的網站都基於 NGINX 運行，差不多相同比例 (53.7%) 的中國網站在 NGINX 開源版上運行。而 NGINX 存在難於動態配置、管理功能影響業務等問題，為瞭解決這些問題，OpenNJet 由此誕生。</span></span></p><p><span><span>OpenNJet 基於 NGINX1.19 基礎 fork 並獨立演進，具有高性能、穩定、易擴展的特點，通過數據面與控制面的隔離，能夠在不重啓進程的情況下基於動態配置能力進行配置的實時更新。最近還推出了 OpenNJet K8s Ingress Controller 1.0，基於 OpenNJet 的動態特性、高性能實現，彌補了 NGINX 在雲原生場景中不足，而且提供了豐富的流量管理功能，如動態 location、host/path 路由、負載均衡、動態 upstream、金絲雀發佈、SNI 等。</span></span></p><p><strong><span><span>OSCHINA 本期高手問答（12 月 13 日 - 12 月 19 日）我們請來了嘉賓<a href="https://my.oschina.net/u/6606114" rel="nofollow">單雷老師</a>和大家一起聊聊 NGINX 向雲原生演進那點兒事。</span></span></strong></p><p><strong><span><span>可討論的問題包括但不限於</span></span></strong><strong><span><span>：</span></span></strong></p><ul><li><span><span style="background-color:white"><span>OpenNJet 和 NGINX 是什麼關係？</span></span></span></li><li><span><span style="background-color:white"><span>什麼是雲原生應用引擎？OpenNJet 的有哪些優勢</span></span></span></li><li><span><span style="background-color:white"><span>我們如何解決數據面控制面隔離、國密、動態配置等問題？</span></span></span></li><li><span><span style="background-color:white"><span>讀 NGINX/OpenNJet 源碼的建議</span></span></span></li><li><span><span style="background-color:white"><span>如何上手開發一個開源項目？</span></span></span></li></ul><p><span><span style="background-color:white"><span>其他關於 NGINX、OpenNJet 的更多內容，也歡迎積極提問。</span></span></span></p><h2><span><span style="background-color:white"><span><strong>嘉賓介紹</strong></span></span></span></h2><p><img alt="" height="534" src="https://oscimg.oschina.net/oscnet/up-774dc1b75df829000896339c602574ff319.jpg" width="400" referrerpolicy="no-referrer"></p><p><span><span><strong><span><span style="color:#7030a0">通明智雲產品總監，單雷</span></span></strong></span></span></p><p><span><span>20 年的 IT 行業經驗，精通雲原生以及高性能應用引擎技術。曾在亞信科技歷任研發主管、首席架構師等職務，並主導多個雲原生、高性能應用網關項目的設計開發工作，現任公司應用引擎產品總監。</span></span></p><hr><p><span><span style="background-color:white"><span><span>🎁</span> 為了鼓勵踴躍提問，下一代雲原生應用引擎 OpenNJet 開源社區會在問答結束後從提問者中抽取 5 名幸運會員，贈予精美棉馬甲一件。</span></span></span></p><p><img alt="" height="436" src="https://oscimg.oschina.net/oscnet/up-6f9dfb1df3b4d3c9f22f9a02a21c1be62d5.jpg" width="400" referrerpolicy="no-referrer"></p><blockquote><p><span><span>OpenNJet&nbsp;應用引擎是基於 NGINX 的面向互聯網和<strong>雲原生</strong>應用提供的運行時組態服務程序，作為底層引擎，OpenNJet 實現了 NGINX 雲原生功能增強、安全加固和代碼重構，利用<strong>動態加載機制</strong>可以實現不同的產品形態，如 Web 服務器、流媒體服務器、負載均衡、代理 (Proxy)、應用中間件、API 網關、消息隊列等產品形態等等。OpenNJet 在雲原生架構中作為數據平面，除了提供南北向通信網關的功能以外，還提供了服務網格中東西向通信能力。在原有功能基礎上增加了透明流量劫持、熔斷、遙測與故障注入等新功能特性。</span></span></p><p><span><span>Gitee：<a href="https://gitee.com/njet-rd/njet" rel="nofollow"><span><span>https://gitee.com/njet-rd/njet</span></span></a></span></span></p><p><span><span>官網：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnjet.org.cn%2F" rel="nofollow" target="_blank">https://njet.org.cn/</a></span></span></p></blockquote><p><span style="background-color:#ffffff; color:#27ae60">OSChina 高手問答一貫的風格，不歡迎任何與主題無關的討論和噴子。</span></p><p>下面歡迎大家就 「<span><span>NGINX 向雲原生演進</span></span>」<span><span>&nbsp;</span>相關</span>問題向<span>&nbsp;<a href="https://my.oschina.net/u/6606114" rel="nofollow">單雷老師</a></span><a href="https://my.oschina.net/klblog" rel="nofollow"><strong><span style="color:#000000">&nbsp;</span></strong></a>提問，直接回帖提問既可。</p></div><div class="poll-wrap"></div><div class="additional-remarks"></div><div class="ui basic center aligned segment action"><div class="ui big buttons"><a class="ui basic button collect-btn hover" data-id="2331501" data-user-id="4700705" data-obj-type="2" data-max="99" data-tag-required="" data-current-user-id="" data-recommend-tags=""><i class="star outline icon"></i>收藏 (<span data-collect-count="" data-id="2331501" data-obj-type="2">1</span>)</a><div class="ui basic dropdown share button osc-share dropdown-share" data-tag="share-question"><i class="share icon"></i><span>分享</span><div class="menu"><a class="item" data-platform="weibo" data-value="weibo"><i class="weibo icon"></i>微博</a><a class="item" data-platform="qq" data-value="qq"><i class="qq icon"></i>QQ</a><a class="item" data-platform="wechat" data-value="wechat"><i class="weixin icon"></i>微信</a></div></div></div><div class="ui basic segment"><a class="ban" ban-report="" data-id="2331501" data-obj-type="2" data-url="https://www.oschina.net/question/4700705_2331501"><i class="flag red icon"></i>舉報</a></div></div>
            ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:28:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/question/4700705_2331501</guid>
            <link>https://www.oschina.net/question/4700705_2331501</link>
        </item>
        <item>
            <title>
                <![CDATA[新技術 LINT 可強制 LLM 回答有毒問題]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">美國普渡大學（Purdue University）的研究人員發佈了，一篇名為《<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2312.04782" target="_blank"><span style="background-color:#ffffff">Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs</span></a>》的論文。描述了他們通過利用大模型廠商傾向於公開與提示響應相關的概率數據的特點，設計出了一種可以打破現有 LLM 規則，使其給出「有害」回答的新穎方法。</span></p><p><span style="color:#000000">研究人員使用了一種名為 LINT (LLM interrogation) 的技術，與繞過安全提示的一般越獄方式不同的是，這種技術更具強制性。它涉及到對概率值（logits）或軟標籤（soft labels）的理解 —— 從統計學上講，其可以有效區分安全響應與有害響應。</span></p><blockquote><p><span style="color:#000000">「在我們的研究中，我們揭示了當不良行為者可以訪問模型的輸出 logits 時，LLM alignment 面臨的的一種新威脅，這是開源 LLM 和許多商業 LLM API（如某些 GPT 模型）的共同特徵。它並不依賴於製作特定的提示。相反，它利用了這樣一個事實：即使 LLM 拒絕了有毒請求，有害響應也往往隱藏在輸出 logits 的深處。通過在自動迴歸生成過程中在幾個關鍵輸出位置強行選擇排名較低的輸出&nbsp;tokens，我們可以迫使模型揭示這些隱藏的響應。我們將這一過程稱為模型審訊（model interrogation）。」</span></p></blockquote><p><span style="color:#000000">具體<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.theregister.com%2F2023%2F12%2F11%2Fchatbot_models_harmful_content%2F" target="_blank">表現為</a>，LINT 使用 LLM-based classifier 向 LLM 提出一個有害的問題：比如"説明如何將槍改裝成全自動的"。然後，對響應中的前九個 tokens（如 "It's"、"It"、"We"和 "I"）進行排序，並用這些詞組創建新句子，這樣該 LLM 就會不斷生成回覆。</span></p><p><span style="color:#000000"><img alt="" height="263" src="https://oscimg.oschina.net/oscnet/up-a4cca4832d0d4ac8918b03eacf880f3455b.png" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">研究人員在創建 LINT 原型時，在一個包含 50 個有毒問題的數據集上詢問了 7 個開源 LLM 和 3 個商業 LLM。他們稱：「當模型只被審訊一次時，ASR（攻擊成功率）達到 92%，當被詢問五次時，ASR 可以達到達到 98%。」</span></p><p><span style="color:#000000">這種方法與越獄方法不同，但性能要<span style="background-color:#ffffff">遠遠優於目前最先進的兩種越獄技術：GCG 和 GPTFuzzer。</span>相比之下越獄方法的 ASR 僅為 62%，且運行時間要長&nbsp;10 到 20 倍。「通過我們的方法揭露的有害內容更加相關、完整、清晰。此外，它可以補充越獄策略，從而進一步提高攻擊性能。」</span></p><p><span style="color:#000000">更重要的是，這種技術甚至適用於根據特定任務（如代碼生成）的基礎模型定製的 LLM。研究人員還聲稱，這種技術可以用來損害隱私和安全，迫使模型公開電子郵件地址和猜測弱密碼。</span></p><p><span style="color:#000000">因此，研究人員警告稱，AI&nbsp;界在考慮是否開源 LLM 時應謹慎；並建議最好的解決方案是確保有毒內容被清除，而不是將其隱藏起來。</span></p><p><span style="color:#000000">更多詳情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Farxiv.org%2Fabs%2F2312.04782" target="_blank">查看完整論文</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 09:24:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270686/lint-llm-harmful-content</guid>
            <link>https://www.oschina.net/news/270686/lint-llm-harmful-content</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Apache StreamPark 2.1.2 穩定版正式發佈]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><img height="460" src="https://oscimg.oschina.net/oscnet/up-223b657c0b3fdd8242108df64be06aa7cf7.png" width="1080" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span><strong><span style="color:#333333">近日 Apache StreamPark<span>(Incubating)&nbsp;</span>社區正式發佈了 StreamPark 2.1.2 版本</span></strong><span style="color:#333333">，</span></span><span>在 2.1.2 版本中，支持了最新的 Flink 1.18，Flink Jar 類型的作業支持指定依賴，</span><span>修復了</span><span style="color:#333333">諸多 Bug 和大量改進</span><span>，穩定性和可用性進一步提升，建議所有用戶升級到這個版本</span><span>。</span></p><p><span style="color:#646464"><strong><span>Github:&nbsp;</span></strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark" target="_blank">https://github.com/apache/streampark</a></p><p style="margin-left:0; margin-right:0; text-align:left"><span style="color:#646464"><strong>官&nbsp; &nbsp; &nbsp;網:&nbsp;</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstreampark.apache.org%2Fdownload" target="_blank">https://streampark.apache.org/download</a></p><p><span style="color:#444444">歡迎&nbsp;</span><strong><span style="color:#444444">使用、關注、star、fork</span></strong><span style="color:#444444">&nbsp;</span></p><h1><span>新特性解讀</span></h1><h4><span style="color:#0052ff"><span style="background-color:#0053cd"><strong>&nbsp;</strong></span><span>&nbsp; </span></span><strong>更好的支持 JAR 類型作業</strong></h4><p><span><span>在&nbsp;StreamPark 中將 Flink 作業按照開發模式分為&nbsp;Custom Code&nbsp;和 Flink SQL<span>&nbsp;</span><span>兩種類型</span>，Custom Code 是需要用戶編寫代碼編譯成 JAR 類型的 Flink 作業，在以前的版本中該類型的作業不支持在 StreamPark 平台側指定作業依賴，要求用戶自己解決作業需要的依賴，通常做法是需要將這些依賴打包到項目裏，生成一個 FatJar (uber-jar)。社區收到很多用戶的反饋，大家普遍希望 StreamPark 平台側針對 JAR 類型的作業能像 Flink SQL 作業一樣，可以自由的指定作業的依賴。</span></span></p><p><span><span>同時，我們也看到 Apache Doris, Apache Paimon 等社區都開發了基於 Flink CDC 一鍵集成數據的組件&nbsp;</span><span style="color:#888888">(doris-flink-connector 和 paimon-action)</span><span>，該組件都提供了作業遷移的入口，但作業運行時依賴需要用戶手動添加。</span></span></p><p><span><span>鑑於這些原因，在 StreamPark 2.1.2 裏，特別針對 JAR 類型的作業支持了指定依賴的能力，使得用戶部署這類作業更加簡單。</span></span><span>以下是兩個示例，演示瞭如何利用該特性，來快速部署 Doris 和 Paimon 數據遷移類型的作業：</span></p><p><iframe frameborder="0" height="370" scrolling="no" src="https://player.bilibili.com/player.html?aid=280306310&amp;bvid=BV17c411d7Jy&amp;cid=1317452428&amp;p=1&amp;autoplay=0" style="box-sizing: inherit; color: rgb(51, 51, 51); font-family: -apple-system, BlinkMacSystemFont, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Segoe UI&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;" width="750" referrerpolicy="no-referrer"></iframe></p><p><span style="background-color:#000000; color:#ffffff">StreamPark 讓 Doris 數據集成更簡單&nbsp;</span><br> &nbsp;</p><p><iframe frameborder="0" height="370" scrolling="no" src="https://player.bilibili.com/player.html?aid=323428873&amp;bvid=BV1Sw411W7QK&amp;cid=1333574131&amp;p=1&amp;autoplay=0" style="box-sizing: inherit; color: rgb(51, 51, 51); font-family: -apple-system, BlinkMacSystemFont, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Segoe UI&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;" width="760" referrerpolicy="no-referrer"></iframe></p><p><span style="background-color:#000000; color:#ffffff">&nbsp;StreamPark 讓 Paimon 數據集成更簡單&nbsp;</span></p><h4><span style="color:#0052ff"><span style="background-color:#0053cd"><strong>&nbsp;</strong></span><span>&nbsp; </span></span><strong>支持 Flink 1.18</strong></h4><p style="margin-left:0; margin-right:0"><span>作為流處理開發管理框架，StreamPark 在對 Apache Flink 的支持上，一如既往的走在前列。得益於 StreamPark 良好的架構設計，使得支持一個新<span>版本</span>的 Flink 非常容易，因此我們率先支持了<span>&nbsp;</span></span><span style="color:#ff4c00"><span>Flink 1.18</span><span><span>&nbsp;</span><span style="background-color:#ffffff">[1]</span></span></span><span>。在使用上非常的簡單，用戶只需要添加一個 Flink 1.18 的環境即可，作業可以自由的選擇 Flink 版本<span>。</span></span></p><p style="margin-left:0; margin-right:0"><span><span>並且本次適配了更多發行版 Flink，如 CDH 版本的 Flink, 華為雲，騰訊雲 Flink 等。</span></span></p><div style="margin-left:0px; margin-right:0px; text-align:left"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px"><div style="margin-left:0px; margin-right:0px">
          &nbsp;
         </div></div></div></div></div></div></div></div></div></div><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><iframe frameborder="0" height="370" scrolling="no" src="https://player.bilibili.com/player.html?aid=264867610&amp;bvid=BV16Y41117kt&amp;cid=953529774&amp;p=1&amp;autoplay=0" style="box-sizing: inherit;" width="750" referrerpolicy="no-referrer"></iframe></p><p><span style="background-color:#000000; color:#ffffff">&nbsp;支持 Flink 多版本&nbsp;</span></p><p><span style="color:rgba(0, 0, 0, 0.9)"><span style="background-color:#0053cd"><strong><span style="color:#0053cd">&nbsp;</span></strong></span><span>&nbsp;<span>&nbsp;</span></span></span><span style="color:#0053dc"><strong>其他改進和更新</strong></span></p><ul><li><p><span style="color:#444444">修復作業狀態重新映射不生效的 Bug</span>&nbsp;<u>#2822</u></p></li><li><p>改進 Flink 版本的校驗邏輯，適配更多的&nbsp;Flink 版本&nbsp;<u>#2832</u></p></li><li><p>修復作業 「取消狀態」 下可能存在的無法發送報警信息的 Bug&nbsp;<u>#3157</u></p></li><li><p style="margin-left:0; margin-right:0"><span>修復 Ingress 訪問 Flink UI 可能存在的 404 Bug&nbsp;<u>#3302</u></span></p></li><li><p><span><span style="color:#444444">修復團隊為空，導致查詢錯誤的 Bug&nbsp;</span><u>#3365</u></span></p></li><li><p>修復作業參數解析，特定字符解析錯誤導致作業失敗的&nbsp;Bug</p></li><li><p><span style="color:#0052ff"><span style="color:#444444">修復項目編譯時 maven-wrapper 文件損壞導致失敗的 Bug</span></span></p></li><li><p><span style="color:#0052ff"><span style="color:#444444"><span style="color:#444444">Flink 作業的 Pom 信息支持&nbsp;exclusion，有效避免 JAR 衝突問題</span></span></span></p></li></ul><h1><span>Release Note</span></h1><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:left"><span><span style="color:#333333">本次 StreamPark 2.1.2 版本的，完整 Release Note 請訪問：</span><br><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstreampark.apache.org%2Fdownload%2Frelease-note%2F2.1.2" target="_blank">https://streampark.apache.org/download/release-note/2.1.2</a></span></p><h1><span style="color:#000000">感謝貢獻者</span></h1><p><span style="color:#333333"><span>StreamPark 開源社區的發展，離不開廣大用戶羣體的積極反饋和宣傳佈道，更離不開貢獻者們的無私貢獻<span>，</span></span><span style="color:#333333">感謝對此版本做出貢獻的每一位貢獻者<span style="background-color:#ffffff">。</span></span></span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span style="color:#444444"><span style="background-color:#ffffff">特別感謝本次的 Release Manager</span>&nbsp;</span><span style="color:#ff4c00">@龔中強<span style="background-color:#ffffff">[2]</span></span><span style="color:#444444">，<span style="background-color:#ffffff; color:#444444">中強</span></span><span><span style="background-color:#ffffff; color:#444444">在<span style="background-color:#ffffff; color:#444444">發版過程中<span style="background-color:#ffffff">積極的跟蹤問題和推進進度</span>，完美勝任了此次發版工作。</span>感謝<span style="background-color:#ffffff; color:#444444">中強</span>為社區做出的貢獻，也歡迎其他<span>&nbsp;</span><span style="background-color:#ffffff">PPMC member 和&nbsp;</span>Committer 在後續的發版中擔任 Release Manager，幫助社區更快捷、高質量地完成發版。</span></span></p><h1><span>什麼是 StreamPark</span></h1><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>StreamPark 是一個流處理應用程序開發管理框架。初衷是讓流處理更簡單，旨在輕鬆構建和管理流處理應用程序，提供使用 Apache Flink 和 Apache Spark 編寫流處理應用程序的開發框架。同時 StreamPark 提供了一個流處理應用管理平台，核心能力包括但不限於應用開發、調試、交互查詢、部署、運維、實時數倉等，最初開源時項目名稱叫 StreamX ，於 2022 年 8 月更名為 StreamPark，隨後通過投票正式成為 Apache 開源軟件基金會的孵化項目。目前已有騰訊<span>、</span>百度<span>、</span>聯通<span>、天翼雲<span>、</span></span>自如<span>、</span>馬蜂窩<span>、</span>長安汽車等數百家公司生產環境使用。</span></p><h1><span style="color:#000000">🫵&nbsp;加入我們</span></h1><p><span><span style="color:#333333"><span style="background-color:#ffffff">StreamPark 社區一直以來都以用心做好一個項目為原則</span><span style="background-color:#ffffff">，</span><span style="background-color:#ffffff">高度關注項目質量</span><span style="background-color:#ffffff">，努力</span><span style="background-color:#ffffff">建設發展社區。</span><span>加入 Apache 孵化器以來，</span><span style="background-color:#ffffff">認真學習和遵循「The Apache Way」，我們將秉承更加兼容幷包的心態，迎接更多的機遇與挑戰。誠摯</span><span>歡迎更多的貢獻者參與到社區建設中來，和我們一道攜手共建。</span></span></span></p><p><span><span style="color:#333333"><strong>💻 項目地址：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark" target="_blank">https://github.com/apache/streampark</a></span></p><p><span><span style="color:#333333"><strong>🧐 提交問題和建議：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fissues" target="_blank">https://github.com/apache/streampark/issues</a></span></p><p><span><span style="color:#333333"><strong>🥁 貢獻代碼：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fpulls" target="_blank">https://github.com/apache/streampark/pulls</a></span></p><p><span><span style="color:#333333"><strong><strong>📮&nbsp;</strong>Proposal：</strong></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FINCUBATOR%2FStreamPark%2BProposal" target="_blank">https://cwiki.apache.org/confluence/display/INCUBATOR/StreamPark+Proposal</a></span></p><p><span><span style="color:#333333"><strong>📧 訂閲社區開發郵件列表：</strong></span><span style="color:#0080ff">dev@streampark.apache.org</span><span style="color:#0080ff">&nbsp;</span><span style="color:#0080ff"><span style="color:#ff4c00">[3]</span>&nbsp;</span></span></p><p style="margin-left:0; margin-right:0; text-align:left"><span><span style="color:#444444"><strong>💁‍♀️</strong></span><span style="color:#444444"><strong>社區溝通：</strong></span></span></p><p><img height="500" src="https://oscimg.oschina.net/oscnet/up-07a7e385d033088436872afd0571e4c3482.png" width="900" referrerpolicy="no-referrer"></p><p><span style="color:#444444"><strong>參考資料</strong></span></p><p><span><em><span style="color:#666666">[1]&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnightlies.apache.org%2Fflink%2Fflink-docs-release-1.18%2Frelease-notes%2Fflink-1.18" target="_blank">https://nightlies.apache.org/flink/flink-docs-release-1.18/release-notes/flink-1.18</a></span></em></span></p><p><em><span style="color:#666666">[2]&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FGOODBOY008" target="_blank">https://github.com/GOODBOY008</a></span></em></p><p><span><em><span style="color:#666666">[3]&nbsp;<em><span>mailto:dev@streampark.apache.org</span></em></span></em></span><br> &nbsp;</p><p><span style="color:#333333">祝大家安裝、升級順利~~&nbsp;&nbsp;</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 06:27:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270652/apache-streampark-2-1-2-released</guid>
            <link>https://www.oschina.net/news/270652/apache-streampark-2-1-2-released</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[鎧俠向 Linux 基金會捐贈 Software-Enabled Flash SDK]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#121212">幾年前從東芝分離出來的存儲公司 Kioxia（</span>鎧俠<span style="background-color:#ffffff; color:#121212">）向 Linux 基金會捐贈了一個軟件開發工具包 (SDK)，用於建立 Software-Enabled Flash SDK。</span></p><p><span style="background-color:#ffffff; color:#121212">Linux 基金會發布<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.linuxfoundation.org%2Fpress%2Fsoftware-enabled-flash-announces-software-development-kit-sdk" target="_blank">公告稱</a>，「SEF SDK 的發佈是存儲技術領域的一個重要里程碑......SEF 項目對 KIOXIA 突破性地捐贈軟件定義閃存原生 SDK 表示熱烈歡迎，這將為開發人員提供前所未有的能力，使他們能夠為閃存存儲（flash storage）應用開發定製的獨特軟件。」</span></p><p><img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-67690b065c2207474d1a67124aa3ef403da.png" width="300" referrerpolicy="no-referrer">&nbsp; &nbsp;<img alt="" height="228" src="https://oscimg.oschina.net/oscnet/up-1056c78ed4258dcb84497a6e896204821c0.jpg" width="300" referrerpolicy="no-referrer"></p><p>該 SEF SDK 包括示例代碼和文檔，以充分利用 flash media control 的潛力；包括 WAF 減少、延遲控制、對 ZNS 和 FDP 或 Block 等多種協議的支持等。</p><p>SEF 項目旨在通過加強對驅動器的管理、增強工作負載隔離、加強延遲控制以及實現對閃存管理的更多&nbsp;host-control，在現代數據中心中開闢新的用途並最大限度地發揮基於閃存的存儲潛力。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270608/software-enabled-flash-sdk</guid>
            <link>https://www.oschina.net/news/270608/software-enabled-flash-sdk</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[芯瞳正式加入 openKylin，為社區貢獻高質量的國產 GPU 解決方案！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>近日，芯瞳半導體技術（山東）有限公司（以下簡稱「芯瞳」），簽署 openKylin 社區 CLA（Contributor License Agreement 貢獻者許可協議），正式加入 openKylin 開源社區。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="1079" src="https://oscimg.oschina.net/oscnet/up-4c9b13fca5452f4a217f1494d816e96a799.png" width="829" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>芯瞳（Sietium）成立於 2019 年，是一家自主設計研發 GPU 芯片及 GPU 解決方案的高科技公司，以行業先進的計算和圖形渲染平台為依託，用高質量的產品和服務為雲端、終端客戶提供可持續發展的國產 GPU 解決方案；為數字時代的創新與發展提供算力支撐，構建自由算力的文明世界。</span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:center"><img alt="" height="410" src="https://oscimg.oschina.net/oscnet/up-6914c94ad47861f5f685cb96e9bc21450f1.png" width="940" referrerpolicy="no-referrer"></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span><strong><span>加入 openKylin 社區後，芯瞳將參與維護社區 GPU SIG 和 Wayland SIG</span></strong><span>。<strong>憑藉其自研的 GPU 顯卡和深厚的行業經驗，優化 openKylin 環境中顯卡驅動的兼容性，確保與芯瞳顯卡的完美適配</strong>。</span></span></p><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>在 openKylin 平台上，芯瞳顯卡將展現其在圖形顯示、渲染、視頻編解碼和大規模計算等方面的優勢，以此提升 openKylin 的用戶體驗，並提供持續的 GPU 產品升級和技術支持，為用戶提供安全可靠的使用體驗。具體計劃如下：</span></p><ul><li><p style="margin-left:0; margin-right:0"><span>積極參與社區合作，緊密關注社區的發展動態，與社區成員攜手推動 openKylin 社區的生態及品牌建設，努力構建一個健康的生態環境，為開源生態的發展貢獻力量。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>尋求與社區的技術合作，通過聯合調試等方式，使 openKylin 的相關產品能更好地兼容並適應芯瞳的全新系列顯卡，從而提高產品的穩定性和性能。</span></p></li><li><p style="margin-left:0; margin-right:0"><span>在應用層面，芯瞳將持續優化軟件算法，提高系統效率，充分發掘 openKylin 在芯瞳顯卡平台上的性能潛力，從而提升整體性能，為用戶提供卓越的產品體驗。</span></p></li></ul><p style="color:rgba(0, 0, 0, 0.9); margin-left:0; margin-right:0; text-align:justify"><span>通過這一系列的舉措，芯瞳將與 openKylin 社區並肩前行，共同推動 openKylin 社區生態良好發展，為用戶帶來更多的創新和驚喜。同時，芯瞳期待與社區成員進行深入的交流和分享，以推動技術的進步和產業的協同發展，共同為中國開源生態的繁榮作出貢獻。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 03:09:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270607</guid>
            <link>https://www.oschina.net/news/270607</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Facebook 開源 StyleX —— 在 JavaScript 中寫 CSS]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Meta（原 Facebook）<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fstylexjs.com%2Fblog%2Fintroducing-stylex%2F" target="_blank">開源</a></u>了全新的 CSS-in-JS 庫 StyleX。</p><p><img src="https://oscimg.oschina.net/oscnet/up-30f683ba9535a9f16ce5e615736da0460cd.png" referrerpolicy="no-referrer"></p><blockquote><p><em>GitHub 地址：<strong><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Ffacebook%2Fstylex" target="_blank">https://github.com/facebook/stylex</a></u></strong></em></p></blockquote><p>官方介紹道，StyleX 是一個富有表現力、具有確定性、可靠且可擴展的樣式系統。它通過使用編譯時 (compile-time) 工具融合了靜態 CSS 的性能和可擴展性。</p><p>此外，StyleX 不僅僅是一個基於編譯器的 CSS-in-JS 庫，它經過精心設計，可以滿足大型應用程序、可複用組件庫和靜態類型代碼庫的要求。Meta 旗下多款產品如 Facebook、WhatsApp、Instagram、Workplace、Threads 等都在使用 StyleX 作為其 CSS 樣式解決方案。</p><p>StyleX 主要特性</p><ul><li><p><strong>快速</strong>：StyleX 在編譯時和運行時都具備高效的性能。Babel 轉換不會對構建過程產生顯著影響。在運行時，StyleX 避免了使用 JavaScript 插入樣式的開銷，並僅在必要時高效地組合類名字符串。生成的 CSS 經過優化，確保即使是大型網站的樣式也能被瀏覽器快速解析。</p></li><li><p><strong>可擴展</strong>：StyleX 旨在適應像 Meta 這樣的超大型代碼庫。通過原子構建和文件級緩存，Babel 插件能夠處理數萬個組件在編譯時的樣式處理。由於 StyleX 設計為封裝樣式，它允許在隔離環境中開發新組件，並期望一旦在其他組件中使用時能夠可預測地呈現。</p></li><li><p><strong>可預測性</strong>：StyleX 會自動管理 CSS 選擇器的特異性，以確保生成的規則之間不會發生衝突。它為開發人員提供了一個可靠地應用樣式的系統，並確保「最後應用的樣式始終生效」。</p></li><li><p><strong>類型安全</strong>：使用 TypeScript 或 Flow 類型來約束組件接受的樣式，每個樣式屬性和變量都具有完全的類型定義。這有助於提高代碼的可讀性和可維護性，同時減少潛在的錯誤和衝突。</p></li><li><p><strong>樣式去重</strong>：StyleX 鼓勵在同一文件中編寫樣式和組件。這種方法有助於使樣式在長期內更具可讀性和可維護性。StyleX 能夠利用靜態分析和構建時工具來跨組件去重樣式，並刪除未使用的樣式。</p></li><li><p><strong>可測試性</strong>：StyleX 可以配置為輸出調試類名，而不是功能性的原子類名。這可以用於生成快照，以便在對設計進行輕微更改時不會經常變化。通過這種方式，開發人員可以更輕鬆地測試和驗證樣式的正確性，從而提高開發效率和產品質量。</p></li></ul><p><strong>示例代碼</strong></p><pre><code class="language-javascript">import stylex from '@stylexjs/stylex';

const styles = stylex.create({
  root: {
    padding: 10,
  },
  element: {
    backgroundColor: 'red',
  },
});

const styleProps = stylex.apply(styles.root, styles.element);</code></pre><p><strong>下面是一個按鈕組件的示例代碼</strong></p><pre><code class="language-javascript">import * as stylex from "@stylexjs/stylex";

const styles = stylex.create({
  base: {
    appearance: "none",
    borderWidth: 0,
    borderStyle: "none",
    backgroundColor: "blue",
    color: "white",
    borderRadius: 4,
    paddingBlock: 4,
    paddingInline: 8,
  },
});

export default function Button({
  onClick,
  children,
}: Readonly&lt;{
  onClick: () =&gt; void;
  children: React.ReactNode;
}&gt;) {
  return (
    &lt;button {...stylex.props(styles.base)} onClick={onClick}&gt;
      {children}
    &lt;/button&gt;
  );
}</code></pre></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:39:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270597/facebook-stylex-css-in-js</guid>
            <link>https://www.oschina.net/news/270597/facebook-stylex-css-in-js</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Arc 瀏覽器開始 Windows 版 Beta 測試]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>12 月 11 日，Arc 瀏覽器開始 Windows 版 Beta 測試，第一批邀請已在加入等待隊列的用戶中篩選併發送完畢。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e387f48e7934b2c6d34f92595dbaea17a39.png" referrerpolicy="no-referrer"></p><p>感興趣的用戶可以在上線的&nbsp;<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.isarconwindowsyet.com%2F" target="_blank">IsArcOnWindowsYet</a></u>&nbsp;頁面中，填寫表單加入等待隊列。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-688afd52e0b014510739c8706073d792afb.png" referrerpolicy="no-referrer"></p><p><strong>Arc 基於 Chromium 並用 Swift 語言編寫</strong>。它支持 Chrome 瀏覽器擴充功能，同時默認使用 Google 搜索。7 月份，Arc 正式發佈了 1.0。</p><blockquote><p><u><strong><em><a href="https://www.oschina.net/news/251034/arc-browser-1-0-mac-released">Arc 瀏覽器正式發佈 1.0，聲稱是 Chrome 的替代品</a></em></strong></u></p></blockquote><p>Arc 旨在成為一個 「萬維網的操作系統」，並試圖將網頁瀏覽與內置應用程序和功能整合在一起。其內置的功能包括虛擬記事本、拼貼風格的 「easel」 和 「boosts」，該功能允許用戶美化和重新設計網站界面。Arc 的選項卡垂直排列在側邊欄中，側邊欄包含除瀏覽窗口之外的所有瀏覽器功能。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-34b7b76a856863e0f84e96557bd15c058e6.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:11:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270593/arc-for-windows-is-in-beta</guid>
            <link>https://www.oschina.net/news/270593/arc-for-windows-is-in-beta</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Google Colab 現已支持直接使用 🤗 transformers 庫]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section data-tool="mdnice 編輯器" data-website="https://www.mdnice.com" style="font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Roboto, Oxygen, Ubuntu, Cantarell, PingFangSC-regular, PingFangTC-regular, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif;"><p data-tool="mdnice 編輯器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">Google Colab，全稱 Colaboratory，是 Google Research 團隊開發的一款產品。在 Colab 中，任何人都可以通過瀏覽器編寫和執行任意 Python 代碼。它尤其適合機器學習、數據分析和教育目的。從技術上來説，Colab 是一種託管式 Jupyter 筆記本服務。用戶無需設置，就可以直接使用，同時還能獲得 GPU 等計算資源的免費使用權限。</p><figure data-tool="mdnice 編輯器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005864" data-ratio="0.6592592592592592" src="https://oscimg.oschina.net/oscnet/6aca6440-d2d5-4972-8624-54894772e85a.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 編輯器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">通過與 Colab 團隊的共同努力，Colab 託管的運行時鏡像現已默認集成了 Hugging Face transformers 庫，只需簡單執行 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">import transformers</code> 即可輕鬆接入！對於使用 Colab 進行機器學習和深度學習研究的開發者來説，這是一個非常重要的更新。</p><p data-tool="mdnice 編輯器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">如果你想使用最新版本的 transformers，Colab 團隊也提供了一個簡單的命令 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">!pip install transformers --upgrade</code>，以便於隨時更新至最新版本。</p><p data-tool="mdnice 編輯器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">除了提升用戶體驗，這一更新還開啓了一些有趣的新功能。例如，用戶現在可以直接從 Pandas 讀取 Hugging Face 數據集，這將大大簡化數據處理和模型訓練的工作流程。</p><figure data-tool="mdnice 編輯器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img class="rich_pages wxw-img" data-imgfileid="100005865" data-ratio="0.4203703703703704" src="https://oscimg.oschina.net/oscnet/8ecbb7d1-9659-48de-9e0f-64e60f62d9ef.jpg" data-type="jpeg" data-w="1080" style="margin-right: auto;margin-left: auto;width: 100%;border-radius: 5px;display: block;margin-bottom: 15px;" referrerpolicy="no-referrer"></figure><p data-tool="mdnice 編輯器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">本合作和更新還開啓了一些有趣的新功能。例如，用戶現在可以直接從 Pandas 讀取 Hugging Face 數據集，這將大大簡化數據處理和模型訓練的工作流程。</p><p data-tool="mdnice 編輯器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">你可以通過 <code style="font-size: 14px;border-radius: 4px;font-family: &quot;Operator Mono&quot;, Consolas, Monaco, Menlo, monospace;word-break: break-all;color: rgb(155, 110, 35);background-color: rgb(255, 245, 227);padding: 3px;margin: 3px;">hf://datasets/</code> 的方式在 Pandas 中直接讀取 Hugging Face Hub 上的數據集。</p><p data-tool="mdnice 編輯器" style="margin-bottom: 20px;line-height: 1.8em;color: rgb(58, 58, 58);">感謝 Colab 團隊的朋友們，也希望社區的成員們喜歡本次的合作和功能更新！</p></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公眾號 - Hugging Face（gh_504339124f0f）。<br>如有侵權，請聯繫 support@oschina.cn 刪除。<br>本文參與「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源創計劃</a>」，歡迎正在閲讀的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 02:07:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/HuggingFace/blog/10316003</guid>
            <link>https://my.oschina.net/HuggingFace/blog/10316003</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推薦 | 開源調試工具 ixGDB]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-readme-for-ixgdb-release" class="anchor" href="https://gitee.com/deep-spark/ixgdb#readme-for-ixgdb-release"></a>README for ixGDB release</h1><h2><a id="user-content-introduction" class="anchor" href="https://gitee.com/deep-spark/ixgdb#introduction"></a>INTRODUCTION</h2><p>ixGDB is Iluvatar CUDA source-level debugger for Linux OS, based on NVIDIA <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fcuda-gdb">CUDA-GDB</a> 10.2.</p><p>ixGDB provides the following capabilities:</p><ul><li>Provides a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application.</li><li>Supports debugging C/C++ applications and all CUDA applications, which might use CUDA driver APIs or CUDA runtime APIs.</li><li>Supports setting breakpoints.</li></ul><h2><a id="user-content-build-instructions-example-only-adjust-as-needed" class="anchor" href="https://gitee.com/deep-spark/ixgdb#build-instructions-example-only-adjust-as-needed"></a>BUILD INSTRUCTIONS (example only, adjust as needed)</h2><p>First, make sure that libtermcap and other required dependent packages are
installed (try "sudo yum install ncurses-devel"). The "configure" command will
report an error if some packages are missing.</p><p>Please note that the libexpat development headers must be present if ixGDB is to be used for cross-platform debugging.</p><p>Issue the following commands to build ixGDB:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">./configure --program-prefix=cuda- \</span><span id="LC2" class="line">    --enable-cuda \</span><span id="LC3" class="line">    --enable-targets="x86_64-apple-darwin,x86_64-unknown-linux-gnu,\</span><span id="LC4" class="line">    arm-elf-linux-gnu,m68k-unknown-linux-gnu" \</span><span id="LC5" class="line">    CFLAGS='-I/usr/local/cuda/include' \</span><span id="LC6" class="line">    LDFLAGS='-lpthread'</span><span id="LC7" class="line">make</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-using-ixgdb" class="anchor" href="https://gitee.com/deep-spark/ixgdb#using-ixgdb"></a>USING ixGDB</h2><p>All standard GDB commands could be used for both CPU and GPU code debugging. In addition to that, ixGDB provides CUDA-specific command families like "info cuda ..." to query GPU states, "cuda .." to control debugger focus on GPU and "[get|set] cuda .." to alter/query CUDA debugger configuration. If you want to know more about how to use ixGDB, please go to Iluvatar CoreX support <a href="https://gitee.com/link?target=https%3A%2F%2Fsupport.iluvatar.com%2F%23%2FDocumentCentre%3Fid%3D1%26nameCenter%3D1%26productId%3D">official site</a> and use "ixgdb" as the keyword to find document "SDK Tools User Guide", which includes detailed usage of ixGDB.</p><h2><a id="user-content-communication" class="anchor" href="https://gitee.com/deep-spark/ixgdb#communication"></a>COMMUNICATION</h2><p><a href="https://gitee.com/deep-spark/ixgdb/issues">Gitee Issues</a>: bug reports, feature requests, install issues, usage issues, etc.</p><h2><a id="user-content-license" class="anchor" href="https://gitee.com/deep-spark/ixgdb#license"></a>LICENSE</h2><p>Licensee's use of the GDB third party component is subject to the terms and conditions of GNU GPL v3:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">This product includes copyrighted third-party software licensed under the terms of the GNU General Public License v3 ("GPL v3"). All third-party software packages are copyright by their respective authors.</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses.</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Component    License</span><span id="LC2" class="line">ixGDB        GPL v3</span></pre><div class="markdown-code-block-copy-btn"></div></div></div>]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:59:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/deep-spark/ixgdb</guid>
            <link>https://gitee.com/deep-spark/ixgdb</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 語言大模型的推理技巧]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><p><img src="https://oscimg.oschina.net/oscnet/7d0eafbb-7a1e-416d-83e4-ac07a8583a4b.jpg" referrerpolicy="no-referrer"></p><p style="margin-left:8px; margin-right:8px"><span>本文探討了一系列<span style="background-color:#efefef">語言大模型的</span>推理優化技巧，涵蓋 KV 緩存、量化和稀疏性等方法，並分享瞭如何有效實施這些技術。對於想要優化 Transformer 模型，以期提升推理速度或效</span><span>率的人來説</span><span>值得一讀。</span></p><p>&nbsp;</p><p><span>本文作者為機器學習研究員 Finbarr Timbers，他曾是 DeepMind 的工程師。</span><span>（本文由 OneFlow 編譯發佈，轉載請聯繫授權。原文：</span><span>https://www.artfintel.com/p/transformer-inference-tricks）</span></p><p>&nbsp;</p><p><strong><span style="color:#3f3f3f">作者 |&nbsp;</span></strong><strong><span>Finbarr Timbers</span></strong></p><p><strong><span style="color:#3f3f3f">OneFlow 編譯</span></strong></p><p><strong><span style="color:#3f3f3f">翻譯｜楊婷、宛子琳</span></strong></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">1</span></strong></span></p><span id="OSC_h2_1"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">鍵值（KV）緩存</span></strong></span></h2><p>&nbsp;</p><p><span>目前，鍵值（KV）緩存是最常見（也是最重要）的解碼器優化方法。在解碼器模型中，對於每次解碼迭代，提示的鍵和值將是相同的。此外，一旦你運行了一個詞元，該詞元的鍵和值將在後續的每個迭代中保持不變。因此，你可以緩存提示，並在解碼時逐漸將每個詞元的 KV 張量添加到緩存中，這樣可以減少大量計算。在注意力機制中，我們能夠將形狀為（batch, context_length, feature_dim）的兩個張量相乘，變為將形狀為（batch, 1, feature_dim）的查詢張量與形狀為（batch, context_length, feature_dim）的 KV 張量相乘。因此，採樣的複雜度不再是二次方，這使我們能夠獲得更長上下文長度的良好解碼（採樣）性能。</span></p><p>&nbsp;</p><p><span>實際上，這會在你的實現中增加複雜性，因為現在你不僅僅是運行純函數，而且有了狀態（state），所以即便一個序列已經完成了推理，你仍需要持續運行推理（<span style="color:#888888"><em>參見 Google MaxText 的實現，https://github.com/google/maxtext</em></span>）。</span></p><p>&nbsp;</p><p><span>KV 緩存需要 2 * n_layers * n_heads * d_head 個參數。對於 GPT-3，其中 n_layers = 96，n_heads = 96，d_head = 128，這意味着每個上下文中的詞元需要 2.4m 個參數。使用典型的 16 位精度，每個詞元需要 5MB；如果上下文窗口有 2048 個詞元，那就需要將 10GB 的 HBM 用於 KV 緩存。這雖然昂貴，但每 GB 的消耗都物有所值。</span></p><p>&nbsp;</p><p><span>這些內存需求是在消費級 GPU 上訓練語言大模型如此困難的重要原因之一。目前最強大的消費級顯卡是 4090，只有 24GB 的 HBM。雖然其每秒浮點運算次數（FLOPS）可與企業級芯片相媲美，但其內存限制要低得多，這使得難以將權重和 KV 緩存置入內存。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">2</span></strong></span></p><span id="OSC_h2_2"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">推測性解碼</span></strong></span></h2><p>&nbsp;</p><p><span>推測性解碼是一種在計算能力充裕時使用的技術，通常用於本地推理設置。它利用了現代加速器的特性，即在批次數據上運行推理所需的時間與在單個數據點上運行推理的時間相同。以 A100 為例，你可以在相同的時間內對多達 160 個數據點進行推理，所需推理時間與單個數據點相同。因此，現在已經出現了許多利用這一特性的技術，如束搜索（beam search）、MCTS（蒙特卡洛樹搜索）或推測性解碼。</span></p><p>&nbsp;</p><p><span>推測性解碼包括兩個模型：一個小而快的模型以及大而慢的模型。由於現代解碼器的推理速度與參數數量成正比，使用較小的模型可以在大型模型運行一次推理所需的時間內運行多次推理。</span></p><p>&nbsp;</p><p><span>現代解碼器模型（如 GPT 系列）使用了自迴歸採樣技術，即要對 N 個詞元的序列進行採樣，模型會進行 N 次推理，每次推理都要使用前一次推理的結果。</span></p><p>&nbsp;</p><p><span>在推測性解碼中，你會並行運行這兩個模型。快速模型會運行一批推理並猜測大模型將預測哪些詞元，然後將這些猜測相疊加。與此同時，大模型在後台運行，檢查較小模型是否記錄了相同結果。較小模型能夠在大模型進行一次推理的時間內進行多次猜測。然而，鑑於我們有多餘的計算能力，大模型能夠並行評估所有猜測。因此，我們支付順序生成序列成本的唯一地方是在較小的模型上。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/59e12167-b54f-4b1c-bcb5-4479facca980.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>推測性解碼的主要缺點是它需要一個「草稿（draft）」模型，該模型能夠預測較大模型的輸出，而且你必須讓兩個模型同時存在於同一台機器的內存中（或者在多 GPU 設置下的同一節點上）。這增加了複雜性，需要額外的工作，因為你必須訓練兩個模型（原始模型和「草稿」模型）。此外，任何性能提升都受限於小模型能夠在多大程度上精確地預測大模型。如果小模型始終能夠準確預測大模型的行為，那麼我們就可以直接使用它！因此，推測性解碼能夠發揮作用的程度存在根本差距。HuggingFace 聲稱它通常可以將解碼速率提高一倍，這與原始論文（<span style="color:#888888"><em>https://arxiv.org/abs/2211.17192</em></span>）中聲稱的 2 至 3 倍的提升一致。</span></p><p>&nbsp;</p><p><span>最近出現了一種試圖改進推測性解碼的前向解碼（Lookahead Decoding）技術（<span style="color:#888888"><em>https://lmsys.org/blog/2023-11-21-lookahead-decoding/</em></span>），該技術讓模型生成 n-gram，然後在無需草稿模型的情況下遞歸匹配這些 n-gram。這種技術被稱為 Jacobi 解碼（來自他們的博客截圖），可能是對貪婪解碼的潛在改進。Jacobi 解碼的工作原理是在生成詞元的每一點上生成 n 個詞元，對整個序列進行「猜測」。然後，將其與先前的猜測相驗證，如果兩者匹配，就接受該猜測。這可以在沒有副作用的情況下減少時延，因為在最壞的情況下，它會變成貪婪解碼。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/d545a2df-0729-4b32-a7dc-390784598002.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>前向解碼通過保留解碼過程中生成的 n-gram，並嘗試將它們用作猜測，進一步改進了這一技術。鑑於已生成的文本與將要生成的文本之間存在很高的相關性，這也有可能以極低的成本，顯著改進時延。這一技巧非常巧妙。考慮到這項技術才發佈不久，我非常好奇它在實際場景中的性能表現。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/cdf7da2f-7ac8-475c-90ff-a2baacf4b4c3.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">3</span></strong></span></p><span id="OSC_h2_3"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">有效稀疏性</span></strong></span></h2><p>&nbsp;</p><p><span>在僅解碼器 Transformer 中，模型核心是注意力機制，可總結為如下的注意力方程：</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/51f1f3da-d1e8-4cff-b3c2-b1f2f9a7af30.png" referrerpolicy="no-referrer"></p><p><span>softmax 操作會使非最大值變得很小。</span></p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/a66d9144-d636-4c8e-a891-8a79045d5e40.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p style="text-align:left"><span>因此，我們將數值張量（在注意力方程中用 V 表示）與一個主要由零（zero）組成的張量相乘。結果，注意力機制的輸出中包含了大量的零，最高可達 97%（<span style="color:#888888"><em>https://x.com/YesThisIsLion/status/1647747069086666752?s=20）</em></span>。類似地，在多層感知器網絡（MLP）中的每個 ReLU 之後，我們也得到了大量稀疏性。</span></p><p>&nbsp;</p><p><span>不幸的是，現在要實際利用這一點比較困難。如果權重中存在稀疏性，那麼可通過結構化稀疏性（例如</span><span>tor<span>‍</span>ch.sparse</span><span>）做大量工作，但目前還不清楚系統能夠多大程度地利用激活的稀疏性。</span></p><p>&nbsp;</p><p><span>可以進行的一個優化是：如果某個激活為零，那麼可以跳過加載與該激活對應的權重，並避免相應計算。據我所知，這並未很好地得到主流張量計算程序的支持，但對於</span><span>Llama.cpp</span><span>等自定義推理實現來説，這一優化比較容易實現。</span></p><p>&nbsp;</p><p><span>這是因為激活是每個詞元的函數，因此有效稀疏性也是隨機分佈在詞元上。因此，這種優化的效果會隨着批大小的增加呈指數級衰減。假設我們的有效稀疏性為 X%，批大小為 N，那麼對於一個給定激活的所有條目在整個批次中都為零的概率可以表示為 X^N。我製作了一張表格，列出了不同 X 和 N 值的情況。這種衰減效應非常顯著。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/a82589c0-f80d-4a43-aab4-348ae7b1f293.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>因此，除批大小為 1 的情況，利用這一方法十分困難，即使在這種情況下，使用推測性解碼通常更為有效。但如果你想要在本地運行推理，並且確實需要降低時延，這可能是一個很棒的技巧。</span></p><p>&nbsp;</p><p style="margin-left:8px; margin-right:8px; text-align:center"><span><strong><span style="color:#f6ab00">4</span></strong></span></p><span id="OSC_h2_4"></span><h2 style="margin-left:8px; margin-right:8px; text-align:center"><span style="color:#1e2380"><strong><span style="color:#1e2380">量化</span></strong></span></h2><p>&nbsp;</p><p><span>量化是人們更為熟悉的技巧之一。我之前已經寫過量化的相關內容 (<span style="color:#888888"><em>https://finbarrtimbers.substack.com/p/efficient-llm-inference</em></span>），所以不打算在具體方法上花費太多時間。我們很難精確度量量化的效果。GPTQ 論文等文獻所使用的模型與 SOTA 模型差距較大，因為大型實驗室並未公開其所使用的模型，並且學術界無法與大型實驗室所擁有的資源相匹敵。</span></p><p>&nbsp;</p><p><img src="https://oscimg.oschina.net/oscnet/1bcc4196-002b-475d-a149-812f9b6e70b7.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p><p><span>例如，GPTQ 報告了 OPT 和 BLOOM 模型的量化結果，這些結果遠不如當前的一系列開源模型，更不用説 GPT-4 了。</span></p><p>&nbsp;</p><p><span>當然，大型實驗室並未公開其研究進展，而我看到的大部分個案報告都來自那些試圖在消費級硬件上運行較小模型的人，這種硬件的內存非常有限。我認為，很多業餘愛好者（即非大型實驗室研究人員）都被在本地運行龐大模型的吸引力所誘惑，因此他們對量化產生了濃厚興趣。但實際上，量化並不具備固有優勢！從第一性原則出發，如果你有兩個位數相同的模型，它們應該具有相同數量的詞元/秒，並且應該具有類似的性能水平。只有在使用更高精度格式的位數時做得很糟，才會有較大差異。</span></p><p>&nbsp;</p><p><span>但文獻中的觀點與我的直覺不一致。上述 GPTQ 論文發現，將模型量化為低至 4 倍的精度時，性能的下降微乎其微。我認為，這是因為性能更差的模型更容易在量化過程中保持其性能不受損。如果假設兩個相同的 LLM，一個經過 2 萬億詞元的訓練，另一個經過 5000 億詞元的訓練（分別稱為 LLM-2T、LLM-500B），在進行量化時，我認為經過更多詞元訓練的模型在性能上受到的影響更大，因為它應該更充分地利用這些詞元。我們仍然預計經過量化的 LLM-2T 會優於 LLM-500B，但我認為從 LLM-2T 到經過量化的 LLM-2T 的性能下降，會比從 LLM-500B 到經過量化的 LLM-500B 的下降更顯著。</span></p><p>&nbsp;</p><p><span>注：雖然上述論點很有説服力，但實際上並沒有相關的文獻支持。量化似乎確實非常接近於「免費的午餐」。</span></p><p>&nbsp;</p><p style="text-align:left"><span>近期的研究，如關於 k-bit 推理規模定律的論文（<span style="color:#888888"><em>https://arxiv.org/abs/2212.09720</em></span>），在一系列 LLM 架構上進行了大量實驗，得出了不同的位數分配對模型性能的影響。他們研究了在給定精度水平下使用 N 個參數的模型與使用 2N 個參數和一半精度的模型之間的權衡。其結果非常引人注目，與未進行量化的性能幾乎沒有差別（至少對於 4 位或更多位而言）。</span></p><p>&nbsp;</p><p style="text-align:center"><img src="https://oscimg.oschina.net/oscnet/e695f055-912b-4be6-99bf-e046a7191224.png" referrerpolicy="no-referrer"></p><p><span>基本上，他們發現可以將精度降至 4 位而不損失任何性能，量化幾乎不會導致任何權衡。你可以運行一個小 4 倍的模型而不會顯著降低性能。由於在現代加速器上推理性能等於處理的位數（即使用較少精度時每秒可以獲得更多的運算次數），這點很有幫助。</span></p><p>&nbsp;</p><p><span>因此，我的結論是：推薦採納「k-bit 推理論文」的建議。然而，對於生產負載，我對使用低於 8 位的精度還有些猶豫。fp8 是目前現代加速器本地支持的最低精度浮點格式，即使如此，支持也是有限的。我建議在 fp8 精度下進行訓練和推理，並觀察進一步量化可能帶來的精度損失對你的用例來説是否可以接受。當生產環境中缺乏來自這些平台（例如 Nvidia 和 Torch/JAX 團隊）的本地支持時，我很難推薦在生產環境中使用更低級別的精度。</span></p><p>&nbsp;</p><p><span>根據我從文獻中瞭解到的（這與我的直覺相符），fp8 嚴格來説優於 int8，但在硬件上的支持有限。如果你在一個 GPU 資源充沛的組織，並且能夠將 H100 用於所有任務，那麼請使用 fp8。否則，也可以使用 int8，而且相比起來要容易得多（PyTorch 使其變得相當容易，儘管 API 不太穩定）。</span></p><p>&nbsp;</p><p><span>關於實際進行模型量化，PyTorch 團隊已經撰寫了一篇關於如何具體操作的文章（<span style="color:#888888"><em>https://pytorch.org/blog/accelerating-generative-ai/</em></span>），並提供了一系列 API 用於簡化操作，儘管它們不太穩定。此外，bitsandbytes 是另一個出色的量化庫，不過我個人還未使用過。</span></p><p>&nbsp;</p><p><span>（特別感謝@cis_female 與我討論稀疏性的複雜性，以及@nostalgebraist 糾正量化部分中的錯誤。我現在認為，證據表明，至少量化到 4 位或更多位，在性能方面的權衡非常小。）</span></p><p>&nbsp;</p><p>&nbsp;</p><span id="OSC_h2_5"></span><h2 style="margin-left:8px; margin-right:8px; text-align:left">&nbsp;</h2><p><span style="background-color:#ffffff; color:#888888">其他人都在看</span></p><span id="OSC_h3_6"></span><h3 style="text-align:left">&nbsp;</h3><ul><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247491309%26idx%3D1%26sn%3D407fefb7ec76a2c9fdfe1ae960f7de4d%26chksm%3Dfe4190dbc93619cd0b9bfecb979e142a125fd8548d323dd9bdc2cbeb619400202e6e1affced0%26scene%3D21%23wechat_redirect" target="_blank">大型語言模型的推理演算</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493088%26idx%3D1%26sn%3Dff319e3b8cc19f165232c3226779588c%26chksm%3Dfe426bd6c935e2c0946fdaa96378d04123f31eb797e39c8ab0d619bbb00db8b665354a167583%26scene%3D21%23wechat_redirect" target="_blank">LoRA 微調語言大模型的實用技巧</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492976%26idx%3D1%26sn%3Dd919a508ce238048ae44e58b9cc06b71%26chksm%3Dfe426b46c935e2500178c2d2c8845fcd3e47fdeb5ec51f55b80cbca5f7b78382cdb3e6fe6a32%26scene%3D21%23wechat_redirect" target="_blank">可復現的語言大模型推理性能指標</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492990%26idx%3D1%26sn%3D50844c8911834baf44863a9e3754175f%26chksm%3Dfe426b48c935e25ede3f772624ba262011b1b48f8ee78ac6d3b1daa5aaf71e7583828740b5cd%26scene%3D21%23wechat_redirect" target="_blank">ChatGPT 規模化服務的經驗與教訓</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493053%26idx%3D1%26sn%3Dfe7a51fbda920626b55d8919dd780e05%26chksm%3Dfe426b8bc935e29d3a806dfd6682619ee46effa39b09777907d5bccfd80d8cd639580c6f6e23%26scene%3D21%23wechat_redirect" target="_blank">機器學習硬件十年：性能變遷與趨勢</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492951%26idx%3D1%26sn%3D873b7c63ea18d638a9570bb582cddbb5%26chksm%3Dfe426b61c935e277e17fd2d4b06fa3ec998479ae87d84312f064dbae0e65875a7cb45829807d%26scene%3D21%23wechat_redirect" target="_blank">微調語言大模型選 LoRA 還是全參數？</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247492811%26idx%3D1%26sn%3D916e330a2a4152dab3192635c3e475fa%26chksm%3Dfe426afdc935e3eb2f371ff5f56247c95800ce91a950a89bea871c26ddc4c3d13371acf03978%26scene%3D21%23wechat_redirect" target="_blank">語言大模型推理性能工程：最佳實踐</a></p></li><li><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU5ODY2MTk3Nw%3D%3D%26mid%3D2247493030%26idx%3D1%26sn%3D58a43ed078977019c997a110526d7c02%26chksm%3Dfe426b90c935e28688b6e317a991bedaaa164471a275d64e60851a09b00f7f6b718e27d7b411%26scene%3D21%23wechat_redirect" target="_blank">語言大模型的分佈式訓練與高效微調指南</a></p></li></ul><span id="OSC_h3_7"></span><h3 style="text-align:left">&nbsp;</h3><p><strong><span>試用 OneFlow: <a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fgithub.com%2FOneflow-Inc%2Foneflow%2F" target="_blank">github.com/Oneflow-Inc/oneflow/</a></span></strong></p><p style="color:#3f3f3f; margin-left:8px; margin-right:8px; text-align:left"><img src="https://oscimg.oschina.net/oscnet/f2b38f8c-5887-4315-9787-03816b68ada4.png" referrerpolicy="no-referrer"></p><p>&nbsp;</p></div><p style="color:#858585">本文分享自微信公眾號 - OneFlow（OneFlowTechnology）。<br> 如有侵權，請聯繫 support@oschina.cn 刪除。<br> 本文參與「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源創計劃</a>」，歡迎正在閲讀的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 12 Dec 2023 01:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/oneflow/blog/10320747</guid>
            <link>https://my.oschina.net/oneflow/blog/10320747</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[開源 MoE 模型 Mixtral 8x7B 性能超過 GPT-3.5]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>大模型創業公司 Mistral AI 終於<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">介紹了</a></u>前兩天「開源」的&nbsp;MoE 模型 <strong>Mixtral 8x7B</strong>。</p><blockquote><p><strong><em><u><a href="https://www.oschina.net/news/270317/mixtral-8x7b-32kseqlen">Mistral AI 用「磁鏈鏈接」開源了 87 GB 的 8x7B MoE 模型</a></u></em></strong></p></blockquote><p>官方稱，Mixtral 8x7B 是開放權重的高質量<strong>稀疏混合專家模型 (SMoE)</strong>，採用 Apache 2.0 License 開源。在大多數基準測試中，Mixtral 的成績都優於 Llama 2-70B，且推理速度提升了 6 倍。而且在大多數標準基準測試中超過 GPT-3.5。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7a689c4f538b591b9744038a052717945e6.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-84fefd9ee6c091c07c894031a1af2faf2e3.png" referrerpolicy="no-referrer"><img alt="" src="https://oscimg.oschina.net/oscnet/up-9f9aaad324856028fb1e796beb2d7685020.png" referrerpolicy="no-referrer"></p><p>因此，Mistral AI 稱 Mixtral 是最強大的開放權重模型，也是成本/性能權衡方面的最佳模型。</p><p><strong>Mixtral 主要特性</strong></p><p>• 32k 上下文<br> • 支持英語、法語、意大利語、德語和西班牙語<br> • 性能超過 Llama 2 系列和 GPT-3.5<br> • 在代碼生成方面具有強勁性能<br> • 在 MT-Bench 上獲得 8.3 分</p><p>Mixtral 作為稀疏混合專家網絡，是一個純解碼器模型，其中前饋塊從 8 組不同的參數組中選擇。在每一層，對於每個 token，路由網絡選擇兩組「專家」來處理 token 並相加地結合它們的輸出。</p><p>Mixtral 總共有 45B 個參數，但每個 token 只使用 12B 個參數。因此，它以與 12B 模型相同的速度和成本處理輸入和生成輸出。</p><p>更多細節查看：<em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmistral.ai%2Fnews%2Fmixtral-of-experts%2F" target="_blank">https://mistral.ai/news/mixtral-of-experts/</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 10:18:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270511/mixtral-of-experts</guid>
            <link>https://www.oschina.net/news/270511/mixtral-of-experts</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[榮耀申請魔方大模型商標]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="background-color:#ffffff; color:#222222">天眼查信息顯示，榮耀終端有限公司近日申請註冊「榮耀魔方大模型」商標，國際分類為網站服務，當前商標狀態為等待實質審查。</span></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-7baf34d7d00360b976559630121d67b0da4.png" width="700" referrerpolicy="no-referrer"></p><p><span style="background-color:#ffffff; color:#222222">此前，該公司曾申請兩枚「MAGIC&nbsp;大模型」商標。榮耀 CEO 趙明曾發文稱，榮耀即將推出自研端側 AI 大模型和全新雲服務。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 11 Dec 2023 08:25:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/270492</guid>
            <link>https://www.oschina.net/news/270492</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[雙十一彈性能力支撐 - ECI 穩定性建設]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h3_1"></span><h3>一、關於 ECI</h3><p style="text-align:justify">背景從 2018 年正式發佈，<strong>ECI 已經打磨了整整 4 個年頭</strong>，如今也已經快速成長為了阿里雲 serverless 容器的基礎設施，服務着阿里內外眾多的公有云客戶與雲產品，每天承接着數百萬的彈性容器創建。</p><p style="text-align:justify">然而，ECI 這些年卻未參與到集團雙十一大促，雙十一可以説是阿里技術人的閲兵，能不能承接住雙十一的流量成為了檢驗一個產品是否穩定可靠的重要標準。但一切都是水到渠成，就在今年，ASI 開始與 ECI 對接，嘗試讓 ECI 承接雙十一大促的彈性的 30W 覈算力，我們都知道雙十一大促對於整個阿里集團的意義，使命將至，我們必將全身心地投入到對接、壓測、護航的工作中。經過長達兩個多月的業務適配、壓測、備戰，最終完成了雙十一大促的彈性容器的圓滿交付。這背後，離不開 ASI、ECI 以及參與到其中的每一位腳踏實地、用心鑽研、保駕護航的同學的努力。ECI 今年首次作為集團大促彈性基礎設施，根據線上數據統計，大促期間 ECI 彈性資源使用共計約 400W 核，從資源的瞬時彈性、保有規模、系統穩定性等多方面對雲原生系統都是一次巨大的考驗。作為底層的計算單元，ECI 此次也成功頂住了雙十一彈性流量洪峯的考驗，在感嘆 serverless、容器這些技術發展迅猛的同時，對於全新的系統架構穩定性的考驗也不小。</p><p style="text-align:justify">如今再回過頭來看 ECI 的第一次雙十一，我們有必要做一次全面的總結，我們為集團彈性保障做了哪些工作，哪些是將來可以複用的工作，哪些是可以給其他的團隊作為借鑑的技術和經驗，以及哪些地方還可以做的更好，為下一次大促做準備。</p><p style="text-align:justify">本文我們將為大家介紹，<strong>ECI 這些年在穩定性方面做了哪些工作，以及是如何來為集團雙十一保駕護航的。</strong></p><span id="OSC_h3_2"></span><h3>二、遇到的挑戰</h3><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-484afd33bc5b8c5f4774ab7fddf53a98_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">大規模併發帶來的穩定性挑戰遇到的最大挑戰首先是大規模併發帶來的。容器保有量增多之後，從容器實例生產方面來看對於雲管控系統是不小的考驗，尤其是對於彈性場景來講，需要在極短的時間進行實例的生產，鏡像的大規模拉取，進而保障容器的成功啓動。</p><p style="text-align:justify"><strong>如何能保障實例的大規模成功生產</strong>，如何先於線上發現問題，以及即使出現了問題如何第一時間止血並進行故障恢復，這對於集團雙十一期間的業務重保都是尤為重要的。除此之外，對於公有云環境來講，不能影響到其他的公有云客戶也是需要重點關注的，因此需要具備一套完整的穩定性保障體系以及故障應對方案以確保雙十一期間的業務能夠順利進行。實例生產系統穩定性 ECI 和 ECS 共用一套資源調度系統，相對於 ECS 容忍度為分鐘級別的應用來講，ECI 實例頻繁的創建刪除對調度系統的要求更為苛刻，對系統容量以及穩定性保障方面提了更高的要求。服務可用性保障 ECI 安全沙箱由於某種原因異常（OOM/物理機宕機/kernel panic），導致不健康情況。這種情況下，k8s 層面如果不從 endpoint 上摘除這個 ECI Pod，會導致請求通過負載均衡依然可以路由到這台不健康的 ECI 上，會導致業務請求成功率下降，因此對於集團業務服務可用性保障也是尤為重要的。</p><span id="OSC_h3_3"></span><h3>三、ECI 穩定性技術建設</h3><p style="text-align:justify">穩定性保障從需求收集準備階段開始，雙十一大促持續兩個月之久，為了配合集團全鏈路驗收，ECI 自身的穩定性保障工作也隨之緊鑼密鼓地進行。</p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-353460c616617e11f187c76359272f10_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">穩定性的保障貫穿了整個大促過程，大促前慎重/減少系統變更以排除人為因素的幹擾，敬畏發佈，多次壓測演預案確保系統穩定性，不斷提升系統抵抗力穩定性和系統恢復力穩定性，以保障大促的順利進行，最後通過問題覆盤沉澱出可複製的大客戶重保策略，這對於未經過雙十一實戰演練具有積極的意義。</p><p style="text-align:justify">因此我們梳理出了整個大促期間圍繞穩定性方面做的主要工作，主要包括風險控制、關鍵業務依賴梳理、技術保障、壓測預案、運行時保障、故障運維能力、以及最後的覆盤優化，希望以此能對今後的大促工作作為指導，並沉澱出穩定性治理的經驗。接下來我們對此次大促涉及到的主要穩定性保障方法以及如何應用進行介紹。</p><p style="text-align:justify">實例生產保障 VM 複用技術實例生產行為的保障是集團彈性使用 ECI 的重中之重。一個典型的實例生產過程如圖所示，<strong>ECS 和 ECI 在控制面共用一套管控系統，</strong>ECI 管控側調用資源調度系統之後會分配計算資源之後會調用 pync（阿里雲單機管控組件），進而調用 avs(阿里雲單機網絡組件) 和 tdc(阿里雲單機存儲組件) 分別生產網卡與磁盤。在此過程中，對於調用 ECS 依賴的 open api 接口較重，在大規模創建刪除場景很快成為系統瓶頸，此前我們專門針對容器實例高頻創建刪除場景開發了 VM 複用功能，對於高頻場景刪除容器實例的場景，延遲 vm 的回收，並複用容器實例的網卡、鏡像、計算資源，降低對管控系統整體的衝擊，以此來保障實例生產系統的穩定性，從此次雙十一的實戰演練效果來看，vm 複用取得了很好的效果，管控系統容量整體處於正常水位，保障了集團雙十一實例穩定的彈性能力。</p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-dcd697c3f23312c91e09c0272f34fbd8_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">重調度機制對於庫存不足或者遠程服務調用超時等情況，為了保障實例生產的最終一致性，對於 ECI 實例生產我們設計了相應的故障處理策略策略，取值如下：fail-back：失敗自動恢復。即 Pod 創建失敗後自動嘗試重新創建 fail-over：失敗轉移。效果等同於 fail-backfail-fast：快速失敗。即 Pod 創建失敗後直接報錯故障處理策略本質上是一種重調度的策略。原生的 k8s 調度支持重調度，即調度失敗後會將 pod 重新放入調度隊列等待下次調度，類比 k8s 的重調度行為，當 eci 管控系統收到創建請求的時候，首先會進入一個隊列，然後有個異步定時任務會將創建從隊列中撈起，提交到異步工作流進行實際的資源生產、以及容器的啓動等。即便是結合了多可用區和多規格的優化，異步工作流依然有可能失敗的，比如資源的爭搶、內網 ip 不足、啓動失敗等，這時候就需要將創建請求再次重回隊列，等待被重新調度生產。</p><p style="text-align:justify">我們目前對於<strong>故障處理策略</strong>：</p><p style="text-align:justify">1、失敗的任務會一直重試，但是我們會計算每個任務的執行週期，重試次數越多，執行週期越長，以達到退避效果。</p><p style="text-align:justify">2、優先級策略會考慮用戶級別、任務類型、任務上次失敗的原因等因素，優先級高的任務優先提交執行。</p><p style="text-align:justify">3、每次調度失敗的原因都會以標準事件的方式通知到 k8s 集羣。隊列裏的任務的整個執行流程的狀態機如下：</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-e09200aea1a82cf515617f061a7bb54a_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">所有執行失敗的任務都會重新進入隊列，等待被再次調度。由於任務會在任何一步失敗，所以所有生產出來的資源都會回滾，回滾結束後，進入初始狀態。初始狀態的任務會被拉起執行，然後提交到異步生產。如果生產失敗，就會再次回到等待調度的狀態。如果生產成功，任務就結束，到達終態。基於我們的重調度機制，可以極大的減少由於生產系統抖動造成實例生產失敗的情況，對於容器啓動成功率要求高的場景可以保障實例生產的最終一致性，對於容器啓動成功率要求不那麼嚴格的場景可以快速失敗，由上層業務進行處理。</p><p style="text-align:justify">服務容錯降級對於故障場景，系統依賴服務的降級也是十分重要的。大多數進行限流降級的方案主要關注點在服務的穩定性，當調用鏈路中某個資源依賴出現異常，例如，表現為 timeout，異常比例升高的時候，則對這個資源的調用進行限制，並讓請求快速失敗或返回預設靜態值，避免影響到其它的資源，最終產生雪崩的效果。ECI 目前實現了基於歷史日誌自學習進行無損降級、本地 cache 降級、流控降級 3 級降級機制框架，ECS/ECI openapi 全面接入，內部依賴 200+接口接入，根據每個接口的調用頻率、RT 分佈、超時時間設置來單獨分析，選擇合適的降級策略，設置合理的閥值，能讓系統出問題時，智能降級從而進行系統保護。<strong>一個典型的降級機制實現過程如圖：</strong></p><p style="text-align:center"><img src="https://pic1.zhimg.com/80/v2-1239ddaa0f4eab60e7996e6fec9fe364_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify"><strong>當有非資源類核心 API 新請求進入</strong>，如果歷史緩存數據未過期則直接返回緩存數據，結束業務邏輯反之則請求遠程接口。如果請求成功，返回數據，對數據進行緩存，同時將緩存數據以日誌方式存入 sls cache log 日誌用於未來降級，結束業務邏輯當遠程請求失敗時觸發降級策略：如果失敗指標（例如指定時間內異常比例）在預設時間窗口內未達到配置的降級策略閾值，則直接拋出相應業務異常，結束業務邏輯如達到降級策略閾值則按以下順序實行降級策略：從 sls 緩存日誌查找歷史日誌數據作為降級返回值，同時將返回值重新寫入緩存，結束業務邏輯如果 sls 緩存日誌沒有相應日誌則返回：預設靜態值或空值，結束業務邏輯對於一些跟用戶資源無關，更新少，屬於全局參數的服務/接口，以上通用降級策略和方案可能因為降級規則閾值難以界定而無法有效執行。</p><p style="text-align:justify">針對這些接口採用 dubbo 異常直接降級的策略涉及到降級或熔斷的條件：自動降級 (可選利用 Sentinal 進行自動降級)： 超時，異常，限流手動開關支持核心非資源 api 直接進行 openapi 本地降級 cache 對於嚴重的系統故障，可以將核心幾個 describe api 進行 openapi 本地 cache，發生故障，或有雪崩出現時，全部切到 openapi 本地 cache，在降級影響面的同時，也能減輕對下層服務的調用壓力來贏取恢復時間。</p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-0ce94b0355dad7e7cbc7463c70e8ad87_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify">依賴服務非創建鏈路 dubbo 或 http 請求進行本地 moc 對於幾乎不會頻繁變化的依賴服務，通過每日 sls 分析進行 kv 的存儲，當故障發生時，降級為備用，讓降級影響面趨向於 0。其他服務降級機制大分頁流控 cache 創建類 api 進行依賴 dubbo 或 http 服務降級，<strong>異步補償操作類 api 進行鏈路降級，</strong>取消非必需依賴數據庫降級 ro 庫流量降級隔離，用戶級別流量切到灰度 api 級別流量切到灰度/獨立線程池日誌 debug 及調用鏈路跟蹤使用 apicontext 實現詳細日誌 debug 及調用全鏈路跟蹤能力核心 api debug 日誌建設，支持按用戶開啓 debug 日誌打印 requestId 貫穿到 dao，支持隨時採樣，及時發現 dao 異常調用服務依賴降級容錯機制可以在保障服務穩定性的前提上，利用相關接口的歷史緩存數據，基於 SLS 日誌無損降級，當 SLS 無數據的時候也可以採用本地靜態數據兜底，構建有效返回值，在服務觸發流控降級熔斷後，大部分用戶不會感知到服務異常。</p><p style="text-align:justify">在內部的多次故障演練中，服務降級機制可以有效保護系統由於發生故障帶來的系統癱瘓。服務可用性保障在傳統的 Kubernetes 集羣中，如果 Node 變得不可用且達到時間閾值，那麼會將 Node 上的 Pod 進行驅逐，重新在其他 Node 上拉起新的 Pod。而在 Serverless 場景下，ECI 管控會通過異步檢測機制檢測不健康 ECI，修改狀態為不可用，同時增加導致不可用的事件，告知 ECI 用戶，之後 ECI 會通過主動運維的手段治癒不健康 ECI，之後觸發控制面將 ECI 恢復為 Ready 狀態，<strong>主要過程如圖所示：</strong></p><p style="text-align:center"><img src="https://pic4.zhimg.com/80/v2-790b2ccc925edb4964dede90b43256af_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify"><strong>處理不健康 ECI 的流程：</strong></p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-4e6eaa3bddc8488957271e480923d34a_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:justify"><strong>恢復 Ready ECI 流程預案&amp;壓測除了技術方面的保障，故障注入、應急預案、壓測演練在穩定性建設中也尤為重要。</strong>在雙十一活動期間我們內部進行了多次壓測演練，對系統中常見的性能瓶頸進行故障注入，用以模擬故障的發生，同時制定應急預案，以此應對故障已經發生時的場景。通過多次的壓測摸高，一方面可以評估系統容量的承載上限，另一方面可以藉此機會進行大規模壓測演練，驗證系統降級方案並對系統穩定性進行評估。預警&amp;監控大促進行時，預警和監控是保證系統運行時穩定性的重要措施。通過監控和預警可以及時發現系統故障，進而快速進行恢復。</p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-bfb30f596763e9976d35e307e15d74a2_720w.webp" referrerpolicy="no-referrer"></p><p style="text-align:center"><img src="https://pic3.zhimg.com/80/v2-c27bfedd2fd094067dc495bbdab8340a_720w.webp" referrerpolicy="no-referrer"></p><span id="OSC_h3_4"></span><h3>四、系統的健壯性</h3><p style="text-align:justify">思考沉澱一個健壯的系統不僅需要減少問題的發生，同時要具備故障發現以及故障快速恢復的能力。除了預警和監控，運維能力建設也十分重要。</p><p style="text-align:justify"><strong>一個系統的健壯性體現在系統的容量，</strong>系統的容錯能力以及系統依賴的各個資源的 sla，尤其是在雲上覆雜的資源環境下，由於「木桶效應」，某一項依賴資源的很可能造成整個系統的直接不可用。因此，隨着系統不斷完善，我們需要通過混沌工程等方法來找出當前系統的「弱點」進而對其進行專項優化，進而提升整個系統的健壯性；其二對於系統的故障恢復以及降級能力也很重要，歷史上 ECS/ECI 管控多次由於單用戶或系統某個環節變慢，導致系統全鏈路雪崩，最終導致 P1P2 故障，ECS/ECI 管控是阿里雲最複雜的管控系統，複雜的業務邏輯，內部系統依賴，非常多的環節出問題都有可能導致全鏈路某個應用雪崩進而全局不可用，因此，對於故障已經來臨時，依賴降級能力能非常有效的保護我們的系統，這也是穩定性建設的一個十分重要的方向。</p><span id="OSC_h3_5"></span><h3>五、總結</h3><p style="text-align:justify">未來展望隨着雙十一最後一波流量高峯結束，ECI 順利通過了對阿里人最嚴苛的技術考驗--雙十一，本文圍繞此次參與雙十一活動的經歷做出總結，希望可以為今後 ECI 穩定性方面的建設積累經驗，當然，這對 ECI 來説也僅僅是一步試金石，作為雲原生時代的基礎設施，ECI 任重而道遠，共勉！</p><p style="text-align:justify">本文出品及鳴謝： 柳密、羽雲、景奇、存誠、 煜楓、景止、皓瑜、月懸、佐井、尚哲、湧泉、十刀、 木名、秉辰、易觀、冬島、不物、瀟洛、 懷歡、 嘗君、寒亭、伯琰。</p><p style="text-align:justify"><strong><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdeveloper.aliyun.com%2Farticle%2F1389034%3Futm_content%3Dg_1000385342" target="_blank">原文鏈接</a></strong></p><p style="text-align:justify"><strong>本文為阿里雲原創內容，未經允許不得轉載。</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Wed, 06 Dec 2023 08:48:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/yunqi/blog/10319647</guid>
            <link>https://my.oschina.net/yunqi/blog/10319647</link>
            <author>
                <![CDATA[阿里云云棲號]]>
            </author>
        </item>
    </channel>
</rss>
