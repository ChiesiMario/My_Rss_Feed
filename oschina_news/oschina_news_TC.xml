<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[開源中國-最新資訊]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[開源中國-最新資訊 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Tue, 23 Jan 2024 03:12:25 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[驚訝！史上最年輕的 Apache Committer 誕生！！！]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-b74c559a04078fb6182d6af9815c351d338.png" referrerpolicy="no-referrer"></p><p>在 Apache StreamPark 被數以千計的星星和諸多榮譽的眷顧下，用戶越來越多，遍佈各個行業。這一切的背後都是因為有一幫熱愛開源，願意投身開源項目建設的貢獻者。這其中有一批 「00 後」 的新星正悄然在社區嶄露頭角，他們正用自己的實際行動書寫着自己的開源故事。Apache StreamPark 社區近日迎來了兩位 「00」 後 Committer，下面讓我們一睹兩位 Committer 的風采。</p><span id="OSC_h1_1"></span><h1><strong>新晉 Committer&nbsp;之張超篇</strong></h1><hr><p><img height="648" src="https://oscimg.oschina.net/oscnet/up-63b75bc7d4a38c12ff48b29d44c7cf3bbe6.png" width="1216" referrerpolicy="no-referrer"></p><p><br><span style="background-color:#ffffff; color:rgba(0, 0, 0, 0.9)">很開心能夠被提名為 Apache StreamPark 的 Committer！非常感謝支持社區的每一位小夥伴！也感謝參與項目時社區的各位技術大拿們給予的指導和幫助！</span></p><p><iframe frameborder="no" height="400px" scrolling="no" src="https://player.bilibili.com/player.html?aid=666424225&amp;bvid=BV1Ra4y1C7EE&amp;cid=1411725695&amp;p=1" width="720px" referrerpolicy="no-referrer"></iframe></p><span id="OSC_h4_2"></span><h4>&nbsp;</h4><span id="OSC_h4_3"></span><h4><strong><strong><strong><strong><strong>主要貢獻</strong></strong></strong></strong></strong></h4><ol><li><p>Apache StreamPark 進入 Apache 孵化器期間參與從 MySQL 到 H2 數據庫的適配</p></li><li><p>變更 Mybatis-Plus 配置方式，簡化配置文件配置項</p></li><li><p>完善用戶模塊刪除功能，部分前後端功能開發與適配，並追蹤後續討論、改進</p></li><li><p>參與整理、完成代碼風格和質量指南文檔，並參與相關代碼規範的推進工作</p></li><li><p>參與官網文檔的改進，完成中英文文檔編寫各 2 篇、翻譯 15&nbsp;篇以及其他文檔完善</p></li></ol><span id="OSC_h4_4"></span><h4><strong><strong><strong><strong><strong>結緣社區</strong></strong></strong></strong></strong></h4><p>對我來説，Apache 不僅僅是一個技術社區，它更像是一種信仰。在我還是個初學者的時候，提起 Apache Tomcat、Apache Maven 這樣的頂級項目，它們在我的心中就好像是編程世界的神話傳説。對於熱愛編程的我而言，Apache 代表着技術的極致、創新的精神，激勵着我不斷前行。想到有一天可能成為這個信仰的一份子，和全球的技術大牛們肩並肩，這種感覺，就像是夢想變成現實。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-7e1ecdc00c6b91bb4bab730f471cf204ac8.png" referrerpolicy="no-referrer"></p><p>我參與 Apache StreamPark 提交的第一個 PR 是處理客戶端發起 GET 請求時攜帶了 「    」問題，是在 2022 年 8 月 28 日提交的，只用了一行箭頭函數的代碼就解決！持續貢獻到現在也已經一年多了。</p><p>在我印象中，那時 Apache&nbsp;StreamPark 還叫 StreamX，整個團隊都在為加入 Apache 孵化器做準備，這時項目急需對 H2 數據庫進行適配，而我正好是 Mybatis-Plus 的開發者。PMC 成員華傑哥[1]邀請我一同參與，我立馬答應了下來，然後開始從零瞭解 Apache StreamPark。即刻懷揣着興奮的心情，連夜肝代碼。第一個挑戰是 Windows 下的開發環境配置，消耗九牛二虎之力，總算終於跑通了代碼！等我抬頭一看發現天都亮了——時間真會玩，就這麼偷偷溜走了: )。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-270fbabed9d20e29eabf0f3261a698241f2.jpg" referrerpolicy="no-referrer"></p><p>後面我在陸陸續續參與開發中也遇到很多問題，記得印象很深的一次是有一天晚上在進行 Apache StreamPark 前端開發時，遇到一個前端組件的 BUG，處理了半天無果，最後求助社區的 PPMC 成員思柱哥[2]。思柱哥不僅幫我解決了問題，還手把手教我原理，真的很感謝思柱哥！這也是我切身感受到參與開源的樂趣之一，有很多技術大拿願意帶着你成長。</p><p>今年八月，我有幸去北京參加了 Apache Community Over Code 2023[3]。跟 Apache 社區的大佬們近距離接觸，真是開眼界了。Apache Way 最讓我印象深刻的就是 「社區大於代碼」 這個理念。特別是聽了 Apache Pulsar PMC 成員小狐狸小姐姐的演講後，我就豁然開朗：開源項目不僅僅是代碼，也需要一個文檔來告訴你它怎麼運作。因此我主動請纓參與文檔的完善工作，幫助社區文檔更友好！</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-02f03dc61579e20ce426bef16ae9accec92.png" referrerpolicy="no-referrer"></p><p>我每天都會堅持寫一篇博客，哪怕沒什麼可寫的技術題材，我也會堅持隨便積累一點。順帶一提我在任何地方都可以打開電腦寫代碼，譬如咖啡廳、飛機上、地鐵上等，甚至我去韓國旅遊時也在隨便找的一家咖啡廳處理 issue。</p><span id="OSC_h4_5"></span><h4><strong>社區印象</strong></h4><p>社區的大佬們真的各顯神通，在 Apache&nbsp;StreamPark 社區裏經常有熱心技術大拿願意提供幫助。並且也進一步地學習到不少開發規範和實用技巧，就比如讓代碼 「一塵不染」 的工具 Spotless、以及 Vue3 的知識點掌握等！在 Apache&nbsp;StreamPark 有一個使用登記收集的 Issue，其中有一條特別引起了我的注意：來自 「袁隆平農業高科技股份有限公司」 的使用登記。這家公司的名字源於雜交水稻之父袁隆平老先生，我為身處的社區能服務於這樣一家公司感到超級自豪。參與開源真的是一件很酷的事情！</p><span id="OSC_h4_6"></span><h4><strong>寄，語</strong></h4><p>從 Apache&nbsp;StreamPark 進入 Apache 孵化器，到現在一週年已經過去啦，期間讓人感動的瞬間數不勝數！期待更多小夥伴的加入和陪伴，咱們一同見證其從孵化器畢業，進一步為實時領域發展作出貢獻！</p><span id="OSC_h1_7"></span><h1><strong>新晉 Committer&nbsp;之蔡燦材篇</strong></h1><hr><p><img height="648" src="https://oscimg.oschina.net/oscnet/up-8120852d62a802a07553d710655b8046808.png" width="1216" referrerpolicy="no-referrer"></p><p>非常榮幸被提名為 Apache&nbsp;StreamPark 的 Committer！感謝社區的信任和肯定！感謝歷次討論、設計、協作和 review 過程中社區開發者給予的指導和幫助！</p><span id="OSC_h4_8"></span><h4><strong><strong><strong><strong><strong>主要貢獻</strong></strong></strong></strong></strong></h4><ol><li><p>參與 Kubernetes V2 Operator 重構 streampark-flink-kubernetes 模塊</p></li><li><p>測試、修復 streamp-flink-kubernetes 模塊並補充單元測試</p></li><li><p>積極參與方案討論、規範制定和代碼 Review</p></li><li><p>修復並完善官網文檔</p></li></ol><span id="OSC_h4_9"></span><h4><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>結緣社區</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></h4><p>Apache&nbsp;StreamPark 算是我第一個正式參與的開源項目,與社區結緣主要是因為 StremaPark PPMC 成員餘林穎哥[4](Al-assad)。那時候我想加入開源社區，但又覺得自己實力太菜，所以一直沒付諸行動。後面 Al-assad 就問我要不要參與 Apache&nbsp;StreamPark 社區，説有很多工作可以做，那時候我還有點怕自己會拖後腿，但大佬説有他兜底。我也就抱着試一試態度去參加了。</p><p>剛進社區，我就接下了三個 feature（真是初生牛犢不怕虎)，真實情況是因為那三個 feature 是很類似的需求，我就想一個是做，兩個也是做，那就一起接過來算了。一開始環境搭建在社區的各位大佬的幫助下，也是花了一星期時間成功搭建好了，後面就開始漫長的 debug 之旅。</p><p>在 debug 過程中，我也發現 Apache&nbsp;StreamPark 的一些問題，提了一些 PR 進行修復。令我比較驚訝的是，我的每個 PR 社區都回復的很快，並提了很多改進的意見、讓我學到了很多，也讓我更有動力。印象比較深的是：在完成第一個 feature 時，我寫的代碼其實很醜陋，幾個 PMC 成員們非凡不嫌棄還提了很多修改意見、鼓勵我慢慢改進，最後還是林穎哥出手才讓我第一個 feature 成功完成，那時候感覺自己幫了倒忙，感覺很不好意思。但 Al-assad 鼓勵我説是新人是這樣的啦，多進行嘗試就好了。在對第一個 feature 進行認真 review 代碼後，我後面兩個 feature 也順利完成了（雖然也是磕磕絆絆）。還記得最後一個 feature 合併的時候，我的心情都激動得快哭了，原來我也能做到自己曾經大學可望而可不及的事情（開源之夏）。記得之前有人看某個開源項目的時候，跟我説，你看這個人和你和你差不多大已經完成了三個 feature 了，我那時候在想什麼時候我也能這麼牛叉就好了，沒想到過了沒多久我也成為了 「這樣的人」，哈哈。</p><p>一開始，我打算搞完這三個 feature 就不參與社區了，後面認識了社區很多優秀的開發者讓我改變了這個想法，Apache&nbsp;StreamPark 社區開發者們非常的團結友善，核心貢獻者們更是犧牲休息時間投入到項目中，知道這一點的時候我十分吃驚。所以在這裏你能認識一羣真正熱愛代碼的人，我十分感謝在 Apache&nbsp;StreamPark 社區遇到每一位開發者，他們教會了我很多。在這個社區，我彷彿意識到開源精神的某種含義。</p><span id="OSC_h4_10"></span><h4><strong>寄，語</strong></h4><p>當然是希望 Apache&nbsp;StreamPark 社區越來越好啦，畢竟是我參加開源以來第一個加入的社區，期待新的小夥伴能加入進來，一起建設社區。如果是新手也不用害怕，我也是新手不也成功加入進來了嗎。祝 Apache StreamPark 能夠早日孵化成功, 祝各位 Apache&nbsp;StreamPark 社區的小夥伴都玩得開心，have fun！</p><span id="OSC_h1_11"></span><h1>**&nbsp;加，入 我，們&nbsp;**</h1><p>Apache&nbsp;StreamPark 是一個流處理應用程序開發管理框架。初衷是讓流處理更簡單，旨在輕鬆構建和管理流處理應用程序，提供使用 Apache Flink 和 Apache Spark 編寫流處理應用程序的開發框架，未來將支持更多其他引擎。同時，Apache&nbsp;StreamPark 提供了一個流處理應用管理平台，核心能力包括但不限於應用開發、調試、交互查詢、部署、運維、實時數倉等，於 2022 年 9 月通過投票正式成為 Apache 開源軟件基金會的孵化項目。</p><p>Apache&nbsp;StreamPark 社區一直以來都以用心做好一個項目為原則，高度關注項目質量，努力建設發展社區。我們時刻保持開發者謙遜樸素的本質，認真學習和遵循「The Apache Way」，秉承更加兼容幷包的心態，迎接更多的機遇與挑戰。誠摯歡迎更多的貢獻者參與到社區建設中來，和我們一道攜手共建。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-fb1f818f6981c4a0f1366a2ec6f60a7252e.png" referrerpolicy="no-referrer"></p><p>💻 項目地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark" target="_blank">https://github.com/apache/streampark</a><br> 🧐 提交問題和建議：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fissues" target="_blank">https://github.com/apache/streampark/issues</a><br> 🥁 貢獻代碼：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fstreampark%2Fpulls" target="_blank">https://github.com/apache/streampark/pulls</a><br> 📮 Proposal：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FINCUBATOR%2FStreamPark%2BProposal" target="_blank">https://cwiki.apache.org/confluence/display/INCUBATOR/StreamPark+Proposal</a><br> 📧 訂閲社區開發郵件列表：**dev@streampark.apache.org&nbsp;<br> 💁‍♀️&nbsp;<strong>社區溝通：</strong><br><img alt="" src="https://oscimg.oschina.net/oscnet/up-d2fb1e3ea10be8a185704b27e79d83d2ad4.png" referrerpolicy="no-referrer"></p><p><strong>參考資料</strong><br> [1]&nbsp;<em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwolfboys" target="_blank">https://github.com/wolfboys</a></em><br> [2]&nbsp;<em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwangsizhu0504" target="_blank">https://github.com/wangsizhu0504</a></em><br> [3]&nbsp;<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcommunityovercode.org" target="_blank">https://communityovercode.org</a><br> [4]&nbsp;<em><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FAl-assad" target="_blank">https://github.com/Al-assad</a></em></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:30:22 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/streampark/blog/10890500</guid>
            <link>https://my.oschina.net/streampark/blog/10890500</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[SQLite 3.45 發佈，添加對 JSONB 格式支持]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>SQLite 3.45 <u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.sqlite.org%2Fchanges.html%23version_3_45_0" target="_blank">已發佈</a></u>。</p><blockquote><p>SQLite 是一個 C 語言庫，實現了一個小型、快速、獨立、高可靠性、全功能的 SQL 數據庫引擎。SQLite 是世界上使用最多的數據庫引擎。SQLite 的源代碼屬於公共領域，每個人都可以免費使用，用於任何目的。</p></blockquote><p>新版本重寫了所有 JSON 函數，改為新的內部解析樹格式 JSONB。JSONB 格式可以序列化並存儲在數據庫中，避免在使用 JSON 值時，還需要進行不必要的重複解析。因此通過支持 JSONB，SQLite 在處理 JSON 數據時能夠更高效靈活。</p><p>此外，JSONB 保留所有舊功能，並且所有 JSON 函數也可以接受 JSONB 二進制內容。</p><p>SQLite 承諾其生成的 JSONB 格式良好，但是由於 JSONB 本質上就是一個 BLOB，因此可能存在格式不正確的 JSONB，而 SQLite 在處理格式不正確的 JSONB，就可能出現不同行為，從而發生錯誤。不過 SQLite 也保證，格式錯誤的 JSONB 永遠不會導致內存錯誤或是類似問題，也不會出現系統漏洞。</p><p>&nbsp;</p><p>詳情查看&nbsp;<em><u><a href="https://www.oschina.net/news/269878/sqlite-lands-jsonb" target="_blank">SQLite 重寫了 JSON 函數 —— 推出 JSONB</a></u></em></p></div>
                                    ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:27:22 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276404/sqlite-3-45-released</guid>
            <link>https://www.oschina.net/news/276404/sqlite-3-45-released</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2024-1 月 | WHLUG（武漢 Linux 用戶組）活動回顧，這是沒有過的全新版本]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div><div><div><p style="margin-left:0; margin-right:0">內容來源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Findex%2Fzh" target="_blank">deepin 社區</a></p><hr><p style="margin-left:0; margin-right:0">2024 年 1 月 20 日下午，由 deepin（深度）社區聯合 Linux 中國、龍蜥社區（OpenAnolis）、華中科技大學網絡空間安全學院開源俱樂部一起舉辦的 Linux 愛好者線下沙龍活動（WHLUG）在武漢未來科技城成功舉辦。</p></div></div><p style="margin-left:0; margin-right:0">本次活動聚集了近 40 名來自武漢地區的 Linux 愛好者線下參與，大家共同探討技術話題，分享技術經驗和見解，現場氣氛熱烈。現在就讓我們一起來回顧本次活動的精彩瞬間吧！</p></div><p style="margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="611" src="https://storage.deepin.org/thread/202401220819117663_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059112961557.png" width="825" referrerpolicy="no-referrer"></p><div><div>
  來自 deepin 團隊的研發工程師 Black desk 在會議上與參會者分享了一些 Linux 內核提供的容器相關特性的用戶態接口使用方法、非 root 用戶使用 Linux 容器的限制，和其他有趣的 Linux 容器技術細節，同時也深入介紹了玲瓏方案產生的背景、希望去解決的問題，以及玲瓏的優勢和目前存在的問題。
 </div></div><div>
 &nbsp;
</div><div><img alt="" height="603" src="https://storage.deepin.org/thread/202401220820237378_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113054411.png" width="827" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div>
   開源已經邁入了「新生代」，我們需要資深的開發者，也需要更張揚的年輕人。在本次分享中，來自華中科技大學開放原子開源俱樂部的慕冬亮老師，便從「領路人」的視角，為大家分享了他是如何帶領學生走進 Linux 內核的世界，一同挖掘、修復 Linux 內核漏洞，並通過內部審核機制保障內核補丁正確性的故事。
  </div></div></div><div>
 &nbsp;
</div><div>
 &nbsp;
</div><div><img alt="" height="617" src="https://storage.deepin.org/thread/202401220820504191_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113182099.png" width="825" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div>
   那從學生的視角來看，開源參與又會有哪些不一樣呢？來自華中科技大學的朱儁星同學結合自身從 0-1 參與開源的經歷，為大家介紹了他是如何從「發佈項目無人問津、參與項目無從下手」的小白，逐步深入探索、找到組織，並進階成為開源項目的核心貢獻者的。他強調，「開源不僅僅是開放源代碼，更是一種分享合作、共同進步的精神」，希望大家能夠拋開顧慮，勇敢嘗試。
  </div></div></div><div>
 &nbsp;
</div><div><img alt="" height="461" src="https://storage.deepin.org/thread/202401220821331561_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113277879.png" width="826" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div>
   虛擬網卡是雲計算重要組成部分，目前正面臨着性能和功能的雙重挑戰。在本次活動中，來自龍蜥社區高性能網絡 SIG 的成員衡琪，向大家分享了 Anolis OS 是如何通過優化 Virtio 協議來增強網絡性能，從而解決這一挑戰。衡琪在分享中提到了一些具體的優化措施，如制定新的 Virtio 標準，包括 Inner Hash、Virtio Checksum 修復、NetDIM、Device Stats 等技術。
  </div></div><div>
  &nbsp;
 </div></div><div><img alt="" height="615" src="https://storage.deepin.org/thread/202401220822105140_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113375653.png" width="827" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div><div><div><p style="margin-left:0; margin-right:0">除了主題演講之外，來自現場愛好者的閃電演講的分享，也贏得了滿堂喝彩。</p><p style="margin-left:0; margin-right:0">鍾同學為大家分享了自己在 Treeland 及窗管調優方面的經驗與探索，向大家展示了他是如何基於 Treeland 進行窗管深度客製化，以及在三個月內打造的台前調度。這個分享引發了現場觀眾的興趣和探討，大家對於鍾同學的經驗和成果表示讚賞，並積極參與了與之相關的交流和討論。</p></div></div></div><div><img alt="" height="463" src="https://storage.deepin.org/thread/202401220822396312_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113493557.png" width="828" referrerpolicy="no-referrer"></div><div>
 &nbsp;
</div><div>
 &nbsp;
</div><div><div><div><p style="margin-left:0; margin-right:0">桑同學結合自己的工作和大家分享了有關路由器和無線網卡的相關知識，並講述了在工作中遇到的一些有趣的事情。其中，當提到客戶經常將需求誤認為 BUG 並直接反饋給技術同學時，更是引發了現場從事 B 端業務的研發同學們的共鳴，並引發了一系列討論和交流。</p><p style="margin-left:0px; margin-right:0px; text-align:center"><img alt="" height="608" src="https://storage.deepin.org/thread/202401220823104278_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059113564231.png" width="822" referrerpolicy="no-referrer"></p><p style="margin-left:0; margin-right:0">&nbsp;</p></div></div></div><div><div><div><h1><strong>網上相會，何如線下相見</strong></h1><p style="margin-left:0; margin-right:0">WHLUG 的創辦希望幫助大家在武漢遇到同為 Linux 愛好者的小夥伴，一起在某個週末，交流技術，學習技術。基本的形式如下：</p></div></div></div><div><img alt="" height="217" src="https://storage.deepin.org/thread/202401220823526444_%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17059114451088.png" width="807" referrerpolicy="no-referrer"></div><div><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">&nbsp;</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start">我們熱切的希望能夠有更多武漢當地的學校、企業、開源愛好者加入我們，共同參與到 WHLUG 活動的策劃和組織中。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:start"><strong>所以，我們會面向核心成員招募組織人，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwj.qq.com%2Fs2%2F13816139%2F8b53%2F" target="_blank">如果您感興趣，請報名。</a>期待與您一起共建！</strong></p><p style="text-align:center"><img height="271" src="https://oscimg.oschina.net/oscnet/up-07d49751e8486d624c7c08f7166f237fa1c.png" width="276" referrerpolicy="no-referrer"></p><hr><p><strong>內容來源：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2F2024-01-whlug%2F" target="_blank">deepin 社區</a></strong></p><p><strong>瞭解 WHLUG（武漢 Linux 用戶組）：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fwelcome-to-whlug%2F" target="_blank">https://www.deepin.org/zh/welcome-to-whlug/</a></strong></p><p><strong>往期 WHLUG 活動回顧：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.deepin.org%2Fzh%2Fcategory%2Fwhlug-news%2F" target="_blank">https://www.deepin.org/zh/category/whlug-news/</a></strong></p></div></div>
                                    ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:12:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276400</guid>
            <link>https://www.oschina.net/news/276400</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Agents-Flex —— 大語言模型應用開發框架]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="margin-left:0px; margin-right:0px; text-align:left">Agents-Flex 是一個優雅的 LLM（大語言模型） 應用開發框架。其使用 Java 開發，對標 LangChain。</p><h2 style="margin-left:0; margin-right:0; text-align:left">基礎能力</h2><ul><li>LLM 的訪問能力</li><li>Prompt、Prompt Template 定義加載的能力</li><li>Function Calling 定義、調用和執行等能力</li><li>Embedding</li><li>Vector Storage</li><li>豐富的內容加載器</li><li>豐富的文本分割器</li><li>LLM Chain</li><li>Agents Chain</li></ul><h2 style="margin-left:0; margin-right:0; text-align:left">簡單對話</h2><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">使用 OpenAi 大語言模型:</p><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">OpenAiConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">OpenAiConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"sk-rts5NF6n*******"</span><span>);</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">OpenAiLlm</strong><span>(</span><span>config</span><span>);</span></span><span><strong style="color:#445588">Prompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SimplePrompt</strong><span>(</span><span style="color:#dd2200">"請寫一個關於小兔子戰勝大灰狼的故事。"</span><span>);</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>llmInstance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"---&gt;"</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><strong style="color:#445588">Thread</strong><span>.</span><span style="color:#008080">sleep</span><span>(</span><span style="color:#009999">10000</span><span>);</span></span><span><span>}</span></span></pre><div style="text-align:center">&nbsp;</div></div></div><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">使用 「通義千問」 大語言模型:</p><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">QwenLlmConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">QwenLlmConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"sk-28a6be3236****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setModel</span><span>(</span><span style="color:#dd2200">"qwen-turbo"</span><span>);</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">QwenLlm</strong><span>(</span><span>config</span><span>);</span></span><span><strong style="color:#445588">Prompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SimplePrompt</strong><span>(</span><span style="color:#dd2200">"請寫一個關於小兔子戰勝大灰狼的故事。"</span><span>);</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>llmInstance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"---&gt;"</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><strong style="color:#445588">Thread</strong><span>.</span><span style="color:#008080">sleep</span><span>(</span><span style="color:#009999">10000</span><span>);</span></span><span><span>}</span></span></pre><div style="text-align:center">&nbsp;</div></div></div><p style="color:#40485b; margin-left:0; margin-right:0; text-align:left">使用 「訊飛星火」 大語言模型:</p><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">SparkLlmConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlmConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setAppId</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiSecret</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlm</strong><span>(</span><span>config</span><span>);</span></span><span><strong style="color:#445588">Prompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SimplePrompt</strong><span>(</span><span style="color:#dd2200">"請寫一個關於小兔子戰勝大灰狼的故事。"</span><span>);</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>llmInstance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"---&gt;"</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><strong style="color:#445588">Thread</strong><span>.</span><span style="color:#008080">sleep</span><span>(</span><span style="color:#009999">10000</span><span>);</span></span><span><span>}</span></span></pre><div style="text-align:center">&nbsp;</div></div></div><h2 style="margin-left:0; margin-right:0; text-align:left">歷史對話示例</h2><div style="text-align:left"><div><pre><span><strong>public</strong><strong>static</strong><strong style="color:#445588">void</strong><strong style="color:#990000">main</strong><span>(</span><strong style="color:#445588">String</strong><span>[]</span><span>args</span><span>)</span><strong>throws</strong><strong style="color:#445588">InterruptedException</strong><span>{</span></span><span><strong style="color:#445588">SparkLlmConfig</strong><span>config</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlmConfig</strong><span>();</span></span><span><span>config</span><span>.</span><span style="color:#008080">setAppId</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiKey</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span>config</span><span>.</span><span style="color:#008080">setApiSecret</span><span>(</span><span style="color:#dd2200">"****"</span><span>);</span></span><span><span style="color:#888888">// 創建一個大模型</span></span><span><strong style="color:#445588">Llm</strong><span>llm</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">SparkLlm</strong><span>(</span><span>config</span><span>);</span></span><span><span style="color:#888888">//創建一個歷史對話的 prompt</span></span><span><strong style="color:#445588">HistoriesPrompt</strong><span>prompt</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">HistoriesPrompt</strong><span>();</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"您想問什麼？"</span><span>);</span></span><span><strong style="color:#445588">Scanner</strong><span>scanner</span><span>=</span><strong style="color:#000000">new</strong><strong style="color:#445588">Scanner</strong><span>(</span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">in</span><span>);</span></span><span><span style="color:#888888">//等待用戶從控制枱輸入問題</span></span><span><strong style="color:#445588">String</strong><span>userInput</span><span>=</span><span>scanner</span><span>.</span><span style="color:#008080">nextLine</span><span>();</span></span><span><strong style="color:#000000">while</strong><span>(</span><span>userInput</span><span>!=</span><strong>null</strong><span>){</span></span><span><span>prompt</span><span>.</span><span style="color:#008080">addMessage</span><span>(</span><strong style="color:#000000">new</strong><strong style="color:#445588">HumanMessage</strong><span>(</span><span>userInput</span><span>));</span></span><span><span style="color:#888888">//向大模型提問</span></span><span><span>llm</span><span>.</span><span style="color:#008080">chat</span><span>(</span><span>prompt</span><span>,</span><span>(</span><span>instance</span><span>,</span><span>message</span><span>)</span><span>-&gt;</span><span>{</span></span><span><strong style="color:#445588">System</strong><span>.</span><span style="color:#008080">out</span><span>.</span><span style="color:#008080">println</span><span>(</span><span style="color:#dd2200">"&gt;&gt;&gt;&gt; "</span><span>+</span><span>message</span><span>.</span><span style="color:#008080">getContent</span><span>());</span></span><span><span>});</span></span><span><span style="color:#888888">//繼續等待用戶從控制枱輸入內容</span></span><span><span>userInput</span><span>=</span><span>scanner</span><span>.</span><span style="color:#008080">nextLine</span><span>();</span></span><span><span>}</span></span><span><span>}</span></span></pre></div></div></div>
                                                                ]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:10:30 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/agents-flex</guid>
            <link>https://www.oschina.net/p/agents-flex</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推薦 | 雲原生量子計算平台，量易伏]]>
            </title>
            <description>
                <![CDATA[<p>簡體中文 | <a href="https://gitee.com/baidu/qcompute/blob/master/README.md">English</a></p><h1><a id="user-content-量易伏---qcomputesdk" class="anchor" href="https://gitee.com/baidu/qcompute#%E9%87%8F%E6%98%93%E4%BC%8F---qcomputesdk"></a>量易伏 - QComputeSDK</h1><ul><li><a href="https://gitee.com/baidu/qcompute#%E7%89%B9%E8%89%B2">特色</a></li><li><a href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4">安裝步驟</a><ul><li><a href="https://gitee.com/baidu/qcompute#%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE">環境設置</a></li><li><a href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85-QComputeSDK">安裝 QComputeSDK</a></li><li><a href="https://gitee.com/baidu/qcompute#%E8%BF%90%E8%A1%8C">運行</a></li><li><a href="https://gitee.com/baidu/qcompute#%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0">重大更新</a></li></ul></li><li><a href="https://gitee.com/baidu/qcompute#%E5%85%A5%E9%97%A8%E4%B8%8E%E5%BC%80%E5%8F%91">入門與開發</a><ul><li><a href="https://gitee.com/baidu/qcompute#%E6%A1%88%E4%BE%8B%E5%85%A5%E9%97%A8">案例入門</a></li><li><a href="https://gitee.com/baidu/qcompute#API-%E6%96%87%E6%A1%A3">API 文檔</a></li><li><a href="https://gitee.com/baidu/qcompute#%E5%BC%80%E5%8F%91">開發</a></li></ul></li><li><a href="https://gitee.com/baidu/qcompute#%E4%BA%A4%E6%B5%81%E4%B8%8E%E5%8F%8D%E9%A6%88">交流與反饋</a></li><li><a href="https://gitee.com/baidu/qcompute#%E4%BD%BF%E7%94%A8-qcomputesdk-%E7%9A%84%E5%B7%A5%E4%BD%9C">使用 QComputeSDK 的工作</a></li><li><a href="https://gitee.com/baidu/qcompute#faq">FAQ</a></li><li><a href="https://gitee.com/baidu/qcompute#copyright-and-license">Copyright and License</a></li></ul><p><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2F">Quantum Leaf (量易伏)</a> 是<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum.baidu.com%2F">百度量子計算研究所</a>旗下全球首個雲原生量子計算平台。用戶可以使用量易伏進行量子編程，量子模擬和運行真實量子計算機。量易伏旨在為量子基礎設施即服務 (Quantum infrastructure as a Service, QaaS) 提供量子基礎開發環境。</p><p><img src="https://release-data.cdn.bcebos.com/github-qleaf%2F%E9%87%8F%E6%98%93%E4%BC%8F%E5%9B%BE%E6%A0%87.png" alt="" referrerpolicy="no-referrer"></p><p><a href="https://gitee.com/baidu/qcompute/blob/master/LICENSE"><img src="https://img.shields.io/badge/license-Apache%202.0-green" alt="" referrerpolicy="no-referrer"></a><img src="https://img.shields.io/badge/build-passing-green" alt="" referrerpolicy="no-referrer"><img src="https://img.shields.io/badge/Python-3.9--3.11-blue" alt="" referrerpolicy="no-referrer"><img src="https://img.shields.io/badge/release-v3.3.0-blue" alt="" referrerpolicy="no-referrer"></p><p><img src="https://img.shields.io/badge/OS-MacOS%20%7C%20Windows%20%7C%20Linux-lightgrey.svg?style=flat-square" alt="" referrerpolicy="no-referrer"></p><p>本安裝包是 QComputeSDK 的 Python 語言實現的全量量子開源計算框架。它採用經典量子混合的編程模式並預製多種先進模塊，用戶不僅可以在量子環境對象 (QEnv) 下快速搭建電路，也可以將它用於各類複雜量子算法的研發。QComputeSDK 內置多類本地高性能模擬器和雲端模擬器/真機調用接口，用戶可以將電路在本地模擬器快速模擬驗證也可以將電路任務進一步提交至雲端真實量子硬件（超導、離子阱）以及高性能模擬器執行。</p><h2><a id="user-content-特色" class="anchor" href="https://gitee.com/baidu/qcompute#%E7%89%B9%E8%89%B2"></a>特色</h2><ul><li>輕鬆上手
<ul><li>近 50 篇教程案例，還在不斷地增加</li><li>量子電路本地可視化</li><li>全自動調用相關計算模塊，完成預訂流程</li></ul></li><li>功能豐富
<ul><li>支持電路嵌套的量子子程序功能</li><li>本地高性能模擬器支持 32 量子比特的模擬運算</li><li>雲端高性能異構模擬器支持更大規模量子模擬</li><li>支持多種噪聲模型的模擬</li><li>基於英偉達 cuQuantum 的本地 GPU 模擬器</li><li>基於 Gaussian/Fork 態的本地光量子模擬器</li></ul></li><li>真實量子算力
<ul><li>接入百度自研超導量子計算機 QPUQian</li><li>接入中科院精密測量院離子阱量子計算機 IonAPM</li><li>接入中科院物理所超導量子計算機 IoPCAS</li></ul></li></ul><h2><a id="user-content-安裝步驟" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4"></a>安裝步驟</h2><h3><a id="user-content-環境設置" class="anchor" href="https://gitee.com/baidu/qcompute#%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE"></a>環境設置</h3><p>推薦使用 Anaconda 創建虛擬環境，</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">conda create <span class="nt">-n</span> qcompute_env <span class="nv">python</span><span class="o">=</span>3.10</span><span id="LC2" class="line">conda activate qcompute_env</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><blockquote><p>Anaconda 請從<a href="https://gitee.com/link?target=https%3A%2F%2Fwww.anaconda.com%2Fdownload%23downloads">官網下載</a></p></blockquote><blockquote><p>注意： 無論使用 Anaconda 還是原生 Python ，Python 版本都應 &gt;= 3.9</p></blockquote><h3><a id="user-content-安裝-qcomputesdk" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%AE%89%E8%A3%85-qcomputesdk"></a>安裝 QComputeSDK</h3><p>通過 <code>pip</code> 完成安裝，</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pip <span class="nb">install </span>qcompute</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>用戶也可以選擇下載全部文件後進行本地安裝。我們推薦此種方式安裝以及二次 SDK 開發，可以方便的形成本地開發閉環，更方便調試等動作。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone https://github.com/baidu/QCompute.git</span><span id="LC2" class="line"><span class="nb">cd </span>QCompute</span><span id="LC3" class="line">pip <span class="nb">install</span><span class="nt">-e</span><span class="nb">.</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-運行" class="anchor" href="https://gitee.com/baidu/qcompute#%E8%BF%90%E8%A1%8C"></a>運行</h3><p>如果用戶選擇下載全部文件，現在可以試着運行一段程序來驗證是否安裝成功。這裏我們運行 QComputeSDK 提供的測試腳本，</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python <span class="nt">-m</span> Test.PostInstall.PostInstall_test</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>該腳本中包括執行本地與雲端任務測試，雲端測試前需要在命令行輸入用戶 Token ，Token 可登陸<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Ftoken">量易伏官網</a>查看。如不需要做雲端測試可運行 <code>Ctrl+c</code> 結束。</p><blockquote><p>注意：通過 pip 安裝請跳過此步。</p></blockquote><h3><a id="user-content-重大更新" class="anchor" href="https://gitee.com/baidu/qcompute#%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0"></a>重大更新</h3><p>從 QComputeSDK 3.0.0 版本開始，開發者可以通過 QComputeSDK 運行百度自研超導量子計算機乾始。設備定期對外提供服務，可以從<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fservices">量易伏真機詳情頁</a>查看真機開放時間。該設備在 QComputeSDK 中的後端名為<code>CloudBaiduQPUQian</code>.</p><h2><a id="user-content-入門與開發" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%85%A5%E9%97%A8%E4%B8%8E%E5%BC%80%E5%8F%91"></a>入門與開發</h2><h3><a id="user-content-案例入門" class="anchor" href="https://gitee.com/baidu/qcompute#%E6%A1%88%E4%BE%8B%E5%85%A5%E9%97%A8"></a>案例入門</h3><p>QComputeSDK 是一個實現後台接入真實量子硬件的量子計算開發框架。建立起了量子計算與量子硬件的橋樑，為量子算法和應用的研發落地提供強有力的支撐，也提供了豐富的案例供開發者學習。</p><p>在這裏，我們提供了初級、中級、高級案例供大家學習。初級案例中展示了使用 QComputeSDK 可以快速上手的簡單示例，包括量子態製備、經典量子混合編程、以及將電路任務提交到量子計算機上執行等。中級案例中是 QComputeSDK 的進階用法，包括模塊的使用、內置轉換器的使用等。高級案例中則是進階量子算法在 QComputeSDK 上的實現示例，我們為這些算法都配套了詳細的教程文檔。建議用戶下載 QComputeSDK 全部文件安裝，本地運行進行實踐。</p><ul><li><p><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1">初級案例</a></p><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/GHZ_Local.py">GHZ 態製備（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/GHZ_Cloud.py">GHZ 態製備（雲端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/HCNOT_Local.py">貝爾態製備（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/HCNOT_Cloud.py">貝爾態製備（雲端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Hybrid_Local.py">經典量子混合語言示例（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Hybrid_Cloud.py">經典量子混合語言示例（雲端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Interact_Local.py">經典量子信息交互示例（本地）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Interact_Cloud.py">經典量子信息交互示例（雲端）</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/QPUCase_BaiduQPUQian.py">百度量子自研超導真機運行示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/QPUCase_IonAPM.py">中科院精密測量院離子阱運行示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/QPUCase_IoPCAS.py">中科院物理所超導真機運行示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/PhotonicFookCase_local.py">基於 Fock 態的光量子線路模擬</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/PhotonicGaussianCase_local.py">基於 Gaussian 態的光量子線路模擬</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/SimulatorCase_CuQuantum_Local.py">基於 cuQuantum 的 GPU 模擬器</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Ubqc.py">盲量子計算示例</a></li></ol><ul><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise">量子噪聲模擬</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/AddNoise.py">對電路添加噪聲示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/CompressNoiseTest.py">量子噪聲壓縮模塊示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/OneQubitNoiseTest.py">一量子位電路含噪模擬</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/ParallelNoiseSimulationTest.py">多進程並行的噪聲模擬</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/Noise/TwoQubitNoiseTest.py">兩量子位電路含噪模擬</a></li></ol></li></ul></li><li><p><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2">中級案例</a></p><ul><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/0_OutputFormatControl">輸出信息設置</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/0_OutputFormatControl/Turorials/OutputFormatControl_ZH.md">教程</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/tutorials/machine_learning/QClassifier_CN.ipynb">結果打印信息設置示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/tutorials/machine_learning/VSQL_CN.ipynb">輸出文件自動清理示例</a></li></ol></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules">通用模塊</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/Tutorials/OpenModules_ZH.md">教程</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/0_OpenModules.py">模塊使用示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/1_InverseCircuitModule.py">量子電路逆操作模塊</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/2_ReverseCircuitModule.py">量子電路反操作模塊</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/3_UnrollProcedureModule.py">子程序展開模塊</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/4_UnrollCircuitModule.py">量子門分解模塊</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/1_OpenModules/5_CompressGateModule.py">量子門壓縮模塊</a></li></ol></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors">轉換器</a><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/Tutorials/OpenConvertors_ZH.md">教程</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/0_Circuit.py">電路序列化示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/1_DrawConsole.py">終端繪圖示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/2_InternalStruct.py">電路序列和反序列示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/3_JSON.py">電路轉 JSON 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/4_QASM.py">電路轉 QASM 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/5_Qobj.py">電路轉 QOBJ 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/6_IonQ.py">電路轉 IonQ 示例</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_2/2_OpenConvertors/7_XanaduSF.py">電路轉 Xanadu 示例</a></li></ol></li></ul></li><li><p><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3">高級案例</a></p><ol><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/0_SuperdenseCoding/Tutorial-Superdense/Superdense_CN.md">量子超密編碼</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/1_Deutsch-Jozsa/Tutorial-DJ/Deutsch-Jozsa_CN.md">Deutsch-Jozsa 算法</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/2_PhaseEstimation/Tutorial-phase/Phase_CN.md">量子相位估計 (QPE)</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/3_Grover/Tutorial-Grover/Grover_CN.md">格羅弗搜索算法</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/4_ShorAlgorithm/tutorial/Shor_CN.md">Shor 算法</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/5_VQE/Tutorial-VQE/VQE_CN.md">變分量子基態求解器 (VQE)</a></li><li><a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_3/6_VQSD/Tutorial-VQSD/VQSD_CN.md">變分量子態對角化 (VQSD)</a></li></ol></li></ul><p>在最近的更新中，QComputeSDK 加入了本地光量子計算模擬器 (LocalBaiduSimPhotonic) 。與傳統的量子電路模型不同，光量子計算具有其獨特的運行方式。QComputeSDK 在架構上支撐起光學體系，也成為了首個集成通用量子計算與光量子計算雙體系的量子開發套件。感興趣的讀者請參見<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fpqs%2Ftutorial-introduction">光量子計算模擬器教程</a>。</p><h3><a id="user-content-api-文檔" class="anchor" href="https://gitee.com/baidu/qcompute#api-%E6%96%87%E6%A1%A3"></a>API 文檔</h3><p>瞭解更多 QComputeSDK 使用方法，請參考 <a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fdocs%2Fqcompute%2Flatest%2F">API 文檔</a>，包含了供用戶使用的所有函數和類的詳細説明與用法。</p><h3><a id="user-content-開發" class="anchor" href="https://gitee.com/baidu/qcompute#%E5%BC%80%E5%8F%91"></a>開發</h3><p>QComputeSDK 中包括量子計算架構、量子模擬器、量子案例以及擴展功能等。對於需要涉及架構或模擬器源碼的開發者，建議下載全部文件並本地安裝調試。對於使用 QComputeSDK 研發算法應用的開發者或科研工作者，建議以 <a href="https://gitee.com/baidu/qcompute/blob/master/Example/Level_1/GHZ_Cloud.py">GHZ_Cloud.py</a> 作為代碼框架，修改和使用這個文件可以有效幫助熟悉本量子開發套件的語法。建議開發者熟悉 QComputeSDK 的電路模型構造，注意量子位輸出順序為高位。</p><h2><a id="user-content-交流與反饋" class="anchor" href="https://gitee.com/baidu/qcompute#%E4%BA%A4%E6%B5%81%E4%B8%8E%E5%8F%8D%E9%A6%88"></a>交流與反饋</h2><ul><li>我們非常歡迎您提交問題、報告與建議，您可以通過以下渠道反饋
<ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbaidu%2FQCompute%2Fissues">GitHub Issues</a> / <a href="https://gitee.com/baidu/qcompute/issues">Gitee Issues</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Ffeedback">量易伏官網-意見反饋</a></li><li>量易伏官方郵箱 <a href="mailto:quantum@baidu.com">quantum@baidu.com</a></li></ul></li><li>技術交流 QQ 羣：1147781135，歡迎掃碼進羣</li></ul><p><img src="https://release-data.cdn.bcebos.com/github-qleaf%2Fqrcode.png" alt="" referrerpolicy="no-referrer"></p><h2><a id="user-content-使用-qcomputesdk-的工作" class="anchor" href="https://gitee.com/baidu/qcompute#%E4%BD%BF%E7%94%A8-qcomputesdk-%E7%9A%84%E5%B7%A5%E4%BD%9C"></a>使用 QComputeSDK 的工作</h2><p>我們非常歡迎開發者使用 QComputeSDK 進行量子應用研發，如果您的工作有使用 QComputeSDK，也非常歡迎聯繫我們。以下為基於 QComputeSDK 開發的量子應用：</p><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqep%2Ftutorial-overview">量噪 (QEP, Quantum Error Processing)</a>，百度量子計算研究所研發的量子噪聲處理工具集，主要功能包括量子性能評估、量子噪聲刻畫、量子噪聲緩釋和量子糾錯。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fbqc%2Ftutorial-bqc">盲量子計算 (UBQC, Universal Blind Quantum Computation)</a>，百度量子計算研究所研發的基於 UBQC 協議的盲計算代理服務。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqapp%2Ftutorial-overview">QAPP</a> 是基於 QComputeSDK 開發的量子計算解決方案工具集，提供包括量子化學、組合優化、機器學習在內的諸多領域問題的量子計算求解服務。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqsvt%2Ftutorial-overview">量子奇異值變換 (QSVT, Quantum Singular Value Transformation)</a>，百度量子計算研究所研發的量子奇異值變換工具集，主要功能包括量子奇異值變換，對稱量子信號處理，以及哈密頓量模擬。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqfinance%2Ftutorial-option-pricing">量子金融 QFinance</a> ，百度量子計算研究所研發的量子金融庫，提供用於期權定價的量子蒙特卡羅方法。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fpqs%2Ftutorial-introduction">光量子計算模擬器 (PQS, Photonic Quantum Simulator)</a>，百度研究院量子計算研究所研發的光量子計算模擬器，支持基於 Gaussian 態和 Fock 態的光量子線路模擬。</li></ul><h2><a id="user-content-faq" class="anchor" href="https://gitee.com/baidu/qcompute#faq"></a>FAQ</h2><ol><li><p>問：<strong>使用 QComputeSDK 可以做什麼？它有哪些應用場景？</strong></p><p>答：QComputeSDK 是一個基於 Python 的量子計算開發框架，可以用於構建、運行和優化量子算法。我們在 QComputeSDK 建設了全面且完善的基礎設施用於支持各類量子算法的實現，因此在量子應用的研發上它具有廣泛的應用場景。具體工作可以參考但不限於 QComputeSDK 中的<a href="https://gitee.com/baidu/qcompute/blob/master/Extensions">擴展功能</a>。</p></li><li><p>問：<strong>想用 QComputeSDK 做量子編程，但對量子計算不是很瞭解，該如何入門？</strong></p><p>答：Nielsen 和 Chuang 所著的《量子計算與量子信息》是量子計算領域公認的經典入門教材。建議讀者首先學習這本書的第一、二、四章，介紹了量子計算中的基本概念、數學和物理基礎、以及量子電路模型。讀者也可以在<a href="https://gitee.com/link?target=https%3A%2F%2Fqulearn.baidu.com%2F">量易簡</a>上學習，這是一個在線量子學習知識庫，不僅包含量子計算教程，還有豐富的視頻課程。讀者還可以下載<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqmobile">量易伏 APP</a>，APP 上的量子小調包含豐富有趣的量子樣例，幫助讀者隨時隨地的學習。</p></li><li><p>問：<strong>QComputeSDK 是否免費？</strong></p><p>答：QComputeSDK 是免費的。QComputeSDK 是開源 SDK 並攜帶多類本地模擬器，用戶執行本地模擬任務是免費的。當用戶通過 QComputeSDK 將任務提交給雲端模擬器或真機運行時，會扣除一定點數。詳細的扣點規則可以參考<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2FquickGuide%2Faccount">用戶指南</a>。用戶在創建賬戶時我們會贈送點數，點數餘額可以在<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fprofile">個人中心</a>查看。</p></li><li><p>問：<strong>點數不足怎麼辦？</strong></p><p>答：點數目前僅用於資源控制。點數不足時可以從量易伏官網的<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Ffeedback">意見反饋</a>或<a href="https://gitee.com/link?target=https%3A%2F%2Fquantum-hub.baidu.com%2Fqmobile">量易伏 APP</a>的用戶反饋提交點數申請。我們會在三個工作日內處理您的請求。</p></li></ol><h2><a id="user-content-copyright-and-license" class="anchor" href="https://gitee.com/baidu/qcompute#copyright-and-license"></a>Copyright and License</h2><p>QComputeSDK 使用 <a href="https://gitee.com/baidu/qcompute/blob/master/LICENSE">Apache-2.0 license</a> 許可證。</p><h2><a id="user-content-作者" class="anchor" href="https://gitee.com/baidu/qcompute#%E4%BD%9C%E8%80%85"></a>作者</h2><ul><li>劉樹森</li><li>賀暘</li><li>江雲帆</li><li>張文學</li><li>孫文贇</li><li>付永凡</li><li>陳建蕭</li><li>沈豪傑</li><li>呂申進</li><li>王友琪</li></ul>]]>
            </description>
            <pubDate>Tue, 23 Jan 2024 02:04:30 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/baidu/qcompute</guid>
            <link>https://gitee.com/baidu/qcompute</link>
        </item>
        <item>
            <title>
                <![CDATA[德國程序員因報告漏洞被判罰 2.4 萬元]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p><span style="color:#000000">德國於利希地方法院近日<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.heise.de%2Fnews%2FWarum-ein-Sicherheitsforscher-im-Fall-Modern-Solution-verurteilt-wurde-9601392.html" target="_blank">宣佈</a>了一項最新判決結果，認定一名程序員因未經授權訪問第三方計算機系統和刺探數據，違反《德國刑法典》（StGB）中所謂的黑客條款 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.gesetze-im-internet.de%2Fstgb%2F__202a.html" target="_blank">202a</a> 而處以 3000 歐元的罰款（約 2.35 萬元），同時承擔所有的訴訟費用。</span></p><p><img height="203" src="https://oscimg.oschina.net/oscnet/up-1685fc08f7024fc47d741af2e54fc872cf5.png" width="700" referrerpolicy="no-referrer"></p><p><span style="color:#000000">2021 年 6 月，這位名為 Hendrik H. 的研究人員在為 IT 服務公司 Modern Solution GmbH 的一位客戶排除軟件故障時發現，Modern Solution 的代碼通過 MySQL 連接至一台 MariaDB 數據庫服務器。而訪問遠程服務器的密碼則以純文本形式存儲在程序文件 MSConnect.exe 中，任何人使用簡單的文本編輯器就能打開該文件查看內容，並找到未加密的硬編碼密碼。</span></p><p><span style="color:#000000">也正是因為這個唾手可得的密碼，導致任何人都可以登錄遠程服務器訪問 Modern Solution 的客戶的數據，同時還可以訪問存儲在該數據庫服務器上的供應商所有客戶的數據。總的來説，這個數據庫漏洞暴露了近 70 萬條客戶記錄，包括姓名、電子郵件地址、電話號碼、銀行信息、密碼以及對話和通話記錄等。</span></p><p><span style="color:#000000">在發現這一漏洞後，該程序員在一名技術博客作者 Mark Steier 的幫助下聯繫了相關公司，後者隨後修復了安全漏洞，並報警追究這名程序員的責任。2021 年 9 月，德國警方扣押了 Hendrik H. 的電腦，因為 Modern Solution 指控他是通過內部信息獲得的密碼，並聲稱他是競爭對手。</span></p><p><span style="color:#000000">2023 年 6 月，德國於利希地方法院以 Modern Solution 軟件保護不力為由，支持了 Hendrik H 的訴訟請求。但亞琛地區法院指令於利希地方法院再次審理此案，原先的裁定被推翻。2024 年 1 月 17 日，於利希地方法院最終宣判對 Hendrik H. 處以罰款，並責令其支付訴訟費用。</span></p><p><span style="color:#000000">這一判決不可避免的在廣大網絡安全專家和研究人員當中引起了爭議。Steier 發帖<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwortfilter.de%2Fentdecker-des-datenlecks-modern-solution-heute-vor-gericht%2F" target="_blank">表示</a>，這一判決從根本上就是錯誤的。「幾乎以純文本形式保存的密碼並不構成第 202 條所要求的'special security'。法官無法對此作出判斷是可以理解的，但這樣一來就必須就這個問題聽取專家的意見。遺憾的是，這並沒有發生。」</span></p><p><span style="color:#000000">不過，該判決尚未具有法律約束力。</span><span style="background-color:#ffffff; color:#323232">被告的辯護律師辯稱，即使法院判定他有罪，他的當事人的行為也是為了公眾利益。</span><span style="color:#000000">被指控的程序員已於 1 月 19 日宣佈，正在對判決提出上訴。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 09:41:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276369/germany-programmer-fined-security</guid>
            <link>https://www.oschina.net/news/276369/germany-programmer-fined-security</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Visual Studio 2022 17.9 Preview 3 發佈]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>Visual Studio 2022 17.9 Preview 3 現已發佈。此版本增強了 Code Search（也稱為 All-In-One Search）體驗，現在用戶可以在整個解決方案中搜索任何單詞或字符串，對代碼庫中的文件和符號結果進行補充。並且可以搜索代碼庫中的局部變量名稱、註釋中的單詞、參數名稱或任何其他字符串。</p><p>新增的文本支持將有助於完善使用 Code Search 的體驗。更專業的文本搜索體驗，可在 <span style="background-color:#ffffff; color:#333333">Find in Files (Ctrl+Shift+F)<span>&nbsp;</span></span>&nbsp;和 <span style="background-color:#ffffff; color:#333333">Quick Find (Ctrl+F)<span>&nbsp;</span></span>中可獲取更多選項和功能。</p><p><img alt="" height="324" src="https://oscimg.oschina.net/oscnet/up-820f75d056a71a0f336d9f2b66556367c6c.png" width="500" referrerpolicy="no-referrer"></p><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>默認的 &nbsp;Code Search 體驗（Ctrl+T 或 Ctrl+,）將在適用時包含精確的文本匹配，包括以前未包括的註釋和局部變量等項目。與 files、types 和 members 相比，文本結果的優先級較低，因此不會影響當前的工作流程。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>對於 </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="background-color:#ffffff; color:#333333">text-only&nbsp;</span><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>視圖，可以通過執行以下操作之一篩選出純文本結果：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li>單擊 search bow 下方的 「text (x:)」 按鈕，</li><li>在搜索查詢前加上「x:」前綴，</li><li>使用鍵盤快捷鍵「Shift+Alt+F」，或者</li><li>轉到菜單選項「Edit &gt; Go To &gt; Go To Text」。</li></ul><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>在 text-only&nbsp;體驗中，還可以通過搜索欄最右側的按鈕切換「Match case」、「Match whole word」和「Use regular expressions」。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:left"><span><span><span><span><span style="color:#333333"><span><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><img alt="" height="324" src="https://oscimg.oschina.net/oscnet/up-959a0534f74574e5f1b4d52a4608423dda3.png" width="500" referrerpolicy="no-referrer"></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:left"><span style="background-color:#ffffff; color:#333333">詳情可<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fvisualstudio%2F17-9-preview-3-brings-exciting-changes-to-code-search%2F" target="_blank">查看</a></span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdevblogs.microsoft.com%2Fvisualstudio%2F17-9-preview-3-brings-exciting-changes-to-code-search%2F" target="_blank">更新公告</a><span style="background-color:#ffffff; color:#333333">。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 06:23:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276349/visual-studio-2022-17-9-preview3-released</guid>
            <link>https://www.oschina.net/news/276349/visual-studio-2022-17-9-preview3-released</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[北京獲準向公眾開放的生成式 AI 大模型產品佔全國近半]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>北京市第十六屆人民代表大會第二次會議於日前召開，會上透露，2023,年，北京獲準向公眾開放的生成式人工智能大模型產品佔全國近一半。今年，北京將推動人工智能模型對標國際先進水平，加快在政務、醫療、工業、生活服務等領域應用。</p><p>北京市市長殷勇作政府工作報告時指出，2023 年，北京加快建設國際科技創新中心，加強科技領軍人才尤其是青年人才培養引進，實施基礎研究領先行動和關鍵核心技術攻堅戰行動，推動在京國家實驗室高質量運行，支持新型研發機構開展有組織科研，加快構建以企業為主導的產學研深度融合新範式。</p><p>北京鞏固提升高精尖產業發展優勢，出台通用人工智能、人形機器人等 30 餘項細分產業支持政策，新設 4 支政府高精尖產業基金，一批創新藥品、醫療器械獲批上市，小米智能手機工廠、理想汽車旗艦工廠提前投產。</p><p>北京精心打造全球數字經濟標杆城市，率先建成全球性能領先的區塊鏈基礎設施，新增 5G 基站 3 萬個，獲準向公眾開放的生成式人工智能大模型產品佔全國近一半，「京通」「京辦」「京智」三個智慧城市應用終端快速升級拓展，高級別自動駕駛示範區實現 160 平方公里連片運行，全國首個數據基礎制度先行區啓動建設，數字經濟增加值佔地區生產總值比重達 42.9%。</p><p>殷勇説，今年，北京將加快發展新質生產力。實施製造業重點產業鏈高質量發展行動，提升產業鏈供應鏈韌性和安全水平。加強原創新藥和高端醫療器械研發，培育生物製造等醫藥健康產業新增長點。推動新能源汽車產業高質量發展，積極佈局電機、電池、電控等關鍵零部件產業鏈。推進超高清視頻全產業鏈優化升級。促進新能源、新材料、商業航天、低空經濟等戰略性新興產業發展，開闢量子、生命科學等未來產業新賽道。優化專精特新企業梯隊培育體系，助力更多企業發展壯大。</p><p>殷勇指出，今年，北京將促進平台經濟有序競爭、創新發展，推動先進數字技術向中小企業深度普及，構建開放共享、充滿活力的創新生態。提升人工智能底層技術和基礎底座自主可控能力，推動人工智能模型對標國際先進水平，加快在政務、醫療、工業、生活服務等領域應用，保持人工智能研發應用領先水平。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 05:55:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276347</guid>
            <link>https://www.oschina.net/news/276347</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[RustDesk 新增 2FA 雙重認證功能，增強遠程桌面訪問安全性]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>RustDesk <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Frustdesk%2Frustdesk%2Freleases%2Ftag%2Fnightly" target="_blank">nightly</a>&nbsp;新增 2FA 雙重認證功能，增強遠程桌面訪問安全性，歡迎大家試用反饋。</p><p><img height="1246" src="https://oscimg.oschina.net/oscnet/up-0de6626da796bb7195b23fc861ee98e2f12.jpg" width="1708" referrerpolicy="no-referrer"></p><p><img height="623" src="https://oscimg.oschina.net/oscnet/up-eef5d3bce35b2b05039bb7678d9ebbea95b.jpg" width="854" referrerpolicy="no-referrer"></p><p><img height="1270" src="https://oscimg.oschina.net/oscnet/up-d4017aeb3ba844b73de27c16d258a40944d.jpg" width="1780" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:20:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276342/rustdesk-2fa</guid>
            <link>https://www.oschina.net/news/276342/rustdesk-2fa</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[到底什麼樣的 Java 項目用 Solon 好？？？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">就像華為講的，不要因為愛國而特意買華為手機。Solon 也是，<strong>有需要就用不需要就跳過</strong>（按正常的需求選擇）：</p><ul><li>信創需要國產化，應該用 Solon 或者 Solon Cloud（有案例）</li><li>軍工項目要國產化，應該用 Solon 或者 Solon Cloud（有案例）</li><li>嵌入式設備，內存有限，算力差，可以用 Solon 或者 Solon Native（有案例）</li><li>客戶的希望你內存更少，可以用 Solon （有案例）</li><li>別的框架用膩了，可以用 Solon （有案例）</li><li>有新系統開發想嘗新的框架，可以用 Solon （有案例）</li><li>老系統要輕量化改造，可以用 Solon（有案例）</li></ul><p style="color:#24292e; margin-left:0; margin-right:0; text-align:start">作為後來者，大家的疑或是會多一些。有問題，可以去交流羣裏多交流。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 04:14:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276341</guid>
            <link>https://www.oschina.net/news/276341</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[如何寫好大模型提示詞？來自大賽冠軍的經驗分享（進階篇）]]>
            </title>
            <description>
                <![CDATA[<div class="content"><blockquote><p><strong>編者按</strong>：近期，如何通過 Prompt Engineering 最大程度發揮大模型的潛力已成為一個熱點話題。人們越來越關注如何通過 Prompt Engineering 技術低成本地用好大模型。</p><p>今天我們推薦的這篇文章，作者認為 Prompt Engineering 需要結合藝術與科學，需要在理解技術背景的同時，發揮創造力和戰略思維。</p><p>本系列文章詳細介紹了作者在新加坡首屆 GPT-4 Prompt Engineering 大賽中使用的策略技巧，包括：<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">使用 CO-STAR 框架構建提示語、使用分隔符明確語義單元</a>、利用 system prompts 添加行為約束、僅依靠 GPT-4 對數據集進行分析等。這些技巧都得到了實例驗證，證明瞭 Prompt Engineering 的重要作用。</p><p>本文屬於該系列文章的第二部分，詳細介紹適合進階使用的 Prompt Engineering 高級策略。</p></blockquote><p><strong>作者 |&nbsp;Sheila Teo</strong></p><p><strong>編譯&nbsp;|&nbsp;嶽揚</strong></p><p><strong>🚢🚢🚢歡迎小夥伴們加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技術軟件及技術交流羣</a>，追蹤前沿熱點，共探技術難題~</strong></p><p>上個月，我有幸獲得新加坡首屆 GPT-4 提示工程（Prompt Engineering）大賽相關獎項，該比賽由新加坡政府科技署（GovTech）組織，匯聚了超過 400&nbsp;位優秀的參與者。</p><p><strong>提示工程（Prompt Engineering）是一門融合了藝術和科學的學科——這門學科不僅需要理解技術，還需要一定的創造力和戰略思維。</strong> 以下是我在學習過程中學到的提示工程（Prompt Engineering）策略彙編，這些策略可以驅動任何大語言模型（LLM）精準執行需求，甚至超常發揮！</p><blockquote><p>作者注：</p><p>在撰寫本文時，我力圖摒棄已在網上廣泛討論和記錄的傳統提示工程（Prompt Engineering）技術。相反，撰寫本文的目的是給大家介紹我在實驗中學到的新見解，以及對某些技術的不同理解。希望你會喜歡閲讀這篇文章！</p></blockquote><p>本系列文章包括以下內容，其中🔵指的是適合初學者的提示語（prompt）技巧（<a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzkzMTI3MTg5MQ%3D%3D%26mid%3D2247484732%26idx%3D1%26sn%3Dbb155ad71f69a8b6aefe7f8557192620%26chksm%3Dc26cc080f51b4996fbb197d6a1fbdce5a45aad000747178e06abea12c89a5601101309012e68%26scene%3D21%23wechat_redirect" target="_blank">見基礎篇</a>），而🔴指的是高級策略（本文的重點）：</p><p>1.[🔵] 使用&nbsp;CO-STAR&nbsp;框架構建提示語</p><p>2.[🔵]&nbsp;使用分隔符（delimiters）將提示語分段</p><p><strong>3.[🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;創建&nbsp;system prompts</strong>（譯者注："guardrails"&nbsp;指的是一種保護機制或限制，用於確保大語言模型生成的內容符合特定標準或要求，防止產生不準確、不合適或有害的信息。）</p><p><strong>4.[🔴]&nbsp;僅使用&nbsp;LLM（無需插件或代碼）分析數據集——將介紹一個使用&nbsp;GPT-4&nbsp;分析真實&nbsp;Kaggle&nbsp;數據集的實踐示例</strong></p><h1><strong>01 [🔴]&nbsp;使用&nbsp;LLM guardrails&nbsp;創建&nbsp;system prompts</strong></h1><p>在進入正題之前，需要注意的是本節只適用於具有 System Prompt 功能的 LLM，而不像基礎篇和本文的其他章節那樣適用於任何 LLM。最著名的 LLM 當然是&nbsp;ChatGPT ，因此在本節中我們將以 ChatGPT 作為示例。</p><h2><strong>1.1 圍繞&nbsp;System Prompt&nbsp;的術語</strong></h2><p>首先，讓我們來理清術語，特別是關於 ChatGPT 的三種術語的使用：這三種術語在 ChatGPT 幾乎可以互換使用：&nbsp;"System Prompts"、"System Messages "和&nbsp;"Custom Instructions"。這讓很多人（包括我在內！）感到困惑，以至於&nbsp;OpenAI&nbsp;特意發佈了一篇文章來解釋這些術語。以下是其摘要：</p><ul><li><strong>"System Prompts"和"System Messages"是通過 Chat Completions API 以編程方式與 ChatGPT 進行交互時使用的術語。</strong></li><li><strong>另一方面，"Custom Instructions"是通過 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchat.openai.com%2F" target="_blank">https://chat.openai.com/</a>&nbsp; 用戶界面與 ChatGPT 交互時使用的術語。</strong></li></ul><p><img src="https://oscimg.oschina.net/oscnet/up-121921e25f98c551f3af8e6f70943254843.png" alt="" referrerpolicy="no-referrer"></p><p>Image from Enterprise DNA Blog</p><p>不過總的來説，這三個術語指的是同一件事，所以不要被這些術語混淆了！後續部分，本文將使用&nbsp;"System Prompts"一詞。現在，讓我們進入正題！</p><h2><strong>1.2 什麼是 System Prompts ？</strong></h2><p>System Prompts 是一種額外的提示語（prompt），我們可以在其中提供有關 LLM 行為方式的 instructions。它被認為是額外的提示語（prompt），因為它不屬於您給 LLM 的&nbsp;"正常"&nbsp;提示語（即 User Prompts）。</p><p>在聊天中，每當您給 LLM 發送新的提示語（prompt）時，System Prompts 都會像過濾器一樣，LLM 會在回答您的新提示語（prompt）之前自動應用這些提示語（prompt）。這意味着 System Prompts 在 LLM 做出回答時都會被考慮進去。</p><h2><strong>1.3 何時使用 System Prompt ？</strong></h2><p>您心中可能會想到的第一個問題是：為什麼我應該在 System Prompts 中提供 instruction，而不是在我向與 LLM 的新對話的第一個提示語（prompt）中提供 instruction，然後再與 LLM 進行更多的對話呢？</p><p>答案是，因為 LLM 的對話記憶是有限的。在後一種情況下，隨着對話的繼續，LLM 很可能會"忘記"您在聊天中提供的第一條提示語（prompt），從而使這些 instruction （指令）過時。</p><p>另一方面，如果在 System Prompts 中提供了 instruction （指令），那麼這些 System Prompts &nbsp;會與聊天中提供的每個新提示語一起發送。這可以確保 LLM 在聊天過程中繼續接收這些 instruction，無論聊天過程變得多長。</p><p>總結：</p><p>在整個聊天過程中，使用 System Prompts 提供您希望 LLM 在回答時記住的 instruction 。</p><h2><strong>1.4 System Prompt 應包括哪些內容？</strong></h2><p>System Prompt 通常應包括以下類別的 instruction ：</p><ul><li><strong>目標任務的定義（Task definition）</strong> ，這樣 LLM 在整個對話過程中都會記住它必須做什麼。</li><li><strong>輸出格式（Output format）</strong> ，這樣 LLM 在整個對話過程中都會記住它應該如何做出回答。</li><li><strong>防範措施（Guardrails）</strong> ，這樣 LLM 在整個對話過程中都會記住它不應該如何做出回答。Guardrails 是 LLM governance 中的新興領域，指的是 LLM 被允許操作的行為邊界。</li></ul><p>例如，System Prompt 可能是這樣的：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p><p>If the text does not contain sufficient information to answer the question,&nbsp;do not make up information and give the answer as&nbsp;「NA」.</p><p>You are only allowed to answer questions related to&nbsp;[insert scope].&nbsp;Never answer any questions related to demographic information such as age,&nbsp;gender,&nbsp;and religion.</p></blockquote><p>各部分內容涉及的類別如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-cab48977b545bba61bf371683a3bdc105af.png" alt="" referrerpolicy="no-referrer"></p><p>Breaking down a System Prompt&nbsp;—&nbsp;Image by author</p><h2><strong>1.5 但是，"正常"的聊天提示語又是什麼呢？</strong></h2><p>現在你可能會想：聽起來 System Prompt 中已經提供了很多信息。那我應該在聊天的"正常"提示語（即 User Prompts）中寫些什麼呢？</p><p>System Prompt 概述了當前的一般任務。在上面的 System Prompt 示例中，任務已被定義為只使用一段特定的文本來回答問題，並且 LLM 被指示以{"Question":&nbsp;"Answer"}的格式進行回答。</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p><p>You will respond with a JSON object in this format:&nbsp;{「Question」:&nbsp;「Answer」}.</p></blockquote><p>在這種情況下，聊天過程中的每個 User Prompt 都將簡化為你希望 LLM 用文本回答的問題。例如，某個用戶的提問可能是「這段文本是關於什麼的？（What is the text about?）」然後 LLM 會回答説&nbsp;{"這段文本是關於什麼的？（What is the text about?）":&nbsp;"這段文本是關於……（The text is about..）"}。</p><p>但是，讓我們進一步概括這個任務示例。在這種情況下，我們可以將上述 System Prompt &nbsp;的第一行從：</p><blockquote><p>You will answer questions using this text:&nbsp;[insert text].</p></blockquote><p>編輯為：</p><blockquote><p>You will answer questions using the provided text.</p></blockquote><p>現在，每個用戶在聊天時的提示語（prompt）將包括進行問題回答的文本和要回答的問題，例如：</p><blockquote><p>&lt;text&gt;</p><p>[insert text]</p><p>&lt;/text&gt;</p><p>&lt;question&gt;</p><p>[insert question]</p><p>&lt;/question&gt;</p></blockquote><p>在這裏，還將使用 XML 標籤作為分隔符，以便以結構化的方式向 LLM 提供所需的兩個信息片段。<strong>XML 標籤中使用的名詞「text」和「question」與 System Prompt 中使用的名詞相對應，這樣 LLM 就能理解標籤與 System Prompt instructions 之間的關係。</strong></p><p>總之， System Prompt 應給出總體任務 instructions，而每個 User Prompt 應提供任務執行的具體細節。例如，在本例中，這些具體的細節是 text 和 question。</p><h2><strong>1.6 LLM guardrails&nbsp;動態化</strong></h2><p>上面通過 System Prompt 中的幾句話添加了 guardrails 。這些 guardrails 會被固定下來，在整個聊天過程中都不會改變。但是如果您希望在對話的不同階段設置不同的 guardrails ，該怎麼辦？</p><p>對於使用 ChatGPT Web 界面的用戶來説，目前還沒有直接的方法來做到這一點。不過，如果您正在通過編程方式與 ChatGPT 進行交互，那你就走運了！隨着人們對構建有效的 LLM guardrail 的關注度越來越高，一些開源軟件包也應運而生，它們可以讓你以編程方式設置更詳細、更動態的 guardrail。</p><p>其中值得注意的是英偉達團隊開發的&nbsp;NeMo Guardrails[1]，它允許您配置用戶和 LLM 之間預期的對話流程，從而在聊天的不同時間點設置不同的 guardrail ，實現隨着聊天進展而不斷演變的動態 guardrails 。我強烈推薦您去了解一下！</p><h1><strong>02 [🔴]&nbsp;僅使用&nbsp;LLM（無需插件或代碼）分析數據集</strong></h1><p>您可能已經聽説過 OpenAI 在 ChatGPT 的 GPT-4 中推出的高級數據分析插件，該插件僅高級（付費）賬戶可以使用。它允許用戶將數據集上傳到 ChatGPT，並直接在數據集上運行代碼，從而進行精確的數據分析。</p><p>但你知道嗎，使用 LLM 分析數據集並不一定需要這樣的插件？讓我們先來瞭解一下單純使用 LLMs 分析數據集的優勢和侷限性。</p><h2><strong>2.1 大語言模型不擅長的數據集分析類型</strong></h2><p>正如您可能已經知道的那樣，LLM 在進行精確數學計算方面的能力有限，因此它們不適合完成需要對數據集進行精確定量分析的任務，比如：</p><ul><li><strong>描述性統計（Descriptive Statistics）</strong> ：通過諸如均值或方差的測量來定量總結數值列。</li><li><strong>相關性分析（Correlation Analysis）</strong> ：獲取列之間精確的相關係數。</li><li><strong>統計分析（Statistical Analysis）</strong> ：比如假設檢驗（hypothesis testing），以確定各組數據點之間是否存在統計意義上的顯著差異。</li><li><strong>機器學習（Machine Learning）</strong> ：在數據集上執行預測建模，比如使用線性迴歸（linear regressions）、梯度提升樹（gradient boosted trees）或神經網絡（neural networks）。</li></ul><p>方便在數據集上執行這些定量任務正是 OpenAI 推出高級數據分析插件的原因，這樣編程語言就可以在數據集上運行代碼來執行此類任務。</p><p>那麼，為什麼有人只想使用 LLM 而不使用此類插件來分析數據集呢？</p><h2><strong>2.2 大語言模型擅長的數據集分析類型</strong></h2><p>LLM 非常擅長識別模式和趨勢（patterns and trends）。這種能力源自它們在多樣化和海量數據上的廣泛訓練，使它們能夠識別那些可能無法立即察覺的複雜模式。</p><p>這使它們非常適合執行基於數據集進行模式識別的任務，例如：</p><ul><li><strong>異常檢測（Anomaly detection）</strong> ：基於一列或多列的數值，識別偏離常規的異常數據點。</li><li><strong>聚類（Clustering）</strong> ：將具有相似特徵的數據點分組。</li><li><strong>跨列關係（Cross-Column Relationships）</strong> ：通過分析不同列之間的關係，可以揭示數據中的複雜模式和趨勢。</li><li><strong>文本分析（Textual Analysis）（針對基於文本的列）</strong> ：基於主題或情感進行分類。</li><li><strong>趨勢分析（Trend Analysis）（針對具有時間特徵的數據集）</strong> ：識別列中跨時間的模式、季節性變化或趨勢。</li></ul><p>對於這類基於模式的任務，單獨使用 LLM 可能會比使用代碼在更短的時間內獲得更好的結果！讓我們用一個例子來充分説明這一點。</p><h2><strong>2.3 僅使用&nbsp;LLM&nbsp;分析&nbsp;Kaggle&nbsp;數據集</strong></h2><p>我們將使用一個廣受歡迎的真實&nbsp;Kaggle&nbsp;數據集[2]，該數據集專為進行客戶人格分析而準備，其中一家公司試圖對其客戶羣體進行細分，以便更好地瞭解其客戶。</p><p>為了便於之後驗證 LLM 的分析結果，我們取該數據集的 50&nbsp;行為一個子集，並只保留最相關的列。之後，用於分析的數據集將如下所示，其中每一行代表一位客戶，每一列描述客戶信息：</p><p><img src="https://oscimg.oschina.net/oscnet/up-7c8caf80c5954ae73d0746053b0109b7737.png" alt="" referrerpolicy="no-referrer"></p><p>First 3 rows of dataset&nbsp;—&nbsp;Image by author</p><p>假設你在公司的營銷團隊工作。你的任務是利用這些客戶信息數據集來指導營銷工作。這是一項分兩步走的任務：首先，利用數據集劃分多個具有實際意義的客戶細分羣體。接下來，提出如何最好地針對每個客戶羣體進行創意營銷。現在，這是一個實際的商業問題，LLM 的模式發現（pattern-finding，對於步驟 1 ）能力在這個問題上確實可以大顯身手。</p><p>讓我們按照以下方式為這個任務制定提示語（prompt），將使用&nbsp;4&nbsp;種&nbsp;prompt engineering&nbsp;技術（後續會詳細介紹[3]）：</p><p><strong>1. 將複雜的任務分解為簡單的步驟</strong></p><p><strong>2. 參考每個步驟的中間輸出</strong></p><p><strong>3. 格式化 LLM 的回答</strong></p><p><strong>4. 將&nbsp;instructions&nbsp;與數據集分開</strong></p><blockquote><p>System Prompt:</p><p>I want you to act as a data scientist to analyze datasets.&nbsp;Do not make up information that is not in the dataset.&nbsp;For each analysis I ask for,&nbsp;provide me with the exact and definitive answer and do not provide me with code or instructions to do the analysis on other platforms.</p><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>GPT-4 的回答如下，接下來我們將數據集以 CSV 字符串的形式傳遞給它。</p><p><img src="https://oscimg.oschina.net/oscnet/up-e81a5f6ea27da5b058438541aa821009931.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p>隨後，GPT-4 將按照我們要求的 markdown 格式回覆分析結果：</p><p><img src="https://oscimg.oschina.net/oscnet/up-72040eca14932d7eb9289119eb13af4d0a8.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-03f08b1a6bfadee91b96716822f50f0c4a4.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-f81c09e878806d7f71c9773c8291d3611d5.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><h2><strong>2.4 驗證&nbsp;LLM&nbsp;的分析結果</strong></h2><p>為了簡潔起見，我們將挑選 LLM 生成的 2 個客戶羣體進行驗證，比如 Young Families 和 Discerning Enthusiasts。</p><h3><strong>2.4.1 Young Families</strong></h3><ul><li>LLM 總結的該人羣特徵：1980 年後出生，已婚或同居，收入中等偏低，有孩子，經常進行小額購買。</li><li>LLM 將數據集中的這些行聚類到了 Young Families 這個羣體中：3、4、7、10、16、20</li></ul><p>深入數據集，這些行的完整數據如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-01030fc251ada675444cc2b84f87f861ce5.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Young Families&nbsp;—&nbsp;Image by author</p><p>LLM 識別出的這部分客戶資料，確實對應於所識別的客戶羣體。甚至能夠在我們事先未經過預處理的情況下對具有空值的資料進行聚類！</p><h3><strong>2.4.2 Discerning Enthusiasts</strong></h3><ul><li>LLM 總結的該人羣特徵：年齡跨度廣，可能是任何婚姻狀況，高收入，子女狀況各異，購買支出高。</li><li>LLM 認為該人羣對應的數據行：2、5、18、29、34、36</li></ul><p>深入數據集，這些行的完整數據如下：</p><p><img src="https://oscimg.oschina.net/oscnet/up-2844275d166caf555082adb378ad4e1e7b6.png" alt="" referrerpolicy="no-referrer"></p><p>Full data for Discerning Enthusiasts&nbsp;—&nbsp;Image by author</p><p>再次與 LLM 識別出的人羣資料非常吻合！</p><p>這個例子展示了 LLM 在發現模式、解釋和提煉多維數據集，並將其提煉為有意義的見解方面的能力，同時確保其分析深深紮根於數據集的事實。</p><h2><strong>2.5 如果我們使用 ChatGPT 的高級數據分析插件會怎樣呢？</strong></h2><p>為了保證分析的完整性，我嘗試使用相同的提示語（prompt），並請求 ChatGPT 使用代碼執行相同的分析，這就激活了它的高級數據分析插件。這個想法是讓插件直接在數據集上運行 K-Means 等聚類算法的代碼，以獲得各個用戶羣體的特徵，然後綜合每個羣體的數據來提供營銷策略。</p><p>然而，儘管數據集只有 50&nbsp;行，進行了多次嘗試都導致出現以下錯誤信息而沒有任何輸出：</p><p><img src="https://oscimg.oschina.net/oscnet/up-10c7526d7a4b7d296cef8b48699d37762a9.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 1&nbsp;—&nbsp;Image by author</p><p><img src="https://oscimg.oschina.net/oscnet/up-fa3318a26ba4ab2800121fb5ab4295345f5.png" alt="" referrerpolicy="no-referrer"></p><p>Error and no output from Attempt 2&nbsp;—&nbsp;Image by author</p><p>現在使用高級數據分析插件，在數據集上執行較簡單的任務（如計算描述性統計數據或創建圖表）似乎很容易實現，但需要某些計算算法的較高級任務有時可能會由於計算限制或其他原因導致錯誤或無輸出。</p><h2><strong>2.6 那麼......何時使用 LLM 分析數據集？</strong></h2><p>答案是取決於分析的數據類型。</p><p>對於需要精確數學計算或複雜的、基於規則處理的任務，傳統的編程方法仍然更勝一籌。</p><p>對於基於模式識別（pattern-recognition）的任務，使用傳統的編程或算法方式可能比較具有挑戰性或更耗時。然而，LLM 擅長此類任務，甚至可以提供額外的內容輸出，比如用來支持其分析的附件和 Markdown 格式的完整分析報告。</p><blockquote><p><strong>歸根結底，是否使用 LLM 取決於手頭任務的性質，要在 LLM 在模式識別方面的優勢與傳統編程技術提供的精確性和特異性之間取得平衡。</strong></p></blockquote><h2><strong>2.7 現在回到提示工程（prompt engineering）！</strong></h2><p><strong>在本節結束之前，讓我們回顧一下用於生成此數據集分析的提示語（prompt），並分解所使用的 prompt engineering 關鍵技術。</strong></p><blockquote><p>Prompt:</p><p>#&nbsp;CONTEXT&nbsp;#</p><p>I sell wine.&nbsp;I have a dataset of information on my customers:&nbsp;[year of birth,&nbsp;marital status,&nbsp;income,&nbsp;number of children,&nbsp;days since last purchase,&nbsp;amount spent].</p><p>#############</p><p>#&nbsp;OBJECTIVE&nbsp;#</p><p>I want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group.&nbsp;Use this step-by-step process and do not use code:</p><p>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</p><p>For each cluster found,</p><p>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</p><p>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</p><p>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</p><p>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</p><p>#############</p><p>#&nbsp;STYLE&nbsp;#</p><p>Business analytics report</p><p>#############</p><p>#&nbsp;TONE&nbsp;#</p><p>Professional,&nbsp;technical</p><p>#############</p><p>#&nbsp;AUDIENCE&nbsp;#</p><p>My business partners.&nbsp;Convince them that your marketing strategy is well thought-out and fully backed by data.</p><p>#############</p><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;</p><p>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</p><p>#############</p><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p><strong>技巧 1：將複雜任務分解為簡單步驟</strong></p><p>LLM 擅長執行簡單任務，但在複雜任務上表現一般。因此，<strong>對於像這樣的複雜任務，重要的是要把任務分解成簡單的步驟説明，讓 LLM 遵循。</strong> 這樣做的目的是向 LLM 提供你自己執行任務時會採取的步驟。</p><p>在本例中，步驟如下：</p><blockquote><p><strong>Use this step-by-step process and do not use code:</strong></p><p><strong>1.&nbsp;CLUSTERS:&nbsp;Use the columns of the dataset to cluster the rows of the dataset,&nbsp;such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values.&nbsp;Ensure that each row only belongs to 1 cluster.</strong></p><p><strong>For each cluster found,</strong></p><p><strong>2.&nbsp;CLUSTER_INFORMATION:&nbsp;Describe the cluster in terms of the dataset columns.</strong></p><p><strong>3.&nbsp;CLUSTER_NAME:&nbsp;Interpret&nbsp;[CLUSTER_INFORMATION]&nbsp;to obtain a short name for the customer group in this cluster.</strong></p><p><strong>4.&nbsp;MARKETING_IDEAS:&nbsp;Generate ideas to market my product to this customer group.</strong></p><p><strong>5.&nbsp;RATIONALE:&nbsp;Explain why&nbsp;[MARKETING_IDEAS]&nbsp;is relevant and effective for this customer group.</strong></p></blockquote><p>與簡單地將整體任務交給 LLM 相比，例如「將客戶分組，然後提出針對每個羣體的營銷策略」。通過逐步説明，LLM 更有可能提供正確的結果。</p><p><strong>技巧 2：引用每個步驟的中間輸出</strong></p><p>在向 LLM 提供逐步説明時，可將每個步驟的中間輸出命名為大寫的變量名，例如 CLUSTERS、CLUSTER_INFORMATION、CLUSTER_NAME、MARKETING_IDEAS 和 RATIONALE。</p><p><strong>使用大寫字母是為了將這些變量名與給出的 instructions 內容區分開。稍後可以使用方括號引用這些中間輸出，如[VARIABLE_NAME]。</strong></p><p><strong>技巧 3：規範大模型回答的格式</strong></p><p>在這裏，要求使用 Markdown 報告格式，以美化 LLM 的回答。在這裏，中間輸出中的變量名又派上了用場，可以決定報告的結構。</p><blockquote><p>#&nbsp;RESPONSE:&nbsp;MARKDOWN REPORT&nbsp;#</p><p>&lt;For each cluster in&nbsp;[CLUSTERS]&gt;</p><p>—&nbsp;Customer Group:&nbsp;[CLUSTER_NAME]</p><p>—&nbsp;Profile:&nbsp;[CLUSTER_INFORMATION]</p><p>—&nbsp;Marketing Ideas:&nbsp;[MARKETING_IDEAS]</p><p>—&nbsp;Rationale:&nbsp;[RATIONALE]</p><p>&lt;Annex&gt;<br><strong>Give a table of the list of row numbers belonging to each cluster,&nbsp;in order to back up your analysis.&nbsp;Use these table headers:&nbsp;[[CLUSTER_NAME],&nbsp;List of Rows].</strong></p></blockquote><p>事實上，您甚至可以隨後要求 ChatGPT 將報告提供為可下載文件，這樣您就可以根據其回答來撰寫最終的報告文檔。</p><p><img src="https://oscimg.oschina.net/oscnet/up-792771629e0046fa10609d608023fce4df5.png" alt="" referrerpolicy="no-referrer"></p><p>Saving GPT-4's response as a file&nbsp;—&nbsp;Image by author</p><p><strong>技巧 4：將任務説明與數據集分開</strong></p><p>您會注意到我們在第一個提示語中並沒有將數據集提供給 LLM。相反，提示語只包含了數據集分析的任務説明，並在最後加上了以下內容：</p><blockquote><p>#&nbsp;START ANALYSIS&nbsp;#</p><p>If you understand,&nbsp;ask me for my dataset.</p></blockquote><p>ChatGPT 隨後回覆説它理解了，我們將在下一個提示語中將數據集作為 CSV 字符串傳遞給它：</p><p><img src="https://oscimg.oschina.net/oscnet/up-3b139d01a6cf26df1f930d6e67dc5b8be24.png" alt="" referrerpolicy="no-referrer"></p><p>GPT-4's response&nbsp;—&nbsp;Image by author</p><p><strong>但為什麼要將 instructions 與數據集分開呢？</strong></p><p>簡單明瞭的答案是，LLM 的上下文窗口存在限制，即在一句提示語中可以輸入的 tokens 數量存在限制。同時包含 instructions 和數據的長提示語（long prompt）可能會超過這個限制，從而導致截斷（truncation）和信息丟失（loss of information）。</p><p>更復雜的答案是，<strong>將 instructions 和數據集分開可以幫助 LLM 保持清晰的理解，降低遺漏信息的可能性。</strong> 你可能遇到過這樣的情況，即 LLM "不小心忘記了"&nbsp;你發送的較長提示語給出的某個 instruction ——例如，如果你要求給出 100&nbsp;字的回答，而 LLM 卻給了您一個較長的段落。通過先接收 instructions ，再接收 instructions 所針對的數據集，LLM 可以先消化它應該執行的任務，然後再對接下來提供的數據集執行 instructions 。</p><p>不過請注意，只有聊天型&nbsp;LLM&nbsp;才能實現&nbsp;instruction&nbsp;和數據集的分離，因為它們會保留對話記憶，而&nbsp;completion LLM&nbsp;則不會（譯者注：completion LLM 指的是一種能夠根據給定的提示語來生成完整文本或完成特定任務的語言模型。這種模型通常不具備對話記憶，而是專注於根據提示語生成連貫的文本）。</p><p><strong>Thanks for reading!</strong></p><p><strong>END</strong></p><p><strong>🚢🚢🚢歡迎小夥伴們加入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaihai-idp.yuque.com%2Fmwvla8%2Fdoc%3F%23" target="_blank">AI 技術軟件及技術交流羣</a>，追蹤前沿熱點，共探技術難題~</strong></p><p><strong>參考資料</strong></p><p>[1]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FNVIDIA%2FNeMo-Guardrails" target="_blank">https://github.com/NVIDIA/NeMo-Guardrails</a></p><p>[2]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fimakash3011%2Fcustomer-personality-analysis" target="_blank">https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis</a></p><p>[3]<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41%23544b" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41#544b</a></p><p><strong>本文經原作者授權，由 Baihai IDP 編譯。如需轉載譯文，請聯繫獲取授權。</strong></p><p><strong>原文鏈接：</strong></p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41" target="_blank">https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/IDP/blog/10920438</guid>
            <link>https://my.oschina.net/IDP/blog/10920438</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[🔥 周熱點 | 純血 HarmonyOS NEXT 亮相；雲風宣佈開源自研遊戲引擎；ReiserFS 作者在獄中迴應被 Linux 內核棄用.....]]>
            </title>
            <description>
                <![CDATA[回顧一週熱門資訊。2024.01.15-2024.01.21]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:51:00 GMT</pubDate>
            <guid isPermaLink="false">https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</guid>
            <link>https://mp.weixin.qq.com/s?__biz=MzA4OTI5NjUwOA==&#38;mid=2649094122&#38;idx=1&#38;sn=75d3821b09abb3147c5c679ffac2df70&#38;chksm=880c4cf9bf7bc5ef0754d108ffbf048aeba8c46cb52ebb828c6266e2816cf4009bea3e8df259&#38;token=871504646&#38;lang=zh_CN#rd</link>
        </item>
        <item>
            <title>
                <![CDATA[助力 AI 技術共享，螞蟻開源又一核心技術 「因果學習系統 OpenASCE」]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">當地時間 2023 年 12 月 10 日，機器學習和人工智能領域的頂級國際會議 NeurIPS (Neural Information Processing Systems) 在美國路易斯安那州新奧爾良市開幕，來自全球產業界和學術領域的人工智能專家齊聚一堂。</p><h1>首個分佈式全鏈路因果學習系統 OpenASCE</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">大會第一天，螞蟻集團在主題為 「知識增強 AI 在垂直行業的應用探索」 的研討會上正式開源了業界首個分佈式全鏈路因果學習系統 OpenASCE (Open All-Scale Causal Engine) 。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">項目 GitHub：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2FOpen-All-Scale-Causal-Engine%2FOpenASCE" target="_blank">https://github.com/Open-All-Scale-Causal-Engine/OpenASCE</a></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-8d931ba2b03a853fcf7d06c1a780d171b96.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">因果推斷主要研究如何從數據中推斷因果關係，是數據科學領域的重要分支，而傳統的機器學習則主要依賴數據中的相關關係。融合因果推斷和機器學習可以同時發揮兩者的強項，我們稱之為因果學習。因果學習作為一種深入理解數據和決策背後關係的技術，在數據驅動的運營和決策中扮演着重要的角色。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 根植於螞蟻集團多年積累的實踐經驗和技術突破，相較於業界已有的一些開源框架，支持全鏈路大規模因果學習，包含因果發現、因果效應估計和歸因，覆蓋了因果各個領域的相應實現。在因果發現上，OpenASCE 支持分佈式貝葉斯網絡結構搜索，能夠處理百節點百萬樣本數據；同時支持基於連續優化的因果發現，支持萬級節點億級樣本數據。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">OpenASCE 實現的大規模分佈式因果糾偏樹可以在 4 小時內完成 1 億樣本的訓練任務，是業界唯一的分佈式因果提升樹實現。此外，OpenASCE 還沉澱了 20 多個工業級因果學習算法，包括 15 個以上因果技術和深度學習結合的因果表徵學習方法，有效降低了因果技術的工業應用門檻，在螞蟻集團內部多個場景實現了規模化應用。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img alt="" src="https://oscimg.oschina.net/oscnet/up-da7d981d440607195474f083b7a1cbb7862.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">在信貸風控領域，通過 OpenASCE 的因果學習方法，可以更準確地識別出風險因素和客戶行為之間的因果關係，大幅提高了風險控制的精度和效率。在營銷優化方面，OpenASCE 能夠幫助營銷人員有效尋找 「營銷敏感人羣」，提升業務指標。在推薦場景中，因果推斷可以幫助機器學習糾正數據中的偏置，去除偽相關，學習更穩定的因果關係。</p><h1>開源開放，共建社區</h1><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">作為一家科技公司，螞蟻集團將 OpenASCE 開源，為業界提供一套大規模、高性能的因果學習技術，並通過開源吸引全球開發者共同參與項目的建設和完善，促進全鏈路因果學習系統領域的發展和創新。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">技術開源是螞蟻集團的重要技術戰略，我們希望通過開源建立起開放、包容的技術生態，讓更多人共享技術紅利。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">截至目前，螞蟻集團已在數據庫、雲原生、中間件等基礎軟件領域開源了 1700 多個倉庫、積累了 100 多個社區頭部開源項目。《COPU2022 中國開源發展藍皮書》顯示，螞蟻開源影響力排名國內前三，其中重點開源的 9 大技術均為支撐支付寶的核心技術。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:47:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276338</guid>
            <link>https://www.oschina.net/news/276338</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[因政治濫用，OpenAI 將一家 AI 初創公司拉黑]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>OpenAI 於日前<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.yahoo.com%2Fnews%2Fopenai-suspends-developer-over-chatgpt-bot-that-impersonated-a-presidential-candidate-214854456.html" target="_blank">封禁</a>了一家開發 Chatbot 的 AI 初創公司 Delphi。因為該公司基於 GPT-4，出於政治目的設計了一個模仿美國民主黨總統候選人 Dean Phillips 的機器人 Dean.Bot；以與潛在支持者互動並傳播候選人的信息，幫助其競選。</p><p><img height="220" src="https://oscimg.oschina.net/oscnet/up-5e13ec80f847c98829315c5bf8ebf46f766.png" width="500" referrerpolicy="no-referrer"></p><p>事實上，美國、英國、印度、巴基斯坦和南非等國都計劃在 2024 年進行大選。為了防止其技術被濫用，OpenAI 在本月早些時候曾發表了一篇<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenai.com%2Fblog%2Fhow-openai-is-approaching-2024-worldwide-elections" target="_blank">長文</a>，介紹其計劃採取的一些措施；其中明確表示不允許人們開發用於政治活動和遊説的應用，並且還特別提到了禁止"冒充候選人的聊天機器人 "。</p><p>雖然 Dean.Bot 網站有提供免責聲明，告知訪問者所有的交互都將由聊天機器人生成，而不是 Phillips 本人。但這種使用方式還是直接違反了 OpenAI 的政策。公司發言人在給《華盛頓郵報》的一份聲明中也證實了被 OpenAI 封禁的消息。Delphi 的 OpenAI 帳戶據稱於上週五被封禁，隨後該公司就停止了對 Dean.Bot 的訪問權限。</p><p>現在訪問該網站的用戶仍然會看到免責聲明，但會顯示聊天機器人本身已因"技術故障"而宕機："Apologies, DeanBot is away campaigning right now!"&nbsp;</p><p>這也是 OpenAI 首次因開發者違反其 AI 濫用準則而採取審查措施。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:34:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</guid>
            <link>https://www.oschina.net/news/276333/openai-suspends-developer-over-chatgpt-bot</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[ElasticSearch 集羣災難：別放棄，也許能再搶救一下 | 京東雲技術團隊]]>
            </title>
            <description>
                <![CDATA[<div class="content"><span id="OSC_h1_1"></span><h1>1 前言</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">Elasticsearch 作為一個分佈式搜索引擎，自身是高可用的；但也架不住一些特殊情況的發生，如：</p><ul><li><p style="color:#222222; margin-left:0; margin-right:0">集羣超過半數的 master 節點丟失，ES 的節點無法形成一個集羣，進而導致集羣不可用；</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">索引 shard 的文件損壞，分片無法被正常恢復，進而導致索引無法正常提供服務</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">本地盤節點，多數據節點故障，舊節點無法再次加入集羣，數據丟失</p></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">針對上述的情況，今天來聊一聊相關的解決方案。</p><span id="OSC_h1_2"></span><h1>2 基礎知識</h1><span id="OSC_h2_3"></span><h2>2.1 集羣經典架構</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">在聊解決方案之前，首先來看一看 ES 集羣層面的基本知識，es 的集羣組成通常如圖 1-1 所示<img alt="es 集羣角色.png" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-06-15-270pb9YSxO96kLhga.png" referrerpolicy="no-referrer"></p><div><p>圖 1-1 es 常用集羣架構</p> 如圖 1-1 所示，為生產環境 es 集羣的經典架構，主要由專有主節點、專有協調節點和數據節點組成： 
</div><ul><li><p style="color:#222222; margin-left:0; margin-right:0"><strong>專有主節點 (<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23master-node" rel="nofollow" target="_blank">Master-eligible node</a>)</strong>: 具有 master 角色的節點，這使其有資格被選為主節點，只存儲集羣元信息包含 cluster、index、shard 級別的元數據；該種角色節點被選舉為 master 之後，將作為整個 ES 集羣的大腦，負責維護集羣層面的元信息，創建刪除索引等工作。該種節點的個數必須為奇數，通常我們固定為 3 個，如果該類節點丟失半數，es 集羣將無法維持 es 節點形成一個集羣。</p></li><li><p style="color:#222222; margin-left:0; margin-right:0"><strong>專有協調節點 (網關節點)</strong>: 該種節點不具有任何角色，僅僅用來處理 es 請求；比如（1）將寫請求的數據歸類轉發到數據所屬的節點（2）查詢請求的二次聚合計算。通常我們也會給該類節點保留<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23node-ingest-node" rel="nofollow" target="_blank">ingest 角色</a><span>&nbsp;</span>，ingest 的主要作用是對數據進行預處理；比如：字段重命名、給數據文檔打上指紋和清洗數據等功能主要通過<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fpipeline.html" rel="nofollow" target="_blank">pipeline 能力</a>進行處理</p></li><li><p style="color:#222222; margin-left:0; margin-right:0"><strong>數據節點 (<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23data-node" rel="nofollow" target="_blank">Data node</a>)</strong>: 存儲數據和集羣元信息，執行與數據相關的操作，如 CRUD、搜索和聚合。在數據節點上打上不同的屬性，可以使其成為 hot、warm、cold 數據節點，在 es7.9 版本之後配置略有不同，但是原理基本不變。</p></li></ul><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果沒有顯示設置節點角色，es 的每個節點都會含有以上三種角色。除此之後還有<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23remote-node" rel="nofollow" target="_blank">Remote-eligible node</a><span>&nbsp;</span>、<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23ml-node" rel="nofollow" target="_blank">ml-node</a>和<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fmodules-node.html%23transform-node" rel="nofollow" target="_blank">Transform nodes</a>等角色需要顯示的配置，節點才會有該角色。</p><span id="OSC_h2_4"></span><h2>2.2 集羣元信息</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">集羣完全啓動主要包含選舉主節點、元信息、主分片、數據恢復等重要階段；如圖 2-1 所示[1]。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><img alt="5211704611687_.pic.jpg" src="https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2024-01-07-15-22jRx48QjPfQUT8WnF.jpg" referrerpolicy="no-referrer"></p><div><p>圖 2-1 es 集羣啓動流程</p></div><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">主節點選舉的過程，不是本文的重點，而是集羣元信息的選舉。被選舉出的 master 和集羣元信息新舊程度沒有關係；master 節點被選舉出來之後，它所要完成的第一個任務，即是選舉集羣元信息。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（1）Master 選舉成功之後，判斷其持有的集羣狀態中是否存在 STATE_NOT_RECOVERED_BLOCK，如果不存在，則説明元數據已<br> 經恢復，跳過 gateway 恢復過程，否則等待。<code>org.elasticsearch.gateway.GatewayService#clusterChanged</code></p><pre><code>//跳過元數據恢復
if (state.blocks().hasGlobalBlock(STATE_NOT_RECOVERED_BLOCK) == false) {
            // already recovered
            return;
 }
 //此處省略部分代碼。
 //進入 gateway 恢復過程   
performStateRecovery(enforceRecoverAfterTime, reason); 
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（2）Master 從各個節點主動獲取元數據信息。<code>org.elasticsearch.gateway.Gateway#performStateRecovery</code></p><pre><code># 獲取元信息核心代碼
 final String[] nodesIds = clusterService.state().nodes().getMasterNodes().keys().toArray(String.class);
        logger.trace("performing state recovery from {}", Arrays.toString(nodesIds));
        final TransportNodesListGatewayMetaState.NodesGatewayMetaState nodesState = listGatewayMetaState.list(nodesIds, null).actionGet();
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（3）從獲取的元數據信息中選擇版本號最大的作為最新元數據;元信息包括集羣級、索引級。</p><pre><code>## org.elasticsearch.gateway.Gateway#performStateRecovery

    public void performStateRecovery(final GatewayStateRecoveredListener listener) throws GatewayException {
# 省略若干行代碼

## 進入 allocation 階段；
## final Gateway.GatewayStateRecoveredListener recoveryListener = new GatewayRecoveryListener();
## listener 為 GatewayStateRecoveredListener   
 listener.onSuccess(builder.build());    
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">（4）兩者確定之後，調用 allocation 模塊的 reroute，對未分配，的分片執行分配，主分片分配過程中會異步獲取各個 shard 級別元數據。</p><pre><code>#主要實現方法為如下方法   
#org.elasticsearch.gateway.GatewayService.GatewayRecoveryListener#onSuccess
## 主要工作是構建集羣狀態（ClusterState），其中的內容路由表，依賴 allocation 模塊協助完成，調用 allocationService.reroute 進，入下一階段：異步執行分片層元數據的恢復，以及分片分配。updateTask 線程結束.
   
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><strong>ES 中存儲的數據</strong>：（1）state 元數據信息；（2）index Lucene 生成的索引文件；（3）translog 事務日誌。<br><strong>元數據信息</strong>：</p><ol><li><p style="color:#222222; margin-left:0; margin-right:0">nodes/0/_state/*.st，集羣層面元信息 MetaData（clusterUUID 、 settings 、templates 等）；</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">nodes/0/indices/{index_uuid}/_state/*.st，索引層面元信息 IndexMetaData（ numberOfShards 、mappings 等）；</p></li><li><p style="color:#222222; margin-left:0; margin-right:0">nodes/0/indices/{index_uuid}/0/_state/*.st，分片層面元信息 ShardStateMetaData（version 、indexUUID、primary 等）。</p></li></ol><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">上述信息被持久化到磁盤：持久化的 state 不包括某個分片存在於哪個節點這種內容路由信息，集羣完全重啓時，依靠 gateway 的 recovery 過程重建 RoutingTable 和 RoutingNode。當讀取某個文檔時， 根據路由算法確定目的分片後，再從 RoutingTable 中查找分片位於哪個節點，然後將請求轉發到目的節點[1]。</p><blockquote><p style="color:#999999; margin-left:0; margin-right:0">⚠️ 注意：在 es7.0.0 之後 es 的元信息存儲方式發生變化；<br> es7.0.0 之後元信息存儲改使用 lucene 的方式存儲，見<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F50928" rel="nofollow" target="_blank">pr50928 Move metadata storage to Lucene</a>)</p></blockquote><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">7.10.2 專有主節點，集羣元數據</p><pre><code>./
|-- _state
|   |-- _39h.cfe
|   |-- _39h.cfs
|   |-- _39h.si
|   |-- node-0.st
|   |-- segments_50d
|   `-- write.lock
`-- node.lock
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">6.8.13 專有主節點，集羣元數據</p><pre><code>./
|-- _state
|   |-- global-230.st
|   `-- node-2.st
|-- indices
|   |-- -hiy4JnoRfqUJHTJoNUt4Q
|   |   `-- _state
|   |       `-- state-4.st
|   `-- ylJKVlqISGOi8EkpxHE_2A
|       `-- _state
|           `-- state-6.st
`-- node.lock
</code></pre><span id="OSC_h1_5"></span><h1>3 災難場景與處理方法</h1><span id="OSC_h2_6"></span><h2>3.1 master 節點丟失</h2><blockquote><p style="color:#999999; margin-left:0; margin-right:0">⚠️ 注意本文所述的 master 節點個數，假設前提均為 3 個</p></blockquote><span id="OSC_h3_7"></span><h3>場景 1 master 節點丟失過半</h3><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">master 節點是控制整個集羣；當該種節點角色丟失過半，由於集羣中投票節點永遠不可能達到 quorum 無法選主，將無法維持 es 節點形成一個集羣；雖然集羣無法形成一個集羣，但所仍幸 master-eligible 節點存活,我們可以使用如下手段進行處理。</p><span id="OSC_h4_8"></span><h4>es7.0.0 版本之前</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">1 修改剩餘節點的 elasticsearch.yaml 配置如下,修改 quorum 的個數，然後啓動剩餘的節點，形成一個新的集羣；</p><pre><code>discovery.zen.minimum_master_nodes: 1
discovery.zen.ping.unicast.hosts:
- masters-0
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">2 重建補充之前丟失的 master-eligible 節點，加入集羣之後.<br> 3 將集羣配置修改為舊的配置，再逐一重啓下集羣中的節點，先從 master-eligible 開始.</p><span id="OSC_h4_9"></span><h4>es7.0.0（包含）版本之後.</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">在 es7.0.0 版本之後,由於 es 修改集羣的啓動配置,新增配置<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.0%2Fdiscovery-settings.html%23unicast.hosts" rel="nofollow" target="_blank">discovery.seed_hosts</a><span>&nbsp;</span>和<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.0%2Fdiscovery-settings.html%23initial_master_nodes" rel="nofollow" target="_blank">cluster.initial_master_nodes</a>；es 集羣第一次啓動時稱為 bootstrap，該過程將配置文件中的 cluster.initial_master_node 作為初始的投票節點<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fmodules-discovery-voting.html" rel="nofollow" target="_blank">Voting configurations</a>，投票節點具有選舉 master 和 commit cluster state 的權利，超過半數以上同意即投票成功。如果在集羣健康的場景下，我們需要下線超過半數的 master-eligible；則必須首先使用投票配置排除 API 從投票配置中排除受影響的節點。</p><pre><code>POST _cluster/voting_config_exclusions?node_names={node_names}
POST _cluster/voting_config_exclusions?node_ids={node_ids}
DELETE _cluster/voting_config_exclusions
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">但是如果丟失的 master 節點超過半數，則可以使用新的集羣處理工具 elasticsearch-node unsafe-bootstrap<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F37696" rel="nofollow" target="_blank">pr37696</a><span>&nbsp;</span>和 elasticsearch-node detach-cluster<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F37979" rel="nofollow" target="_blank">pr37979</a></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">面對丟失半數 master-eligible，es7.0.0（包含）版本之後的處理步驟如下：<br> 1 使用<code>bin/elasticsearch-node unsafe-bootstrap</code>命令讓唯一主節點以不安全的方式改寫投票節點，就像重新進行 bootstrap 一樣，自己使用持久化的 cluster state 形成一個新集羣<br> 2 其他數據節點無法加入新集羣因為 UUID 不同 (es 使用 UUID 作為節點和集羣的唯一表示，每個節點都會持久化當前集羣的 UUID)，使用<code>bin/elasticsearch-node detach-cluster</code>命令讓節點離開之前的集羣<br> 3 啓動數據節點和新的 master-eligible 節點 (如下補充兩個新的 master-eligible)，他會加入新集羣中</p><pre><code>cluster.initial_master_nodes:
- {master-0}
- {new-master-1}
- {new-master-2}
discovery.seed_hosts:
- {master-ip-0}
- {new-master-ip-1}
- {new-master-ip-2}
</code></pre><span id="OSC_h3_10"></span><h3>場景 2 master 節點全部丟失</h3><span id="OSC_h4_11"></span><h4>es7.0.0 版本之前</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">1 關閉 security 功能 (如果開啓了, 最好先關閉 security 插件功能)：<br> 1.1 因為新啓動的 master 節點, 沒有數據節點 (如果只配置了一個 master 的角色), security 插件的初始化無法完成, 各類接口不好調用<br> 1.2 如果給新啓動的 master 節點, 配置了 master and data 角色, 則 security 插件會初始化成功. 會插入 index, 但是這個 index 會和原來的 data 節點上保存的衝突. 不知道怎麼解.<br> elastic 官方<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fconfiguring-security.html" rel="nofollow" target="_blank">xpack-security</a>;關閉鑑權：<code>xpack.security.enabled:false</code><br> 2 啓動足夠的新 master-eligible 節點形成一個新集羣.</p><pre><code>discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.unicast.hosts:
- {new-masters-1}
- {new-masters-2}
- {new-masters-3}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">3 修改數據節點的為新 master 的地址,並且刪除掉節點上的_state(因為新集羣的 cluster UUID 不一致)，同上<br> 4 啓動數據節點，數據被恢復加入到集羣</p><span id="OSC_h4_12"></span><h4>es7.0.0（包含）版本之後</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">已經沒有 cluster state 了，唯一的希望是數據節點上的 index 數據；恢復方式藉助<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fnode-tool.html" rel="nofollow" target="_blank">elasticsearch-node</a><span>&nbsp;</span>工具</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">1 關閉 security 功能 (如果開啓了, 最好先關閉 security 插件功能),原因同上<br> 2 啓動足夠的新 master-eligible 節點形成一個新集羣</p><pre><code>cluster.initial_master_nodes:
- {new-master-0}
- {new-master-1}
- {new-master-2}
discovery.seed_hosts:
- {new-master-ip-0}
- {new-master-ip-1}
- {new-master-ip-2}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">3<span>&nbsp;</span><code>bin/elasticsearch-node detach-cluster</code>命令讓數據節點離開之前的集羣</p><pre><code>./bin/elasticsearch-node detach-cluster
------------------------------------------------------------------------

    WARNING: Elasticsearch MUST be stopped before running this tool.

------------------------------------------------------------------------

You should only run this tool if you have permanently lost all of the
master-eligible nodes in this cluster and you cannot restore the cluster
from a snapshot, or you have already unsafely bootstrapped a new cluster
by running `elasticsearch-node unsafe-bootstrap` on a master-eligible
node that belonged to the same cluster as this node. This tool can cause
arbitrary data loss and its use should be your last resort.

Do you want to proceed?

Confirm [y/N] y
Node was successfully detached from the cluster
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">4<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fdangling-indices-list.html" rel="nofollow" target="_blank">查詢 dangling 索引</a>,<code>GET /_dangling</code>, 改 api 引入 es7.9 版本於<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F58176" rel="nofollow" target="_blank">pr58176</a><br> 5 啓動數據節點並使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fdangling-index-import.html" rel="nofollow" target="_blank">Import dangling indexAPI</a>將 index 數據 import 到 cluster state 中 (官方推薦，es7.9 版本之後). 或者，配置<code>gateway.auto_import_dangling_indices: true</code>引入於 es7.6 版本<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F49174" rel="nofollow" target="_blank">pr49174</a>（es7.6.0-7.9.0 可用該配置，在 7.6 版本之前不需要配置默認加載 dangling 索引）並啓動數據節點</p><pre><code>POST /_dangling/{index-uuid}?accept_data_loss=true
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">6 導入完成之後，索引 recovery 之後即可進行讀寫</p><blockquote><blockquote><p style="color:#999999; margin-left:0; margin-right:0">注意</p></blockquote><p style="color:#999999; margin-left:0; margin-right:0"><strong>Q1</strong>: 為什麼 7.6.0 之後需要配置,才能處理懸空索引（dangling index）才能讓數據加入新集羣，7.6.0 之後沒有懸空索引嗎？<br><strong>A1</strong>: 其實也是有的，只不過在 es2 版本將配置移除（對應<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F10016" rel="nofollow" target="_blank">pr10016</a>）,默認自動加載 dangling index（es2.0-es7.6）; 具體實現於<code>org.elasticsearch.gateway.DanglingIndicesState#processDanglingIndices</code><span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F49174" rel="nofollow" target="_blank">es7.6 再次引入 dangling 配置</a>，es7.9 引入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.9%2Fdangling-index-import.html" rel="nofollow" target="_blank">dangling index rest api</a></p></blockquote><blockquote><p style="color:#999999; margin-left:0; margin-right:0"><strong>Q2</strong>: 什麼是 dangling 索引？<br><strong>A2</strong>: 當一個節點加入集羣時，如果發現存儲在其本地數據目錄中的任何分片（shard）不存在於集羣中，將認為這些分片屬於「懸空」索引。懸空索引產生的場景（1）在 Elasticsearch 節點離線時刪除了多個<code>cluster.indices.tombstones.size</code><span>&nbsp;</span>索引，節點再次加入集羣集羣 （2）master 節點丟失，數據節點重新加入新的集羣等</p></blockquote><span id="OSC_h2_13"></span><h2>3.2 數據節點故障</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">數據節點災難故障之後，無法恢復加入集羣；可將數據物理複製到新的節點，然後按照 master 節點丟失的方式，將數據節點加入集羣即可。</p><span id="OSC_h2_14"></span><h2>3.3 分片不能夠自動分配</h2><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">查看索引分片為什麼無法分配，<code>POST _cluster/allocation/explain</code></p><span id="OSC_h3_15"></span><h3>3.3.1 分片正常</h3><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果分片數據正常，那麼我們可以嘗試重試分配分片任務;<code>POST _cluster/reroute?retry_failed</code></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">獲取索引的 shard 在那些節點上，使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.0%2Findices-shards-stores.html" rel="nofollow" target="_blank">_shard_stores api</a></p><pre><code>GET indexName1/_shard_stores
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">使用<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F7.10%2Fcluster-reroute.html" rel="nofollow" target="_blank">cluster reroute</a>重新分配</p><pre><code># 嘗試分配副本 
POST /_cluster/reroute
{
  "commands": [
    {
      "allocate_replica": {
        "index": "{indexName1}",
        "shard": {shardId},
        "node": "{nodes-9}"
      }
    }
  ]
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果是主分片無法分配，可以嘗試如下命令進行分配</p><pre><code>POST /_cluster/reroute
{
  "commands": [
    {
      "allocate_stale_primary": {
        "index": "{indexName1}",
        "shard": {shardId},
        "node": {nodes-9},
        "accept_data_loss": true
      }
    }
  ]
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">如果主分片確實是無法分配，只能選擇丟失該分片的數據，分配一個空的主分片</p><pre><code>POST /_cluster/reroute
{
  "commands": [
    {
      "allocate_empty_primary": {
        "index": "{indexName1}",
        "shard": {shardId},
        "node": "{nodes-9}",
        "accept_data_loss": true
      }
    }
  ]
}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">es5.0 版本之前參考; https://www.elastic.co/guide/en/elasticsearch/reference/2.4/cluster-reroute.html</p><span id="OSC_h3_16"></span><h3>3.3.2 分片數據損壞</h3><span id="OSC_h4_17"></span><h4>shard corrupted</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">錯誤參考<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdiscuss.elastic.co%2Ft%2Fcorrupted-elastic-index%2F135932" rel="nofollow" target="_blank">Corrupted elastic index</a></p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start"><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2F6.5%2Fshard-tool.html" rel="nofollow" target="_blank">shard-tool</a><span>&nbsp;</span>es6.5 版本引入，該操作需要 stop 節點<br> elasticsearch-shard 工具 es6.5 版本引入<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F33848" rel="nofollow" target="_blank">pr33848</a><br> elasticsearch-shard remove-corrupted-data 的 es7.0.0 引入<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F32281" rel="nofollow" target="_blank">pr32281</a></p><pre><code>bin/elasticsearch-shard remove-corrupted-data --index {indexName} --shard-id {shardId}
## 示列：修復索引 twitter 的 0 號分片
bin/elasticsearch-shard remove-corrupted-data --index twitter --shard-id 0

## 如果--index 和--shard-id 換成索引分片目錄參數--dir，則直接修復 data 和 translog
bin/elasticsearch-shard remove-corrupted-data --dir /var/lib/elasticsearchdata/nodes/0/indices/P45vf_YQRhqjfwLMUvSqDw/0
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">修復完成之後，啓動節點，如果分片不能夠自動分配,使用 reroute 命令進行 shard 分片</p><pre><code>POST /_cluster/reroute{
  "commands":[
    {
      "allocate_stale_primary":{
        "index":"index42",
        "shard":0,
        "node":"node-1",
        "accept_data_loss":false
      }
    }
  ]}
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">5 版本之前可以通過索引級別配置，進行修復<br> index.shard.check_on_startup: fix ，該配置在 es6.5 版本移除<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F32279" rel="nofollow" target="_blank">pr32279</a></p><span id="OSC_h4_18"></span><h4>translog 損壞</h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">修復 translog 操作，需要 stop 節點。</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">修復工具 elasticsearch-translog es5.0.0 引入<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F19342" rel="nofollow" target="_blank">pr19342</a><br> elasticsearch-shard remove-corrupted-data translog 的 es7.4.1 開始引入，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F47866" rel="nofollow" target="_blank">pr47866</a>elasticsearch-shard 可以直接清除 translog，也可以像上文中指定--dir 那樣進行修復 translog</p><pre><code>bin/elasticsearch-shard remove-corrupted-data --index  --shard-id   --truncate-clean-translog
## 示列：修復索引 twitter 的 0 號分片
bin/elasticsearch-shard remove-corrupted-data --index twitter --shard-id 0 --truncate-clean-translog
</code></pre><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">清除完成之後使用 cluster reroute 進行恢復</p><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">5 版本之前可以通過索引級別配置，進行修復<br> index.shard.check_on_startup: fix ，該配置在 es6.5 版本移除<span>&nbsp;</span><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fpull%2F32279" rel="nofollow" target="_blank">pr32279</a></p><span id="OSC_h4_19"></span><h4><code>segments_N 文件丟失</code></h4><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">該種場景的文件損壞是最難修復的；官方還未提供工具，我們正在自己調研中</p><span id="OSC_h1_20"></span><h1>4 參考</h1><p style="color:#222222; margin-left:0; margin-right:0; text-align:start">[1]<span>&nbsp;</span>elasticsearch 集羣啓動流程<br> [2]https://www.elastic.co/guide/en/elasticsearch/reference/7.9/dangling-indices-list.html<br> [3]https://www.elastic.co/guide/en/elasticsearch/reference/7.10/node-tool.html</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">作者：京東科技，楊松柏</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left">來源：京東雲開發者社區，轉載請註明來源</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 03:19:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4090830/blog/10920402</guid>
            <link>https://my.oschina.net/u/4090830/blog/10920402</link>
            <author>
                <![CDATA[京東雲開發者]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[CursusDB —— 面向文檔的內存數據庫]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>CursusDB 是一種面向文檔的快速開源內存數據庫，提供安全性、持久性、分佈性、可用性和類似 SQL 的查詢語言。</p><p>CursusDB 的設想是創建無限可擴展的東西，同時又不會真正減慢速度。假設你有 10 億個文檔存儲在分佈在 100 個節點的 1 個集合中，當集羣同時在所有節點上啓動非插入操作時，集羣將在查詢 1000 萬個文檔所需的時間內查詢 10 億個文檔。這就是並行搜索的力量。Cursus 系統可同時在用戶集合的多個部分中進行搜索。一個集羣可以同時查詢數千個節點。將主節點視為多個或一個集合的碎片。每個集合都會鎖定插入、更新和刪除，但由於 CursusDB 的分佈式設計，它就像一個併發交換機，允許大量併發事務。一個集羣或多個集羣採取操作，這些操作作為請求同時轉發到 1 個或多個節點。一致性和可靠性是設計 CursusDB 時的主要目標之一。</p><p style="text-align:start"><strong><span><span><span><span><span style="color:#1f2328"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>特性</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></strong></p><ul><li>使用共享密鑰和 OR TLS 保護集羣和節點通信</li><li>運行時內存中的數據</li><li>並行搜索。同時搜索多個節點內的集合部分。</li><li>自動為所有節點生成唯一的所有文檔的 $id 鍵</li><li>具有基本（R、RW）權限的數據庫用戶</li><li>集羣和節點認證</li><li>專門針對讀取的集羣節點數據複製和同步</li><li>JSON 對象插入</li><li>非結構化集合</li><li>集羣和客戶端身份驗證</li><li>節點（插入、更新、刪除）實時轉發給觀察者</li><li>如果連接丟失，節點觀察者自動重新連接</li><li>類似 SQL 的查詢語言（CDQL - Cursus 文檔查詢語言）</li><li>低延遲</li><li>高可用</li><li>默認情況下使用共享密鑰和用戶確保安全</li><li>高度可配置</li><li>輕量級核心代碼總共不到 6000 行代碼</li><li>基於<code>log-max-lines</code>配置的文件日誌記錄和自動日誌截斷</li><li>自動重新連接任何丟失的節點或節點副本</li><li>如果 .curodeconfig 中的 automatic-backup 設置為 true，則自動備份節點</li><li>如果 .curodeconfig 中的 automatic-backup-cleanup 設置為 true，則自動清理節點備份。</li><li>如果配置了自動備份，則在數據損壞時自動恢復節點</li><li>節點數據 (.cdat) 和節點備份 (/backups/.cdat.{unixtime}) 是在關機或備份時通過序列化-加密 (chacha20poly1305)-壓縮 (DEFLATE) 將內存中的數據序列化、加密並逐塊壓縮後創建的。</li><li>......</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:33:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/cursusdb</guid>
            <link>https://www.oschina.net/p/cursusdb</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推薦 | 中文對話 0.2B 小模型 ChatLM-Chinese-0.2B]]>
            </title>
            <description>
                <![CDATA[<div align="center"><h1><a id="user-content-中文對話 02b 小模型-chatlm-chinese-02b" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D02b%E5%B0%8F%E6%A8%A1%E5%9E%8B-chatlm-chinese-02b"></a>中文對話 0.2B 小模型 ChatLM-Chinese-0.2B</h1><p>中文  | <a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/README.en.md">English</a></p></div><h1><a id="user-content-一介紹" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%80%E4%BB%8B%E7%BB%8D"></a>一、👋介紹</h1><p>現在的大語言模型的參數往往較大，消費級電腦單純做推理都比較慢，更別説想自己從頭開始訓練一個模型了。本項目的目標是整理生成式語言模型的訓練流程，包括數據清洗、tokenizer 訓練、模型預訓練、SFT 指令微調、RLHF 優化等。</p><p>ChatLM-mini-Chinese 為中文對話小模型，模型參數只有 0.2B（算共享權重約 210M），可以在最低 4GB 顯存的機器進行預訓練（<code>batch_size=1</code>，<code>fp16</code>或者<code> bf16</code>），<code>float16</code>加載、推理最少只需要 512MB 顯存。</p><ul><li>公開所有預訓練、SFT 指令微調、DPO 偏好優化數據集來源。</li><li>使用<code>Huggingface</code>NLP 框架，包括<code>transformers</code>、<code>accelerate</code>、<code>trl</code>、<code>peft</code>等。</li><li>自實現<code>trainer</code>，支持單機單卡、單機多卡進行預訓練、SFT 微調。訓練過程中支持在任意位置停止，及在任意位置繼續訓練。</li><li>預訓練：整合為端到端的<code>Text-to-Text</code>預訓練，非<code>mask</code>掩碼預測預訓練。
<ul><li>開源所有數據清洗（如規範化、基於 mini_hash 的文檔去重等）、數據集構造、數據集加載優化等流程；</li><li>tokenizer 多進程詞頻統計，支持<code>sentencepiece</code>、<code>huggingface tokenizers</code>的 tokenizer 訓練；</li><li>預訓練支持任意位置斷點，可從斷點處繼續訓練;</li><li>大數據集（GB 級別）流式加載、支持緩衝區數據打亂，不利用內存、硬盤作為緩存，有效減少內存、磁盤佔用。配置<code>batch_size=1, max_len=320</code>下，最低支持在 16GB 內存+4GB 顯存的機器上進行預訓練；</li><li>訓練日誌記錄。</li></ul></li><li>SFT 微調：開源 SFT 數據集及數據處理過程。
<ul><li>自實現<code>trainer</code>支持 prompt 指令微調， 支持任意斷點繼續訓練；</li><li>支持<code>Huggingface trainer</code>的<code>sequence to sequence</code>微調；</li><li>支持傳統的低學習率，只訓練 decoder 層的微調。</li></ul></li><li>偏好優化：使用 DPO 進行全量偏好優化。
<ul><li>支持使用<code>peft lora</code>進行偏好優化；</li><li>支持模型合併，可將<code>Lora adapter</code>合併到原始模型中。</li></ul></li><li>支持下游任務微調：<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract">finetune_examples</a>給出<strong>三元組信息抽取任務</strong>的微調示例，微調後的模型對話能力仍在。</li></ul><p>如果需要做基於小模型的檢索增強生成（RAG），可以參考我的另一個項目<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese">Phi2-mini-Chinese</a>，代碼見<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2FPhi2-mini-Chinese%2Fblob%2Fmain%2Frag_with_langchain.ipynb">rag_with_langchain.ipynb</a></p><p>🟢<strong>最近更新</strong></p><details><summary><b>2024-01-07</b></summary>
- 添加數據清洗過程中基於 mini hash 實現的文檔去重（在本項目中其實是數據集的樣本去重），防止模型遇到多次重複數據後，在推理時吐出訓練數據。<br>
- 添加`DropDatasetDuplicate`類實現對大數據集的文檔去重。<br></details><details><summary><b>2023-12-29</b></summary>
- 更新模型代碼（權重不變），可以直接使用`AutoModelForSeq2SeqLM.from_pretrained(...)`加載模型使用。<br>
- 更新 readme 文檔。<br></details><details><summary><b>2023-12-18</b></summary>
- 補充利用`ChatLM-mini-0.2B`模型微調下游三元組信息抽取任務代碼及抽取效果展示 。<br>
- 更新 readme 文檔。<br></details><details><summary><b>2023-12-14</b></summary>
- 更新 SFT、DPO 後的模型權重文件。 <br>
- 更新預訓練、SFT 及 DPO 腳本。 <br>
- 更新`tokenizer`為`PreTrainedTokenizerFast`。 <br>
- 重構`dataset`代碼，支持動態最大長度，每個批次的最大長度由該批次的最長文本決定，節省顯存。 <br>
- 補充`tokenizer`訓練細節。 <br></details><details><summary><b>2023-12-04</b></summary>
- 更新`generate`參數及模型效果展示。<br>
- 更新 readme 文檔。<br></details><details><summary><b>2023-11-28</b></summary>
- 更新 dpo 訓練代碼及模型權重。<br></details><details><summary><b>2023-10-19</b></summary>
- 項目開源， 開放模型權重供下載。 <br></details><h1><a id="user-content-二️chatlm-02b-chinese 模型訓練過程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%8C%EF%B8%8Fchatlm-02b-chinese%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>二、🛠️ChatLM-0.2B-Chinese 模型訓練過程</h1><h2><a id="user-content-21-預訓練數據集" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#21-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"></a>2.1 預訓練數據集</h2><p>所有數據集均來自互聯網公開的<strong>單輪對話</strong>數據集，經過數據清洗、格式化後保存為 parquet 文件。數據處理過程見<code>utils/raw_data_process.py</code>。主要數據集包括：</p><ol><li>社區問答 json 版 webtext2019zh-大規模高質量數據集，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fbrightmart%2Fnlp_chinese_corpus">nlp_chinese_corpus</a>。共 410 萬，清洗後剩餘 260 萬。</li><li>baike_qa2019 百科類問答，見：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F107726">https://aistudio.baidu.com/datasetdetail/107726</a>，共 140 萬，清醒後剩餘 130 萬。</li><li>中國醫藥領域問答數據集，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FToyhom%2FChinese-medical-dialogue-data">Chinese-medical-dialogue-data</a>，共 79 萬，清洗後剩餘 79 萬。</li><li><del>金融行業問答數據，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F609821974">https://zhuanlan.zhihu.com/p/609821974</a>，共 77 萬，清洗後剩餘 52 萬。</del><strong>數據質量太差，未採用。</strong></li><li>知乎問答數據，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fwangrui6%2FZhihu-KOL">Zhihu-KOL</a>，共 100 萬行，清洗後剩餘 97 萬行。</li><li>belle 開源的指令訓練數據，介紹：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>，下載：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2FBelleGroup">BelleGroup</a>，僅選取<code>Belle_open_source_1M</code>、<code>train_2M_CN</code>、及<code>train_3.5M_CN</code>中部分回答較短、不含複雜表格結構、翻譯任務（沒做英文詞表）的數據，共 370 萬行，清洗後剩餘 338 萬行。</li><li>維基百科（Wikipedia）詞條數據，將詞條拼湊為提示語，百科的前<code>N</code>個詞為回答，使用<code>202309</code>的百科數據，清洗後剩餘 119 萬的詞條提示語和回答。Wiki 下載：<a href="https://gitee.com/link?target=https%3A%2F%2Fdumps.wikimedia.org%2Fzhwiki%2F">zhwiki</a>，將下載的 bz2 文件轉換為 wiki.txt 參考：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fapertium%2FWikiExtractor">WikiExtractor</a>。</li></ol><p>數據集總數量 1023 萬：Text-to-Text 預訓練集：930 萬，評估集：2.5 萬（因為解碼較慢，所以沒有把評估集設置太大）。<del>測試集：90 萬。</del>
SFT 微調和 DPO 優化數據集見下文。</p><h2><a id="user-content-22-模型" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#22-%E6%A8%A1%E5%9E%8B"></a>2.2 模型</h2><p>T5 模型（Text-to-Text Transfer Transformer），詳情見論文: <a href="https://gitee.com/link?target=https%3A%2F%2Farxiv.org%2Fabs%2F1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>。</p><p>模型源碼來自 huggingface，見：<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftransformers%2Fblob%2Fmain%2Fsrc%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py%23L1557">T5ForConditionalGeneration</a>。</p><p>模型配置見<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese%2Fblob%2Fmain%2Fconfig.json">model_config.json</a>，官方的<code>T5-base</code>：<code>encoder layer</code>和<code>decoder layer </code>均為為 12 層，本項目這兩個參數修改為 10 層。</p><p>模型參數：0.2B。詞表大小：29298，僅包含中文和少量英文。</p><h2><a id="user-content-23-訓練過程" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#23-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"></a>2.3 訓練過程</h2><p>硬件：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 預訓練階段：</span></span><span id="LC2" class="line">CPU: 28 vCPU Intel<span class="o">(</span>R<span class="o">)</span> Xeon<span class="o">(</span>R<span class="o">)</span> Gold 6330 CPU @ 2.00GHz</span><span id="LC3" class="line">內存：60 GB</span><span id="LC4" class="line">顯卡：RTX A5000<span class="o">(</span>24GB<span class="o">)</span><span class="k">*</span> 2</span><span id="LC5" class="line"></span><span id="LC6" class="line"><span class="c"># sft 及 dpo 階段：</span></span><span id="LC7" class="line">CPU: Intel<span class="o">(</span>R<span class="o">)</span> i5-13600k @ 5.1GHz</span><span id="LC8" class="line">內存：32 GB</span><span id="LC9" class="line">顯卡：NVIDIA GeForce RTX 4060 Ti 16GB <span class="k">*</span> 1</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li><p><strong>tokenizer 訓練</strong>： 現有<code>tokenizer</code>訓練庫遇到大語料時存在 OOM 問題，故全量語料按照類似<code>BPE</code>的方法根據詞頻合併、構造詞庫，運行耗時半天。</p></li><li><p><strong>Text-to-Text 預訓練</strong>：學習率為<code>1e-4</code>到<code>5e-3</code>的動態學習率，預訓練時間為 8 天。訓練損失：</p></li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/train_loss.png" alt="traing loss" referrerpolicy="no-referrer"></p><ol start="3"><li><strong>prompt 監督微調（SFT）</strong>：使用<code>belle</code>指令訓練數據集（指令和回答長度都在 512 以下），學習率為<code>1e-7</code>到<code>5e-5</code>的動態學習率，微調時間 2 天。微調損失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/sft_loss.png" alt="finetune loss" referrerpolicy="no-referrer"></p><ol start="4"><li><strong>dpo 直接偏好優化</strong>：數據集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>作為<code>chosen</code>文本，步驟<code>2</code>中 SFT 模型對數據集中的 prompt 做批量<code>generate</code>，得到<code>rejected</code>文本，耗時 1 天，dpo 全量偏好優化，學習率<code>le-5</code>，半精度<code>fp16</code>,共<code>2</code>個<code>epoch</code>，耗時 3h。dpo 損失：</li></ol><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/dpo_loss.png" alt="dpo loss" referrerpolicy="no-referrer"></p><h2><a id="user-content-24-對話效果展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#24-%E5%AF%B9%E8%AF%9D%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA"></a>2.4 對話效果展示</h2><h3><a id="user-content-241-stream-chat" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#241-stream-chat"></a>2.4.1 stream chat</h3><p>默認使用<code>huggingface transformers</code>的 <code>TextIteratorStreamer</code>實現流式對話，只支持<code>greedy search</code>，如果需要<code>beam sample</code>等其他生成方式，請將<code>cli_demo.py</code>的<code>stream_chat</code>參數修改為<code>False</code>。
<img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/stream_chat.gif" alt="" referrerpolicy="no-referrer"></p><h3><a id="user-content-242-對話展示" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#242-%E5%AF%B9%E8%AF%9D%E5%B1%95%E7%A4%BA"></a>2.4.2 對話展示</h3><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/show1.png" alt="" referrerpolicy="no-referrer"></p><p>存在問題：預訓練數據集只有 900 多萬，模型參數也僅 0.2B，不能涵蓋所有方面，會有答非所問、廢話生成器的情況。</p><h1><a id="user-content-三使用説明" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%B8%89%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"></a>三、📑使用説明</h1><h2><a id="user-content-31-快速開始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#31-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"></a>3.1 快速開始：</h2><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="kn">from</span><span class="nn">transformers</span><span class="kn">import</span><span class="n">AutoTokenizer</span><span class="p">,</span><span class="n">AutoModelForSeq2SeqLM</span></span><span id="LC2" class="line"><span class="kn">import</span><span class="nn">torch</span></span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="n">model_id</span><span class="o">=</span><span class="s">'charent/ChatLM-mini-Chinese'</span></span><span id="LC5" class="line"><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="k">if</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span><span class="k">else</span><span class="s">'cpu'</span><span class="p">)</span></span><span id="LC6" class="line"></span><span id="LC7" class="line"><span class="n">tokenizer</span><span class="o">=</span><span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span></span><span id="LC8" class="line"><span class="n">model</span><span class="o">=</span><span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span><span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="n">txt</span><span class="o">=</span><span class="s">'如何評價 Apple 這家公司？'</span></span><span id="LC11" class="line"></span><span id="LC12" class="line"><span class="n">encode_ids</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">([</span><span class="n">txt</span><span class="p">])</span></span><span id="LC13" class="line"><span class="n">input_ids</span><span class="p">,</span><span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'input_ids'</span><span class="p">]),</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">encode_ids</span><span class="p">[</span><span class="s">'attention_mask'</span><span class="p">])</span></span><span id="LC14" class="line"></span><span id="LC15" class="line"><span class="n">outs</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">my_generate</span><span class="p">(</span></span><span id="LC16" class="line"><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC17" class="line"><span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span></span><span id="LC18" class="line"><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span></span><span id="LC19" class="line"><span class="n">search_type</span><span class="o">=</span><span class="s">'beam'</span><span class="p">,</span></span><span id="LC20" class="line"><span class="p">)</span></span><span id="LC21" class="line"></span><span id="LC22" class="line"><span class="n">outs_txt</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outs</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">(),</span><span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></span><span id="LC23" class="line"><span class="k">print</span><span class="p">(</span><span class="n">outs_txt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">Apple 是一家專注於設計和用戶體驗的公司，其產品在設計上注重簡約、流暢和功能性，而在用戶體驗方面則注重用戶的反饋和使用體驗。作為一家領先的科技公司，蘋果公司一直致力於為用戶提供最優質的產品和服務，不斷推陳出新，不斷創新和改進，以滿足不斷變化的市場需求。</span><span id="LC2" class="line">在 iPhone、iPad 和 Mac 等產品上，蘋果公司一直保持着創新的態度，不斷推出新的功能和設計，為用戶提供更好的使用體驗。在 iPad 上推出的 iPad Pro 和 iPod touch 等產品，也一直保持着優秀的用戶體驗。</span><span id="LC3" class="line">此外，蘋果公司還致力於開發和銷售軟件和服務，例如 iTunes、iCloud 和 App Store 等，這些產品在市場上也獲得了廣泛的認可和好評。</span><span id="LC4" class="line">總的來説，蘋果公司在設計、用戶體驗和產品創新方面都做得非常出色，為用戶帶來了許多便利和驚喜。</span><span id="LC5" class="line"></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-32-從克隆倉庫代碼開始" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#32-%E4%BB%8E%E5%85%8B%E9%9A%86%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E5%BC%80%E5%A7%8B"></a>3.2 從克隆倉庫代碼開始</h2><p>本項目模型為<code>TextToText</code>模型，在預訓練階段、SFT 階段、RLFH 階段的<code>prompt</code>、<code>response</code>等字段，請務必加上<code>[EOS]</code>句子結束標記。<br>
本項目模型為<code>TextToText</code>模型，在預訓練階段、SFT 階段、RLFH 階段的<code>prompt</code>、<code>response</code>等字段，請務必加上<code>[EOS]</code>句子結束標記。<br>
本項目模型為<code>TextToText</code>模型，在預訓練階段、SFT 階段、RLFH 階段的<code>prompt</code>、<code>response</code>等字段，請務必加上<code>[EOS]</code>句子結束標記。</p><h3><a id="user-content-321-克隆項目" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#321-%E5%85%8B%E9%9A%86%E9%A1%B9%E7%9B%AE"></a>3.2.1 克隆項目：</h3><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">git clone <span class="nt">--depth</span> 1 https://github.com/charent/ChatLM-mini-Chinese.git</span><span id="LC2" class="line"></span><span id="LC3" class="line"><span class="nb">cd </span>ChatLM-mini-Chinese</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-322-安裝依賴" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#322-%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"></a>3.2.2 安裝依賴</h3><p>本項目推薦使用<code>python 3.10</code>，過老的 python 版本可能不兼容所依賴的第三方庫。</p><p>pip 安裝：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">pip <span class="nb">install</span><span class="nt">-r</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>如果 pip 安裝了 CPU 版本的 pytorch，可以通過下面的命令安裝 CUDA 版本的 pytorch：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># pip 安裝 torch + cu118</span></span><span id="LC2" class="line">pip3 <span class="nb">install </span>torch <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu118</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>conda 安裝：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">conda <span class="nb">install</span><span class="nt">--yes</span><span class="nt">--file</span> ./requirements.txt</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h3><a id="user-content-323-下載預訓練模型及模型配置文件" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#323-%E4%B8%8B%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"></a>3.2.3 下載預訓練模型及模型配置文件</h3><p>用<code>git</code>命令從<code>Hugging Face Hub</code>下載模型權重及配置文件，需要先安裝<a href="https://gitee.com/link?target=https%3A%2F%2Fdocs.github.com%2Fzh%2Frepositories%2Fworking-with-files%2Fmanaging-large-files%2Finstalling-git-large-file-storage">Git LFS</a>，然後運行:</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 使用 git 命令下載 huggingface 模型，先安裝[Git LFS]，否則下載的模型文件不可用</span></span><span id="LC2" class="line">git clone <span class="nt">--depth</span> 1 https://huggingface.co/charent/ChatLM-mini-Chinese</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="nb">mv </span>ChatLM-mini-Chinese model_save</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>也可以直接從<code>Hugging Face Hub</code>倉庫<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>手工下載，將下載的文件移動到<code>model_save</code>目錄下即可。</p><h2><a id="user-content-33-tokenizer 訓練" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#33-tokenizer%E8%AE%AD%E7%BB%83"></a>3.3 Tokenizer 訓練</h2><p>原本打算直接用現成的<code>tokenizer</code>庫訓練的（如<code>sentencepiece</code>），但是數據集一大就容易 OOM。另外預訓練數據集各個領域的語料不平衡，會產生很多不必要的合併。最後使用<code>jieba</code>分詞對所有的預訓練語料切詞後統計詞頻，只保留出現 1500 次以上的字、詞，參照<code>PreTrainedTokenizerFast</code>的<code>BPE model</code>的保存格式，構造<code>tokenzier</code>，最後轉換為<code>PreTrainedTokenizerFast</code>。核心代碼如下，詳細的處理過程見<code>utils/train_tokenizer.py</code>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c1"># 構造 merge 數組</span></span><span id="LC2" class="line"><span class="n">words_merge_list</span><span class="o">=</span><span class="p">[]</span></span><span id="LC3" class="line"><span class="k">for</span><span class="n">word</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span></span><span id="LC4" class="line"><span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span></span><span id="LC5" class="line"><span class="k">if</span><span class="n">n</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">:</span></span><span id="LC6" class="line"><span class="c1"># a, b 切分 12345 示例： 1 2345,  12 345,   123 45,   1234 5</span></span><span id="LC7" class="line"><span class="k">for</span><span class="n">i</span><span class="ow">in</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span></span><span id="LC8" class="line"><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]),</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="p">])</span></span><span id="LC9" class="line"></span><span id="LC10" class="line"><span class="k">if</span><span class="n">a</span><span class="ow">in</span><span class="n">words_dict</span><span class="ow">and</span><span class="n">b</span><span class="ow">in</span><span class="n">words_dict</span><span class="p">:</span></span><span id="LC11" class="line"><span class="n">words_merge_list</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>本項目還提供了使用預訓練模型自帶的<code>tokenizer</code>根據自己的語料重新訓練<code>tokenizer</code>的例子，見<code>train_tokenizer.ipynb</code>。注意，重新訓練<code>tokenizer</code>後，預訓練模型的權重將無法使用，需要重新訓練模型權重，因為<code>token</code>對應的<code>id</code>變了。</p><h2><a id="user-content-34-text-to-text-預訓練" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#34-text-to-text-%E9%A2%84%E8%AE%AD%E7%BB%83"></a>3.4 Text-to-Text 預訓練</h2><ol><li>預訓練數據集示例</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"對於花園街，你有什麼瞭解或看法嗎？"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"花園街（是香港油尖旺區的一條富有特色的街道，位於九龍旺角東部，北至界限街，南至登打士街，與通菜街及洗衣街等街道平行。現時這條街道是香港著名的購物區之一。位於亞皆老街以南的一段花園街，也就是</span><span class="se">\"</span><span class="s2">波鞋街</span><span class="se">\"</span><span class="s2">整條街約 150 米長，有 50 多間售賣運動鞋和運動用品的店舖。旺角道至太子道西一段則為排檔區，售賣成衣、蔬菜和水果等。花園街一共分成三段。明清時代，花園街是芒角村栽種花卉的地方。此外，根據歷史專家鄭寶鴻的考證：花園街曾是 1910 年代東方殷琴拿煙廠的花園。縱火案。自 2005 年起，花園街一帶最少發生 5 宗縱火案，當中 4 宗涉及排檔起火。2010 年。2010 年 12 月 6 日，花園街 222 號一個賣鞋的排檔於凌晨 5 時許首先起火，濃煙湧往旁邊住宅大廈，消防接報 4"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li><p>jupyter-lab 或者 jupyter notebook:</p><p>見文件<code>train.ipynb</code>，推薦使用 jupyter-lab，避免考慮與服務器斷開後終端進程被殺的情況。</p></li><li><p>控制枱：</p><p>控制枱訓練需要考慮連接斷開後進程被殺的，推薦使用進程守護工具<code>Supervisor</code>或者<code>screen</code>建立連接會話。</p><p>首先要配置<code>accelerate</code>，執行以下命令， 根據提示選擇即可，參考<code>accelerate.yaml</code>，<em>注意：DeepSpeed 在 Windows 安裝比較麻煩</em>。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">accelerate config</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>開始訓練，如果要使用工程提供的配置請在下面的命令<code>accelerate launch</code>後加上參數<code>--config_file ./accelerate.yaml</code>，<em>該配置按照單機 2xGPU 配置。</em></p><p><em>預訓練有兩個腳本，本項目實現的 trainer 對應<code>train.py</code>，huggingface 實現的 trainer 對應<code>pre_train.py</code>，用哪個都可以，效果一致。本項目實現的 trainer 訓練信息展示更美觀、更容易修改訓練細節（如損失函數，日誌記錄等），均支持斷點繼續訓練，本項目實現的 trainer 支持在任意位置斷點後繼續訓練，按<code>ctrl+c</code>退出腳本時會保存斷點信息。</em></p><p>單機單卡：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer</span></span><span id="LC2" class="line">accelerate launch ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">python pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>單機多卡：
<code>2</code>為顯卡數量，請根據自己的實際情況修改。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>從斷點處繼續訓練：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py train <span class="nt">--is_keep_training</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer</span></span><span id="LC5" class="line"><span class="c"># 需要在`pre_train.py`中的`train`函數添加`resume_from_checkpoint=True`</span></span><span id="LC6" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 pre_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div></li></ol><h2><a id="user-content-35-sft 微調" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#35-sft%E5%BE%AE%E8%B0%83"></a>3.5 SFT 微調</h2><p>SFT 數據集全部來自<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2FLianjiaTech%2FBELLE">BELLE</a>大佬的貢獻，感謝。SFT 數據集分別為：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Fgenerated_chat_0.4M">generated_chat_0.4M</a>、<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_0.5M_CN">train_0.5M_CN</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FBelleGroup%2Ftrain_2M_CN">train_2M_CN</a>，清洗後剩餘約 137 萬行。
sft 指令微調數據集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"解釋什麼是歐洲啓示錄"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"歐洲啓示錄（The Book of Revelation）是新約聖經的最後一卷書，也被稱為《啓示錄》、《默示錄》或《約翰默示錄》。這本書從宗教的角度描述了世界末日的來臨，以及上帝對世界的審判和拯救。 書中的主題包括來臨的基督的榮耀，上帝對人性的懲罰和拯救，以及魔鬼和邪惡力量的存在。歐洲啓示錄是一個充滿象徵和暗示的文本，對於解讀和理解有許多不同的方法和觀點。"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>參考<code>data</code>目錄下的示例<code>parquet</code>文件製作自己的數據集，數據集格式：<code>parquet</code>文件分兩列，一列<code>prompt</code>文本，表示提示語，一列<code>response</code>文本，表示期待的模型輸出。
微調細節見<code>model/trainer.py</code>下的<code>train</code>方法, <code>is_finetune</code>設置為<code>True</code>時，將進行微調，微調默認會凍結 embedding 層和 encoder 層，只訓練 decoder 層。如需要凍結其他參數，請自行調整代碼。</p><p>運行 SFT 微調：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c"># 本項目實現的 trainer， 添加參數`--is_finetune=True`即可, 參數`--is_keep_training=True`可從任意斷點處繼續訓練</span></span><span id="LC2" class="line">accelerate launch <span class="nt">--multi_gpu</span><span class="nt">--num_processes</span> 2 ./train.py <span class="nt">--is_finetune</span><span class="o">=</span>True</span><span id="LC3" class="line"></span><span id="LC4" class="line"><span class="c"># 或者使用 huggingface trainer, 多 GPU 請用 accelerate launch --multi_gpu --num_processes gpu 個數 sft_train.py</span></span><span id="LC5" class="line">python sft_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-36-rlhf 強化學習人類反饋優化方法" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#36-rlhf%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"></a>3.6 RLHF（強化學習人類反饋優化方法）</h2><p>偏好方法這裏介紹常見的兩種：PPO 和 DPO，具體實現請自行搜索論文及博客。</p><ol><li><p>PPO 方法（近似偏好優化,Proximal Policy Optimization）<br>
步驟 1：使用微調數據集做有監督微調（SFT， Supervised Finetuning）。<br>
步驟 2：使用偏好數據集（一個 prompt 至少包含 2 個回覆，一個想要的回覆，一個不想要的回覆。多個回覆可以按照分數排序，最想要的分數最高）訓練獎勵模型（RM， Reward Model）。可使用<code>peft</code>庫快速搭建 Lora 獎勵模型。<br>
步驟 3：利用 RM 對 SFT 模型進行有監督 PPO 訓練，使得模型滿足偏好。</p></li><li><p>使用 DPO（直接偏好優化，Direct Preference Optimization）微調（<strong>本項目採用 DPO 微調方法，比較節省顯存</strong>）
在獲得 SFT 模型的基礎上，無需訓練獎勵模型，取得正向回答（chosen）和負向回答（rejected）即可開始微調。微調的<code>chosen</code>文本來自原數據集<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fc-s-ale%2Falpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>，拒絕文本<code>rejected</code>來自 SFT 微調 1 個 epoch 後的模型輸出，另外兩個數據集：<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FSkepsun%2Fhuozi_rlhf_data_json">huozi_rlhf_data_json</a>和<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fbeyond%2Frlhf-reward-single-round-trans_chinese">rlhf-reward-single-round-trans_chinese</a>，合併後共 8 萬條 dpo 數據。</p><p>dpo 數據集處理過程見<code>utils/dpo_data_process.py</code>。</p></li></ol><p>DPO 偏好優化數據集示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="w"></span><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"為給定的產品創建一個創意標語。，輸入：可重複使用的水瓶。"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"chosen"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">保護地球，從擁有可重複使用的水瓶開始！</span><span class="se">\"</span><span class="s2">"</span><span class="p">,</span></span><span id="LC4" class="line"><span class="w"></span><span class="nl">"rejected"</span><span class="p">:</span><span class="w"></span><span class="s2">"</span><span class="se">\"</span><span class="s2">讓你的水瓶成為你的生活伴侶，使用可重複使用的水瓶，讓你的水瓶成為你的夥伴</span><span class="se">\"</span><span class="s2">"</span></span><span id="LC5" class="line"><span class="w"></span><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>運行偏好優化：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="c">#  多 GPU 請用 accelerate launch --multi_gpu --num_processes gpu 個數 dpo_train.py</span></span><span id="LC2" class="line">python dpo_train.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h2><a id="user-content-37-推理" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#37-%E6%8E%A8%E7%90%86"></a>3.7 推理</h2><p>確保<code>model_save</code>目錄下有以下文件，這些文件都可以在<code>Hugging Face Hub</code>倉庫<a href="https://gitee.com/link?target=https%3A%2F%2Fhuggingface.co%2Fcharent%2FChatLM-mini-Chinese">ChatLM-Chinese-0.2B</a>中找到：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">ChatLM-mini-Chinese</span><span id="LC2" class="line">├─model_save</span><span id="LC3" class="line">|  ├─config.json</span><span id="LC4" class="line">|  ├─configuration_chat_model.py</span><span id="LC5" class="line">|  ├─generation_config.json</span><span id="LC6" class="line">|  ├─model.safetensors</span><span id="LC7" class="line">|  ├─modeling_chat_model.py</span><span id="LC8" class="line">|  ├─special_tokens_map.json</span><span id="LC9" class="line">|  ├─tokenizer.json</span><span id="LC10" class="line">|  └─tokenizer_config.json</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol><li>控制枱運行：</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python cli_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><ol start="2"><li>API 調用</li></ol><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">python api_demo.py</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>API 調用示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">curl <span class="nt">--location</span><span class="s1">'127.0.0.1:8812/api/chat'</span><span class="se">\</span></span><span id="LC2" class="line"><span class="nt">--header</span><span class="s1">'Content-Type: application/json'</span><span class="se">\</span></span><span id="LC3" class="line"><span class="nt">--header</span><span class="s1">'Authorization: Bearer Bearer'</span><span class="se">\</span></span><span id="LC4" class="line"><span class="nt">--data</span><span class="s1">'{</span></span><span id="LC5" class="line"><span class="s1">    "input_txt": "感冒了要怎麼辦"</span></span><span id="LC6" class="line"><span class="s1">}'</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/api_example.png" alt="api demo" referrerpolicy="no-referrer"></p><h2><a id="user-content-38-下游任務微調" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#38-%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83"></a>3.8 下游任務微調</h2><p>這裏以文本中三元組信息為例，做下游微調。該任務的傳統深度學習抽取方法見倉庫<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。抽取出一段文本中所有的三元組，如句子<code>《寫生隨筆》是冶金工業 2006 年出版的圖書，作者是張來亮</code>，抽取出三元組<code>(寫生隨筆,作者,張來亮)</code>和<code>(寫生隨筆,出版社,冶金工業)</code>。</p><p>原始數據集為：<a href="https://gitee.com/link?target=https%3A%2F%2Faistudio.baidu.com%2Fdatasetdetail%2F11384">百度三元組抽取數據集</a>。加工得到的微調數據集格式示例：</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line"><span class="p">{</span></span><span id="LC2" class="line"><span class="w"></span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"></span><span class="s2">"請抽取出給定句子中的所有三元組。給定句子：《家鄉的月亮》是宋雪萊演唱的一首歌曲，所屬專輯是《久違的哥們》"</span><span class="p">,</span></span><span id="LC3" class="line"><span class="w"></span><span class="nl">"response"</span><span class="p">:</span><span class="w"></span><span class="s2">"[(家鄉的月亮,歌手,宋雪萊),(家鄉的月亮,所屬專輯,久違的哥們)]"</span></span><span id="LC4" class="line"><span class="p">}</span></span></pre><div class="markdown-code-block-copy-btn"></div></div></div><p>可以直接使用<code>sft_train.py</code>腳本進行微調，腳本<a href="https://gitee.com/charent/ChatLM-mini-Chinese/blob/main/finetune_examples/info_extract/finetune_IE_task.ipynb">finetune_IE_task.ipynb</a>裏麪包含詳細的解碼過程。訓練數據集約<code>17000</code>條，學習率<code>5e-5</code>，訓練 epoch<code>5</code>。微調後其他任務的對話能力也沒有消失。</p><p><img src="https://gitee.com/charent/ChatLM-mini-Chinese/raw/main/img/ie_task_chat.png" alt="信息抽取任務微調後的對話能力" referrerpolicy="no-referrer"></p><p>微調效果：
將<code>百度三元組抽取數據集</code>公開的<code>dev</code>數據集作為測試集，對比傳統方法<a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fcharent%2Fpytorch_IE_model">pytorch_IE_model</a>。</p><table><thead><tr><th align="left">模型</th><th align="center">F1 分數</th><th align="center">精確率 P</th><th align="center">召回率 R</th></tr></thead><tbody><tr><td align="left">ChatLM-Chinese-0.2B 微調</td><td align="center">0.74</td><td align="center">0.75</td><td align="center">0.73</td></tr><tr><td align="left">ChatLM-Chinese-0.2B 無預訓練</td><td align="center">0.51</td><td align="center">0.53</td><td align="center">0.49</td></tr><tr><td align="left">傳統深度學習方法</td><td align="center">0.80</td><td align="center">0.79</td><td align="center">80.1</td></tr></tbody></table><p>備註：<code>ChatLM-Chinese-0.2B 無預訓練</code>指直接初始化隨機參數，開始訓練，學習率<code>1e-4</code>，其他參數和微調一致。</p><h1><a id="user-content-四引用" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E5%9B%9B%E5%BC%95%E7%94%A8"></a>四、🎓引用</h1><p>如果你覺得本項目對你有所幫助，歡迎引用。</p><div class="white"><div class="highlight markdown-code-block"><pre><span id="LC1" class="line">@<span class="n">misc</span>{<span class="n">Charent2023</span>,</span><span id="LC2" class="line"><span class="n">author</span>={<span class="n">Charent</span><span class="n">Chen</span>},</span><span id="LC3" class="line"><span class="n">title</span>={<span class="n">A</span><span class="n">small</span><span class="n">chinese</span><span class="n">chat</span><span class="n">language</span><span class="n">model</span><span class="n">with</span><span class="m">0</span>.<span class="m">2</span><span class="n">B</span><span class="n">parameters</span><span class="n">base</span><span class="n">on</span><span class="n">T5</span>},</span><span id="LC4" class="line"><span class="n">year</span>={<span class="m">2023</span>},</span><span id="LC5" class="line"><span class="n">publisher</span> = {<span class="n">GitHub</span>},</span><span id="LC6" class="line"><span class="n">journal</span> = {<span class="n">GitHub</span><span class="n">repository</span>},</span><span id="LC7" class="line"><span class="n">howpublished</span> = {\<span class="n">url</span>{<span class="n">https</span>://<span class="n">github</span>.<span class="n">com</span>/<span class="n">charent</span>/<span class="n">ChatLM</span>-<span class="n">mini</span>-<span class="n">Chinese</span>}},</span><span id="LC8" class="line">}</span></pre><div class="markdown-code-block-copy-btn"></div></div></div><h1><a id="user-content-五其他事項" class="anchor" href="https://gitee.com/charent/ChatLM-mini-Chinese#%E4%BA%94%E5%85%B6%E4%BB%96%E4%BA%8B%E9%A1%B9"></a>五、🤔其他事項</h1><p>本項目不承擔開源模型和代碼導致的數據安全、輿情風險或發生任何模型被誤導、濫用、傳播、不當利用而產生的風險和責任。</p>
]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:15:00 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/charent/ChatLM-mini-Chinese</guid>
            <link>https://gitee.com/charent/ChatLM-mini-Chinese</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | vivo 海量微服務架構最新實踐]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="rich_media_content js_underline_content
                       autoTypeSetting24psection
            " id="js_content"><section style="font-size: 15px;line-height: 1.6;"><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(219, 219, 219);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgba(0, 0, 0, 0.5);font-size: 14px;text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">作者：來自 vivo 互聯網中間件團隊</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="text-align: left;justify-content: flex-start;display: flex;flex-flow: row;margin-bottom: 10px;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;align-self: flex-start;flex: 0 0 auto;background-color: rgb(234, 241, 255);border-style: solid;border-width: 0px 0px 0px 4px;border-color: rgb(48, 97, 207) rgb(48, 97, 207) rgb(48, 97, 207) rgb(21, 151, 239);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">本文根據羅亮老師在「2023 vivo 開發者大會"現場演講內容整理而成。公眾號回覆【2023 VDC】獲取互聯網技術分會場議題相關資料。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: left;" powered-by="xiumi.us"><section style="text-align: justify;line-height: 1.8;padding-right: 5px;padding-left: 5px;color: rgb(160, 160, 160);"><p style="text-wrap: wrap;">vivo 微服務平台為全球 5 億+用戶背後的全網十萬級機器、萬級微服務提供服務，在高效實踐過程中，vivo 中間件平台團隊輸出了一套業務適用的微服務架構最佳實踐--架構能力矩陣、高效的開源中間件組件全生命週期管理策略，走出了一條從開源到開源+自研的技術演進路徑，通過微服務引擎升級和統一平台建設較好解決了面臨的問題與挑戰。</p></section></section><section style="margin-right: 0%;margin-bottom: -5px;margin-left: 0%;text-align: right;line-height: 1;font-size: 5px;transform: translate3d(5px, 0px, 0px);" powered-by="xiumi.us"><section style="width: 0px;display: inline-block;vertical-align: top;border-bottom: 0.6em solid rgb(160, 160, 160);border-right: 0.6em solid rgb(160, 160, 160);border-top: 0.6em solid transparent !important;border-left: 0.6em solid transparent !important;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>一、vivo 從 0 到 1 的微服務架構工程實踐</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.1 為什麼需要微服務及落地挑戰</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">伴隨業務的高速發展，業務的複雜度越來越高，用戶規模和訪問量也越來越大；項目的迭代速度越來越快，交付效率要求也越來越高。與此同時，服務的集羣規模越來越大，部署架構越來越複雜，故障範圍也越來越不可控。此外，突增的業務流量時刻考驗着服務的水平擴容能力，創新業務的快速孵化也對服務的可擴展性提出了更高的要求。想要解決以上問題，業務架構會朝着微服務架構方向演進。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014317" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/0efbe187-74ec-4c6c-9f03-665534c87cb6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">正是在這樣的背景下，vivo 於 2015 年開始微服務架構改造，在落地過程中碰到了以下問題：</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>一是</strong>：服務數量多，配置修改生效、服務發佈等變更場景效率低下；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>二是</strong>：業務鏈路長，高可用保障難，問題與故障定位耗時長，服務的維護成本高；</p><p style="margin-bottom: 10px;text-wrap: wrap;"><strong>三是</strong>：大量的跨服務通訊，性能和訪問體驗優化提升難度大；</p><p style="text-wrap: wrap;"><strong>四是</strong>：一個業務鏈路涉及大量的上下游團隊，對接溝通的協作成本高；</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">為瞭解決以上落地過程中的開發、運維、團隊協作等難題，我們需要建設配套的微服務架構技術體系。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.2 vivo 微服務架構最佳實踐-架構能力矩陣</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">建設一套微服務架構技術體系，助力業務又快又好地構建微服務工程，需要哪些技術能力？我們對微服務架構的主要業務場景進行了分析，在業務實踐過程中，微服務主要會涉及同步、異步、定時任務三大核心業務場景。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014318" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/b246fe36-cacc-49af-a105-9c44ade187ea.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>同步調用</strong>場景：涉及的技術能力主要是 RPC 框架、註冊中心、服務治理；</p><p style="margin-bottom: 10px;">在<strong>異步調用</strong>場景：涉及的技術能力主要是消息中間件；</p><p>在<strong>定時任務</strong>場景：涉及的技術能力主要是分佈式任務調度。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了上面介紹的框架和系統，業務在微服務架構改造過程中，需要的能力全貌是怎樣的？</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在深度參與業務微服務架構改造過程中，我們對最佳實踐能力項進行了抽象，從而形成了 vivo 內部的微服務架構最佳實踐總結-架構能力矩陣，總計近 30 項能力。為了更直觀的呈現這些能力，我們從接入層、服務層、數據層的三層架構分層，開發、運維等 DevOps 的關鍵環節對架構能力進行了梳理，如下圖所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014319" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d1fb2470-8f85-4233-b3f2-010196469737.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;"><br></p></section><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在開發環節：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>在開發接口時</strong>，我們要實現內外網接口分離，保障接口的安全性，為此我們要接入網關來隔離內外網接口；在接入層和服務層，我們可以通過治理平台來實現限流、熔斷、降級等能力，保障業務的高可用。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在構建內部服務時</strong>，我們要儘可能實現服務無狀態，通過 RPC 框架實現內部接口的 RPC 相互調用，具備異常重試能力，提升服務的魯棒性；在編碼過程中，我們通過接入配置中心實現代碼與配置分離，具備運行時動態調整配置的能力，提高服務的變更效率。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在異步調用場景</strong>，我們可以通過接入消息中間件實現業務間的相互解耦、流量削峯；在定時任務場景，我們可以通過分佈式任務調度系統，實現失敗任務的自動轉移能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，我們可以通過落地存儲與計算分離能力，實現服務層和數據層的解耦，便於分層擴容，具備面向未來更大規模業務的擴展能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在數據層</strong>，通過落地讀寫分離、冷熱分離等能力，提升系統性能，節省存儲成本；同時將這些能力通過研發框架進行封裝，便於業務側複用。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">在運維環節：</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">我們可以藉助 CDN 實現網站的動靜分離訪問，減小系統的請求壓力；在日常運維過程中，我們要實現服務的可灰度、可回滾；服務節點無單點；同時藉助容器技術快速實現彈性伸縮能力；提升系統的故障恢復速度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在部署時</strong>，通過部署與發佈分離，可以較好規避發佈變更時產生的問題，即服務部署成功，並且健康檢查通過後再發布到生產環境，減小故障的影響範圍。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在遇到嚴重的系統故障時</strong>，需要具備使用備份數據從零恢復的能力，同時對所有已知的故障場景要有對應的預案，提升系統的故障應對能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在數據運維上</strong>，我們要確保數據屬主唯一，避免多個業務對同一個數據庫進行訪問；同時也要實現業務數據和大數據的存儲隔離，避免相互影響。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">除了以上能力之外，我們<strong>還要</strong>實現業務的安全合規，建設覆蓋 Metric、Trace、Log 的可觀測能力體系，便於對故障問題的定位排查；在多機房層面，需要具備同城雙活、異地多活等跨機房容災能力。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.3 vivo 微服務平台能力</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">為了更好落地以上最佳實踐，我們構建了一套從接入層、服務層、消息層、框架層到存儲層的平台能力，完整的平台能力地圖如下圖所示：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014320" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/3b0a6fb7-b253-48eb-921d-5056d3d9204a.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">在<strong>接入層</strong>，我們提供了四層流量網關和七層微服務 API 網關；在服務層提供了服務/流量治理平台、配置中心、註冊中心、接口管理平台、分佈式任務調度等系統。</p><p style="margin-bottom: 10px;">在<strong>消息層</strong>提供了消息中間件；在框架層提供了腳手架，可快速集成日誌、配置、限流/熔斷、MySQL/Redis 等 SDK，以及 RPC 框架。</p><p style="margin-bottom: 10px;">在<strong>存儲層</strong>提供了 DaaS 平台，包含 MySQL、Redis、ElasticSearch、MongoDB、文件服務等系統能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">為了更好排查故障問題，我們在可觀測領域構建了監控中心、日誌中心、調用鏈等系統；此外，還有更好支撐服務構建、變更發佈的 CICD 系統和 IT 基礎設施的配置管理系統 CMDB。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">截止 2019 年，vivo 基本完成了從 0 到 1 的微服務平台能力煙囪式建設。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">快速構建這些能力的過程，離不開開源組件的賦能。例如微服務 API 網關背後的 zuul，註冊中心背後的 ZooKeeper 和 etcd，RPC 框架的 Dubbo 和 bRPC；配置中心的 Apollo 和 Nacos，流量治理的 hystrix 和 sentinel，消息中間件的 RabbitMQ 和 RocketMQ，任務調度的 xxl-job；如下圖所示。</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014321" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d29efb2f-0028-42ec-b597-77c064aef74c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">在此，我們也通過 VDC(vivo 開發者大會) 平台，感謝開源社區的賦能，助力 vivo 微服務架構技術體系從 0 到 1 的快速構建。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">1.4 vivo 微服務現狀</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">截止當前，vivo 的微服務平台為全球分佈在 60+個國家/地區的 5 億+用戶提供服務；其中 vivo 現有萬級的微服務，覆蓋全網機器規模十萬級，每天處理高達 8000 億次的 RPC 調用次數，流量的峯值 QPS 達到千萬級以上。<span style="text-align: center;letter-spacing: 0.034em;"></span></p><p><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014323" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/89e7af31-f3b1-48ab-8291-f6bdfc728e87.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在支撐如此規模的微服務過程中，特別是在 2020 年以後，我們碰到了較多的問題與挑戰，為瞭解決這些問題，我們使用了微服務引擎升級和統一平台建設的解決方案；下面來一起看看我們碰到了哪些問題與挑戰？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>二、微服務引擎升級與統一平台建設</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.1 面臨的問題與挑戰</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我們知道，註冊中心和配置中心是微服務架構領域的技術基石；下面給大家説明下我們在這兩個基石系統實踐過程中遇到的<strong>問題與挑戰</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014324" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/fd67ab5a-9617-4bd1-8251-85c51a7af775.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是註冊中心，眾所周知，ZK 是 CP 特性，在註冊中心場景有較多不可用的問題，此外還有跨機房多活能力缺失，集羣故障半徑大等問題；寫性能無法水平擴展，在大規模 Dubbo 服務場景中，接口級註冊模型註冊的數據量大，在業務高頻變更期間網卡的帶寬峯值會超過 1000Gbps。此外還有業務易混用，功能缺失；內部的多個技術棧使用不同的註冊中心，跨技術棧調用的研發運維成本高等問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在配置中心場景，存在應用、組件配置的變更通道不統一，故障場景配置回滾慢，變更審計日誌分散，業務恢復耗時長等問題；配置變更下發的時效不滿足業務要求，內部存在多套配置中心，都需要和業務研發流程打通，存在審批、審計、回滾等功能沒有對齊的問題；此外在功能和安全上，還需要實現內部的配置定時生效，配置加解密等需求，配置訪問通道符合公司的安全要求。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">從以上的問題與挑戰中可以看出，基於開源組件快速構建的微服務底層引擎在 vivo 的內部業務場景中存在較多的可用性、性能&amp;容量、研發運維、功能&amp;安全問題與挑戰。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="font-size: 16px;color: rgb(65, 95, 255);" powered-by="xiumi.us"><p style="text-wrap: wrap;">2.2 註冊中心引擎升級</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">為瞭解決以上的問題與挑戰，我們需要進行技術升級，首先給大家介紹的是註冊中心的<strong>解決方案</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014325" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bc83c41-6a74-4125-9924-22bc26ac900c.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">針對 Dubbo 接口級服務發現導致 ZK 註冊中心流量過大的問題，業界同行都在往應用級服務發現遷移來構建解決方案；通過 Dubbo 開源社區官網的介紹，我們可以看到，應用級服務發現是適應雲原生，支持更大規模的服務發現模型；</p><p style="margin-bottom: 10px;">將 Dubbo 接口級服務發現模型升級為應用級，可降低單機 50% 的內存消耗，降低註冊中心集羣 90% 的存儲與推送壓力，從架構上支持百萬實例集羣規模；</p><p>因此我們需要將 Dubbo 框架服務發現模型從接口級升級為應用級，徹底解決註冊數據量大，對註冊中心請求壓力大的問題，同時具備面向雲原生微服務架構的擴展能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，針對註冊中心的可用性、性能&amp;容量、研發運維等問題，我們需要建設滿足 AP 特性、支持跨機房多活的統一註冊中心，使用 Session+Data 分離架構，Data 層持久化數據，Session 層處理和客戶端的長連接，無狀態 Session 層能較好收斂客戶端請求，實現讀寫流量隔離，具備較好的橫向擴展能力，真正解決註冊中心的性能、容量和擴展性問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>綜上</strong>，我們需要構建 Dubbo 應用級服務發現能力，構建 Session+Data 分離的統一註冊中心，內部的項目代號為 vns。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">從上面的技術方案分析中，我們可以看到，通過應用級註冊可以徹底解決註冊中心的流量突刺問題；通過 Session+Data 雙層分離架構可以實現業務無感知的多集羣拆分，有效縮小故障半徑，那如何來<strong>落地</strong>呢？</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014326" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/444085d9-cd4f-491a-b55b-5ec48ccb6c28.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;">我們首先想到的就是上圖左側的技術方案，通過構建暴露 gRPC 協議、支持應用級註冊的 vns 系統，海量的 Dubbo 服務通過雙註冊來實現遷移；但是在經過詳細的技術分析之後，我們發現該方案存在明顯的<strong>耦合問題：</strong></p><p style="margin-bottom: 10px;">首先是 Dubbo 應用級註冊升級的進展依賴 vns 系統的建設進度，Dubbo 框架依賴穩定的 vns SDK，Dubbo 框架和 vns 系統之間存在進度依賴問題；</p><p style="margin-bottom: 10px;">其次還存在回滾依賴問題，當 vns 系統因灰度異常回滾時，Dubbo 應用級註冊升級進度也會同步回滾；</p><p style="margin-bottom: 10px;">同理當 Dubbo 流量切換異常回滾時，vns 的業務接入進度也會回退。</p><p style="margin-bottom: 10px;">此外，部分不迭代的業務可能需要繼續使用接口級註冊，無法實現 ZK 註冊中心的完全下線。</p><p>為瞭解決以上問題，我們對技術方案進行了升級，改用通過 vns 系統暴露和支持 ZK 協議，實現 Dubbo 應用級註冊升級和 vns 系統的能力建設解耦；當 vns 系統的能力建設進展還未達到生產環境要求時，我們可以通過引入一套新的 ZK 集羣來支持 Dubbo 的應用級註冊模型升級；當 vns 的能力成熟度達到生產環境的要求後，可以對引入的 ZK 集羣進行替代，整個過程可以根據系統建設進展和可用性保障要求，進行可控的灰度放量和回滾操作，控制變更風險；最終，vns 通過暴露 ZK+gRPC 雙協議滿足業務的接入訴求。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><section powered-by="xiumi.us"><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在整個技術方案落地過程中，我們始終堅持業務導向原則，實現業務升級和遷移的零|低成本；採用穩妥、完善的升級遷移方案，確保過程可灰度、可回滾、可觀測；大家可以看到，我們通過兼容 ZK 協議，最大限度的保障 Dubbo 業務的平滑升級，切換方案做到了可灰度可回滾可觀測，在減少升級成本的同時，降低項目落地風險，最終實現 ZK 註冊中心的完全下線。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.3 配置中心引擎升級</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介紹完註冊中心，我們再來看看配置中心的解決方案，配置中心主要解決的是配置通道不統一，性能不達標，無法滿足內部的業務需求等問題。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014327" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/4591d606-98ae-4070-823d-8aff13aa8f98.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">上圖左側是我們最新的配置中心技術架構圖，右側是統一配置通道的示意圖，我們通過支持應用配置與組件配置的統一配置通道，實現了配置管理能力的收斂統一，在此基礎上，建設一鍵審批/審計/回滾等能力，實現了和內部業務研發流程的打通，減少人力運維投入；此外，在新版配置中心上，我們也實現了較多的高可用、性能、安全、可觀測能力增強等業務訴求；在配置中心升級過程中，我們追求業務的無感知升級，通過兼容原有配置中心對外開放的接口，實現了新系統的平滑升級，原有系統優雅下線。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，和註冊中心的升級方案類似，在配置中心的技術方案設計中，我們也較好的遵循了業務導向原則。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.4 統一微服務平台建設</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介紹完註冊中心和配置中心等微服務引擎的技術升級方案，我們再來看下從 0 到 1 快速構建的煙囪式微服務平台會面臨哪些問題和挑戰？</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014328" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/2dcc08b7-f604-43a4-85d3-b7c2ae6eedf3.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">從上圖左側示意圖中可以看到，我們快速構建的微服務平台存在 10 個以上的模塊，每個模塊都有獨立的入口，用戶使用平台的易用性很低；此外，這些模塊在建設過程中，還需要重複對接雲平台、單點登錄、權限、工單、監控、CMDB 等公共服務系統；系統審計日誌分散，不便於快速定位因變更引起的問題；綜上，煙囪式微服務平台存在多入口，功能重複對接，運維、研發成本高，故障排查與恢復效率低，易用性不足等問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">要解決煙囪式微服務平台的問題，需要構建更合理的產品方案，我們對用戶的使用現狀進行了分析：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014329" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/f3ad12b5-778a-41ec-9496-bd59e3ab55a6.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us">通過系統埋點數據發現，煙囪式微服務平台中用戶使用頻率最高的兩個系統分別是配置中心、服務治理。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">通過上圖左側的 PV/UV 餅狀圖數據，大家可以發現：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: center;"><p style="text-align: left;">配置中心的用戶訪問主要集中在配置的【查詢與變更】、【變更記錄與審批】和配置變更相關的 2 個頁面上，服務治理的用戶訪問主要集中在【服務概覽】、【服務查詢】和服務相關的 2 個頁面上。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">基於埋點數據，我們可以看到用戶的訪問集中在少數的幾個功能上，通過整合各個系統模塊高頻使用的功能，建設統一的平台入口，實現系統間聯動，這也給我們如何建設統一平台提供了較好的思路。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，在對各個模塊的技術架構進行分析時，我們識別到了位於最底層、技術依賴程度最高的兩個系統：配置中心、註冊中心，這兩個系統非常適合作為統一平台建設的技術底座。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014330" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/9bb1b5bf-7287-4f3d-884c-ea3b5ab7d7eb.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">區別於煙囪式微服務平台的多個系統模塊獨立對接 CICD 等研發平台，在統一微服務平台建設中，我們升級為統一平台對接 CICD 等研發平台；我們的建設思路是，以配置中心/註冊中心為底座來建設統一微服務平台：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>：基於統一的配置通道與 CICD 等研發平台系統進行聯動，建設一鍵審批、回滾能力，整合研發流程，降低對接成本；</p><p><strong>二是</strong>：通過統一平台的建設，實現平台間聯動，建設高階的自動化水平，支撐業務進一步提升持續服務能力。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">2.5 引擎升級&amp;統一平台建設總結</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">接下來，對我們前面講到的內容做一個總結：在大規模、海量業務的微服務架構實踐過程中，我們通過引擎升級和統一平台能力建設較好的解決了碰到的問題與挑戰。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014331" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/16e89ce4-7fa5-497c-9a09-4b45df61c722.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在升級和建設過程中，我們需要保證現有業務的連續性，保障不發生因底層引擎升級和平台建設導致的可用性問題。因此，引擎升級和統一平台建設的工作需要建立在高可用保障的基礎上；換句話來説，可用性是我們所有工作的底座。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在這個基礎上，我們實現註冊中心和配置中心的引擎升級，完成應用級註冊模型升級；在這個過程中，解決底層引擎的擴展性、容量、性能、可維護性和安全性等問題；最後，我們要建設統一的微服務平台能力，實現平台間聯動，構建自動/自助化使用能力；賦能業務。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">大家可以看到，通過完整的方案介紹，在上圖右側我們呈現了微服務架構實踐過程中的價值分層邏輯，即在可用性的基礎上，提升系統的擴展性、容量、性能、可維護、安全性等能力；然後再在此基礎上，交付更高的研發效率，更好的用戶使用體驗。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="padding: 3px;display: inline-block;border-bottom: 1px solid rgb(65, 94, 255);font-size: 17px;color: rgb(65, 94, 255);"><p>三、微服務架構升級的總結與展望</p></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">介紹完我們的解決方案後，最後來説明下我們對微服務架構升級的總結與思考，以及對未來的展望。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.1 擁抱開源的實用主義</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">在構建微服務架構技術體系的過程中，我們始終堅持擁抱開源，迭代業務適用的技術平台；結合內部業務的實際情況，我們走出了一條從開源到開源+自研的研發路徑。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014332" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8f26fca0-069c-4375-850c-26e6d7c79812.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">在從 0 到 1 的平台能力建設過程中，我們引入開源組件進行能力快速構建，快速交付滿足業務的需求；始終堅持業務適用原則，不過度設計，支撐業務的快速迭代；以上階段，我們稱之為「拿來主義」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在面向更大規模、海量業務實踐過程中，為瞭解決碰到的問題與挑戰，我們在開源的基礎上進行增強，自研部分能力來解決億級用戶規模下內部業務的功能，性能，容量，研發流程打通等需求；這個階段，我們稱之為「實用主義」。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在技術平台迭代過程中，我們始終堅持 2 個原則，一是簡單有效原則，堅持用最簡單的解決方案來解決問題；二是迭代和演進原則，堅持平台持續迭代和演進的原則；前期基於開源組件快速搭建能力，再基於實際的業務需求和痛點來落地自研架構；在這個過程中，始終堅持業務適用，不為了技術而技術，避免大而全的技術架構。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">此外，也要説明一個常見的誤區，我們為什麼不完全自研？vivo 的微服務平台建設從開源社區獲益良多，堅持不閉門造車，站在巨人肩膀上，持續引入優秀特性來支撐業務的快速發展，同時也會考慮將部分行業適用的通用優秀特性反饋給社區，和社區共同成長。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.2&nbsp;中間件組件全生命週期管理</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">大家可以看到，vivo 的微服務架構技術體系引入了較多的開源組件，在實踐過程中，我們摸索出了一套完整的中間件組件全生命週期管理策略。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014333" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e436a13c-2e63-49df-a26a-8784d90e98a0.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">我們先來看看業務的訴求和底層技術的<strong>特點</strong>：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(62, 62, 62);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">首先是業務的訴求：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">業務期望更高的迭代交付效率；</span></p></li><li><p style="margin-bottom: 10px;">快速引入新技術，使用新技術助力業務創新，但很多時候新技術往往意味着成熟度不足，可能存在較多問題；</p></li><li><p style="margin-bottom: 10px;"><span style="letter-spacing: 0.034em;">業務的不斷創新與發展，對組件的性能、容量要求越來越高；</span><br></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">對業務來説，高效迭代交付需求是第一位的。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin: 10px 0% 8px;text-align: left;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;vertical-align: top;border-left: 3px solid rgb(65, 95, 255);border-bottom-left-radius: 0px;padding-left: 8px;align-self: flex-start;flex: 0 0 auto;"><section style="color: rgb(14, 14, 13);text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">然而，底層技術有它自己的特點：</p></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><ol class="list-paddingleft-1" style="list-style-type: decimal;"><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">技術的發展有它的客觀規律，需要經歷萌芽期 → 膨脹期 → 低谷期→ 復甦期→ 成熟期等多個階段；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">缺</span><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">乏約束的技術體系必然隨着時間推移而腐化，治理不及時會成為技術債務，阻塞業務發展；</span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">同類中間件組件的快速引入會有重複建設的效率問題；</span></span></p></li><li><p style="margin-bottom: 10px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 15px;letter-spacing: 0.578px;text-wrap: wrap;">中間件組件的技術升級週期客觀上都比較長。</span></span><span style="letter-spacing: 0.034em;"></span></p></li></ol></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">實踐證明，只有足夠穩健的底層技術能力才能更好支撐業務的高效迭代。在這個過程中，如何兼顧效率與質量？尊重客觀規律，確保整個過程都有明確的目標和方向，避免走偏，慢就是快。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我們認為，完善的中間件組件全生命週期管理策略，首先需要在所有的技術團隊中形成價值共識；再通過組件掃描和組件地圖等手段及時對組件全貌進行洞察；在組件的標準化治理和運營階段實現有規範，補短板；同時在新技術引入時，通過完善的新技術引入規範，覆蓋功能/性能/容量/擴展性/成熟度/使用成本等維度；在組件的版本治理上，使用基線版本治理方案，輸出明確的使用標準/版本升級方案/版本收斂策略；最後，在組件的成熟度管理上，我們可以藉助 Gartner(高德納) 技術成熟度説明和組件能力矩陣，不斷提升組件的成熟度。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">綜上，為更高效的支撐業務，在組件管理上我們使用了更加入寬鬆的引入策略，同時也會對組件的全生命週期進行嚴格管理，踐行寬入嚴出策略，通過完善的中間件組件全生命週期管理助力業務跑的更快，走的更遠。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.3 引擎升級探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">展望未來，我們會堅持和踐行引擎升級和平台建設的<strong>持續迭代思路</strong>：</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">首先是對引擎升級的探索，通過引入新技術來解決當前碰到的研發效率、成本等痛點問題：</p><p style="text-wrap: wrap;"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014334" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/8bd511dc-cb3c-4056-a1f1-5fca3b345d4b.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在研發效率方向</strong>，存在的痛點問題如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>，組件 SDK 的升級週期長，碎片化問題嚴重；</p><p><strong>二是</strong>，當前 vivo 內部主要的是 Java、C++技術棧，新業務形態孵化可能會引入新的技術棧，需能夠較好解決跨技術棧的問題。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">想要較好的解決以上問題，需要探索基於 Java Agent/SideCar 技術的標準 ServiceMesh 模式，將 RPC、MQ 等中間件能力下沉，透明化實現微服務治理、高可用等能力增強，同時組件具備熱升級能力。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">此外，<strong>在成本方向</strong>，存在的痛點問題如下：</p><section style="margin-top: 10px;margin-bottom: 10px;" powered-by="xiumi.us"><section style="margin-bottom: -2.25em;margin-right: 5px;background-color: rgb(247, 247, 247);"><section style="padding: 10px;margin-bottom: 5px;" powered-by="xiumi.us"><section style="text-align: left;"><p style="margin-bottom: 10px;"><strong>一是</strong>， MQ 等重資源型應用的 CPU、存儲資源利用率差異大；</p><p><strong>二是</strong>，部分事件驅動場景機器資源利用率低。</p></section></section></section><section style="margin-left: auto;width: 2.25em;height: 2.25em;border-right: 5px solid transparent;border-bottom: 5px solid transparent;"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">要解決以上問題，我們可以通過升級 MQ 組件，落地存算分離技術，探索計算存儲資源利用率優化方案。另外，還可以探索 Serverless 技術，實現平台化託管運維，降低資源成本，天然適合小程序、快應用等事件驅動業務場景。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>綜上</strong>，在引擎升級探索上，我們會基於業務需求和痛點問題，探索和落地 ServiceMesh/Serverless/存算分離等雲原生技術。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><span style="font-size: 16px;color: rgb(65, 95, 255);">3.4 平台建設探索</span></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">講完引擎升級探索，我們再來看看在平台建設上的探索：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014335" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/d67670bb-72fa-43e3-93a8-1c5f33dfca51.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">作為技術平台團隊，我們在持續積極的探索「平台工程」理念，從現在的 DevOps 實踐到平台工程，也是團隊協作理念的再次升級。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">我們知道，DevOps 於 2009 年出現，2015 年在國內火起來，它是一種文化、方法論，是敏捷理念從開發到運維的延伸。DevOps 的理念是：踐行誰構建誰運行，開發運維一體化，實現業務的高效交付。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">但是，DevOps 在實際落地過程中存在以下問題：</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">「DevOps 團隊」的中心化與去中心化取捨問題</p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;">【<strong>中心化</strong>】指的是，獨立的 DevOps 團隊，即不在業務團隊中配置 DevOps 能力，而把 DevOps 人員集中起來組建團隊，這種完全中心化的模式本質上和 DevOps 文化相矛盾。同時根據康威定律，可能會製造新的效能瓶頸。「獨立的 DevOps 團隊」在 2014 年被 Thoughtworks「技術雷達」列為 Hold (停止採用)。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">【<strong>去中心化</strong>】指的是，將 DevOps 能力分散在業務團隊，這種做法會將大量的和基礎設施相關的工作職責劃給業務團隊；這種方式會隨之出現基礎設施和服務治理缺失、系統穩定性降低、研發和 DevOps 效能浪費等諸多問題。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us">因此，想要踐行好 DevOps，必須在中心化與去中心化之間取得平衡。</p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section powered-by="xiumi.us"><p style="text-wrap: wrap;">此外，從平台能力上講，DevOps 平台往往更側重於建設流程和工具鏈，而在使用這些建設的工具技術平台過程中會大大增加業務開發團隊的認知負荷，存在無法較好向業務開發團隊屏蔽底層基礎設施複雜性的問題。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">平台工程的概念，是在 2017 年首次出現，於 2022 年在國內興起。平台工程的定義是，一套用來構建和運營支持軟件交付和生命週期管理的自助式內部開發者平台的機制和架構；它的特點是：平台在演進中提供足夠的透明度、敏捷性，在建設過程中形成適合業務架構的高效協作模式。在這一過程中逐步將知識體系固化到平台中，從而使得工程方式標準化、流程化和規模化並持續改善；它踐行的理念是：一個可用的、高效的平台並非一個技術團隊埋頭苦幹就可以產出的；恰恰相反，一個成功的平台工程需要企業各個組織部門合作、協調、推廣並根據實際使用反饋不斷迭代。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;">在具體實踐中，平台工程約定了「業務團隊」和「平台團隊」兩個團隊，其中「業務團隊」負責業務研發，「平台團隊」負責平台建設；「平台團隊」通過將技術知識沉澱到「平台工程」，隱藏和抽象底層基礎設施的複雜性，實現基礎設施即代碼，為「業務團隊」賦能增效；同時，基於「業務團隊」在使用「平台工程」的過程中的不斷反饋來持續改進平台的自助化產品能力，構建一整套覆蓋 DevOps 全鏈路的簡單易用平台產品；可以看到，平台工程是一種最佳實踐，和我們當前的團隊協作模式匹配度非常高。</p></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100014336" data-ratio="0.562962962962963" data-s="300,640" src="https://oscimg.oschina.net/oscnet/e9b65caa-221d-4d7f-ae0c-c6722aa53455.png" data-type="png" data-w="1080" style="" referrerpolicy="no-referrer"></p><p><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><strong>在平台建設的整體規劃上：</strong></p><section style="margin-top: 10px;margin-bottom: 10px;text-align: center;" powered-by="xiumi.us"><section style="display: inline-block;width: 100%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);padding: 10px;"><section style="text-align: justify;" powered-by="xiumi.us"><p style="text-wrap: wrap;"><strong>當前階段</strong>：我們構建的統一微服務平台會持續探索「平台工程」理念，沉澱配置中心、註冊中心等平台的技術知識與最佳實踐，構建和打磨業務自助化使用的平台能力。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>展望未來</strong>：我們會通過明確的北極星指標，牽引平台提供更高的研發效率和更好的開發者體驗。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在研發效率上</strong>，我們追求單位時間內更多的代碼產出和需求交付；此外我們也追求更好的開發者體驗，通過降低用戶使用平台的打斷次數和平台問題的人工支撐次數，提升業務團隊和平台團隊兩個團隊的開發體驗。</p><p style="text-wrap: wrap;"><br></p><p style="text-wrap: wrap;"><strong>在具體的落地路徑上</strong>，我們始終以開發者用戶為中心，針對研發工作中時間消耗較多的場景進行優化，通過北極星指標牽引，形成覆蓋 IDE+PaaS 的平台工程實踐路徑，持續迭代優化平台能力，提升研發效率與開發者體驗。</p></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section style="margin-right: 0%;margin-bottom: 20px;margin-left: 0%;justify-content: flex-start;display: flex;flex-flow: row;" powered-by="xiumi.us"><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section><section style="display: inline-block;vertical-align: middle;width: 20%;align-self: center;flex: 0 0 auto;"><section style="text-align: center;color: rgb(45, 66, 87);font-size: 11px;" powered-by="xiumi.us"><p>END</p></section></section><section style="display: inline-block;vertical-align: middle;width: 40%;align-self: center;flex: 0 0 auto;"><section style="margin-top: 0.5em;margin-bottom: 0.5em;" powered-by="xiumi.us"><section style="border-top: 1px dotted rgb(90, 98, 114);"><svg viewBox="0 0 1 1" style="float:left;line-height:0;width:0;vertical-align:top;"></svg></section></section></section></section><section style="margin-top: 10px;margin-bottom: 10px;text-align: left;" powered-by="xiumi.us"><section style="padding-left: 1em;padding-right: 1em;display: inline-block;text-align: center;"><span style="display: inline-block;padding: 0.3em 0.5em;border-radius: 0.5em;background-color: rgb(65, 94, 255);color: rgb(255, 255, 255);" title="" opera-tn-ra-cell="_$.pages:0.layers:0.comps:161.title1"><p>猜你喜歡</p></span></section><section style="border-width: 1px;border-style: solid;border-color: transparent;margin-top: -1em;padding: 20px 10px 10px;background-color: rgb(239, 239, 239);text-align: center;"><section style="font-size: 14px;text-align: left;" powered-by="xiumi.us"><ul class="list-paddingleft-1" style="list-style-type: disc;"><li><p><span style="font-size: 14px;letter-spacing: 0.578px;text-align: left;text-wrap: wrap;background-color: rgb(239, 239, 239);"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247498140%26idx%3D2%26sn%3D66854883c362d9145d89f72f267a7773%26chksm%3Debdb890edcac0018fa02b33bf9eff448548362f5174c829a6e20ee6ab656d9ea12b31c0a1e0f%26scene%3D21%23wechat_redirect" textvalue="Spring 七種事務傳播性介紹" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Spring 七種事務傳播性介紹</a></span><br></p></li><li><p><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497989%26idx%3D1%26sn%3Da98e270e4612356756966bd9d90d80ee%26chksm%3Debdb8997dcac0081e35a2c9ba681902e703f8c52406ee49fcedaafbba77b7dc3279f56305782%26scene%3D21%23wechat_redirect" textvalue="vivo 數據庫備份恢復系統演化" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 數據庫備份恢復系統演化</a></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497821%26idx%3D1%26sn%3D80e04511f5a5d5acfee4a44a8a8b3e31%26chksm%3Debdb88cfdcac01d954242fd24907b69c542e43fcb99ebe6a03d66858194e03ad14105281b62f%26scene%3D21%23wechat_redirect" textvalue="vivo 容器平台資源運營實踐" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">vivo 容器平台資源運營實踐</a></span></p></li><li><p><span style="letter-spacing: 0.034em;"><a target="_blank" href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzI4NjY4MTU5Nw%3D%3D%26mid%3D2247497810%26idx%3D1%26sn%3Dfb5334c9637cdde4b5125f69ed32e89f%26chksm%3Debdb88c0dcac01d6faf82e4d44e8421616ec9128f46ea494339a599c346b13212b9f1d774886%26scene%3D21%23wechat_redirect" textvalue="Hudi 在 vivo 湖倉一體的落地實踐" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">Hudi 在 vivo 湖倉一體的落地實踐</a></span></p></li></ul></section></section></section><p style="text-wrap: wrap;" powered-by="xiumi.us"><br></p><section class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe" data-pluginname="mpprofile" data-id="MzI4NjY4MTU5Nw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/4g5IMGibSxt45QXJZicZ9gaNU2mRSlvqhQd94MJ7oQh4QFj1ibPV66xnUiaKoicSatwaGXepL5sBDSDLEckicX1ttibHg/0?wx_fmt=png" data-nickname="vivo 互聯網技術" data-alias="vivoVMIC" data-signature="分享 vivo 互聯網技術乾貨與沙龍活動，推薦最新行業動態與熱門會議。" data-from="0" data-is_biz_ban="0"></mp-common-profile></section></section><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div><p style="color: #858585; font-size: 13px;">本文分享自微信公眾號 - vivo 互聯網技術（vivoVMIC）。<br>如有侵權，請聯繫 support@oschina.cn 刪除。<br>本文參與「<a href="https://www.oschina.net/sharing-plan" target="_blank">OSC 源創計劃</a>」，歡迎正在閲讀的你也加入，一起分享。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 02:06:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/vivotech/blog/10773883</guid>
            <link>https://my.oschina.net/vivotech/blog/10773883</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[一站式文件存儲 X File Storage 發佈 2.1.0 版本]]>
            </title>
            <description>
                <![CDATA[<div class="content"><h3><img alt="logo" height="200px" src="https://x-file-storage.xuyanwu.cn/assets/logo.svg" referrerpolicy="no-referrer"><br><span>原名 X Spring File Storage 現已捐贈至 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdromara.org%2Fzh" target="_blank">dromara</a> 開源組織</span></h3><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.dromara.org" target="_blank">x-file-storage.dromara.org</a> | <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn" target="_blank">x-file-storage.xuyanwu.cn</a> | <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fspring-file-storage.xuyanwu.cn" target="_blank">spring-file-storage.xuyanwu.cn</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fcentral.sonatype.com%2Fsearch%3Fq%3Dorg.dromara.x-file-storage" target="_blank"><img src="https://img.shields.io/maven-central/v/org.dromara.x-file-storage/x-file-storage-core.svg?label=Maven%20Central" referrerpolicy="no-referrer"></a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.apache.org%2Flicenses%2FLICENSE-2.0" target="_blank"><img src="https://img.shields.io/badge/license-Apache%202-green.svg" referrerpolicy="no-referrer"></a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Findex.html" target="_blank"><img src="https://img.shields.io/badge/JDK-8+-blue.svg" referrerpolicy="no-referrer"></a><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fdromara%2Fx-file-storage" target="_blank"><img alt="github star" src="https://img.shields.io/github/stars/dromara/x-file-storage.svg?style=social" referrerpolicy="no-referrer"></a><a href="https://gitee.com/dromara/x-file-storage"><img alt="star" src="https://gitee.com/dromara/x-file-storage/badge/star.svg?theme=dark" referrerpolicy="no-referrer"></a><br><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fjq.qq.com%2F%3F_wv%3D1027%26k%3DeGfeNqka" target="_blank"><img alt="" src="https://img.shields.io/badge/QQ%E7%BE%A4-515706495-orange" referrerpolicy="no-referrer"></a></p><h1><span>📚簡介</span></h1><p>一行代碼將文件存儲到本地、FTP、SFTP、WebDAV、阿里雲 OSS、華為雲 OBS、七牛雲 Kodo、騰訊雲 COS、百度雲 BOS、又拍雲 USS、MinIO、 Amazon S3、GoogleCloud Storage、FastDFS、 Azure Blob Storage、Cloudflare R2、金山雲 KS3、美團雲 MSS、京東雲 OSS、天翼雲 OOS、移動，雲 EOS、沃雲 OSS、網易數帆 NOS、Ucloud US3、青雲 QingStor、平安雲 OBS、首雲 OSS、IBM COS、其它兼容 S3 協議的存儲平台。查看 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%25AD%2598%25E5%2582%25A8%25E5%25B9%25B3%25E5%258F%25B0" target="_blank">所有支持的存儲平台</a></p><p>💡 通過 WebDAV 連接到 Alist 後，可以使用百度網盤、天翼雲盤、阿里雲盤、迅雷網盤等常見存儲服務，查看 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Falist-doc.nn.ci%2Fdocs%2Fwebdav" target="_blank">Alist 支持的存儲平台</a></p><p>GitHub：https://github.com/dromara/x-file-storage Gitee：https://gitee.com/dromara/x-file-storage</p><p>文檔 1：https://x-file-storage.dromara.org 文檔 2：https://x-file-storage.xuyanwu.cn 文檔 3：https://spring-file-storage.xuyanwu.cn</p><hr><h1><span>📜更新內容</span></h1><ul><li><p>新增 FastDFS 存儲平台</p></li><li><p>新增 Azure Blob Storage 存儲平台</p></li><li><p>新增複製文件，支持跨存儲平台複製</p></li><li><p>新增移動（重命名）文件，支持跨存儲平台移動（重命名）</p></li><li><p>新增大文件手動分片上傳（斷點續傳），1.0.0 版本早已支持大文件自動分片上傳</p></li><li><p>新增計算哈希功能，上傳下載時可以邊處理邊計算</p></li><li><p>上傳無需強制獲取文件大小，上傳未知大小的文件更友好</p></li><li><p>優化 SpringBoot 自動配置兼容非 SpringWeb 環境</p></li><li><p>優化 FileKey 獲取方式，避免空指針異常</p></li><li><p>優化上傳代碼結構</p></li><li><p>優化異常處理</p></li><li><p>優化進度監聽器</p></li><li><p>修復上傳時設置縮略圖保存名稱錯誤的 BUG</p></li><li><p>兼容低版本 SpringBoot(2.0.x) 的依賴注入</p></li><li><p>修復華為雲 OBS 上傳進度問題</p></li><li><p>修復 MultipartFile 存儲到本地時，在某些情況下輸入流未關閉的問題</p></li><li><p>修復，又拍雲 USS 上傳縮略圖文件時 Response 未關閉的問題</p></li></ul><hr><h1><span>📦使用</span></h1><p>點擊 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%25BF%25AB%25E9%2580%259F%25E5%2585%25A5%25E9%2597%25A8" target="_blank">快速入門</a> 查看全部存儲平台的使用方法！</p><h4><span>🔧 配置</span></h4><p>這裏以阿里雲 OSS 為例，<code>pom.xml</code> 引入本項目，這裏默認是 <code>SpringBoot</code> 環境，其它環境參考 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E8%2584%25B1%25E7%25A6%25BBSpringBoot%25E5%258D%2595%25E7%258B%25AC%25E4%25BD%25BF%25E7%2594%25A8" target="_blank">脫離 SpringBoot 單獨使用</a></p><pre><code><span>&lt;!--&nbsp;引入本項目&nbsp;--&gt;</span><span>&lt;<span>dependency</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>groupId</span>&gt;</span>org.dromara.x-file-storage<span>&lt;/<span>groupId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>artifactId</span>&gt;</span>x-file-storage-spring<span>&lt;/<span>artifactId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>version</span>&gt;</span>2.1.0<span>&lt;/<span>version</span>&gt;</span><span>&lt;/<span>dependency</span>&gt;</span><span>&lt;!--&nbsp;引入&nbsp;阿里雲&nbsp;OSS&nbsp;SDK，如果使用其它存儲平台，就引入對應的&nbsp;SDK&nbsp;&nbsp;--&gt;</span><span>&lt;<span>dependency</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>groupId</span>&gt;</span>com.aliyun.oss<span>&lt;/<span>groupId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>artifactId</span>&gt;</span>aliyun-sdk-oss<span>&lt;/<span>artifactId</span>&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>&lt;<span>version</span>&gt;</span>3.16.1<span>&lt;/<span>version</span>&gt;</span><span>&lt;/<span>dependency</span>&gt;</span></code></pre><p><code>application.yml</code> 配置文件中添加以下基礎配置</p><pre><code><span>dromara:</span>
&nbsp;&nbsp;<span>x-file-storage:</span>&nbsp;<span>#文件存儲配置</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>default-platform:</span>&nbsp;<span>aliyun-oss-1</span>&nbsp;<span>#默認使用的存儲平台</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>aliyun-oss:</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>-</span>&nbsp;<span>platform:</span>&nbsp;<span>aliyun-oss-1</span>&nbsp;<span>#&nbsp;存儲平台標識</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>enable-storage:</span>&nbsp;<span>true</span>&nbsp;&nbsp;<span>#&nbsp;啓用存儲</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>access-key:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>secret-key:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>end-point:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>bucket-name:</span>&nbsp;<span>??</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>domain:</span>&nbsp;<span>??</span>&nbsp;<span>#&nbsp;訪問域名，注意「/」結尾，例如：https://abc.oss-cn-shanghai.aliyuncs.com/</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>base-path:</span>&nbsp;<span>test/</span>&nbsp;<span>#&nbsp;基礎路徑</span></code></pre><h4><span>🔨編碼</span></h4><p>在啓動類上加上<code>@EnableFileStorage</code>註解</p><pre><code><span>@EnableFileStorage</span><span>@SpringBootApplication</span><span>public</span>&nbsp;<span><span>class</span>&nbsp;<span>SpringFileStorageTestApplication</span>&nbsp;</span>{

&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;<span>static</span>&nbsp;<span>void</span>&nbsp;<span>main</span><span>(String[]&nbsp;args)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SpringApplication.run(SpringFileStorageTestApplication<span>.<span>class</span>,<span>args</span>)</span>;
&nbsp;&nbsp;&nbsp;&nbsp;}

}
</code></pre><h4><span>✨開始上傳</span></h4><p>支持 File、MultipartFile、byte[]、InputStream、URL、URI、String、HttpServletRequest，大文件會自動分片上傳。如果想支持更多方式，請閲讀 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E6%2596%2587%25E4%25BB%25B6%25E9%2580%2582%25E9%2585%258D%25E5%2599%25A8" target="_blank">文件適配器</a> 章節</p><pre><code><span>@RestController</span><span>public</span>&nbsp;<span><span>class</span>&nbsp;<span>FileDetailController</span>&nbsp;</span>{

&nbsp;&nbsp;&nbsp;&nbsp;<span>@Autowired</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>private</span>&nbsp;FileStorageService&nbsp;fileStorageService;<span>//注入實列</span>

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上傳文件
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>upload</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>//只需要這一行代碼即可上傳成功</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(file).upload();
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上傳文件，成功返回文件&nbsp;url
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload2"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;String&nbsp;<span>upload2</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FileInfo&nbsp;fileInfo&nbsp;=&nbsp;fileStorageService.of(file)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPath(<span>"upload/"</span>)&nbsp;<span>//保存到相對路徑下，為了方便管理，不需要可以不寫</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setObjectId(<span>"0"</span>)&nbsp;&nbsp;&nbsp;<span>//關聯對象 id，為了方便管理，不需要可以不寫</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setObjectType(<span>"0"</span>)&nbsp;<span>//關聯對象類型，為了方便管理，不需要可以不寫</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.putAttr(<span>"role"</span>,<span>"admin"</span>)&nbsp;<span>//保存一些屬性，可以在切面、保存上傳記錄、自定義存儲平台等地方獲取使用，不需要可以不寫</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.upload();&nbsp;&nbsp;<span>//將文件上傳到對應地方</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileInfo&nbsp;==&nbsp;<span>null</span>&nbsp;?&nbsp;<span>"上傳失敗！"</span>&nbsp;:&nbsp;fileInfo.getUrl();
&nbsp;&nbsp;&nbsp;&nbsp;}

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上傳圖片，成功返回文件信息
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;圖片處理使用的是&nbsp;https://github.com/coobird/thumbnailator
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload-image"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>uploadImage</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(file)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.image(img&nbsp;-&gt;&nbsp;img.size(<span>1000</span>,<span>1000</span>))&nbsp;&nbsp;<span>//將圖片大小調整到&nbsp;1000*1000</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.thumbnail(th&nbsp;-&gt;&nbsp;th.size(<span>200</span>,<span>200</span>))&nbsp;&nbsp;<span>//再生成一張&nbsp;200*200&nbsp;的縮略圖</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.upload();
&nbsp;&nbsp;&nbsp;&nbsp;}

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;上傳文件到指定存儲平台，成功返回文件信息
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload-platform"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>uploadPlatform</span><span>(MultipartFile&nbsp;file)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(file)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPlatform(<span>"aliyun-oss-1"</span>)&nbsp;&nbsp;&nbsp;&nbsp;<span>//使用指定的存儲平台</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.upload();
&nbsp;&nbsp;&nbsp;&nbsp;}

&nbsp;&nbsp;&nbsp;&nbsp;<span>/**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;直接讀取&nbsp;HttpServletRequest&nbsp;中的文件進行上傳，成功返回文件信息
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;使用這種方式有些注意事項，請查看文檔&nbsp;基礎功能-上傳&nbsp;章節
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span>@PostMapping</span>(<span>"/upload-request"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;<span><span>public</span>&nbsp;FileInfo&nbsp;<span>uploadPlatform</span><span>(HttpServletRequest&nbsp;request)</span>&nbsp;</span>{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>return</span>&nbsp;fileStorageService.of(request).upload();
&nbsp;&nbsp;&nbsp;&nbsp;}
}
</code></pre><h4><span>🎨其它操作</span></h4><pre><code><span>//手動構造文件信息，可用於其它操作</span>
FileInfo&nbsp;fileInfo&nbsp;=&nbsp;<span>new</span>&nbsp;FileInfo()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPlatform(<span>"huawei-obs-1"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setBasePath(<span>"test/"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setPath(<span>"aa/"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setFilename(<span>"image.png"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.setThFilename(<span>"image.png.min.jpg"</span>);

<span>//文件是否存在</span><span>boolean</span>&nbsp;exists&nbsp;=&nbsp;fileStorageService.exists(fileInfo);
<span>//下載</span><span>byte</span>[]&nbsp;bytes&nbsp;=&nbsp;fileStorageService.download(fileInfo).bytes();
<span>//刪除</span>
fileStorageService.delete(fileInfo);
<span>//其它更多操作</span></code></pre><p>如果將文件記錄保存到數據庫中，還可以更方便的根據 URL 進行操作了，詳情請閲讀 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%259F%25BA%25E7%25A1%2580%25E5%258A%259F%25E8%2583%25BD%3Fid%3D%25E4%25BF%259D%25E5%25AD%2598%25E4%25B8%258A%25E4%25BC%25A0%25E8%25AE%25B0%25E5%25BD%2595" target="_blank">保存上傳記錄</a> 章節</p><pre><code><span>//直接從數據庫中獲取&nbsp;FileInfo&nbsp;對象，更加方便執行其它操作</span>
FileInfo&nbsp;fileInfo&nbsp;=&nbsp;fileStorageService.getFileInfoByUrl(<span>"https://abc.def.com/test/aa/image.png"</span>);

<span>//文件是否存在</span><span>boolean</span>&nbsp;exists&nbsp;=&nbsp;fileStorageService.exists(<span>"https://abc.def.com/test/aa/image.png"</span>);
<span>//下載</span><span>byte</span>[]&nbsp;bytes&nbsp;=&nbsp;fileStorageService.download(<span>"https://abc.def.com/test/aa/image.png"</span>).bytes();
<span>//刪除</span>
fileStorageService.delete(<span>"https://abc.def.com/test/aa/image.png"</span>);
<span>//其它更多操作</span></code></pre><p>點擊 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fx-file-storage.xuyanwu.cn%2F%23%2F%25E5%25BF%25AB%25E9%2580%259F%25E5%2585%25A5%25E9%2597%25A8" target="_blank">快速入門</a> 查看全部存儲平台的使用方法！</p><hr><p>&nbsp;</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 22 Jan 2024 00:50:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/276310</guid>
            <link>https://www.oschina.net/news/276310</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[《互聯網法律白皮書（2023 年）》發佈]]>
            </title>
            <description>
                <![CDATA[<div class="content"><p>2024 年 1 月 19 日，中國信息通信研究院（簡稱「中國信通院」）互聯網法律研究中心舉辦第七屆互聯網法律研討會，<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSfy0OltG0u4n1YKNDSXmqg" target="_blank">發佈</a>《互聯網法律白皮書（2023 年）》。系統梳理過去一年國內外重要互聯網立法活動，深入分析我國互聯網立法的最新成果和國際互聯網立法進展，對未來互聯網立法工作進行了展望，希望能為社會各界瞭解互聯網領域立法情況提供有價值的參考。</p><p><strong>白皮書核心觀點</strong></p><ol><li><p>我國網絡空間法治化邁向新階段。我國加快推進互聯網立法進程，在網絡安全、數據安全、個人信息保護、互聯網平台等方面明確管理要求，為構建網絡綜合治理體系提供了法治保障。2023 年，我國互聯網法治建設繼續順應全球信息化發展大勢，立足我國互聯網發展實踐，不斷深化對依法治網的規律性認識，在保安全的基礎上向促發展邁進，查漏補缺、完善制度、細化規則。</p></li><li><p>我國互聯網法律體系日趨健全完善。我國深入貫徹落實黨中央關於網絡強國、數字中國的決策部署，積極推進互聯網立法工作，不斷完善相關法律制度規範，基本形成了具有中國特色的互聯網法律體系。網絡設施安全防護進一步加強，保障數據安全和促進數據價值釋放法律制度同步推進，互聯網平台發展的法治環境日益優化，新技術新模式發展逐步規範。</p></li><li><p>全球互聯網立法展現新趨勢新動向。2023 年，全球網絡安全立法持續推進，數據跨境流動和數據共享流通成為新的立法關注點，互聯網平台對網絡信息內容的管理義務進一步強化，全球加快對人工智能等新技術新應用的立法應對，各國加快推進互聯網立法進程為保障其國內發展、強化國際競爭提供製度保障。</p></li><li><p>構建適應數字經濟和實體經濟融合發展的互聯網法律體系。在習近平新時代中國特色社會主義思想指引下，我國需適應數字經濟發展要求，構建完善互聯網法律體系，補齊法制短板，通過分類分級制度實現精準施策。應對人工智能等新技術發展趨勢，為新技術的規範發展提供法治支撐。深入佈局研究互聯網立法前沿問題，探索構建數據基礎法律制度。進一步健全互聯網法律法規，提高我國數字經濟治理體系和治理能力現代化水平，以法治護航數字經濟行穩致遠。</p></li></ol><p><img height="280" src="https://oscimg.oschina.net/oscnet/up-2e6e5be1f936754aa5edce38c01aada886b.png" width="500" referrerpolicy="no-referrer"></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-559d5e7fe744cce93a4ae209e492aee5880.png" width="500" referrerpolicy="no-referrer"></p><p><img height="272" src="https://oscimg.oschina.net/oscnet/up-7ae8f0a460439255c90efe36af5171f1195.png" width="500" referrerpolicy="no-referrer"></p><p><img height="271" src="https://oscimg.oschina.net/oscnet/up-ab6fa6ae131b9d94129e631761e1ee414c9.png" width="500" referrerpolicy="no-referrer"></p><p><img height="278" src="https://oscimg.oschina.net/oscnet/up-9998e54e517d56fe321f2268e243548c9f8.png" width="500" referrerpolicy="no-referrer"></p><p><img height="279" src="https://oscimg.oschina.net/oscnet/up-60418af84fcfa23ba48a3f631cff64713c5.png" width="500" referrerpolicy="no-referrer"></p><p><img height="277" src="https://oscimg.oschina.net/oscnet/up-6f1f74832bb661a34b0429d057d5aca4d93.png" width="500" referrerpolicy="no-referrer"></p><p><img height="275" src="https://oscimg.oschina.net/oscnet/up-e099e03ebfcf5a5101d332999447594f51a.png" width="500" referrerpolicy="no-referrer"></p><p><img height="285" src="https://oscimg.oschina.net/oscnet/up-ef189c3cccb8a9ec7d7e5540ddac9d9026d.png" width="500" referrerpolicy="no-referrer"></p><p><img height="284" src="https://oscimg.oschina.net/oscnet/up-9a859a2d1c378f75b300162a287fa4ffd38.png" width="500" referrerpolicy="no-referrer"></p><p><img height="284" src="https://oscimg.oschina.net/oscnet/up-39b8a71bbc15699a837a443f84672b1781c.png" width="500" referrerpolicy="no-referrer">&nbsp;</p></div>
                                    ]]>
            </description>
            <guid isPermaLink="false">https://www.oschina.net/news/276402</guid>
            <link>https://www.oschina.net/news/276402</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
    </channel>
</rss>
