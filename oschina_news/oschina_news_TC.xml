<?xml version="1.0" encoding="UTF-8"?>
<rss
    xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"


>
    <channel>
        <title>
            <![CDATA[開源中國-最新資訊]]>
        </title>
        <link>https://www.oschina.net/news/project</link>
        <atom:link href="https://rsshub.app/oschina/news" rel="self" type="application/rss+xml" />
        <description>
            <![CDATA[開源中國-最新資訊 - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]>
        </description>
        <generator>RSSHub</generator>
        <webMaster>i@diygod.me (DIYgod)</webMaster>
        <language>zh-cn</language>
        <lastBuildDate>Mon, 30 Oct 2023 03:14:50 GMT</lastBuildDate>
        <ttl>120</ttl>
        <item>
            <title>
                <![CDATA[jQuery 4.0 開發進度：已完成 99%]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>根據 jQuery 的 GitHub 里程碑狀態，其 4.0.0 版本的開發進度已完成 99%。</p><p><img height="1018" src="https://static.oschina.net/uploads/space/2023/1030/105430_Ecrh_2720166.png" width="2170" referrerpolicy="no-referrer"></p><p><em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjquery%2Fjquery%2Fmilestone%2F7" target="_blank">https://github.com/jquery/jquery/milestone/7</a></u></em></p><p>可以看到，目前待處理的 issue 僅剩一個，其內容是升級與 ESLint 相關的軟件包，以及修復 linting 錯誤。已經處理完畢的 issue 共計 163 個，內容包括核心變更、構建變更、與 Ajax 相關的改動等，詳情查看&nbsp;<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fjquery%2Fjquery%2Fmilestone%2F7%3Fclosed%3D1" target="_blank">https://github.com/jquery/jquery/milestone/7?closed=1</a></u>。</p><p><img src="https://static.oschina.net/uploads/space/2023/1030/111012_2yQh_2720166.png" referrerpolicy="no-referrer"></p><blockquote><p>jQuery 是一個快速、小型且功能豐富的 JavaScript 庫。通過易於使用的 API（可在多種瀏覽器中使用），使 HTML 文檔的遍歷和操作、事件處理、動畫和 Ajax 等操作變得更加簡單。結合了多功能性和可擴展性，jQuery 改變了數百萬人編寫 JavaScript 的方式。</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 03:11:46 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264038/jquery-4-0-milestone-99-percent-complete</guid>
            <link>https://www.oschina.net/news/264038/jquery-4-0-milestone-99-percent-complete</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[B 站：全年「AIGC」相關視頻播放量 90 億]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="color:#000000">嗶哩嗶哩 (B 站) 日前在其首屆「bilibili 超級科學晚」會上，發佈了一個「五大科學焦點榜單」—— AIGC、室溫超導、腦機接口、黑洞、可控核聚變入選。</span></p><p><span style="color:#000000">並公佈數據稱，過去一年有 2.43 億用戶在 B 站進行學習，是中國在校大學生人數的 5.5 倍。B 站泛知識內容消費人羣中，有 72% 為 00 後。</span></p><p><span style="color:#000000">科學和知識品類佔 B 站用戶搜索排名第 2 位，相關內容播放量佔 B 站 41%，00 後正在成為科學內容消費主力。全年「AIGC」相關視頻播放量 90 億，播放時長達 140 億分鐘。全站 UP 主圍繞 ChatGPT、文心一言、盤古氣象等多個大模型發佈動向，共投稿 330 萬支視頻，是無可爭議的 2023 年熱度最高科學技術領域。</span></p><p><span style="color:#000000">截至目前，B 站累計入駐知識類 UP 主 300 餘萬人。</span></p><p><span style="color:#000000"><img alt="" height="1596" src="https://oscimg.oschina.net/oscnet/up-8324fab9d8b85f93db34425ec9fc991cf4b.jpg" width="350" referrerpolicy="no-referrer"></span></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 03:11:46 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264039</guid>
            <link>https://www.oschina.net/news/264039</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[KubeSphere 社區雙週報 | KubeKey 支持 Web UI]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>KubeSphere 社區雙週報主要整理展示新增的貢獻者名單和證書、新增的講師證書以及兩週內提交過 commit 的貢獻者，並對近期重要的 PR 進行解析，同時還包含了線上/線下活動和佈道推廣等一系列社區動態。</p><p>本次雙週報涵蓋時間為：2023.10.13-2023.10.26。</p><h2>貢獻者名單</h2><p><img src="https://oscimg.oschina.net/oscnet/up-1a76ec80c43e7bcb348d4c83f3acd397fe2.gif" alt="" referrerpolicy="no-referrer"></p><p><img src="https://oscimg.oschina.net/oscnet/up-a1db65d7a7a9dd815a333a3f0f7cac20dde.png" alt="" referrerpolicy="no-referrer"></p><h2>新晉 KubeSphere Contributor</h2><p>兩週內共有 7 位新晉 KubeSphere Contributor，包括在社區分享最佳實踐經驗的小夥伴，感謝各位對 KubeSphere 社區的貢獻！</p><table><thead><tr><th>GitHub ID</th><th>證書</th></tr></thead><tbody><tr><td>Hanmo123</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-Hanmo123.png" target="_blank">下載證書</a></td></tr><tr><td>JoeDerby</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-JoeDerby.png" target="_blank">下載證書</a></td></tr><tr><td>SongJXin</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-SongJXin.png" target="_blank">下載證書</a></td></tr><tr><td>gunine</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-gunine.png" target="_blank">下載證書</a></td></tr><tr><td>jongwooo</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-jongwooo.png" target="_blank">下載證書</a></td></tr><tr><td>studyingwang23</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-studyingwang23.png" target="_blank">下載證書</a></td></tr><tr><td>Leirong Luo</td><td><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpek3b.qingstor.com%2Fkubesphere-community%2Fimages%2Fcontributor-2023-luoleirong.png" target="_blank">下載證書</a></td></tr></tbody></table><h2>近期更新</h2><h3>KubeSphere</h3><h4>1. 修復 K8s 1.26+ 環境中網管地址未正確展示的問題</h4><p>相關 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fpull%2F5950" target="_blank">https://github.com/kubesphere/kubesphere/pull/5950</a></p><p>貢獻者：hongzhouzi</p><h4>2. 修復應用更新時間不正確的問題</h4><p>相關 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fpull%2F5948" target="_blank">https://github.com/kubesphere/kubesphere/pull/5948</a></p><p>貢獻者：king-119</p><h4>3. 鏡像 tag 列表默認按名稱排序</h4><p>相關 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubesphere%2Fpull%2F5957" target="_blank">https://github.com/kubesphere/kubesphere/pull/5957</a></p><p>貢獻者：zhou1203</p><h3>KubeKey</h3><h4>1. 支持 Web UI</h4><p>相關 PR：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fkubesphere%2Fkubekey%2Fpull%2F2007" target="_blank">https://github.com/kubesphere/kubekey/pull/2007</a></p><p>貢獻者：shijilin0116</p><h2>社區動態</h2><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg4NTU0MzEyMg%3D%3D%26mid%3D2247526348%26idx%3D1%26sn%3Dbc88cb295e3d769a8bf7f2f94b17dab7%26chksm%3Dcfa57e71f8d2f7670a55352fd93aafb57f990c2ab407e0996c63888aeb085aa765d61507e31c%26token%3D1638355988%26lang%3Dzh_CN%23rd" target="_blank">ARM 版 openEuler 22.03 部署 KubeSphere v3.4.0 不完全指南</a></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg4NTU0MzEyMg%3D%3D%26mid%3D2247526431%26idx%3D1%26sn%3D035ab125b4109ab78c685b5eba12ab4d%26chksm%3Dcfa571a2f8d2f8b435a22c90e1fec21e4ebd58270e9e2ddd1d292faf77ce79c06026b2402bf0%26token%3D1638355988%26lang%3Dzh_CN%23rd" target="_blank">11 月 4 日成都站 Meetup 分享內容詳情曝光！</a></li></ul><blockquote><p>本文由博客一文多發平台 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fopenwrite.cn%3Ffrom%3Darticle_bottom" target="_blank">OpenWrite</a> 發佈！</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 03:03:46 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4197945/blog/10138824</guid>
            <link>https://my.oschina.net/u/4197945/blog/10138824</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Vercel 發佈免費開源字體 Geist]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Vercel 公司發佈了一款免費開源字體 ——<strong>「<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fvercel.com%2Ffont" target="_blank">Geist</a>」</strong>，稱專門面向設計師和開發者而設計。</p><blockquote><p><em>開源地址：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvercel%2Fgeist-font" target="_blank">https://github.com/vercel/geist-font</a></u><br> 下載地址：</em><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fvercel%2Fgeist-font%2Freleases%2Ftag%2F1.0.0" target="_blank">https://github.com/vercel/geist-font/releases/tag/1.0.0</a></u></p></blockquote><p>Geist 字體由 Vercel 與 Basement Studio 聯手打造，包含<strong> Geist Sans 和 Geist Mono</strong>，分別對應的是<strong>無襯線字體</strong>和<strong>等寬字體</strong>。</p><p>根據官方的介紹，Geist Sans 作為無襯線字體，其設計理念是易讀和簡潔。它是一種現代的幾何字體，基於經典的瑞士字體設計原則打造，被設計用於標題、徽標、海報和其他大尺寸顯示屏。</p><p>Geist Mono 則是 Geist Sans 的完美搭檔。作為等寬字體，它被設計用於代碼編輯器、圖表、終端和其他以文本為基礎的代碼界面。目前提供的版本優先考慮可讀性，並支持無縫集成到編碼環境中。</p><h4><strong>Geist 字體預覽效果</strong></h4><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-e79e19a1fd7bb59d7c6f9780a4b6b40cefd.png" referrerpolicy="no-referrer"></p><p>此外，Geist 受到以下字體的影響和啓發：Inter、Univers、SF Mono、SF Pro、Suisse International、ABC Diatype Mono 和 ABC Diatype。</p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:35:56 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/264031/vercel-geist-font</guid>
            <link>https://www.oschina.net/news/264031/vercel-geist-font</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Weaviate —— 開源向量數據庫]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="project_detail_above_text_link_1" data-tracepid="project_detail_above_text_link"><a style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代 <img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p><span><span><span>Weaviate 是一個開源向量數據庫，</span></span></span><span style="background-color:#ffffff; color:#1f2328">具有強大、可擴展、雲原生且快速的特點。</span><span><span><span>可存儲對象和向量，允許將向量搜索與結構化過濾與雲原生數據庫的容錯性和可擴展性相結合，所有這些都可以通過 GraphQL、REST 和各種語言客戶端進行訪問。</span></span></span></p><p><span style="color:#000000">允許你存儲來自你最喜歡的 ML 模型的數據對象和向量嵌入，並無縫擴展到數十億個數據對象。</span></p><p style="margin-left:0px; margin-right:0px; text-align:start"><span><span><span><span><span style="color:#000000"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span><strong>簡而言之</strong>：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li>Weaviate 是一個開源<a href="https://weaviate.io/blog/what-is-a-vector-database" target="_blank">向量數據庫</a>。</li><li><a href="https://weaviate.io/developers/weaviate/concepts/vector-index">Weaviate 允許你通過使用向量索引</a>來根據數據對象的語義屬性來存儲和檢索數據對象。</li><li>Weaviate 可以獨立使用<span style="background-color:#ffffff; color:#000000">&nbsp;(aka<span>&nbsp;</span></span>bring your vectors<span style="background-color:#ffffff; color:#000000">)<span>&nbsp;</span></span>，也可以與各種可以為您進行向量化並擴展核心功能的<a href="https://weaviate.io/developers/weaviate/modules">模塊一起使用。</a></li><li>Weaviate 具有<a href="https://weaviate.io/developers/weaviate/api/graphql">GraphQL-API</a>，可輕鬆訪問你的數據。</li><li>Weaviate 速度很快（查看<a href="https://weaviate.io/developers/weaviate/benchmarks">開源基準測試</a>）。</li></ul><p>Weaviate 是一個低延遲向量數據庫，對不同媒體類型（文本、圖像等）提供開箱即用的支持。它提供語義搜索、問答提取、分類、可定製模型 (PyTorch/TensorFlow/Keras) 等。Weaviate 以 Go 語言從頭開始構建，同時存儲對象和向量，從而將向量搜索與結構化過濾和雲原生數據庫的容錯性結合起來。所有這些都可以通過 GraphQL、REST 和各種客戶端編程語言進行訪問。</p><p style="margin-left:0px; margin-right:0px; text-align:start"><span><span><span><span><span style="color:#000000"><span><span><span><span><span><span><span><span><span><span><span style="background-color:#ffffff"><span><span><span>Weaviate 可以輕鬆使用最先進的 AI 模型，同時提供專用向量數據庫的可擴展性、易用性、安全性和成本效益。最為顯着地：</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>快速查詢</strong><br>
Weaviate 通常在不到 100 毫秒的時間內對數百萬個對象執行最近鄰 (NN) 搜索。<a href="https://weaviate.io/developers/weaviate/benchmarks">可以在我們的基準</a>頁面上找到更多信息。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>使用 Wea​​viate 模塊攝取任何媒體類型</strong><br>
使用最先進的 AI 模型推理（例如 Transformer）在搜索和查詢時訪問數據（文本、圖像等），讓 Weaviate 管理數據矢量化過程為你 - 或提供你自己的向量。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>組合向量和標量搜索</strong><br>
Weaviate 可以進行高效的組合向量和標量搜索。例如，「過去 7 天內發表的與 COVID-19 大流行相關的文章」。Weaviate 存儲對象和向量，並確保兩者的檢索始終高效。不需要第三方對象存儲。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>實時且持久的</strong><br>
Weaviate 讓你可以搜索數據，即使當前正在導入或更新數據。此外，每次寫入都會寫入預寫日誌 (WAL)，以便立即持久寫入 - 即使發生崩潰也是如此。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>水平可擴展性</strong></span></span></span></span><br><span style="background-color:#ffffff; color:#000000">Scale</span>&nbsp;<span><span><span><span>Weaviate 滿足你的確切需求，例如最大攝取量、最大可能的數據集大小、每秒最大查詢數等。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>高可用性</strong><br>
已列入<a href="https://weaviate.io/developers/weaviate/roadmap">路線圖</a>，並計劃於今年晚些時候發佈。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>成本效益</strong><br>
非常大的數據集不需要完全保存在 Weaviate 的內存中。同時，可以利用可用內存來提高查詢速度。這樣可以有意識地進行速度/成本權衡，以適應每個用例。</span></span></span></span></p></li><li><p style="margin-left:0px; margin-right:0px"><span><span><span><span><strong>對象之間的類似圖形的連接</strong><br>
以類似圖形的方式在對象之間建立任意連接，以類似於數據點之間的真實連接。使用 GraphQL 遍歷這些連接。</span></span></span></span></p></li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:32:56 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/weaviate</guid>
            <link>https://www.oschina.net/p/weaviate</link>
        </item>
        <item>
            <title>
                <![CDATA[Gitee 推薦 | 基於 LLM 大語言模型的知識庫問答系統 FastGPT]]>
            </title>
            <description>
                <![CDATA[<h1><a id="user-content-fastgpt" class="anchor" href="https://gitee.com/xindian/FastGPT#fastgpt"></a>FastGPT</h1><p>FastGPT 是一個基於 LLM 大語言模型的知識庫問答系統，提供開箱即用的數據處理、模型調用等能力。同時可以通過 Flow 可視化進行工作流編排，從而實現複雜的問答場景！</p><h2><a id="user-content--在線體驗" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E5%9C%A8%E7%BA%BF%E4%BD%93%E9%AA%8C"></a>🛸 在線體驗</h2><p>🎉 <a href="https://gitee.com/link?target=https%3A%2F%2Ffastgpt.run%2F">fastgpt.run</a>（服務器在新加坡，部分地區可能無法直連）</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro1.png" alt="Demo" referrerpolicy="no-referrer"></td><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro2.png" alt="Demo" referrerpolicy="no-referrer"></td></tr><tr><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro3.png" alt="Demo" referrerpolicy="no-referrer"></td><td><img src="https://gitee.com/xindian/FastGPT/raw/dev4/.github/imgs/intro4.png" alt="Demo" referrerpolicy="no-referrer"></td></tr></tbody></table><h2><a id="user-content--開發" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E5%BC%80%E5%8F%91"></a>👨‍💻 開發</h2><p>項目技術棧: NextJs + TS + ChakraUI + Mongo + Postgres（Vector 插件）<br><a href="https://gitee.com/xindian/FastGPT/blob/dev4/docSite/i18n/zh-Hans/docusaurus-plugin-content-docs/current/quick-start/dev.md">本地開發 Quick Start</a></p><h2><a id="user-content--部署" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E9%83%A8%E7%BD%B2"></a>🚀 部署</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fsealos.io%2Fdocs%2Fexamples%2Fai-applications%2Finstall-fastgpt-on-desktop">官方推薦 Sealos 部署</a> 無需服務器，代理和域名，高可用。</li><li><a href="https://gitee.com/xindian/FastGPT/blob/dev4/docSite/i18n/zh-Hans/docusaurus-plugin-content-docs/current/deploy/docker.md">docker-compose 部署</a> 單機版。</li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1tV4y1y7Mj%2F%3Fvd_source%3D92041a1a395f852f9d89158eaa3f61b4">由社區貢獻的寶塔部署和本地運行教程</a> 單機版。</li></ul><h2><a id="user-content--roadmap" class="anchor" href="https://gitee.com/xindian/FastGPT#-roadmap"></a><img class="emoji" alt=":point_right:" style="vertical-align: middle" src="https://cn-assets.gitee.com/assets/emoji/point_right-8d392cf32998e3bca12bb7b4ee10dae0.png" width="14" height="14" referrerpolicy="no-referrer"> RoadMap</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fkjqvjse66l.feishu.cn%2Fdocx%2FRVUxdqE2WolDYyxEKATcM0XXnte">FastGpt RoadMap</a></li></ul><h2><a id="user-content-️-交流羣" class="anchor" href="https://gitee.com/xindian/FastGPT#%EF%B8%8F-%E4%BA%A4%E6%B5%81%E7%BE%A4"></a>🏘️ 交流羣</h2><p>添加 wx 進入：<br><img src="https://otnvvf-imgs.oss.laf.run/wx300.jpg" alt="Demo" referrerpolicy="no-referrer"></p><h2><a id="user-content-powered-by" class="anchor" href="https://gitee.com/xindian/FastGPT#powered-by"></a>Powered by</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fmsgbyte%2Ftushan">TuShan: 5 分鐘搭建後台管理系統</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Flabring%2Flaf">Laf: 3 分鐘快速接入三方應用</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Flabring%2Fsealos">Sealos: 快速部署集羣應用</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fsongquanpeng%2Fone-api">One API: 令牌管理 &amp; 二次分發，支持 Azure</a></li></ul><h2><a id="user-content--其他" class="anchor" href="https://gitee.com/xindian/FastGPT#-%E5%85%B6%E4%BB%96"></a>👀 其他</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fkjqvjse66l.feishu.cn%2Fdocx%2FHtrgdT0pkonP4kxGx8qcu6XDnGh">FastGpt 常見問題</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1jo4y147fT%2F">docker 部署教程視頻</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1xh4y1t7fy%2F">公眾號接入視頻教程</a></li><li><a href="https://gitee.com/link?target=https%3A%2F%2Fwww.bilibili.com%2Fvideo%2FBV1Wo4y1p7i1%2F">FastGpt 知識庫演示</a></li></ul><h2><a id="user-content-第三方生態" class="anchor" href="https://gitee.com/xindian/FastGPT#%E7%AC%AC%E4%B8%89%E6%96%B9%E7%94%9F%E6%80%81"></a>第三方生態</h2><ul><li><a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fluolin-ai%2FFastGPT-Enterprise-WeChatbot">luolinAI: 企微機器人，開箱即用</a></li></ul><h2><a id="user-content--star-history" class="anchor" href="https://gitee.com/xindian/FastGPT#-star-history"></a>🌟 Star History</h2><p><a href="https://gitee.com/link?target=https%3A%2F%2Fstar-history.com%2F%23labring%2FFastGPT%26Date"><img src="https://api.star-history.com/svg?repos=labring/FastGPT&amp;type=Date" alt="Star History Chart" referrerpolicy="no-referrer"></a></p>]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:26:56 GMT</pubDate>
            <guid isPermaLink="false">https://gitee.com/xindian/FastGPT</guid>
            <link>https://gitee.com/xindian/FastGPT</link>
        </item>
        <item>
            <title>
                <![CDATA[每日一博 | 智能問答技術在百度搜索中的應用]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p><img src="https://oscimg.oschina.net/oscnet/up-7e074847d19344a1cd39fd61d566f8ba83b.png" alt="" referrerpolicy="no-referrer"></p><p>作者 | Xiaodong</p><blockquote><p>導讀</p><p>本文主要介紹了智能問答技術在百度搜索中的應用。包括機器問答的發展歷程、生成式問答、百度搜索智能問答應用。歡迎大家加入百度搜索團隊，共同探索智能問答技術的發展方向，文末有簡歷投遞方式。</p></blockquote><blockquote><p><em>全文 6474 字，預計閲讀時間 17 分鐘。</em></p></blockquote><h1><strong>01 什麼是機器問答</strong></h1><p>機器問答，就是讓計算機軟件系統自動回答人類提出的描述性問題。例如問：「王小丫的主持的節目叫什麼」，我們可以在百度搜索框裏輸入任意用自然語言描述的問題，並在搜索的首位結果中可以直接得相關答案，如下圖所示：</p><p><img src="https://oscimg.oschina.net/oscnet/up-963d02166a9d2429174f6b8653991ec8f94.png" alt="圖片" referrerpolicy="no-referrer"></p><p>區別於傳統搜索引擎根據多個關鍵詞反饋檢索的網頁鏈接，機器問答根據自然語言描述的問題直接獲取答案，可以極大地提高大家獲取信息的效率。機器問答在生活中無處不在，經統計，有約 40% 的搜索需求、約 30% 的對話需求都跟機器問答相關。</p><p>那麼，百度搜索的機器問答應用現狀如何？目前首條結果可以直接滿足大部分的問答需求，並且，在百度搜索中，不限定用戶問題領域，是一個開放式的問答系統，可以詢問任何信息。</p><h2><strong>1.1 機器問答的發展歷程</strong></h2><p>機器問答的發展歷程如下，與機器學習發展相吻合。</p><p><img src="https://oscimg.oschina.net/oscnet/up-9a139b0eb31cf87b33168f4ad87086aa2cd.png" alt="圖片" referrerpolicy="no-referrer"></p><p><strong>從模型方法的發展上看：</strong></p><p>2013 年以前，大家主要做一些特徵工程相關工作，即給定一個問題和一些候選答案，設計多種字面匹配特徵，並計算問題與答案之間詞的匹配度，例如 BM25 等算法。</p><p>2014~2015 年，隨着深度學習的發展，大家會使用神經網絡來計算問題和答案間表示的語義距離，例如 CNN、RNN 等。</p><p>2016~2017 年，大家會使用 Attention 網絡結構設計各類模型結構，進一步刻畫問題和答案間的深層語義匹配關係。</p><p>2018~2021 年，研究主要集中在訓練模型上，會使用一些更大、效果更好的預訓練模型來完成複雜的問答匹配任務。</p><p>自 2022 年開始，大家更多關注生成模型的應用。</p><p><strong>從數據集的發展上看：</strong></p><p>2013 年，MCTest 出現，以選擇題和完形填空形式為主。</p><p>2016 年，SQuAD 誕生，這是第一個大型閲讀理解數據信息，會根據用戶問題從提供的一篇文章中進行答案抽取。</p><p>2017 年，百度發佈了 DuReader 數據集，這是首箇中文的閲讀理解的數據集。</p><p>2018 年，HotputQA 等發佈，更加深入研究了多跳推理、常識推理等複雜的問答場景。</p><h2><strong>1.2 機器問答建模</strong></h2><p>目前的主流範式：Retriever + Reader</p><p>Retriever = 基於 query 查詢候選。即給定一個 query，獲得該 query 的相關候選，可能是網頁、視頻、表格、知識圖譜等。</p><p>Reader = 從給定候選中獲取答案信息。即在給定候選的基礎上，結合 query 進一步進行答案抽取。</p><p>百度搜索就是一個非常強的一個 Retriever ，它可以提供相關候選查詢，所以我們的研究工作更多集中在 Reader 上，即基於搜索結果如何更好地完成答案抽取。</p><p><img src="https://oscimg.oschina.net/oscnet/up-d2ba5b11127a99934c18d5825cba6ab00b1.png" alt="圖片" referrerpolicy="no-referrer"></p><p>早期的 Reader，主要基於傳統的特徵工程方法，是一個很複雜的系統化 pipeline 流程：先分析 query 獲得期望的答案類型、實體信息、問題類型等，並根據這些信息從候選庫裏檢索若干候選，並設計複雜的匹配特徵來計算 query 和候選的相關性打分，並設計排序函數進行排序，得到排序最高的答案，過程如下圖。</p><p><img src="https://oscimg.oschina.net/oscnet/up-74c5ecc4978e073d0b1799f4735a3b37278.png" alt="圖片" referrerpolicy="no-referrer"></p><p>這個流程是管道串聯的，每一步都存在誤差的積累，整個訓練流程也不可整體迭代，維護成本較高。後來，大家希望找到一種更加端到端的方法來解決以上問題，機器閲讀理解（Machine Reading Comprehension，MRC）被提出出來。</p><p>MRC 的任務的定義是：輸入 Question+Document，直接用一個模型替代複雜流程，輸出 Answer。早期的 MRC 工作會設計一些比較複雜的網絡結構，來對問題和答案之間的關係進行建模。一個比較經典的方法是 BiDAF，它的輸入層是對整個 document 和 query 分別映射到 enbedding 表示上，各自通過 LSTM 等網絡來學習問題和文檔上下文的表示，之後通過 Attention 交互層，採用雙向注意力對 query 和 document 的關係進行建模，在此基礎上再通過 LSTM 網絡獲取更豐富的上下文表示，最終輸出層預測每個位置作為答案開始和終止的概率，概率最高的片段被抽取作為答案。</p><p><img src="https://oscimg.oschina.net/oscnet/up-c79c34662eb1f0af4c7b094a66cf7656acb.png" alt="圖片" referrerpolicy="no-referrer"></p><p>早期的模型結構設計呈現百花齊放的狀態，以期更好解決問題和答案的建模。</p><p>後來，預訓練模型逐漸發展起來，大家意識到，複雜的模型結構設計並不太必要，transformer 就是目前為止最好的模型結構，這樣可以釋放更多研究精力到預訓練工作中，更多關注預訓練的任務設計、 loss 函數、預訓練的數據等。</p><p>在這種情況下，產生了多種預訓練模型，比如説最早的 BERT 和百度的<a href="https://www.oschina.net/action/visit/ad?id=1191" title="ERNIE">ERNIE</a>等，這些預訓練模型會使 MRC 更加簡單，大家會把 query 和 document 整體作為一個序列進行輸入，query 和 document 之間可以用一些特殊符號進行分割。經過預訓練模型的語義表示建模，最後依舊預測答案開始和結束的位置並進行抽取。</p><p><img src="https://oscimg.oschina.net/oscnet/up-b91feb0c3746117307b1b707dbc9b2341c0.png" alt="圖片" referrerpolicy="no-referrer"></p><h1><strong>02 生成式問答</strong></h1><p>近期生成式技術的發展非常火熱，也有非常多的工作發表。</p><p>早期一個比較有代表性的生成式 Reader，是 2017 年的 S-NET，它是針對 MS-MARCO 數據集專門設計的，該數據集的特點是答案來自多篇文章並且與原文中詞彙不一定相同。</p><p>針對這樣的任務，很自然的想法是用生成的方式來解決這個問題。它設計了一套兩階段的流程，第一階段是答案抽取模型，跟我們上面介紹的模型非常一致，並額外引入了 passage 排序任務對候選文章進行相關性排序。第二階段是生成模型，輸入得到抽取結果，生成答案的總結，如下圖所示。</p><p><img src="https://oscimg.oschina.net/oscnet/up-eac42697ab12c4dd61e9625a1bbf3ab4bcf.png" alt="圖片" referrerpolicy="no-referrer"></p><p>可以看出，早期的這些工作跟我們現在所使用的生成式問答流程非常相似，我們還會加一個檢索模塊，就是我們剛才最早提到的 Retriever，然後就是候選抽取、排序、生成。但是，這個工作還是依賴於額外信息來做參考總結。大家會想，是不是可以有一個生成模型，直接生成答案，而不依賴於我們輸入額外的信息知識？</p><p>2019 年的 T5 模型首先解決了這個問題，當時它是採用了一種「預訓練+遷移學習」的思路，將不同 NLP 任務統一到生成範式下，來統一完成問答、機器翻譯、情感分析、對話等一系列任務，且通過百億參數量的大模型（在當時算是比較大的規模）中存儲的知識直接回答問題。它也驗證了不同生成模型的結構，包括 Encoder-Decoder 方式的、Decoder-only 的和混合式的。</p><p>但是，T5 這類模型雖然可以完成一些簡單問答，但還不足以達到可直接使用的商用狀態，它的參數量及訓練方式還存在改進空間，對於一些通用問題也不能直接取得非常好的效果。直到 ChatGPT 的出現，它會採用更大的參數規模（千億級），並有更強的人類回覆對齊能力，去理解用戶指令，從而完成更加複雜的問答。可以説，ChatGPT 是已達商用級別的對話和問答產品。</p><h1><strong>03 百度搜索的智能問答應用</strong></h1><p>百度搜索的問答場景是豐富多樣的。答案抽取方式也有多種，比如説我們可以從百科或者網頁通過信息抽取的方式得到一些知識圖譜，在知識圖譜上來進行答案提取；更通用的方式是從網頁文本中，通過閲讀理解直接抽取答案；還可以通過對一些半結構化的數據，比如表格，來進一步的提取信息，並組織成更結構化的方式展現。不止是文本，也包括對視頻內容的理解和抽取。</p><p><img src="https://oscimg.oschina.net/oscnet/up-4f1d8cf290673670a6b13f27ff8f32c76f0.png" alt="圖片" referrerpolicy="no-referrer"></p><p>面臨着這樣一個豐富多樣的問答場景，我們會有哪些挑戰呢？</p><p><strong>挑戰 1</strong>：機器問答面臨複雜語義理解、推理、上下文建模難點？</p><p><strong>挑戰 2</strong>：面對搜索的高流量和機器問答對複雜模型的需求，如何實現快速響應？</p><p><strong>挑戰 3</strong>：開放領域的搜索場景下網頁數據非常複雜，答案質量參差不齊（錯誤、片面），如何提供正確且高質量的答案？</p><p>、</p><h2><strong>3.1 解決複雜語義理解、推理、上下文建模難點</strong></h2><p>比如最開始的這個例子，如下圖所示，答案中提到一個「她」，就需要做指代消解、對上下文的理解，並且上下文篇幅可能很長，通過深層次的理解才能知道所需的是一個答題節目，而不是其他節目。這個問題的解決依賴一些很複雜的模型。</p><p><img src="https://oscimg.oschina.net/oscnet/up-b0523ebaa060da3070ea6a4d3811d62d1bd.png" alt="圖片" referrerpolicy="no-referrer"></p><p>我們採用的解決方案是「大模型+預訓練」。</p><p>在預訓練中，我們會使用非常豐富的數據，包括幾個階段：</p><ul><li><p>首先，用 T 級別通用文本進行 Pretrain 學習基礎語言模型；</p></li><li><p>並且，使用百 G 級業務日誌進行 Post-pretrain 實現領域和目標遷移；</p></li><li><p>此外，進行細緻的數據挖掘，通過 G 級人工標註數據進行 Finetune 擬合業務效果；</p></li><li><p>最後，通過遠程監督數據增強、標註數據質量識別、薄弱數據自動挖掘和定向標註、用戶行為指引，實現數據和模型的閉環反饋。</p></li></ul><p>而在大模型方面：</p><ul><li><p>使用百億級參數量模型，提升知識記憶和語言理解能力</p></li><li><p>通過長序列建模，充分理解上下文</p></li></ul><p>例如，我們正在使用的一個模型，我們稱之為 DocMRC 模型，它模擬人做閲讀理解答題，閲讀整個文章，邏輯如下圖所示。</p><p><img src="https://oscimg.oschina.net/oscnet/up-efaa14b51d1240a2fed3ef2c93b114b90d9.png" alt="圖片" referrerpolicy="no-referrer"></p><p>輸入層支持長序列建模，將整個 doc segment sents 進行切分；特別的是，我們在每句話前插入 token 表示，CLS 用來匯聚每個句子的表示，整體輸入淺層詞級模型結構來學習局部表示；基於這個表示經過層次化結構學習深層上下文關係；最後輸出 CLS 特殊 token 表示標註，輸出答案。</p><p>輸出層會有兩種輸出：一種是針對問題輸出偏摘要等多句話答案介紹，會使用句子層的輸出，然後做序列標註的輸出；另一種是強調答案中的關鍵內容，可能是幾個實體，會將 token 表示做序列標註預測。</p><h2><strong>3.2 提升整體模型的速度，實現快速響應</strong></h2><p>搜索每天的用戶流量非常大，前面也提到，我們需要用到較大或較複雜模型，整個模型的耗時以及資源消耗也是非常大的。那麼，有沒有其他方式來提升整體模型的速度，實現快速響應及資源平衡？</p><p>剛才介紹的層次化的建模，對模型結構的優化，是一種解決方案。</p><p>另外有一種通用的方式：知識蒸餾，知識蒸餾是將大模型的知識提煉給單個小模型，在效果接近的情況下提升推理速度。這裏我們採用了一種「多 teacher 多階段蒸餾」模式。</p><p>針對問答的業務場景，我們會訓練多個不同的 teacher，通過不同 teacher 的集成來提升學習目標的上限。然後對於多個 teacher 蒸餾，一種基線方案是將每個 teacher 的打分或 loss 加權直接做平均，讓 student 擬合，但是我們認為這種方式可能並不能確保達到非常極致的效果。我們期望根據不同樣本動態做出選擇（因為不同 teacher 的側重有差異），設計了一種多階段蒸餾的模式，並在其中根據數據動態選擇 teacher，如下圖所示。</p><p><strong>第一階段</strong>，Teacher 模型訓練，訓練多 teacher 提升學習上限；</p><p><strong>第二階段</strong>，無監督蒸餾，無標數據很難判斷 teacher 的好壞，所以採用 teacher 間投票的方式，依據梯度方向動態選擇 teacher，剔除可能的噪聲 teacher；</p><p><strong>第三階段</strong>，有監督蒸餾，依據標註樣本對 teacher 動態賦權。</p><p><img src="https://oscimg.oschina.net/oscnet/up-1d06f3320489bb36099a988b3c28560be5c.png" alt="圖片" referrerpolicy="no-referrer"></p><p>通過這樣一種多階段多 teacher 蒸餾的方式，我們最終得到一個效果非常好的 student 模型，甚至超過單個大模型效果。</p><h2><strong>3.3 如何提供正確且高質量的答案</strong></h2><p>搜索場景的問答數據非常複雜，答案質量也參差不齊，很多網頁中可能存在一些錯誤信息或片面介紹，如何提供正確且高質量的答案是我們面臨的第三個挑戰。</p><p>如下圖所示，是搜索中場景的複雜答案的例子。左側是冗長答案，用戶無法快速抓住重點，這種情況下需要一種方式進行總結，用戶才能快速理解的答案關鍵信息，提升滿足效率。抽取式答案提取方式已經無法滿足，我們需要用生成技術對答案進行深層次壓縮總結。</p><p><img src="https://oscimg.oschina.net/oscnet/up-3b0cb10d8ed43151c7cfb9dba4b1f9792a7.png" alt="圖片" referrerpolicy="no-referrer"></p><p>另外，對於單篇文章中提取的答案可能不夠全面，我們需要從多篇網頁中做答案總結，也需要生成模型，如下圖所示。我們從多篇文章中總結答案，並在答案中標註來源，用戶可以清晰看到答案出處。</p><p><img src="https://oscimg.oschina.net/oscnet/up-743951b5f984c8a0ac47dc29838f43e96c9.png" alt="圖片" referrerpolicy="no-referrer"></p><p>綜上，如果要生成全面、高效、正確的答案，就需要有一個更好的生成模型。目前的大語言模型非常多，但怎樣的大語言模型才能完成搜索場景的問答任務呢？</p><h1><strong>04 檢索增強生成</strong></h1><p>目前大語言模型直接做問答還有幾個問題：</p><p><strong>第一</strong>，大預言模型難以記住所有知識，對於一些偏長尾知識可能有錯誤或者不知道的情況；</p><p><strong>第二</strong>，大語言模型的知識容易過時、更新困難，對於新知識無法及時感知；</p><p><strong>第三</strong>，大語言模型的輸出難以驗證，目前用戶的信賴感較差，我們無法完全信賴生成模型直接生成的答案。</p><p><strong>所以在這種情況下，大家希望能有一些方式來進行一些輔助的答案驗證。</strong></p><h2><strong>4.1 檢索增強生成流程</strong></h2><p>針對搜索問答場景，我們設計了檢索增強生成方案，已在百度搜索落地。檢索增強生成是基於搜索引擎補充相關信息，可有效緩解大模型幻覺，來提升答案的正確性、時效性以及可信度。整體流程分為幾個階段：</p><p>1、文檔檢索階段，會檢索得到多種參考來源；</p><p>2、答案抽取階段，會把文章抽取關鍵信息，減輕生成模型負擔；</p><p>3、prompt 組成階段，會根據獲取的參考來源來回答問題，並提供具體要求，比如説在答案內容中序號標註來源；</p><p>4、答案生成階段，將 prompt 輸入生成大模型中，最終得到搜索結果。</p><p><img src="https://oscimg.oschina.net/oscnet/up-4433c15aa52ec41c714f2962fbc7f36d8bf.png" alt="圖片" referrerpolicy="no-referrer"></p><p>如上圖所示，可以看到右側答案是總結了多篇文章的一個結果，並且也會在其中標註上參考來源，這就是我們期望給用戶提供的答案。</p><h2><strong>4.2 生成大模型訓練流程</strong></h2><p>我們生成大模型的訓練流程分為四個階段，如下圖所示，前兩個階段跟目前主流的生成大模型訓練比較接近，後兩個階段我們做了檢索增強生成問答場景下的特殊適配。</p><p><img src="https://oscimg.oschina.net/oscnet/up-2d170b5a9d03db57ccca02b3f9648bfe4a9.png" alt="圖片" referrerpolicy="no-referrer"></p><p><strong>第一階段</strong>，通用預訓練，我們會有一些通用的網頁語料以及垂類語料，比如書籍、表格、對話等，來獲得通用的預訓練基礎模型；</p><p><strong>第二階段</strong>，進行指令微調，我會提供一些通用的指令，使得模型擁有理解指令的能力；</p><p><strong>第三階段</strong>，標註業務指令，並用其做具體的微調，使其能理解搜索場景下的多結果組織的問答場景；</p><p><strong>第四階段</strong>，基於用戶行為反饋做細緻微調，以及通過強化學習等方式，提高生成答案的質量。</p><h2><strong>4.3 通過指令拆解，學習複雜指令</strong></h2><p>搜索的業務場景指令非常複雜，我們會提出非常具體的要求，並提供參考來源。那麼如何讓生成模型來理解這種複雜的指令？一種解決方案是標註很多這類複雜指令，並輸入到生成模型中，但這種方式並不一定是最佳的。如果模型學習這類指令偏多了，反而無法達到更好泛化效果，造成模型效果下降。有沒有其他的方式？</p><p><strong>這裏，我們借鑑推理鏈（CoT）的思想，提出通過指令拆解的方式，學習檢索生成場景下複雜指令。</strong></p><p>上述複雜指令通常可以通過三步簡單步驟完成：</p><p>第一步，選擇能用來回答問題的搜索結果；</p><p>第二步，根據選擇的搜索結果進行答案的組織和生成；</p><p>第三步，用編號的形式，加上參考來源。</p><p>可以看出，對於很複雜的指令，我們可以通過多步拆解變成多個簡單指令，我們會讓模型先去學習並理解簡單指令，之後可能不用太多複雜指令的數據，就能使模型在複雜指令上的表現達到一個非常好的水平。</p><h2><strong>4.4 推理加速及降低資源消耗</strong></h2><p>對於一些判別式模型，可以用蒸餾或一些其他的技術來做。但對於生成模型來説，模型尺寸小了對效果的影響較大，蒸餾並不特別適用，需要有一些其他的加速手段。近期業內有很多相關的工作研究，例如 Inference with Reference，就是針對檢索增強生成的業務場景，通過檢測固定 prefix，從參考中複製固定長度文本作為候選序列，驗證如與模型輸出一致則實現並行解碼多步，如下圖所示。</p><p><img src="https://oscimg.oschina.net/oscnet/up-78ee3ddc4ee98c5d2055bc1c54dc1d0b8d1.png" alt="圖片" referrerpolicy="no-referrer"></p><p>另外也有一些更加通用的生成加速的手段，例如可以用小模型快速生成多步，把小模型的預測結果直接輸入大模型，大模型驗證是否解碼一致，類似前一個工作也可以實現加速，但要求是儘量使我們的小模型和大模型效果接近，預測準確的概率會更大，加速比就會更大。</p><p>最後我給大家留一個問題，大家可以想一想： <strong>「下一代的搜索引擎會是什麼樣子？」</strong> 很期待你的回答，也歡迎與我們一起探討。</p><p><a href="https://www.oschina.net/action/GoToLink?url=mailto%3A%E4%BA%A4%E6%B5%81%26%E7%AE%80%E5%8E%86%E6%8A%95%E9%80%92%E9%82%AE%E7%AE%B1%EF%BC%9Asti01%40baidu.com" target="_blank">交流&amp;簡歷投遞郵箱：sti01@baidu.com</a></p><p>——END——</p><p><strong>推薦閲讀</strong></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247572187%26idx%3D1%26sn%3De62d1cf576fcc45232b67028321a9dd0%26chksm%3Dc03feaa7f74863b11423311a7158d1ae375b5fded421b83756d0af9cbb4db6d5dfc034ac1be4%26scene%3D21%23wechat_redirect" target="_blank">通過 Python 腳本支持 OC 代碼重構實踐（一）：模塊調用關係分析</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571891%26idx%3D1%26sn%3De5ab3e3ad26b8e92b5387e5905d17805%26chksm%3Dc03fe9cff74860d911187fc6e1b70da54e5c9b05453ef602576d2efc03d95b6fda158003ada7%26scene%3D21%23wechat_redirect" target="_blank">CVPR2023 優秀論文 | AIGC 偽造圖像鑑別算法泛化性缺失問題分析</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571772%26idx%3D1%26sn%3D3f9c022e989d2e8e2f7af2e6c7f7db76%26chksm%3Dc03fe940f7486056ca76cb5e0d6e1175d25ec77186ed8f7f26887ae5cb8d24c750acfafdaadb%26scene%3D21%23wechat_redirect" target="_blank">一文搞定專屬碼的設計與開發</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571660%26idx%3D1%26sn%3D867bb1f6c7e9cb68da34a95923a06a5e%26chksm%3Dc03fe8b0f74861a6972c085bcabc20b71647b02a4b312980722ad37ac843f519432393f6b657%26scene%3D21%23wechat_redirect" target="_blank">AI 原生應用速通指南</a></p><p><a href="https://www.oschina.net/action/GoToLink?url=http%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg5MjU0NTI5OQ%3D%3D%26mid%3D2247571596%26idx%3D1%26sn%3Dcc698b9be371c3d0316236551cf73a9d%26chksm%3Dc03fe8f0f74861e6e27850cbee6bad70d2e88f1e9fcaf474db0736232108a8a743d0223fdf40%26scene%3D21%23wechat_redirect" target="_blank">代碼理解技術應用實踐介紹</a></p></div>
                                    ]]>
            </description>
            <pubDate>Mon, 30 Oct 2023 02:21:56 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/4939618/blog/10123217</guid>
            <link>https://my.oschina.net/u/4939618/blog/10123217</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Linux Mint]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Linux Mint 團隊在最新月度報告中<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fblog.linuxmint.com%2F%3Fp%3D4591" target="_blank">提到</a></u>，他們已經開始着手開發對 Wayland 的支持。</p><p>團隊稱這項工作是他們在很長一段時間內必須面對的主要挑戰之一，雖然他們不期待 Wayland 能很快取代 Xorg 作為默認值，無論是在 21.3 中，還是在 22.x 中，但仍然希望做好準備。</p><p>按照計劃，Cinnamon 6.0 計劃在今年的 Mint 21.3 中推出，<strong>並將提供實驗性的 Wayland 支持</strong>。用戶可以從登錄界面在默認 Cinnamon 會話（在 Xorg 上運行）和 Cinnamon on Wayland 之間進行選擇。</p><p>下圖是 Cinnamon on Wayland 的運行截圖：</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-21b9daccab538fe5909df6cef6b172695f5.png" referrerpolicy="no-referrer"></p><p>Linux Mint 團隊表示，他們可能在 2026 年實現對 Wayland 的穩定支持/默認支持。鑑於 Linux Mint 堅持以 Ubuntu LTS 為基礎，因此並不指望在 Ubuntu 24.04 LTS 之前就能支持 Wayland。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 28 Oct 2023 04:19:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263917/linux-mint-wayland-progress</guid>
            <link>https://www.oschina.net/news/263917/linux-mint-wayland-progress</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[智譜 AI 推出第三代基座大模型]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>2023 年 10 月 27 日，智譜 AI 於 2023 中國計算機大會（CNCC）上，推出了<strong>全自研的第三代基座大模型 ChatGLM3</strong>及相關係列產品。</p><p><img height="281" src="https://static.oschina.net/uploads/space/2023/1028/102320_GQzP_2720166.jpg" width="500" referrerpolicy="no-referrer"></p><p>以下彙總摘錄自官方公告：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FSVq458IhrR2GGezA9goumw" target="_blank">https://mp.weixin.qq.com/s/SVq458IhrR2GGezA9goumw</a></u></p><hr><h4><strong>全新技術升級</strong></h4><p><strong>1. 更強大的性能：</strong></p><p>今年以來，這是我們第三次對 ChatGLM 基座模型進行了深度優化。我們採用了獨創的多階段增強預訓練方法，更豐富的訓練數據和更優的訓練方案，使訓練更為充分。</p><p>評測顯示，與 ChatGLM 二代模型相比，在 44 箇中英文公開數據集測試中，ChatGLM3 在國內同尺寸模型中排名首位。其中，MMLU 提升 36%、CEval 提升 33%、GSM8K 提升 179% 、BBH 提升 126%。</p><p><strong>2. 瞄向 GPT-4V 的技術升級：</strong></p><p>瞄向 GPT-4V，ChatGLM3 本次實現了若干全新功能的迭代升級，包括：</p><p><strong>多模態理解</strong>能力的 CogVLM，看圖識語義，在 10 餘個國際標準圖文評測數據集上取得 SOTA；</p><p><strong>代碼增強</strong>模塊 Code Interpreter 根據用戶需求生成代碼並執行，自動完成數據分析、文件處理等複雜任務；</p><p><strong>網絡搜索增強</strong>WebGLM，接入搜索增強，能自動根據問題在互聯網上查找相關資料並在回答時提供參考相關文獻或文章鏈接。</p><p>ChatGLM3 的<strong>語義能力與邏輯能力</strong>大大增強。</p><p><strong>3. 全新的 Agent 智能體能力：</strong></p><p>ChatGLM3 本次集成了自研的 AgentTuning 技術，激活了模型智能體能力，尤其在智能規劃和執行方面，相比於 ChatGLM 二代提升 1000%；開啓國產大模型原生支持工具調用、代碼執行、遊戲、數據庫操作、知識圖譜搜索與推理、操作系統等複雜場景。</p><p><strong>4. Edge 端側模型：</strong></p><p>ChatGLM3 本次推出可手機部署的端測模型 ChatGLM3-1.5B 和 ChatGLM3-3B，支持包括 Vivo、小米、三星在內的多種手機以及車載平台，甚至支持移動平台上 CPU 芯片的推理，速度可達 20 tokens/s。</p><p>精度方面 ChatGLM3-1.5B 和 ChatGLM3-3B 在公開 Benchmark 上與 ChatGLM2-6B 模型性能接近。</p><p><strong>5. 更高效推理/降本增效：</strong></p><p>基於最新的高效動態推理和顯存優化技術，我們當前的推理框架在相同硬件、模型條件下，相較於目前最佳的開源實現，包括伯克利大學推出的 vLLM 以及 Hugging Face TGI 的最新版本，推理速度提升了 2-3 倍，推理成本降低一倍，每千 tokens 僅 0.5 分，成本最低。</p><h4><strong>新一代「智譜清言」上線</strong></h4><p>在全新升級的 ChatGLM3 賦能下，生成式 AI 助手智譜清言已成為國內首個具備代碼交互能力的大模型產品（Code Interpreter）。</p><p>傳送門：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fchatglm.cn%2Fmain%2Fcode" target="_blank">https://chatglm.cn/main/code</a></u></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102833_nbbu_2720166.png" referrerpolicy="no-referrer"></p><p>在這一能力的加持下，ChatGLM3 可支持圖像處理、數學計算、數據分析等使用場景。以下分別為：</p><p><strong>處理數據生成圖表</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102853_C5gK_2720166.png" referrerpolicy="no-referrer"></p><p><strong>畫圖</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102920_iPm4_2720166.png" referrerpolicy="no-referrer"></p><p><strong>上傳 SQL 代碼分析</strong></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102938_Bkdu_2720166.png" referrerpolicy="no-referrer"></p><p>隨着 WebGLM 大模型能力的加入，智譜清言現具有搜索增強能力。智譜清言可以幫助用戶整理出相關問題的網上文獻或文章鏈接，並整理出答案。</p><p><img src="https://static.oschina.net/uploads/space/2023/1028/102955_TE2Q_2720166.jpg" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103014_ein5_2720166.png" referrerpolicy="no-referrer"></p><p>CogVLM 模型則提高了智譜清言的中文圖文理解能力，取得了接近 GPT-4V 的圖片理解能力。它可以回答各種類型的視覺問題，並且可以完成複雜的目標檢測，並打上標籤，完成自動數據標註。</p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103030_N8vg_2720166.png" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103042_fWLm_2720166.jpg" referrerpolicy="no-referrer"></p><p><img src="https://static.oschina.net/uploads/space/2023/1028/103052_suIU_2720166.jpg" referrerpolicy="no-referrer"></p><hr><p>據介紹，自 2022 年初，智譜 GLM 系列模型已支持在昇騰、神威超算、海光 DCU 架構上進行大規模預訓練和推理，當前已支持 10 餘種國產硬件生態，包括昇騰、神威超算、海光 DCU、海飛科、沐曦曦雲、算能科技、天數智芯、寒武紀、摩爾線程、百度崑崙芯、靈汐科技、長城超雲等。</p></div>
                                    ]]>
            </description>
            <pubDate>Sat, 28 Oct 2023 02:31:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263818</guid>
            <link>https://www.oschina.net/news/263818</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[網易雲課堂 Service Worker 運用與實踐]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p><img src="https://oscimg.oschina.net/oscnet/up-ac820c57f42ee1b582f9a6e7a1787375e7a.png" alt="" referrerpolicy="no-referrer"></p><h1>前言</h1><p>本文首先會簡單介紹下前端的常見緩存方式，再引入 Service Worker 的概念，針對其原理和如何運用進行介紹。然後基於 google 推出的第三方庫 Workbox，在產品中進行運用實踐，並對其原理進行簡要剖析。</p><blockquote><p>作者：劉放</p></blockquote><blockquote><p>編輯：Ein</p></blockquote><h1>前端緩存簡介</h1><p>先簡單介紹一下現有的前端緩存技術方案，主要分為 http 緩存和瀏覽器緩存。</p><h2>http 緩存</h2><p>http 緩存都是第二次請求時開始的，這也是個老生常談的話題了。無非也是那幾個 http 頭的問題：</p><h3>Expires</h3><p>HTTP1.0 的內容，服務器使用 Expires 頭來告訴 Web 客戶端它可以使用當前副本，直到指定的時間為止。</p><h3>Cache-Control</h3><p>HTTP1.1 引入了 Cathe-Control，它使用 max-age 指定資源被緩存多久，主要是解決了 Expires 一個重大的缺陷，就是它設置的是一個固定的時間點，客戶端時間和服務端時間可能有誤差。 所以一般會把兩個頭都帶上，這種緩存稱為強緩存，表現形式為： <img src="https://oscimg.oschina.net/oscnet/up-5d55a7877b12164c2b7f2fe4e870e072dc2.png" alt="" referrerpolicy="no-referrer"></p><h3>Last-Modified / If-Modified-Since</h3><p>Last-Modified 是服務器告訴瀏覽器該資源的最後修改時間，If-Modified-Since 是請求頭帶上的，上次服務器給自己的該資源的最後修改時間。然後服務器拿去對比。</p><p>若資源的最後修改時間大於 If-Modified-Since，説明資源又被改動過，則響應整片資源內容，返回狀態碼 200；</p><p>若資源的最後修改時間小於或等於 If-Modified-Since，説明資源無新修改，則響應 HTTP 304，告知瀏覽器繼續使用當前版本。</p><h3>Etag / If-None-Match</h3><p>前面提到由文件的修改時間來判斷文件是否改動，還是會帶來一定的誤差，比如註釋等無關緊要的修改等。所以推出了新的方式。</p><p>Etag 是由服務端特定算法生成的該文件的唯一標識，而請求頭把返回的 Etag 值通過 If-None-Match 再帶給服務端，服務端通過比對從而決定是否響應新內容。這也是 304 緩存。</p><h2>瀏覽器緩存</h2><h3>Storage</h3><p>簡單的緩存方式有 cookie，localStorage 和 sessionStorage。這裏就不詳細介紹他們的區別了，這裏説下通過 localStorage 來緩存靜態資源的優化方案。 localStorage 通常有 5MB 的存儲空間，我們以微信文章頁為例。 查看請求發現，基本沒有 js 和 css 的請求，因為它把全部的不需要改動的資源都放到了 localStorage 中： <img src="https://oscimg.oschina.net/oscnet/up-aa2899a96564193e2509884484b4f1eb12b.png" alt="" referrerpolicy="no-referrer"> 所以微信的文章頁加載非常的快。</p><h3>前端數據庫</h3><p>前端數據庫有 WebSql 和 IndexDB，其中 WebSql 被規範廢棄，他們都有大約 50MB 的最大容量，可以理解為 localStorage 的加強版。</p><h3>應用緩存</h3><p>應用緩存主要是通過 manifest 文件來註冊被緩存的靜態資源，已經被廢棄，因為他的設計有些不合理的地方，他在緩存靜態文件的同時，也會默認緩存 html 文件。這導致頁面的更新只能通過 manifest 文件中的版本號來決定。所以，應用緩存只適合那種常年不變化的靜態網站。如此的不方便，也是被廢棄的重要原因。</p><p>PWA 也運用了該文件，不同於 manifest 簡單的將文件通過是否緩存進行分類，PWA 用 manifest 構建了自己的 APP 骨架，並運用 Servie Worker 來控制緩存，這也是今天的主角。</p><h1>Service Worker</h1><p>Service Worker 本質上也是瀏覽器緩存資源用的，只不過他不僅僅是 Cache，也是通過 worker 的方式來進一步優化。 他基於 h5 的 web worker，所以絕對不會阻礙當前 js 線程的執行，sw 最重要的工作原理就是：</p><p>1、後台線程：獨立於當前網頁線程；</p><p>2、網絡代理：在網頁發起請求時代理，來緩存文件。</p><h2>兼容性</h2><p><img src="https://oscimg.oschina.net/oscnet/up-c50376e8514a0eda4c04fcf7bf1af3f24aa.png" alt="" referrerpolicy="no-referrer"> 可以看到，基本上新版瀏覽器還是兼容滴。之前是隻有 chrome 和 firefox 支持，現在微軟和蘋果也相繼支持了。</p><h2>成熟程度</h2><p>判斷一個技術是否值得嘗試，肯定要考慮下它的成熟程度，否則過一段時間又和應用緩存一樣被規範拋棄就尷尬了。 所以這裏我列舉了幾個使用 Service Worker 的頁面：</p><ul><li>淘寶</li><li>網易新聞</li><li>考拉</li></ul><p>所以説還是可以嘗試下的。</p><h2>調試方法</h2><p>一個網站是否啓用 Service Worker，可以通過開發者工具中的 Application 來查看：</p><p><img src="https://oscimg.oschina.net/oscnet/up-b631c480eabb3662b08968b60c0466ceefd.png" alt="" referrerpolicy="no-referrer"></p><p>被 Service Worker 緩存的文件，可以在 Network 中看到 Size 項為 from Service Worker：</p><p><img src="https://oscimg.oschina.net/oscnet/up-a94d21f7c7ca175656166c4224fae4ba3c9.png" alt="" referrerpolicy="no-referrer"></p><p>也可以在 Application 的 Cache Storage 中查看緩存的具體內容：</p><p><img src="https://oscimg.oschina.net/oscnet/up-7db236cf39ff32cf4e8f86a591a08431b07.png" alt="" referrerpolicy="no-referrer"></p><p>如果是具體的斷點調試，需要使用對應的線程，不再是 main 線程了，這也是 webworker 的通用調試方法：</p><p><img src="https://oscimg.oschina.net/oscnet/up-72a1c5ec7411b0e92530a737fe53db2b158.png" alt="" referrerpolicy="no-referrer"></p><h2>使用條件</h2><p>sw 是基於 HTTPS 的，因為 Service Worker 中涉及到請求攔截，所以必須使用 HTTPS 協議來保障安全。如果是本地調試的話，localhost 是可以的。 而我們剛好全站強制 https 化，所以正好可以使用。</p><h2>生命週期</h2><p>大概可以用如下圖片來解釋：</p><p><img src="https://oscimg.oschina.net/oscnet/up-385c15f6dc80d598f67579d5c0308bb98e5.png" alt="" referrerpolicy="no-referrer"></p><h3>註冊</h3><p>要使用 Service Worker，首先需要註冊一個 sw，通知瀏覽器為該頁面分配一塊內存，然後 sw 就會進入安裝階段。 一個簡單的註冊方式：</p><pre><code>(function() {
    if('serviceWorker' in navigator) {
        navigator.serviceWorker.register('./sw.js');
    }
})()
</code></pre><p>當然也可以考慮全面點，參考網易新聞的註冊方式：</p><pre><code>"serviceWorker" in navigator &amp;&amp; window.addEventListener("load",
    function() {
        var e = location.pathname.match(/\/news\/[a-z]{1,}\//)[0] + "article-sw.js?v=08494f887a520e6455fa";
        navigator.serviceWorker.register(e).then(function(n) {
            n.onupdatefound = function() {
                var e = n.installing;
                e.onstatechange = function() {
                    switch (e.state) {
                        case "installed":
                            navigator.serviceWorker.controller ? console.log("New or updated content is available.") : console.log("Content is now available offline!");
                            break;
                        case "redundant":
                            console.error("The installing service worker became redundant.")
                    }
                }
            }
        }).
        catch(function(e) {
            console.error("Error during service worker registration:", e)
        })
    })
</code></pre><p>前面提到過，由於 sw 會監聽和代理所有的請求，所以 sw 的作用域就顯得額外的重要了，比如説我們只想監聽我們專題頁的所有請求，就在註冊時指定路徑：</p><pre><code>navigator.serviceWorker.register('/topics/sw.js');
</code></pre><p>這樣就只會對 topics/下面的路徑進行優化。</p><h3>installing</h3><p>我們註冊後，瀏覽器就會開始安裝 sw，可以通過事件監聽：</p><pre><code>//service worker 安裝成功後開始緩存所需的資源
var CACHE_PREFIX = 'cms-sw-cache';
var CACHE_VERSION = '0.0.20';
var CACHE_NAME = CACHE_PREFIX+'-'+CACHE_VERSION;
var allAssets = [
    './main.css'
];
self.addEventListener('install', function(event) {

    //調試時跳過等待過程
    self.skipWaiting();


    // Perform install steps
    //首先 event.waitUntil 你可以理解為 new Promise，
    //它接受的實際參數只能是一個 promise，因為,caches 和 cache.addAll 返回的都是 Promise，
    //這裏就是一個串行的異步加載，當所有加載都成功時，那麼 SW 就可以下一步。
    //另外，event.waitUntil 還有另外一個重要好處，它可以用來延長一個事件作用的時間，
    //這裏特別針對於我們 SW 來説，比如我們使用 caches.open 是用來打開指定的緩存，但開啓的時候，
    //並不是一下就能調用成功，也有可能有一定延遲，由於系統會隨時睡眠 SW，所以，為了防止執行中斷，
    //就需要使用 event.waitUntil 進行捕獲。另外，event.waitUntil 會監聽所有的異步 promise
    //如果其中一個 promise 是 reject 狀態，那麼該次 event 是失敗的。這就導致，我們的 SW 開啓失敗。
    event.waitUntil(
        caches.open(CACHE_NAME)
            .then(function(cache) {
                console.log('[SW]: Opened cache');
                return cache.addAll(allAssets);
            })
    );

});
</code></pre><p>安裝時，sw 就開始緩存文件了，會檢查所有文件的緩存狀態，如果都已經緩存了，則安裝成功，進入下一階段。</p><h3>activated</h3><p>如果是第一次加載 sw，在安裝後，會直接進入 activated 階段，而如果 sw 進行更新，情況就會顯得複雜一些。流程如下：</p><p>首先老的 sw 為 A，新的 sw 版本為 B。 B 進入 install 階段，而 A 還處於工作狀態，所以 B 進入 waiting 階段。只有等到 A 被 terminated 後，B 才能正常替換 A 的工作。</p><p><img src="https://oscimg.oschina.net/oscnet/up-5d04b8ca78c8e2f2bc8f02e992c6a540426.png" alt="" referrerpolicy="no-referrer"></p><p>這個 terminated 的時機有如下幾種方式：</p><p>1、關閉瀏覽器一段時間；</p><p>2、手動清除 Service Worker；</p><p>3、在 sw 安裝時直接跳過 waiting 階段</p><pre><code>//service worker 安裝成功後開始緩存所需的資源
self.addEventListener('install', function(event) {
    //跳過等待過程
    self.skipWaiting();
});
</code></pre><p>然後就進入了 activated 階段，激活 sw 工作。</p><p>activated 階段可以做很多有意義的事情，比如更新存儲在 Cache 中的 key 和 value：</p><pre><code>var CACHE_PREFIX = 'cms-sw-cache';
var CACHE_VERSION = '0.0.20';
/**
 * 找出對應的其他 key 並進行刪除操作
 * @returns {*}
 */
function deleteOldCaches() {
    return caches.keys().then(function (keys) {
        var all = keys.map(function (key) {
            if (key.indexOf(CACHE_PREFIX) !== -1 &amp;&amp; key.indexOf(CACHE_VERSION) === -1){
                console.log('[SW]: Delete cache:' + key);
                return caches.delete(key);
            }
        });
        return Promise.all(all);
    });
}
//sw 激活階段,説明上一 sw 已失效
self.addEventListener('activate', function(event) {


    event.waitUntil(
        // 遍歷 caches 裏所有緩存的 keys 值
        caches.keys().then(deleteOldCaches)
    );
});
</code></pre><h3>idle</h3><p>這個空閒狀態一般是不可見的，這種一般説明 sw 的事情都處理完畢了，然後處於閒置狀態了。</p><p>瀏覽器會週期性的輪詢，去釋放處於 idle 的 sw 佔用的資源。</p><h3>fetch</h3><p>該階段是 sw 最為關鍵的一個階段，用於攔截代理所有指定的請求，並進行對應的操作。</p><p>所有的緩存部分，都是在該階段，這裏舉一個簡單的例子：</p><pre><code>//監聽瀏覽器的所有 fetch 請求，對已經緩存的資源使用本地緩存回覆
self.addEventListener('fetch', function(event) {
    event.respondWith(
        caches.match(event.request)
            .then(function(response) {
                //該 fetch 請求已經緩存
                if (response) {
                    return response;
                }
                return fetch(event.request);
                }
            )
    );
});
</code></pre><p>生命週期大概講清楚了，我們就以一個具體的例子來説明下原生的 serviceworker 是如何在生產環境中使用的吧。</p><h2>舉個栗子</h2><p>我們可以以網易新聞的 wap 頁為例,其針對不怎麼變化的靜態資源開啓了 sw 緩存，具體的 sw.js 邏輯和解讀如下：</p><pre><code>'use strict';
//需要緩存的資源列表
var precacheConfig = [
    ["https://static.ws.126.net/163/wap/f2e/milk_index/bg_img_sm_minfy.png",
        "c4f55f5a9784ed2093009dadf1e954f9"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/change.png",
        "9af1b102ef784b8ff08567ba25f31d95"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-download.png",
        "1c02c724381d77a1a19ca18925e9b30c"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-login-dark.png",
        "b59ba5abe97ff29855dfa4bd3a7a9f35"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-refresh.png",
        "a5b1084e41939885969a13f8dbc88abd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon-video-play.png",
        "065ff496d7d36345196d254aff027240"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/icon.ico",
        "a14e5365cc2b27ec57e1ab7866c6a228"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.eot",
        "e4d2788fef09eb0630d66cc7e6b1ab79"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.svg",
        "d9e57c341608fddd7c140570167bdabb"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.ttf",
        "f422407038a3180bb3ce941a4a52bfa2"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/iconfont_1.woff",
        "ead2bef59378b00425779c4ca558d9bd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/index.5cdf03e8.js",
        "6262ac947d12a7b0baf32be79e273083"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/index.bc729f8a.css",
        "58e54a2c735f72a24715af7dab757739"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-bohe.png",
        "ac5116d8f5fcb3e7c49e962c54ff9766"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-mail.png",
        "a12bbfaeee7fbf025d5ee85634fca1eb"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-manhua.png",
        "b8905b119cf19a43caa2d8a0120bdd06"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-open.png",
        "b7cc76ba7874b2132f407049d3e4e6e6"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-app-read.png",
        "e6e9c8bc72f857960822df13141cbbfd"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/logo-site.png",
        "2b0d728b46518870a7e2fe424e9c0085"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_no_pic.png",
        "aef80885188e9d763282735e53b25c0e"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_pc.png",
        "42f3cc914eab7be4258fac3a4889d41d"],
    ["https://static.ws.126.net/163/wap/f2e/milk_index/version_standard.png",
        "573408fa002e58c347041e9f41a5cd0d"]
];
var cacheName = 'sw-precache-v3-new-wap-index-' + (self.registration ? self.registration.scope : '');

var ignoreUrlParametersMatching = [/^utm_/];

var addDirectoryIndex = function(originalUrl, index) {
    var url = new URL(originalUrl);
    if (url.pathname.slice(-1) === '/') {
        url.pathname += index;
    }
    return url.toString();
};
var cleanResponse = function(originalResponse) {
    // If this is not a redirected response, then we don't have to do anything.
    if (!originalResponse.redirected) {
        return Promise.resolve(originalResponse);
    }
    // Firefox 50 and below doesn't support the Response.body stream, so we may
    // need to read the entire body to memory as a Blob.
    var bodyPromise = 'body' in originalResponse ?
        Promise.resolve(originalResponse.body) :
        originalResponse.blob();
    return bodyPromise.then(function(body) {
        // new Response() is happy when passed either a stream or a Blob.
        return new Response(body, {
            headers: originalResponse.headers,
            status: originalResponse.status,
            statusText: originalResponse.statusText
        });
    });
};
var createCacheKey = function(originalUrl, paramName, paramValue,
                              dontCacheBustUrlsMatching) {
    // Create a new URL object to avoid modifying originalUrl.
    var url = new URL(originalUrl);
    // If dontCacheBustUrlsMatching is not set, or if we don't have a match,
    // then add in the extra cache-busting URL parameter.
    if (!dontCacheBustUrlsMatching ||
        !(url.pathname.match(dontCacheBustUrlsMatching))) {
        url.search += (url.search ? '&amp;' : '') +
            encodeURIComponent(paramName) + '=' + encodeURIComponent(paramValue);
    }
    return url.toString();
};
var isPathWhitelisted = function(whitelist, absoluteUrlString) {
    // If the whitelist is empty, then consider all URLs to be whitelisted.
    if (whitelist.length === 0) {
        return true;
    }
    // Otherwise compare each path regex to the path of the URL passed in.
    var path = (new URL(absoluteUrlString)).pathname;
    return whitelist.some(function(whitelistedPathRegex) {
        return path.match(whitelistedPathRegex);
    });
};
var stripIgnoredUrlParameters = function(originalUrl,
                                         ignoreUrlParametersMatching) {
    var url = new URL(originalUrl);
    // Remove the hash; see https://github.com/GoogleChrome/sw-precache/issues/290
    url.hash = '';
    url.search = url.search.slice(1) // Exclude initial '?'
        .split('&amp;') // Split into an array of 'key=value' strings
        .map(function(kv) {
            return kv.split('='); // Split each 'key=value' string into a [key, value] array
        })
        .filter(function(kv) {
            return ignoreUrlParametersMatching.every(function(ignoredRegex) {
                return !ignoredRegex.test(kv[0]); // Return true iff the key doesn't match any of the regexes.
            });
        })
        .map(function(kv) {
            return kv.join('='); // Join each [key, value] array into a 'key=value' string
        })
        .join('&amp;'); // Join the array of 'key=value' strings into a string with '&amp;' in between each
    return url.toString();
};

var hashParamName = '_sw-precache';
//定義需要緩存的 url 列表
var urlsToCacheKeys = new Map(
    precacheConfig.map(function(item) {
        var relativeUrl = item[0];
        var hash = item[1];
        var absoluteUrl = new URL(relativeUrl, self.location);
        var cacheKey = createCacheKey(absoluteUrl, hashParamName, hash, false);
        return [absoluteUrl.toString(), cacheKey];
    })
);
//把 cache 中的 url 提取出來,進行去重操作
function setOfCachedUrls(cache) {
    return cache.keys().then(function(requests) {
        //提取 url
        return requests.map(function(request) {
            return request.url;
        });
    }).then(function(urls) {
        //去重
        return new Set(urls);
    });
}
//sw 安裝階段
self.addEventListener('install', function(event) {
    event.waitUntil(
        //首先嚐試取出存在客戶端 cache 中的數據
        caches.open(cacheName).then(function(cache) {
            return setOfCachedUrls(cache).then(function(cachedUrls) {
                return Promise.all(
                    Array.from(urlsToCacheKeys.values()).map(function(cacheKey) {
                        //如果需要緩存的 url 不在當前 cache 中,則添加到 cache
                        if (!cachedUrls.has(cacheKey)) {
                            //設置 same-origin 是為了兼容舊版本 safari 中其默認值不為 same-origin,
                            //只有當 URL 與響應腳本同源才發送 cookies、 HTTP Basic authentication 等驗證信息
                            var request = new Request(cacheKey, {credentials: 'same-origin'});
                            return fetch(request).then(function(response) {
                                //通過 fetch api 請求資源
                                if (!response.ok) {
                                    throw new Error('Request for ' + cacheKey + ' returned a ' +
                                        'response with status ' + response.status);
                                }
                                return cleanResponse(response).then(function(responseToCache) {
                                    //並設置到當前 cache 中
                                    return cache.put(cacheKey, responseToCache);
                                });
                            });
                        }
                    })
                );
            });
        }).then(function() {

            //強制跳過等待階段,進入激活階段
            return self.skipWaiting();

        })
    );
});
self.addEventListener('activate', function(event) {
    //清除 cache 中原來老的一批相同 key 的數據
    var setOfExpectedUrls = new Set(urlsToCacheKeys.values());
    event.waitUntil(
        caches.open(cacheName).then(function(cache) {
            return cache.keys().then(function(existingRequests) {
                return Promise.all(
                    existingRequests.map(function(existingRequest) {
                        if (!setOfExpectedUrls.has(existingRequest.url)) {
                            //cache 中刪除指定對象
                            return cache.delete(existingRequest);
                        }
                    })
                );
            });
        }).then(function() {
            //self 相當於 webworker 線程的當前作用域
            //當一個 service worker 被初始註冊時，頁面在下次加載之前不會使用它。 claim() 方法會立即控制這些頁面
            //從而更新客戶端上的 serviceworker
            return self.clients.claim();

        })
    );
});

self.addEventListener('fetch', function(event) {
    if (event.request.method === 'GET') {
        // 標識位,用來判斷是否需要緩存
        var shouldRespond;
        // 對 url 進行一些處理,移除一些不必要的參數
        var url = stripIgnoredUrlParameters(event.request.url, ignoreUrlParametersMatching);
        // 如果該 url 不是我們想要緩存的 url,置為 false
        shouldRespond = urlsToCacheKeys.has(url);
        // 如果 shouldRespond 未 false,再次驗證
        var directoryIndex = 'index.html';
        if (!shouldRespond &amp;&amp; directoryIndex) {
            url = addDirectoryIndex(url, directoryIndex);
            shouldRespond = urlsToCacheKeys.has(url);
        }
        // 再次驗證,判斷其是否是一個 navigation 類型的請求
        var navigateFallback = '';
        if (!shouldRespond &amp;&amp;
            navigateFallback &amp;&amp;
            (event.request.mode === 'navigate') &amp;&amp;
            isPathWhitelisted([], event.request.url)) {
            url = new URL(navigateFallback, self.location).toString();
            shouldRespond = urlsToCacheKeys.has(url);
        }
        // 如果標識位為 true
        if (shouldRespond) {
            event.respondWith(
                caches.open(cacheName).then(function(cache) {
                    //去緩存 cache 中找對應的 url 的值
                    return cache.match(urlsToCacheKeys.get(url)).then(function(response) {
                        //如果找到了,就返回 value
                        if (response) {
                            return response;
                        }
                        throw Error('The cached response that was expected is missing.');
                    });
                }).catch(function(e) {
                    // 如果沒找到則請求該資源
                    console.warn('Couldn\'t serve response for "%s" from cache: %O', event.request.url, e);
                    return fetch(event.request);
                })
            );
        }
    }
});
</code></pre><p>這裏的策略大概就是優先在 Cache 中尋找資源，如果找不到再請求資源。可以看出，為了實現一個較為簡單的緩存，還是比較複雜和繁瑣的，所以很多工具就應運而生了。</p><h1>Workbox</h1><p>由於直接寫原生的 sw.js，比較繁瑣和複雜，所以一些工具就出現了，而 Workbox 是其中的佼佼者，由 google 團隊推出。</p><h2>簡介</h2><p>在 Workbox 之前，GoogleChrome 團隊較早時間推出過 sw-precache 和 sw-toolbox 庫，但是在 GoogleChrome 工程師們看來，workbox 才是真正能方便統一的處理離線能力的更完美的方案，所以停止了對 sw-precache 和 sw-toolbox 的維護。</p><h2>使用者</h2><p>有很多團隊也是啓用該工具來實現 serviceworker 的緩存，比如説：</p><ul><li>淘寶首頁</li><li>網易新聞 wap 文章頁</li><li>百度的 Lavas</li></ul><h2>基本配置</h2><p>首先，需要在項目的 sw.js 文件中，引入 Workbox 的官方 js，這裏用了我們自己的靜態資源：</p><pre><code>importScripts(
    "https://edu-cms.nosdn.127.net/topics/js/workbox_9cc4c3d662a4266fe6691d0d5d83f4dc.js"
);
</code></pre><p>其中 importScripts 是 webworker 中加載 js 的方式。</p><p>引入 Workbox 後，全局會掛載一個 Workbox 對象</p><pre><code>if (workbox) {
    console.log('workbox 加載成功');
} else {
    console.log('workbox 加載失敗');
}
</code></pre><p>然後需要在使用其他的 api 前，提前使用配置</p><pre><code>//關閉控制枱中的輸出
workbox.setConfig({ debug: false });
</code></pre><p>也可以統一指定存儲時 Cache 的名稱：</p><pre><code>//設置緩存 cachestorage 的名稱
workbox.core.setCacheNameDetails({
    prefix:'edu-cms',
    suffix:'v1'
});
</code></pre><h2>precache</h2><p>Workbox 的緩存分為兩種，一種的 precache，一種的 runtimecache。</p><p>precache 對應的是在 installing 階段進行讀取緩存的操作。它讓開發人員可以確定緩存文件的時間和長度，以及在不進入網絡的情況下將其提供給瀏覽器，這意味着它可以用於創建 Web 離線工作的應用。</p><h3>工作原理</h3><p>首次加載 Web 應用程序時，Workbox 會下載指定的資源，並存儲具體內容和相關修訂的信息在 indexedDB 中。</p><p>當資源內容和 sw.js 更新後，Workbox 會去比對資源，然後將新的資源存入 Cache，並修改 indexedDB 中的版本信息。</p><p>我們舉一個例子：</p><pre><code>workbox.precaching.precacheAndRoute([
    './main.css'
]);
</code></pre><p><img src="https://oscimg.oschina.net/oscnet/up-d8a79ba50b83bfbc538b960e07f0c707b17.png" alt="" referrerpolicy="no-referrer"></p><p>indexedDB 中會保存其相關信息</p><p><img src="https://oscimg.oschina.net/oscnet/up-b9f1c514f24a2b5121771fa9b59fb0cc82b.png" alt="" referrerpolicy="no-referrer"></p><p>這個時候我們把 main.css 的內容改變後，再刷新頁面，會發現除非強制刷新，否則 Workbox 還是會讀取 Cache 中存在的老的 main.css 內容。</p><p>即使我們把 main.css 從服務器上刪除，也不會對頁面造成影響。</p><p>所以這種方式的緩存都需要配置一個版本號。在修改 sw.js 時，對應的版本也需要變更。</p><h3>使用實踐</h3><p>當然了，一般我們的一些不經常變的資源，都會使用 cdn，所以這裏自然就需要支持域外資源了，配置方式如下：</p><pre><code>var fileList = [
    {
        url:'https://edu-cms.nosdn.127.net/topics/js/cms_specialWebCommon_js_f26c710bd7cd055a64b67456192ed32a.js'
    },
    {
        url:'https://static.ws.126.net/163/frontend/share/css/article.207ac19ad70fd0e54d4a.css'
    }
];


//precache 適用於支持跨域的 cdn 和域內靜態資源
workbox.precaching.suppressWarnings();
workbox.precaching.precacheAndRoute(fileList, {
    "ignoreUrlParametersMatching": [/./]
});
</code></pre><p>這裏需要對應的資源配置跨域允許頭，否則是不能正常加載的。且文件都要以版本文件名的方式，來確保修改後 Cache 和 indexDB 會得到更新。</p><p>理解了原理和實踐後，説明這種方式適合於上線後就不會經常變動的靜態資源。</p><h2>runtimecache</h2><p>運行時緩存是在 install 之後，activated 和 fetch 階段做的事情。</p><p>既然在 fetch 階段發送，那麼 runtimecache 往往應對着各種類型的資源，對於不同類型的資源往往也有不同的緩存策略。</p><h3>緩存策略</h3><p>Workbox 提供的緩存策劃有以下幾種，通過不同的配置可以針對自己的業務達到不同的效果：</p><h3>Stale While Revalidate</h3><p>這種策略的意思是當請求的路由有對應的 Cache 緩存結果就直接返回，</p><p>在返回 Cache 緩存結果的同時會在後台發起網絡請求拿到請求結果並更新 Cache 緩存，如果本來就沒有 Cache 緩存的話，直接就發起網絡請求並返回結果，這對用戶來説是一種非常安全的策略，能保證用戶最快速的拿到請求的結果。</p><p>但是也有一定的缺點，就是還是會有網絡請求佔用了用戶的網絡帶寬。可以像如下的方式使用 State While Revalidate 策略：</p><pre><code>workbox.routing.registerRoute(
    new RegExp('https://edu-cms\.nosdn\.127\.net/topics/'),
    workbox.strategies.staleWhileRevalidate({
        //cache 名稱
        cacheName: 'lf-sw:static',
        plugins: [
            new workbox.expiration.Plugin({
                //cache 最大數量
                maxEntries: 30
            })
        ]
    })
);
</code></pre><h3>Network First</h3><p>這種策略就是當請求路由是被匹配的，就採用網絡優先的策略，也就是優先嚐試拿到網絡請求的返回結果，如果拿到網絡請求的結果，就將結果返回給客戶端並且寫入 Cache 緩存。</p><p>如果網絡請求失敗，那最後被緩存的 Cache 緩存結果就會被返回到客戶端，這種策略一般適用於返回結果不太固定或對實時性有要求的請求，為網絡請求失敗進行兜底。可以像如下方式使用 Network First 策略：</p><pre><code>//自定義要緩存的 html 列表
var cacheList = [
    '/Hexo/public/demo/PWADemo/workbox/index.html'
];
workbox.routing.registerRoute(
    //自定義過濾方法
    function(event) {
        // 需要緩存的 HTML 路徑列表
        if (event.url.host === 'localhost:63342') {
            if (~cacheList.indexOf(event.url.pathname)) return true;
            else return false;
        } else {
            return false;
        }
    },
    workbox.strategies.networkFirst({
        cacheName: 'lf-sw:html',
        plugins: [
            new workbox.expiration.Plugin({
                maxEntries: 10
            })
        ]
    })
);
</code></pre><h3>Cache First</h3><p>這個策略的意思就是當匹配到請求之後直接從 Cache 緩存中取得結果，如果 Cache 緩存中沒有結果，那就會發起網絡請求，拿到網絡請求結果並將結果更新至 Cache 緩存，並將結果返回給客戶端。這種策略比較適合結果不怎麼變動且對實時性要求不高的請求。可以像如下方式使用 Cache First 策略：</p><pre><code>workbox.routing.registerRoute(
    new RegExp('https://edu-image\.nosdn\.127\.net/'),
    workbox.strategies.cacheFirst({
        cacheName: 'lf-sw:img',
        plugins: [
            //如果要拿到域外的資源，必須配置
            //因為跨域使用 fetch 配置了
            //mode: 'no-cors',所以 status 返回值為 0，故而需要兼容
            new workbox.cacheableResponse.Plugin({
                statuses: [0, 200]
            }),
            new workbox.expiration.Plugin({
                maxEntries: 40,
                //緩存的時間
                maxAgeSeconds: 12 * 60 * 60
            })
        ]
    })
);
</code></pre><h3>Network Only</h3><p>比較直接的策略，直接強制使用正常的網絡請求，並將結果返回給客戶端，這種策略比較適合對實時性要求非常高的請求。</p><h3>Cache Only</h3><p>這個策略也比較直接，直接使用 Cache 緩存的結果，並將結果返回給客戶端，這種策略比較適合一上線就不會變的靜態資源請求。</p><h2>舉個栗子</h2><p>又到了舉個栗子的階段了，這次我們用淘寶好了，看看他們是如何通過 Workbox 來配置 Service Worker 的：</p><pre><code>//首先是異常處理
self.addEventListener('error', function(e) {
  self.clients.matchAll()
    .then(function (clients) {
      if (clients &amp;&amp; clients.length) {
        clients[0].postMessage({ 
          type: 'ERROR',
          msg: e.message || null,
          stack: e.error ? e.error.stack : null
        });
      }
    });
});

self.addEventListener('unhandledrejection', function(e) {
  self.clients.matchAll()
    .then(function (clients) {
      if (clients &amp;&amp; clients.length) {
        clients[0].postMessage({
          type: 'REJECTION',
          msg: e.reason ? e.reason.message : null,
          stack: e.reason ? e.reason.stack : null
        });
      }
    });
})
//然後引入 workbox
importScripts('https://g.alicdn.com/kg/workbox/3.3.0/workbox-sw.js');
workbox.setConfig({
  debug: false,
  modulePathPrefix: 'https://g.alicdn.com/kg/workbox/3.3.0/'
});
//直接激活跳過等待階段
workbox.skipWaiting();
workbox.clientsClaim();
//定義要緩存的 html
var cacheList = [
  '/',
  '/tbhome/home-2017',
  '/tbhome/page/market-list'
];
//html 採用 networkFirst 策略，支持離線也能大體訪問
workbox.routing.registerRoute(
  function(event) {
    // 需要緩存的 HTML 路徑列表
    if (event.url.host === 'www.taobao.com') {
      if (~cacheList.indexOf(event.url.pathname)) return true;
      else return false;
    } else {
      return false;
    }
  },
  workbox.strategies.networkFirst({
    cacheName: 'tbh:html',
    plugins: [
      new workbox.expiration.Plugin({
        maxEntries: 10
      })
    ]
  })
);
//靜態資源採用 staleWhileRevalidate 策略，安全可靠
workbox.routing.registerRoute(
  new RegExp('https://g\.alicdn\.com/'),
  workbox.strategies.staleWhileRevalidate({
    cacheName: 'tbh:static',
    plugins: [
      new workbox.expiration.Plugin({
        maxEntries: 20
      })
    ]
  })
);
//圖片採用 cacheFirst 策略，提升速度
workbox.routing.registerRoute(
  new RegExp('https://img\.alicdn\.com/'),
  workbox.strategies.cacheFirst({
    cacheName: 'tbh:img',
    plugins: [
      new workbox.cacheableResponse.Plugin({
        statuses: [0, 200]
      }),
      new workbox.expiration.Plugin({
        maxEntries: 20,
        maxAgeSeconds: 12 * 60 * 60
      })
    ]
  })
);

workbox.routing.registerRoute(
  new RegExp('https://gtms01\.alicdn\.com/'),
  workbox.strategies.cacheFirst({
    cacheName: 'tbh:img',
    plugins: [
      new workbox.cacheableResponse.Plugin({
        statuses: [0, 200]
      }),
      new workbox.expiration.Plugin({
        maxEntries: 30,
        maxAgeSeconds: 12 * 60 * 60
      })
    ]
  })
);
</code></pre><p>可以看出，使用 Workbox 比起直接手擼來，要快很多，也明確很多。</p><h2>原理</h2><p>目前分析 Service Worker 和 Workbox 的文章不少，但是介紹 Workbox 原理的文章卻不多。這裏簡單介紹下 Workbox 這個工具庫的原理。</p><p>首先將幾個我們產品用到的模塊圖奉上：</p><p><img src="https://oscimg.oschina.net/oscnet/up-b22e014db049eea325d28de53c9b6b9cd76.png" alt="" referrerpolicy="no-referrer"></p><p>簡單提幾個 Workbox 源碼的亮點。</p><h3>通過 Proxy 按需依賴</h3><p>熟悉了 Workbox 後會得知，它是有很多個子模塊的，各個子模塊再通過用到的時候按需 importScript 到線程中。 <img src="https://oscimg.oschina.net/oscnet/up-12e98258edc4bd13e5ad48a74c9835ede68.png" alt="" referrerpolicy="no-referrer"></p><p>做到按需依賴的原理就是通過 Proxy 對全局對象 Workbox 進行代理：</p><pre><code>new Proxy(this, {
  get(t, s) {
    //如果 workbox 對象上不存在指定對象，就依賴注入該對象對應的腳本
    if (t[s]) return t[s];
    const o = e[s];
    return o &amp;&amp; t.loadModule(`workbox-${o}`), t[s];
  }
})
</code></pre><p>如果找不到對應模塊，則通過 importScripts 主動加載：</p><pre><code>/**
 * 加載前端模塊
 * @param {Strnig} t 
 */
loadModule(t) {
  const e = this.o(t);
  try {
    importScripts(e), (this.s = !0);
  } catch (s) {
    throw (console.error(`Unable to import module '${t}' from '${e}'.`), s);
  }
}
</code></pre><h3>通過 freeze 凍結對外暴露 api</h3><p>Workbox.core 模塊中提供了幾個核心操作模塊，如封裝了 indexedDB 操作的 DBWrapper、對 Cache Storage 進行讀取的 Cache Wrapper，以及發送請求的 fetchWrapper 和日誌管理的 logger 等等。</p><p>為了防止外部對內部模塊暴露出去的 api 進行修改，導致出現不可預估的錯誤，內部模塊可以通過 Object.freeze 將 api 進行凍結保護：</p><pre><code>var _private = /*#__PURE__*/Object.freeze({
    DBWrapper: DBWrapper,
    WorkboxError: WorkboxError,
    assert: finalAssertExports,
    cacheNames: cacheNames,
    cacheWrapper: cacheWrapper,
    fetchWrapper: fetchWrapper,
    getFriendlyURL: getFriendlyURL,
    logger: defaultExport
  });
</code></pre><h1>總結</h1><p>通過對 Service Worker 的理解和 Workbox 的應用，可以進一步提升產品的性能和弱網情況下的體驗。有興趣的同學也可以對 Workbox 的源碼細細評讀，其中還有很多不錯的設計模式和編程風格值得學習。</p><p><img src="https://oscimg.oschina.net/oscnet/up-a546d4d52ff6cdf625f4d4a4890fd454bec.png" alt="" referrerpolicy="no-referrer"></p><p><strong>-END-</strong></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 10:45:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/youdaotech/blog/5054309</guid>
            <link>https://my.oschina.net/youdaotech/blog/5054309</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Wasmer 開源 WinterJS：Rust 編寫的 Service Worker]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Wasmer 團隊開源了一款用 Rust 編寫的<strong> JavaScript Service Worker：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwasmer.io%2Fposts%2Fannouncing-winterjs-service-workers" target="_blank">WinterJS</a></u></strong>。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-6382fe02fb5cbb80e1cb6951156b73e1143.png" referrerpolicy="no-referrer"></p><p><em>WinterJS 開源地址：<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fwasmerio%2Fwinterjs" target="_blank">https://github.com/wasmerio/winterjs</a></u></em></p><p>據介紹，WinterJS 使用 SpiderMonkey 運行時執行 JavaScript（與 Firefox 使用的運行時相同），並遵循 WinterCG 規範，目的是最大限度地兼容 Cloudflare Workers、Deno Deploy 和 Vercel 等其他服務（因此命名為 WinterJS）。</p><p>WinterJS 除了速度極快，還能通過 WASIX <strong>編譯成 WebAssembly</strong>，因此完全支持在 Wasmer 上運行。</p><ul><li><strong>使用示例</strong></li></ul><p><strong>創建<code>serviceworker.js</code>文件，並返回 "hello world"</strong></p><pre><code class="language-javascript">$ wasmer run wasmer/winterjs --net --mapdir /app:. /app/serviceworker.js</code></pre><pre><code class="language-javascript">addEventListener('fetch', (req) =&gt; {
  req.respondWith(`hello world from ${req.request.url.href}`);
});</code></pre><blockquote><p>Wasmer 是支持 WASI 和 Emscripten 的通用 WebAssembly 運行時，提供基於 WebAssembly 的超輕量級容器，專注於支持在任何平台上運行 WASM 代碼：從桌面端到雲端、以及 IoT 設備，並且能嵌入在任何編程語言中。</p><p><img alt="" src="https://static.oschina.net/uploads/space/2023/0627/173716_02s8_2720166.png" referrerpolicy="no-referrer"></p><p>Wasmer 憑藉其多樣化的支持和專注於從通用桌面應用程序到 「便攜式 ML/AI 應用程序」 的領域，目前仍然是領先的 WASM 運行時之一。</p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 10:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263968/winterjs-service-workers</guid>
            <link>https://www.oschina.net/news/263968/winterjs-service-workers</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[扎克伯克：Meta 明年投入更多工程和計算資源到 AI 領域]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>當地時間 10 月 25 日，在 2023 財年第三季度財報電話會上，Meta CEO 扎克伯格強調，相信生成式 AI 的相關技術將讓人們使用各種應用程序的方式變得更有意義。在未來，Meta 甚至有可能會利用 AI 來根據用戶的興趣為他們直接生成內容。</p><p>扎克伯格表示，AI 將幫助使用 Meta 各大應用的創作者提升內容質量和生產效率，而隨着時間的推移，AI 參與生成的內容在用戶消費內容中的佔比將會越來越大。</p><p>對於公司的後續發展，扎克伯格表示在 2024 年，<strong>就工程和計算資源而言，AI 將成為 Meta 最大的投資領域</strong>。此外，扎克伯格補充道，為了避免引入大量的新員工，<strong>公司將降低一些非 AI 項目的優先級，並將相關人員轉向從事 AI 工作</strong>。</p><p>上月曾報道過，<u><a href="https://www.oschina.net/news/257670/meta-building-llm-rival-openais-gpt4">Meta 正在構建</a></u>新開源大模型，據稱性能超越 Llama 2、比肩 GPT-4，最終目標是加速開發下一代生成式人工智能模型，使其能夠生成更多類似人類的表達。</p><p><img alt="" src="https://static.oschina.net/uploads/space/2023/0911/152426_g2gp_2720166.png" referrerpolicy="no-referrer"></p><p>長期以來，Meta 一直在採用開源方法公開其大模型產品，是業內眾所周知的最大貢獻者之一。僅今年它就向人工智能社區發佈了大量人工智能模型和訓練數據集。其中包括針對編程任務優化的 Code Llama 大語言模型； 可實現數百種語言通用按需翻譯的 SeamlessM4T 模型； 用於創作音樂和聲音的生成式人工智能模型 AudioCraft；語音生成人工智能模型 Voicebox。它還推出了 I-JEPA（一種可以像人類一樣學習的計算機視覺模型）和 FACET（一種基準數據集，旨在幫助研究人員審核計算機視覺模型的偏差）。</p><hr><p>延伸閲讀</p><ul><li><a href="https://www.oschina.net/news/256830/meta-ai-belebele">Meta AI 多語言閲讀理解數據集 Belebele，涵蓋 122 種語言變體</a></li><li><a href="https://www.oschina.net/news/255350/meta-code-llama">Meta 開源基於 Llama 2 的 AI 代碼生成大模型：Code Llama</a></li><li><a href="https://www.oschina.net/news/255168/meta-seamless-m4t">Meta 推出&nbsp;SeamlessM4T，可轉錄和翻譯近 100 種語言</a></li><li><a href="https://www.oschina.net/news/252174/audiocraft-generative-ai-for-music-and-audio">Meta 發佈開源 AI 工具 AudioCraft，文本自動生成音樂</a></li><li><a href="https://www.oschina.net/news/249944/meta-llama-2">Meta 放大招：發佈開源大語言模型 Llama 2，可免費商用</a></li><li><a href="https://www.oschina.net/news/245895/meta-voicebox-generative-ai-model-speech">Meta 發佈語音生成 AI 模型：Voicebox</a></li><li><a href="https://www.oschina.net/news/245705/meta-musicgen">Meta 開源音樂生成模型 MusicGen</a></li><li><a href="https://www.oschina.net/news/242331/mate-multilingual-model-speech">Meta 開源大模型：支持 4000+ 語言識別，1100+ 種語音文本轉換</a></li></ul></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 09:35:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263757</guid>
            <link>https://www.oschina.net/news/263757</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Ubuntu 24.04 進入開發階段，代號 Noble Numbat]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p><span style="color:#000000">Canonical 的 Utkarsh Gupta 在一封發送給 Ubuntu 開發郵件列表的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Flists.ubuntu.com%2Farchives%2Fubuntu-devel%2F2023-October%2F042835.html" target="_blank">電子郵件中宣佈</a>，Ubuntu 24.04 現已開放供開發，並透露了該版本的代號為「Noble Numbat」。</span></p><blockquote><p><span style="color:#000000">我們很高興地宣佈，Noble Numbat 現已開放開發。自動同步已啓用，並將很快運行。和往常一樣，我們預計在初始階段會有大量的構建和自動測試湧入，這將導致一些延遲現象的出現。請協助修復出現的任何故障。</span></p></blockquote><p><span style="color:#000000">根據百度百科，Numbat（袋食蟻獸）是分佈於澳大利亞西南部的一種小型有袋動物，幾乎只以白蟻為食，每天可以吃約 20000 只白蟻。目前僅在少數地區存活，屬於瀕危物種，已被列入《世界自然保護聯盟瀕危物種紅色名錄》。袋食蟻獸的體型小而吻長，牙齒多達 52 枚，超過任何陸生哺乳動物的齒數，齒細，排成長列，長而能伸的舌（長約 10 釐米），用以捕捉白蟻。</span></p><p><span style="color:#000000"><img alt="" height="286" src="https://oscimg.oschina.net/oscnet/up-050f5f99cc77c3757686112b409fb2558f7.jpg" width="500" referrerpolicy="no-referrer"></span></p><p><span style="color:#000000">Ubuntu 24.04 將是 Ubuntu 自 2006 年以來的第 10 個 LTS 版本。Ubuntu 的 LTS 版本將獲得 5 年的安全更新、錯誤修復和精選應用程序更新。Ubuntu Pro 則會在此基礎上額外增加 5 年的安全保障，為現代的 LTS 版本提供了長達十年的支持。</span></p><p><span style="color:#000000">目前對於 Ubuntu 24.04 中將包含的新功能和改進仍然知之甚少。但 <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.omgubuntu.co.uk%2F2023%2F10%2Fubuntu-24-04-development-open" target="_blank">OMG! Ubuntu</a> 指出，對於長期支持版本而言，Ubuntu 在主要新功能、用戶界面的巨大變化等方面往往會比較保守，主要會更加專注于堅實、穩定的體驗。</span></p><p><span style="color:#000000">可以確定的是，24.04 肯定會配備新的 Linux 內核（6.7 或 6.8，視時間而定）、GNOME 46（預計將在三月份發佈）。Canonical 的 Oliver Grawert 還透露，一個不可變的、snap-based Ubuntu 24.04 鏡像將於 4 月份提供下載（但不會是默認推薦下載）。</span></p><p><span style="color:#000000">Ubuntu 24.04 計劃於 2024 年 4 月 25 日正式發佈。其功能凍結階段定於 2024 年 2 月 29 日，beta 版本計劃於 2024 年 4 月 4 日發佈。</span></p><p><img height="501" src="https://oscimg.oschina.net/oscnet/up-78523d04d02ccf21bda56aa13e0dcd0b317.png" width="300" referrerpolicy="no-referrer"></p><p><span style="color:#000000">可在此查看具體的<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdiscourse.ubuntu.com%2Ft%2Fnoble-numbat-release-schedule%2F35649" target="_blank">發佈時間表</a>。</span></p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 09:29:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263754/ubuntu-24-04-noble-numbat</guid>
            <link>https://www.oschina.net/news/263754/ubuntu-24-04-noble-numbat</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[Next.js 14 發佈：Server Actions 已穩定、部分預渲染進入預覽]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>Vercel 公司在 Next.js Conf 2023 上宣佈了&nbsp;<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14" target="_blank">Next.js 14</a></u>。</p><blockquote><p>Vercel 是流行的開源前端框架 Next.js 背後的公司，Next.js 提供了包括服務器端渲染和為 Web 應用程序生成靜態網站在內的功能。Vercel 作為一個開放的雲平台提供了網站託管服務，讓開發者能夠在上面開發、預覽和發佈 Web 應用，同時優化了前端開發者的開發和部署體驗。</p></blockquote><p><img alt="" height="338" src="https://oscimg.oschina.net/oscnet/up-facb05348dbe78400c4b01b68c0fceaa5d3.png" width="600" referrerpolicy="no-referrer"></p><p><strong>Next.js 14 主要變化：</strong></p><ul><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23nextjs-compiler-turbocharged" target="_blank"><strong>Turbopack</strong></a>: App &amp; 頁面路由通過了 5000 項測試 
  <ul><li>本地服務器啓動速度提升&nbsp;<strong>53%</strong></li><li>使用 Fast Refresh 進行代碼更新的速度提升&nbsp;<strong>94%</strong></li></ul></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23forms-and-mutations" target="_blank"><strong>Server Actions (Stable)</strong></a>: 漸進式的增強突變 
  <ul><li>重新驗證緩存數據</li><li>支持簡單的函數調用</li><li>本地支持表單</li></ul></li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23partial-prerendering-preview" target="_blank"><strong>Partial Prerendering (Preview)</strong></a>: 快速初始化靜態響應 + 流式動態內容</li><li><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14%23nextjs-learn-course" target="_blank"><strong>Next.js Learn (New)</strong></a>: 針對 App Router、身份驗證、數據庫等內容的全新免費課程</li></ul><hr><ul><li><strong>Turbopack 通過 5000 項集成測試</strong></li></ul><p>底層採用 Rust 編寫的構建引擎 Turbopack 已通過<code>next dev</code>&nbsp;的 5,000 項集成測試，這些測試包括 7 年的錯誤修復。</p><p>Vercel 稱開發者現在應該使用<code>next dev -turbo</code>會得到更快、更可靠的性能。該公司還表示，一旦 Turbopack 所有測試都通過，它將進入穩定狀態（目前通過了 90% 的測試）。</p><ul><li><strong>Server Actions</strong></li></ul><p>在 Next.js 14 中，Next.js 團隊通過穩定版本的 Server Actions 改進了開發者在編寫數據變更方面的體驗。</p><p>Server Actions 允許開發者定義異步服務器函數，使用 Server Actions 來重新驗證緩存數據、重定向到不同的路由、設置和讀取 cookie 等等。</p><p>現在，只需在 React 組件中定義一個函數，就能在服務器上安全地執行操作。</p><p>下面是一個簡易示例：</p><pre><code>export default function Page() {
  async function create(formData: FormData) {
    'use server';
    const id = await createItem(formData);
  }
 
  return (
    &lt;form action={create}&gt;
      &lt;input type="text" name="name" /&gt;
      &lt;button type="submit"&gt;Submit&lt;/button&gt;
    &lt;/form&gt;
  );
}</code></pre><p>這不僅減少代碼量，還減少了更改數據和重新渲染頁面所需的網絡往返次數，從而提升用戶體驗。</p><ul><li><strong>部分預渲染 (Partial Prerendering)</strong></li></ul><p>Next.js 團隊正在為 Next.js 開發的」部分預渲染「是一種針對具有快速初始靜態響應的動態內容的編譯器優化。</p><p>Partial Prerendering 基於十年來對服務器端渲染 (SSR)、靜態網站生成 (SSG) 和增量靜態重驗證 (ISR) 的研究和開發。</p><p>詳情查看<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fnextjs.org%2Fblog%2Fnext-14" target="_blank"><u>發佈公告</u></a>。</p></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 08:48:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263948/next-js-14</guid>
            <link>https://www.oschina.net/news/263948/next-js-14</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[libnop - C++ 本機對象協議]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="project_detail_above_text_link_1" data-tracepid="project_detail_above_text_link"><a style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代 <img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>libnop 是一個僅用於序列化和反序列化 C++數據類型的頭庫，無需外部代碼生成器或運行時支持庫。唯一的強制性要求是一個支持 C++14 標準的編譯器。</p><hr><p style="color:#1f2328; text-align:start"><strong>libnop 有以下目標：</strong></p><ul><li>使簡單的序列化任務變得容易，使複雜的任務變得易於處理。</li><li>在 C++語言中移除對代碼生成器和模式文件描述數據類型、格式和協議的依賴。</li><li>避免運行序列化操作時可能需要的額外運行時間。</li><li>提供現代功能，如雙向二進制兼容性、數據驗證、類型安全性和類型可替代性。</li><li>以最少的工作量處理內部類型、常見的 STL 類型和容器以及用戶定義的類型。</li><li>生成易於分析的代碼。</li><li>避免動態內存的分配時使用。</li></ul></div>
                                                                ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 07:36:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/p/libnop</guid>
            <link>https://www.oschina.net/p/libnop</link>
        </item>
        <item>
            <title>
                <![CDATA[Next.js 支持在前端代碼中寫 SQL，開倒車還是遙遙領先？]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>下面這張圖來自近日舉辦的&nbsp;Next.js Conf 2023，裏面的代碼使用了名為<strong>「Server Actions」</strong>的特性——在前端代碼中使用 SQL 語句直接操作數據庫。</p><blockquote><p>Next.js 是流行的開源前端框架，其開發商是知名創業公司 Vercel。</p><p>Next.js 提供了包括服務器端渲染和為 Web 應用程序生成靜態網站在內的功能。Vercel 作為一個開放的雲平台提供了網站託管服務，讓開發者能夠在上面開發、預覽和發佈 Web 應用，同時優化了前端開發者的開發和部署體驗。</p></blockquote><p><img alt="" height="533" src="https://oscimg.oschina.net/oscnet/up-e254f1c847ae20e8c530b34f9021da3a4d0.png" width="400" referrerpolicy="no-referrer"></p><p>在最新發布的 Next.js 14 中，Server Actions 已到達穩定階段。其團隊表示，Server Actions 改進了開發者在編寫數據變更方面的體驗。</p><blockquote><p><em><u><a href="https://www.oschina.net/news/263948/next-js-14" target="_blank">Next.js 14 發佈：Server Actions 已穩定、部分預渲染進入預覽</a></u></em></p></blockquote><p>Server Actions 允許開發者定義異步服務器函數，他們可以使用 Server Actions 重新驗證緩存數據、重定向到不同的路由、設置和讀取 cookie 等等。</p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-77eb78e36979830a06c1f44ed2476bb4db1.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-0ad921306554c0716b4b4b0a8bedb71b8ba.png" referrerpolicy="no-referrer"></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-badb8152ef443eb8272d03c220e0450c723.png" referrerpolicy="no-referrer"></p><p>不過目前看來，大多數人對它的評價似乎並不太好 ——</p><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1029/122449_wEe4_2720166.png" referrerpolicy="no-referrer"></p></blockquote></div>
                                    ]]>
            </description>
            <pubDate>Fri, 27 Oct 2023 04:54:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263921/nextjs-server-actions</guid>
            <link>https://www.oschina.net/news/263921/nextjs-server-actions</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[國外物價高，6 美元只能買 50 個 GitHub stars]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>《Wired》雜誌發表文章<em>"<u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fwww.wired.com%2Fstory%2Fgithub-stars-black-market-coders-cheat%2F" target="_blank">The GitHub Black Market That Helps Coders Cheat the Popularity Contest</a></u>"</em>，介紹了交易 GitHub Stars 的地下黑市。</p><blockquote><p><img src="https://static.oschina.net/uploads/space/2023/1028/161939_TSqR_2720166.png" referrerpolicy="no-referrer"></p></blockquote><p>GitHub 平台託管項目的受歡迎程度能夠為部分程序員和創業公司打開一扇大門，他們通過 Stars 獲得關注度、影響力和聲譽。然而地下黑市出售的 Stars 提供了」以假亂真「的方式來讓他們進行作弊。這些虛假 Stars 在某種程度上能幫助程序員和創業公司在聯絡投資人或找工作時留下好印象。</p><p>據介紹，在交易 GitHub Stars 的平台上，支付價值&nbsp;6 美元的以太幣即可購買 50 個 Stars。除了 Stars，其他可量化的指標——如 Forks、Watchers 和 Follower 也可單獨或組合購買。</p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><img src="https://static.oschina.net/uploads/space/2023/1024/174616_4UDX_2720166.png" referrerpolicy="no-referrer"></p><p style="color:#333333; margin-left:0; margin-right:0; text-align:left"><em>via<span>&nbsp;</span><u><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fbaddhi.shop%2Fproduct%2Fbuy-github-followers%2F" target="_blank">https://baddhi.shop/product/buy-github-followers/</a></u></em></p><p>文章寫道，此前初創公司、程序員和投資者在決定僱用誰、為誰工作或投資誰時，會使用這些指標來篩選有潛力的程序員和初創公司。</p><p>但真正決定成功的不僅是這些指標，投資者開始意識到這種評估方式並不可靠，正在改變對 GitHub Stars 等指標的依賴，GitHub 平台也在打擊這些專門用於刷數據的虛假賬號。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 10:44:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263862/github-stars-black-market-coders-cheat</guid>
            <link>https://www.oschina.net/news/263862/github-stars-black-market-coders-cheat</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[小米 14 開機動畫顯示澎湃 OS 基於 Android]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>小米 14、澎湃 OS 等一大波新品已經正式登場<strong>。澎湃 OS 發佈前，不少人都爭論，它不是小米自研的系統，對此雷軍還特意表示，確實不是。</strong></p><p>小米的澎湃 OS 由兩部分組成：一部分是基於安卓系統進行深度進化的，這使得澎湃 OS 可以與安卓系統保持同步，並且能夠使用安卓軟件。</p><p>另一部分則是小米自研發的 Vela 系統，主要用於實現小米產品之間的互聯互通。</p><p>這種系統架構使得澎湃 OS 能夠兼顧兼容性和自主性，既滿足了用戶對豐富應用的需求，又能夠提供更好的硬件軟件一體化體驗。</p><p><strong>發佈會後，有網友從現場展示的新機看到，小米澎湃 OS 開機頁面動畫還是和以前一樣，也有顯示 "Powered by android"，這也算是證實了雷軍之前的説法。</strong></p><p><img alt="" src="https://oscimg.oschina.net/oscnet/up-4b428f25973338129e02686c3a80ff9faf1.png" referrerpolicy="no-referrer"></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 05:17:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263829</guid>
            <link>https://www.oschina.net/news/263829</link>
            <author>
                <![CDATA[來源: 投稿]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[OpenSSL 3.2 發佈首個 Beta]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><a data-traceid="news_detail_above_text_link_1" data-tracepid="news_detail_above_text_link" style="color:#A00; font-weight:bold;" href="https://www.oschina.net/event/2331193" _blank"="">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div><p>OpenSSL 3.2 首個 Beta 版本已發佈。</p><p>OpenSSL 3.2 實現了針對 QUIC 的初步客戶端，QUIC 是 Google 開發的通用傳輸層網絡協議，後來被 IETF 採用。 對於 OpenSSL 3.3 和明年的 OpenSSL 3.4，他們的目標是進一步完成此實現。</p><p>此外還增加了對 TLS 1.3 中 Brainpool 曲線的支持、原始公鑰 (RFC7250) 支持、使用 Brotli 和 Zstd 進行證書壓縮的支持、SM4-XTS 支持、確定性 ECDSA 簽名、AES-GCM-SIV、混合公鑰加密 (HPKE) ），以及其他特性。</p><p>OpenSSL 3.2 還將默認的 SSL/TLS 安全級別從 1 更改為 2。</p><p><a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopenssl%2Fopenssl%2Freleases%2Ftag%2Fopenssl-3.2.0-beta1" target="_blank">下載地址</a> | <a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fopenssl%2Fopenssl%2Fblob%2Fmaster%2FNEWS.md" target="_blank">更新説明</a></p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 04:08:00 GMT</pubDate>
            <guid isPermaLink="false">https://www.oschina.net/news/263824/openssl-3-2-0-beta1-released</guid>
            <link>https://www.oschina.net/news/263824/openssl-3-2-0-beta1-released</link>
            <author>
                <![CDATA[來源: OSCHINA]]>
            </author>
        </item>
        <item>
            <title>
                <![CDATA[2023 CCF 中國開源大會開源商業化分論壇順利召開]]>
            </title>
            <description>
                <![CDATA[<div class="content"><div class="ad-wrap" style="margin-bottom: 8px;"><div data-traceid="news_comment_top_ad" data-tracepid="news_comment_top" style="text-align: center;"><a style="color:#A00;font-weight:bold;" href="https://www.oschina.net/event/2331193" target="_blank">OSC 請你來轟趴啦！1028 蘇州源創會，一起尋寶 AI 時代<img src="https://www.oschina.net/img/hot3.png" align="absmiddle" style="max-height: 32px;max-width: 32px;margin-top: -4px;" referrerpolicy="no-referrer"></a></div></div><p>10 月 21 日至 22 日，由中國計算機學會（CCF）、開放原子開源基金會主辦的 2023 CCF 中國開源大會在長沙順利舉行。其中，開源商業化分論壇由開源中國承辦，開源中國董事長馬越擔任主席。來自開源原生商業公司的諸多專家就開源項目商業化最佳實踐展開分享，為更多開發者和企業提供可借鑑的經驗，共同推動開源生態建設，助力開源生態發展。</p><p><strong>開源中國董事長馬越</strong>以《中國開源商業發展的現狀及思考》為題發表主旨演講。他指出，當前開源創業公司有「七大恨」：沒有品牌、沒有流量、沒有銷售能力、沒有資質、沒有交付能力、沒有現金流、沒有資本渠道。這些都嚴重阻礙了創業公司進一步發展壯大。而解決開源創業公司「七大恨」的關鍵就在於開源創業聯合體。所謂開源創業聯合體，就是提供通用服務模型的價值流平台，實現集成和自動化 IT 價值鏈的插件開放平台，融合市場各類開源或商業生態能力，落地客戶場景服務。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-729f0faa336b257003127266ac12c4b722d.png" width="800" referrerpolicy="no-referrer"></p><p>最後馬越提議，希望能夠集眾多開源力量共建這樣的插件開放平台，繁榮開源商業生態，互通有無。開源中國已積累了十幾年的商業化經驗以及商業化能力，旗下 Gitee 平台也已經入駐了 27 萬多家中小團隊，服務中國 600 多家 100 億元估值以上的大企業。未來，開源中國將通過該插件平台將這些經驗和能力賦能更多開源企業。</p><p><strong>CCF 開源發展委員會常委譚中意</strong>就 「AI to B 的開源和商業化」這兩大方向展開探討。譚中意認為，當前 LLM to B 業務的難點在於，需要找到一個 Killer 場景——有足夠的商業回報，能覆蓋大模型 Finetune 和 Serving 以及 LLM 應用開發和運維的成本。但是 LLM 技術存在先天上的約束：一是無法避免幻覺的問題，To C 業務需要滿足網信辦規定，合規成本很高；二是無法避免概率的問題，To B 的嚴肅場景可能不太適合。因此，突破口可能在於：一是企業內部，對生成內容更講究創意或者實時 Check 的場景，比如內部研發代碼生成工具、遊戲行業的場景；二是電商領域，比如促銷、廣告投放等，因為這是離錢最近的潛力市場，且容易形成數據閉環。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-cf8531b4308c6c6399ab200699cb0ef9b7d.png" width="800" referrerpolicy="no-referrer"></p><p>至於開源在 LLM 產業的關鍵作用，主要有兩個：一是降低成本，開源是降低整個產業創新成本的關鍵，即 AI 民主化。這需要整個學術界和產業界一起努力，來把成本降下來，而且開源底座模型是整個大生態最重要部分；二是建立信任，人工智能要讓人信任，它必須公開透明。一旦這個問題解決了，一個十萬億規模的市場可能起飛。</p><p><strong>PingCAP 副總裁劉松</strong>分享了 TiDB 從開源到 Serverless 的商業化演進邏輯。據其介紹，PingCAP 的商業化之路可以總結為「四部曲」：創建一個滿足時代剛需的開源項目，開源產品獲得規模化的用戶部署與反饋，打造一個全球化的商業化模式，持續創造滿足極致用戶需求的產品形態。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-c12ba68303c5e846f95912614c315de728c.png" width="800" referrerpolicy="no-referrer"></p><p>劉松表示，面對「開源 + 雲」互相推動和演進的新技術環境，TiDB 演進方向從技術領先走向了「技術+體驗」領先。當前，PingCAP 圍繞 TiDB 構建了三大產品形態：TiDB 企業版、TiDB Cloud（全託管）、TiDB Cloud Serverless。其中，TiDB Cloud 提供全託管的 DBaaS （Database-as-a-Service）服務，極大地降低了雲數據庫的使用門檻；TiDB Cloud Serverless 基於雲原生/多雲的設計，採用 AI-Ready 的架構，實現極致低成本、極致彈性，擁有自動化的資源調度能力以及靈活集成 AI 能力等特性。</p><p><strong>統信軟件解決方案中心專家任紫東</strong>以《中國開源操作系統商業發展探索》為題展開分享。任紫東認為，2020 年是國產 Linux 里程碑之年。2019 年前，我國有 10 多家國產操作系統企業，隨着政策引導和市場競爭戰略的選擇，國產操作系統廠商進行了全新的業態整合，2020 年起，初步形成兩家主流國產操作系統企業，統信就是其中之一。及至 2022 年，產業鏈的商業形態形成。頭部操作系統公司規模也呈現「幹百十」特徵：千人以上的操作系統開發隊伍，百人以上的內核研發團隊，十人以上的開源合規律師團隊。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-d9d35f21d4f87673628b2337118d6fb42dc.png" width="800" referrerpolicy="no-referrer"></p><p>當前，統信 UOS 生態是國內最大的自主操作系統生態圈之一。統信己基於服務器操作系統完成諸多主流國外商業軟件的適配，涵蓋了 Oracle、IBM、SAP、微軟等廠商的主流數據庫產品，Google、FaceBook、百度、華為等廠商的主流 AI 人工智能類產品，以及 Oracle、IBM 等廠商的主流中間件產品。</p><p>開源社區做得好，怎麼變成錢？在以《白鯨 DataOps 開源矩陣商業化之路》為題的演講中，<strong>白鯨開源 CEO 郭煒</strong>提到，白鯨花了 8 個月時間，摸索了一套開源商業轉化流程，積累數千萬的商業 Pipeline 以及上百個線索，而投入資源不過是一名銷售人員，沒有任何市場費用。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-c3550e7f84e4a38252e13b77a70f32bcd94.png" width="800" referrerpolicy="no-referrer"></p><p>郭煒把這些成果歸結於幾大原則：「別人做中石油，我做中石化」，即做所有人的朋友；開源項目定位要清晰，商業功能痛點要明確，因此白鯨開源採取了「開源矩陣+OpenCore」的路線；開源商業軟件要重視「行業屬性」，分行業洞察痛點，口碑營銷；不忘初心，牢記使命，不斷升級開源版，商業版才有機會；勇於探索，擁抱新技術，將大模型融入軟件，等等。</p><p>最後他提到，開源風口並沒有過去，而且溫度剛剛好。面對經濟下行週期、資本趨於冷靜以及收入體量要求更高等挑戰，也應看到行業展現出來更多的機會，比如惡性競爭減少，互聯網公司開始付費買工具，訂閲制更容易被接受，海外市場逐步增長等等。</p><p><strong>TDengine 聯合創始人&amp; 商業化 VP 李廣</strong>分享了 TDengine 是如何從開源時序數據庫到工業大數據處理平台的。他表示，一款軟件開源，就意味着可信、可控。TDengine 的商業邏輯就是重塑 2B 銷售模式，以開源建社區與品牌，以開源建 GTM 路徑。通過開源擴大影響力，樹立品牌，形成開發者社區，構建競爭壁壘，快速獲得市場反饋，快速迭代，快速打造生態，獲得用戶信任。另一方面，將傳統的 2B 銷售演變為 2C 的模式，將傳統的登門拜訪演變為線上銷售，將資源型銷售轉化為技術和產品型銷售。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-a48a9ecbd7606788031401fba0b8a83267d.png" width="800" referrerpolicy="no-referrer"></p><p>當前，TDengine 提供三種產品與服務，一是開源社區版——TDengine OSS，主要是為了建立開發者社區，建立生態；二是企業版——TDengine Enterprise，支持獨立部署並按照 TBL（Term Based License）年度服務訂閲或者永久 Licenese 模式銷售；三是雲服務版——TDengine Cloud，在阿里雲、AWS、華為雲等雲平台上直接提供 SaaS 服務，根據數據量和時長計費。</p><p>最後他表示，開源軟件的商業化邏輯已經發生了變化，從關注增長轉向強調利潤。2B 軟件羣龍紛爭的時代結束，只有深入行業黑土地才能生存。</p><p><strong>築棧（KodeRover / Zadig）創始人 &amp; CEO 李倩</strong>分析了公司為什麼選擇深耕中國而不是出海。據悉，在商業化過程中，KodeRover 也面臨過不少問題，比如花了時間打磨產品，但用戶付費意願不強烈；有需求有預算的大客戶找上門，卻因為自身團隊規模過小無法為其提供大型服務等等，不過 KodeRover 最終也地制定了相應策略。李倩將公司的商業化思路總結為：技術上用開源鑄造好基建，打造產品力、品牌力；商業上，中國場景助力「新 IT」 （比如硬科技創新、舊行業升級重整）升級，創造客戶價值；可持續創造價值，廣泛鏈接，為客戶提供最優質的解決方案。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-e0510f159c4ed52d504290463b06b22c059.png" width="800" referrerpolicy="no-referrer"></p><p>李倩提到，Zadig 是生產軟件的軟件，目的是交付數字業務。 Zadig 開源兩年，企業安裝總量近 3 萬，目前是國內雲原生 DevOps 領域落地最廣泛的平台，成為包括字節飛書、極氪、路特斯、小鵬、七牛雲、WiFi 萬能鑰匙、易快報、iMile、TT 語音、鍋圈、藥師幫、大參林、老百姓大藥房、 益豐大藥房、小天才等標杆企業的數千家企業研發工程師每日深度、高頻使用的軟件交付平台。</p><p><strong>EMQ 映雲科技聯合創始人兼 CPO 金髮華</strong>以《EMQ —— 開源數實融合基礎軟件的商業化》為題發表演進。據瞭解，作為全球領先的物聯網基礎軟件提供商，EMQ 創立並主導 LF Edge eKuiper、NanoMQ、Neuron 等多個全球知名邊緣軟件開源項目。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-086857ea6bb66926b0c38eb9fb66172ceae.png" width="800" referrerpolicy="no-referrer"></p><p>金髮華表示，Hosting(Cloud) 模式是未來產品的一個方向。當前，EMQ Cloud 商業服務有三種。一是 Serverless，輕鬆幾步即可獲得一個安全可伸縮的 ServerlessMQTT 服務。全託管特性讓用戶無需關心基礎設施和資源管理，特別適用於個人開發者、中小型項目、開發測試環境以及技術框架的評估。二是專有版，獨立部署的全託管 MQTT 服務，具有更高的性能保障和可定製能力，尤其適用於對性能、穩定性要求較高的企業級項目。三是 BYOC (Bring Your Own Cloud)，用戶在自己的雲上部署 EMQX 集羣,並交由 EMQX 團隊託管，適用於有嚴格數據安全和合規性要求的企業級項目，最大限度地利用現有的雲資源。</p><p><strong>天際科技投資副總裁江志桐</strong>闡述了開源與 AI 時代下的投資邏輯。她表示，在 AI 2.0 時代，從全球市場來看，基於大模型未來增長預期，資本給予顯著溢價。當前通用模型格局明確，出現了微軟、谷歌雙龍頭企業，工具層、應用層、垂直領域湧現大量獨角獸。全球市場都在關注大模型商業化的落地，圍繞效率、創意、情感陪伴 2C/2B 的應用生態繁榮。</p><p style="text-align:center"><img height="533" src="https://oscimg.oschina.net/oscnet/up-d423c2f9a083841554cb3cea84bb20c4056.png" width="800" referrerpolicy="no-referrer"></p><p>與此同時，開源正在加速 AI 2.0 落地，加速 AI 生態繁榮。開源是大模型基礎設施必然選擇，延伸出的服務、應用具有巨大商業機會。AI 開源時代的投資策略以核心人物為中心，佈局早期，發揮產業資源優勢，關鍵人物、網絡效應、稀缺數據，以及軟硬一體都有可能成為企業護城河的因素，也是 AI 開源時代投資重點。</p><p>那麼中國市場的機會在哪裏呢？江志桐認為，主要在於兩方面，一是基礎設施，二是垂直行業。國內大模型的應用還在快速成長，基於開源加速模型落地後，整個 AI 生態裏面也會出現能夠對標全球市場的公司。總之，國內 AI 市場還處於巨頭形成的階段。</p></div>
                                    ]]>
            </description>
            <pubDate>Thu, 26 Oct 2023 02:32:00 GMT</pubDate>
            <guid isPermaLink="false">https://my.oschina.net/u/3859945/blog/10136947</guid>
            <link>https://my.oschina.net/u/3859945/blog/10136947</link>
            <author>
                <![CDATA[原創]]>
            </author>
        </item>
    </channel>
</rss>
